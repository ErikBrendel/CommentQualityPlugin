# id;timestamp;commentText;codeText;commentWords;codeWords
YarnFileStageTestS3ITCase -> private static void setupCustomHadoopConfig() throws IOException;1510999087;Create a Hadoop config file containing S3 access credentials.__<p>Note that we cannot use them as part of the URL since this may fail if the credentials_contain a "/" (see <a href="https://issues.apache.org/jira/browse/HADOOP-3733">HADOOP-3733</a>).;private static void setupCustomHadoopConfig() throws IOException {_		File hadoopConfig = TEMP_FOLDER.newFile()__		Map<String , String > parameters = new HashMap<>()___		_		parameters.put("fs.s3a.access.key", ACCESS_KEY)__		parameters.put("fs.s3a.secret.key", SECRET_KEY)___		parameters.put("fs.s3.awsAccessKeyId", ACCESS_KEY)__		parameters.put("fs.s3.awsSecretAccessKey", SECRET_KEY)___		parameters.put("fs.s3n.awsAccessKeyId", ACCESS_KEY)__		parameters.put("fs.s3n.awsSecretAccessKey", SECRET_KEY)___		try (PrintStream out = new PrintStream(new FileOutputStream(hadoopConfig))) {_			out.println("<?xml version=\"1.0\"?>")__			out.println("<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>")__			out.println("<configuration>")__			for (Map.Entry<String, String> entry : parameters.entrySet()) {_				out.println("\t<property>")__				out.println("\t\t<name>" + entry.getKey() + "</name>")__				out.println("\t\t<value>" + entry.getValue() + "</value>")__				out.println("\t</property>")__			}_			out.println("</configuration>")__		}__		final Configuration conf = new Configuration()__		conf.setString(ConfigConstants.HDFS_SITE_CONFIG, hadoopConfig.getAbsolutePath())___		FileSystem.initialize(conf)__	};create,a,hadoop,config,file,containing,s3,access,credentials,p,note,that,we,cannot,use,them,as,part,of,the,url,since,this,may,fail,if,the,credentials,contain,a,see,a,href,https,issues,apache,org,jira,browse,hadoop,3733,hadoop,3733,a;private,static,void,setup,custom,hadoop,config,throws,ioexception,file,hadoop,config,new,file,map,string,string,parameters,new,hash,map,parameters,put,fs,s3a,access,key,parameters,put,fs,s3a,secret,key,parameters,put,fs,s3,aws,access,key,id,parameters,put,fs,s3,aws,secret,access,key,parameters,put,fs,s3n,aws,access,key,id,parameters,put,fs,s3n,aws,secret,access,key,try,print,stream,out,new,print,stream,new,file,output,stream,hadoop,config,out,println,xml,version,1,0,out,println,xml,stylesheet,type,text,xsl,href,configuration,xsl,out,println,configuration,for,map,entry,string,string,entry,parameters,entry,set,out,println,t,property,out,println,t,t,name,entry,get,key,name,out,println,t,t,value,entry,get,value,value,out,println,t,property,out,println,configuration,final,configuration,conf,new,configuration,conf,set,string,config,constants,hadoop,config,get,absolute,path,file,system,initialize,conf
YarnFileStageTestS3ITCase -> private static void setupCustomHadoopConfig() throws IOException;1512378085;Create a Hadoop config file containing S3 access credentials.__<p>Note that we cannot use them as part of the URL since this may fail if the credentials_contain a "/" (see <a href="https://issues.apache.org/jira/browse/HADOOP-3733">HADOOP-3733</a>).;private static void setupCustomHadoopConfig() throws IOException {_		File hadoopConfig = TEMP_FOLDER.newFile()__		Map<String , String > parameters = new HashMap<>()___		_		parameters.put("fs.s3a.access.key", ACCESS_KEY)__		parameters.put("fs.s3a.secret.key", SECRET_KEY)___		parameters.put("fs.s3.awsAccessKeyId", ACCESS_KEY)__		parameters.put("fs.s3.awsSecretAccessKey", SECRET_KEY)___		parameters.put("fs.s3n.awsAccessKeyId", ACCESS_KEY)__		parameters.put("fs.s3n.awsSecretAccessKey", SECRET_KEY)___		try (PrintStream out = new PrintStream(new FileOutputStream(hadoopConfig))) {_			out.println("<?xml version=\"1.0\"?>")__			out.println("<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>")__			out.println("<configuration>")__			for (Map.Entry<String, String> entry : parameters.entrySet()) {_				out.println("\t<property>")__				out.println("\t\t<name>" + entry.getKey() + "</name>")__				out.println("\t\t<value>" + entry.getValue() + "</value>")__				out.println("\t</property>")__			}_			out.println("</configuration>")__		}__		final Configuration conf = new Configuration()__		conf.setString(ConfigConstants.HDFS_SITE_CONFIG, hadoopConfig.getAbsolutePath())___		FileSystem.initialize(conf)__	};create,a,hadoop,config,file,containing,s3,access,credentials,p,note,that,we,cannot,use,them,as,part,of,the,url,since,this,may,fail,if,the,credentials,contain,a,see,a,href,https,issues,apache,org,jira,browse,hadoop,3733,hadoop,3733,a;private,static,void,setup,custom,hadoop,config,throws,ioexception,file,hadoop,config,new,file,map,string,string,parameters,new,hash,map,parameters,put,fs,s3a,access,key,parameters,put,fs,s3a,secret,key,parameters,put,fs,s3,aws,access,key,id,parameters,put,fs,s3,aws,secret,access,key,parameters,put,fs,s3n,aws,access,key,id,parameters,put,fs,s3n,aws,secret,access,key,try,print,stream,out,new,print,stream,new,file,output,stream,hadoop,config,out,println,xml,version,1,0,out,println,xml,stylesheet,type,text,xsl,href,configuration,xsl,out,println,configuration,for,map,entry,string,string,entry,parameters,entry,set,out,println,t,property,out,println,t,t,name,entry,get,key,name,out,println,t,t,value,entry,get,value,value,out,println,t,property,out,println,configuration,final,configuration,conf,new,configuration,conf,set,string,config,constants,hadoop,config,get,absolute,path,file,system,initialize,conf
YarnFileStageTestS3ITCase -> private static void setupCustomHadoopConfig() throws IOException;1542388684;Create a Hadoop config file containing S3 access credentials.__<p>Note that we cannot use them as part of the URL since this may fail if the credentials_contain a "/" (see <a href="https://issues.apache.org/jira/browse/HADOOP-3733">HADOOP-3733</a>).;private static void setupCustomHadoopConfig() throws IOException {_		File hadoopConfig = TEMP_FOLDER.newFile()__		Map<String , String > parameters = new HashMap<>()___		_		parameters.put("fs.s3a.access.key", S3TestCredentials.getS3AccessKey())__		parameters.put("fs.s3a.secret.key", S3TestCredentials.getS3SecretKey())___		parameters.put("fs.s3.awsAccessKeyId", S3TestCredentials.getS3AccessKey())__		parameters.put("fs.s3.awsSecretAccessKey", S3TestCredentials.getS3SecretKey())___		parameters.put("fs.s3n.awsAccessKeyId", S3TestCredentials.getS3AccessKey())__		parameters.put("fs.s3n.awsSecretAccessKey", S3TestCredentials.getS3SecretKey())___		try (PrintStream out = new PrintStream(new FileOutputStream(hadoopConfig))) {_			out.println("<?xml version=\"1.0\"?>")__			out.println("<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>")__			out.println("<configuration>")__			for (Map.Entry<String, String> entry : parameters.entrySet()) {_				out.println("\t<property>")__				out.println("\t\t<name>" + entry.getKey() + "</name>")__				out.println("\t\t<value>" + entry.getValue() + "</value>")__				out.println("\t</property>")__			}_			out.println("</configuration>")__		}__		final Configuration conf = new Configuration()__		conf.setString(ConfigConstants.HDFS_SITE_CONFIG, hadoopConfig.getAbsolutePath())___		FileSystem.initialize(conf)__	};create,a,hadoop,config,file,containing,s3,access,credentials,p,note,that,we,cannot,use,them,as,part,of,the,url,since,this,may,fail,if,the,credentials,contain,a,see,a,href,https,issues,apache,org,jira,browse,hadoop,3733,hadoop,3733,a;private,static,void,setup,custom,hadoop,config,throws,ioexception,file,hadoop,config,new,file,map,string,string,parameters,new,hash,map,parameters,put,fs,s3a,access,key,s3test,credentials,get,s3access,key,parameters,put,fs,s3a,secret,key,s3test,credentials,get,s3secret,key,parameters,put,fs,s3,aws,access,key,id,s3test,credentials,get,s3access,key,parameters,put,fs,s3,aws,secret,access,key,s3test,credentials,get,s3secret,key,parameters,put,fs,s3n,aws,access,key,id,s3test,credentials,get,s3access,key,parameters,put,fs,s3n,aws,secret,access,key,s3test,credentials,get,s3secret,key,try,print,stream,out,new,print,stream,new,file,output,stream,hadoop,config,out,println,xml,version,1,0,out,println,xml,stylesheet,type,text,xsl,href,configuration,xsl,out,println,configuration,for,map,entry,string,string,entry,parameters,entry,set,out,println,t,property,out,println,t,t,name,entry,get,key,name,out,println,t,t,value,entry,get,value,value,out,println,t,property,out,println,configuration,final,configuration,conf,new,configuration,conf,set,string,config,constants,hadoop,config,get,absolute,path,file,system,initialize,conf
YarnFileStageTestS3ITCase -> private static void setupCustomHadoopConfig() throws IOException;1542388685;Create a Hadoop config file containing S3 access credentials.__<p>Note that we cannot use them as part of the URL since this may fail if the credentials_contain a "/" (see <a href="https://issues.apache.org/jira/browse/HADOOP-3733">HADOOP-3733</a>).;private static void setupCustomHadoopConfig() throws IOException {_		File hadoopConfig = TEMP_FOLDER.newFile()__		Map<String , String > parameters = new HashMap<>()___		_		parameters.put("fs.s3a.access.key", S3TestCredentials.getS3AccessKey())__		parameters.put("fs.s3a.secret.key", S3TestCredentials.getS3SecretKey())___		parameters.put("fs.s3.awsAccessKeyId", S3TestCredentials.getS3AccessKey())__		parameters.put("fs.s3.awsSecretAccessKey", S3TestCredentials.getS3SecretKey())___		parameters.put("fs.s3n.awsAccessKeyId", S3TestCredentials.getS3AccessKey())__		parameters.put("fs.s3n.awsSecretAccessKey", S3TestCredentials.getS3SecretKey())___		try (PrintStream out = new PrintStream(new FileOutputStream(hadoopConfig))) {_			out.println("<?xml version=\"1.0\"?>")__			out.println("<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>")__			out.println("<configuration>")__			for (Map.Entry<String, String> entry : parameters.entrySet()) {_				out.println("\t<property>")__				out.println("\t\t<name>" + entry.getKey() + "</name>")__				out.println("\t\t<value>" + entry.getValue() + "</value>")__				out.println("\t</property>")__			}_			out.println("</configuration>")__		}__		final Configuration conf = new Configuration()__		conf.setString(ConfigConstants.HDFS_SITE_CONFIG, hadoopConfig.getAbsolutePath())___		FileSystem.initialize(conf)__	};create,a,hadoop,config,file,containing,s3,access,credentials,p,note,that,we,cannot,use,them,as,part,of,the,url,since,this,may,fail,if,the,credentials,contain,a,see,a,href,https,issues,apache,org,jira,browse,hadoop,3733,hadoop,3733,a;private,static,void,setup,custom,hadoop,config,throws,ioexception,file,hadoop,config,new,file,map,string,string,parameters,new,hash,map,parameters,put,fs,s3a,access,key,s3test,credentials,get,s3access,key,parameters,put,fs,s3a,secret,key,s3test,credentials,get,s3secret,key,parameters,put,fs,s3,aws,access,key,id,s3test,credentials,get,s3access,key,parameters,put,fs,s3,aws,secret,access,key,s3test,credentials,get,s3secret,key,parameters,put,fs,s3n,aws,access,key,id,s3test,credentials,get,s3access,key,parameters,put,fs,s3n,aws,secret,access,key,s3test,credentials,get,s3secret,key,try,print,stream,out,new,print,stream,new,file,output,stream,hadoop,config,out,println,xml,version,1,0,out,println,xml,stylesheet,type,text,xsl,href,configuration,xsl,out,println,configuration,for,map,entry,string,string,entry,parameters,entry,set,out,println,t,property,out,println,t,t,name,entry,get,key,name,out,println,t,t,value,entry,get,value,value,out,println,t,property,out,println,configuration,final,configuration,conf,new,configuration,conf,set,string,config,constants,hadoop,config,get,absolute,path,file,system,initialize,conf
YarnFileStageTestS3ITCase -> @Test 	public void testRecursiveUploadForYarnS3() throws Exception;1510999087;Verifies that nested directories are properly copied with a <tt>s3a://</tt> file_systems during resource uploads for YARN.;@Test_	public void testRecursiveUploadForYarnS3() throws Exception {_		try {_			Class.forName("org.apache.hadoop.fs.s3.S3FileSystem")__		} catch (ClassNotFoundException e) {_			_			String msg = "Skipping test because S3FileSystem is not in the class path"__			log.info(msg)__			assumeNoException(msg, e)__		}_		testRecursiveUploadForYarn("s3", "testYarn-s3")__	};verifies,that,nested,directories,are,properly,copied,with,a,tt,s3a,tt,file,systems,during,resource,uploads,for,yarn;test,public,void,test,recursive,upload,for,yarn,s3,throws,exception,try,class,for,name,org,apache,hadoop,fs,s3,s3file,system,catch,class,not,found,exception,e,string,msg,skipping,test,because,s3file,system,is,not,in,the,class,path,log,info,msg,assume,no,exception,msg,e,test,recursive,upload,for,yarn,s3,test,yarn,s3
YarnFileStageTestS3ITCase -> @Test 	public void testRecursiveUploadForYarnS3() throws Exception;1512378085;Verifies that nested directories are properly copied with a <tt>s3a://</tt> file_systems during resource uploads for YARN.;@Test_	public void testRecursiveUploadForYarnS3() throws Exception {_		try {_			Class.forName("org.apache.hadoop.fs.s3.S3FileSystem")__		} catch (ClassNotFoundException e) {_			_			String msg = "Skipping test because S3FileSystem is not in the class path"__			log.info(msg)__			assumeNoException(msg, e)__		}_		testRecursiveUploadForYarn("s3", "testYarn-s3")__	};verifies,that,nested,directories,are,properly,copied,with,a,tt,s3a,tt,file,systems,during,resource,uploads,for,yarn;test,public,void,test,recursive,upload,for,yarn,s3,throws,exception,try,class,for,name,org,apache,hadoop,fs,s3,s3file,system,catch,class,not,found,exception,e,string,msg,skipping,test,because,s3file,system,is,not,in,the,class,path,log,info,msg,assume,no,exception,msg,e,test,recursive,upload,for,yarn,s3,test,yarn,s3
YarnFileStageTestS3ITCase -> @Test 	public void testRecursiveUploadForYarnS3() throws Exception;1542388684;Verifies that nested directories are properly copied with a <tt>s3a://</tt> file_systems during resource uploads for YARN.;@Test_	public void testRecursiveUploadForYarnS3() throws Exception {_		try {_			Class.forName("org.apache.hadoop.fs.s3.S3FileSystem")__		} catch (ClassNotFoundException e) {_			_			String msg = "Skipping test because S3FileSystem is not in the class path"__			log.info(msg)__			assumeNoException(msg, e)__		}_		testRecursiveUploadForYarn("s3", "testYarn-s3")__	};verifies,that,nested,directories,are,properly,copied,with,a,tt,s3a,tt,file,systems,during,resource,uploads,for,yarn;test,public,void,test,recursive,upload,for,yarn,s3,throws,exception,try,class,for,name,org,apache,hadoop,fs,s3,s3file,system,catch,class,not,found,exception,e,string,msg,skipping,test,because,s3file,system,is,not,in,the,class,path,log,info,msg,assume,no,exception,msg,e,test,recursive,upload,for,yarn,s3,test,yarn,s3
YarnFileStageTestS3ITCase -> private void testRecursiveUploadForYarn(String scheme, String pathSuffix) throws Exception;1510999087;Verifies that nested directories are properly copied with to the given S3 path (using the_appropriate file system) during resource uploads for YARN.__@param scheme_file system scheme_@param pathSuffix_test path suffix which will be the test's target path;private void testRecursiveUploadForYarn(String scheme, String pathSuffix) throws Exception {_		++numRecursiveUploadTests___		final Path basePath = new Path(scheme + "://" + BUCKET + '/' + TEST_DATA_DIR)__		final HadoopFileSystem fs = (HadoopFileSystem) basePath.getFileSystem()___		assumeFalse(fs.exists(basePath))___		try {_			final Path directory = new Path(basePath, pathSuffix)___			YarnFileStageTest.testCopyFromLocalRecursive(fs.getHadoopFileSystem(),_				new org.apache.hadoop.fs.Path(directory.toUri()), tempFolder, true)___			_			assertFalse(fs.exists(directory))__		} finally {_			_			fs.delete(basePath, true)__		}_	};verifies,that,nested,directories,are,properly,copied,with,to,the,given,s3,path,using,the,appropriate,file,system,during,resource,uploads,for,yarn,param,scheme,file,system,scheme,param,path,suffix,test,path,suffix,which,will,be,the,test,s,target,path;private,void,test,recursive,upload,for,yarn,string,scheme,string,path,suffix,throws,exception,num,recursive,upload,tests,final,path,base,path,new,path,scheme,bucket,final,hadoop,file,system,fs,hadoop,file,system,base,path,get,file,system,assume,false,fs,exists,base,path,try,final,path,directory,new,path,base,path,path,suffix,yarn,file,stage,test,test,copy,from,local,recursive,fs,get,hadoop,file,system,new,org,apache,hadoop,fs,path,directory,to,uri,temp,folder,true,assert,false,fs,exists,directory,finally,fs,delete,base,path,true
YarnFileStageTestS3ITCase -> private void testRecursiveUploadForYarn(String scheme, String pathSuffix) throws Exception;1512378085;Verifies that nested directories are properly copied with to the given S3 path (using the_appropriate file system) during resource uploads for YARN.__@param scheme_file system scheme_@param pathSuffix_test path suffix which will be the test's target path;private void testRecursiveUploadForYarn(String scheme, String pathSuffix) throws Exception {_		++numRecursiveUploadTests___		final Path basePath = new Path(scheme + "://" + BUCKET + '/' + TEST_DATA_DIR)__		final HadoopFileSystem fs = (HadoopFileSystem) basePath.getFileSystem()___		assumeFalse(fs.exists(basePath))___		try {_			final Path directory = new Path(basePath, pathSuffix)___			YarnFileStageTest.testCopyFromLocalRecursive(fs.getHadoopFileSystem(),_				new org.apache.hadoop.fs.Path(directory.toUri()), tempFolder, true)__		} finally {_			_			fs.delete(basePath, true)__		}_	};verifies,that,nested,directories,are,properly,copied,with,to,the,given,s3,path,using,the,appropriate,file,system,during,resource,uploads,for,yarn,param,scheme,file,system,scheme,param,path,suffix,test,path,suffix,which,will,be,the,test,s,target,path;private,void,test,recursive,upload,for,yarn,string,scheme,string,path,suffix,throws,exception,num,recursive,upload,tests,final,path,base,path,new,path,scheme,bucket,final,hadoop,file,system,fs,hadoop,file,system,base,path,get,file,system,assume,false,fs,exists,base,path,try,final,path,directory,new,path,base,path,path,suffix,yarn,file,stage,test,test,copy,from,local,recursive,fs,get,hadoop,file,system,new,org,apache,hadoop,fs,path,directory,to,uri,temp,folder,true,finally,fs,delete,base,path,true
YarnFileStageTestS3ITCase -> private void testRecursiveUploadForYarn(String scheme, String pathSuffix) throws Exception;1542388684;Verifies that nested directories are properly copied with to the given S3 path (using the_appropriate file system) during resource uploads for YARN.__@param scheme_file system scheme_@param pathSuffix_test path suffix which will be the test's target path;private void testRecursiveUploadForYarn(String scheme, String pathSuffix) throws Exception {_		++numRecursiveUploadTests___		final Path basePath = new Path(S3TestCredentials.getTestBucketUriWithScheme(scheme) + TEST_DATA_DIR)__		final HadoopFileSystem fs = (HadoopFileSystem) basePath.getFileSystem()___		assumeFalse(fs.exists(basePath))___		try {_			final Path directory = new Path(basePath, pathSuffix)___			YarnFileStageTest.testCopyFromLocalRecursive(fs.getHadoopFileSystem(),_				new org.apache.hadoop.fs.Path(directory.toUri()), tempFolder, true)__		} finally {_			_			fs.delete(basePath, true)__		}_	};verifies,that,nested,directories,are,properly,copied,with,to,the,given,s3,path,using,the,appropriate,file,system,during,resource,uploads,for,yarn,param,scheme,file,system,scheme,param,path,suffix,test,path,suffix,which,will,be,the,test,s,target,path;private,void,test,recursive,upload,for,yarn,string,scheme,string,path,suffix,throws,exception,num,recursive,upload,tests,final,path,base,path,new,path,s3test,credentials,get,test,bucket,uri,with,scheme,scheme,final,hadoop,file,system,fs,hadoop,file,system,base,path,get,file,system,assume,false,fs,exists,base,path,try,final,path,directory,new,path,base,path,path,suffix,yarn,file,stage,test,test,copy,from,local,recursive,fs,get,hadoop,file,system,new,org,apache,hadoop,fs,path,directory,to,uri,temp,folder,true,finally,fs,delete,base,path,true
YarnFileStageTestS3ITCase -> private void testRecursiveUploadForYarn(String scheme, String pathSuffix) throws Exception;1542388685;Verifies that nested directories are properly copied with to the given S3 path (using the_appropriate file system) during resource uploads for YARN.__@param scheme_file system scheme_@param pathSuffix_test path suffix which will be the test's target path;private void testRecursiveUploadForYarn(String scheme, String pathSuffix) throws Exception {_		++numRecursiveUploadTests___		final Path basePath = new Path(S3TestCredentials.getTestBucketUriWithScheme(scheme) + TEST_DATA_DIR)__		final HadoopFileSystem fs = (HadoopFileSystem) basePath.getFileSystem()___		assumeFalse(fs.exists(basePath))___		try {_			final Path directory = new Path(basePath, pathSuffix)___			YarnFileStageTest.testCopyFromLocalRecursive(fs.getHadoopFileSystem(),_				new org.apache.hadoop.fs.Path(directory.toUri()), tempFolder, true)__		} finally {_			_			fs.delete(basePath, true)__		}_	};verifies,that,nested,directories,are,properly,copied,with,to,the,given,s3,path,using,the,appropriate,file,system,during,resource,uploads,for,yarn,param,scheme,file,system,scheme,param,path,suffix,test,path,suffix,which,will,be,the,test,s,target,path;private,void,test,recursive,upload,for,yarn,string,scheme,string,path,suffix,throws,exception,num,recursive,upload,tests,final,path,base,path,new,path,s3test,credentials,get,test,bucket,uri,with,scheme,scheme,final,hadoop,file,system,fs,hadoop,file,system,base,path,get,file,system,assume,false,fs,exists,base,path,try,final,path,directory,new,path,base,path,path,suffix,yarn,file,stage,test,test,copy,from,local,recursive,fs,get,hadoop,file,system,new,org,apache,hadoop,fs,path,directory,to,uri,temp,folder,true,finally,fs,delete,base,path,true
