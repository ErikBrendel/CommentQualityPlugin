# id;timestamp;commentText;codeText;commentWords;codeWords
ContinuousFileProcessingITCase -> private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData( 		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException;1475792502;Create a file and fill it with content.;private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(_		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {__		assert (hdfs != null)___		org.apache.hadoop.fs.Path tmp =_			new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx)___		FSDataOutputStream stream = hdfs.create(tmp)__		StringBuilder str = new StringBuilder()__		for (int i = 0_ i < LINES_PER_FILE_ i++) {_			String line = fileIdx + ": " + sampleLine + " " + i + "\n"__			str.append(line)__			stream.write(line.getBytes())__		}_		stream.close()__		return new Tuple2<>(tmp, str.toString())__	};create,a,file,and,fill,it,with,content;private,tuple2,org,apache,hadoop,fs,path,string,fill,with,data,string,base,string,file,name,int,file,idx,string,sample,line,throws,ioexception,interrupted,exception,assert,hdfs,null,org,apache,hadoop,fs,path,tmp,new,org,apache,hadoop,fs,path,base,file,name,file,idx,fsdata,output,stream,stream,hdfs,create,tmp,string,builder,str,new,string,builder,for,int,i,0,i,i,string,line,file,idx,sample,line,i,n,str,append,line,stream,write,line,get,bytes,stream,close,return,new,tuple2,tmp,str,to,string
ContinuousFileProcessingITCase -> private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData( 		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException;1479390848;Create a file and fill it with content.;private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(_		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {__		assert (hdfs != null)___		org.apache.hadoop.fs.Path tmp =_			new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx)___		FSDataOutputStream stream = hdfs.create(tmp)__		StringBuilder str = new StringBuilder()__		for (int i = 0_ i < LINES_PER_FILE_ i++) {_			String line = fileIdx + ": " + sampleLine + " " + i + "\n"__			str.append(line)__			stream.write(line.getBytes())__		}_		stream.close()__		return new Tuple2<>(tmp, str.toString())__	};create,a,file,and,fill,it,with,content;private,tuple2,org,apache,hadoop,fs,path,string,fill,with,data,string,base,string,file,name,int,file,idx,string,sample,line,throws,ioexception,interrupted,exception,assert,hdfs,null,org,apache,hadoop,fs,path,tmp,new,org,apache,hadoop,fs,path,base,file,name,file,idx,fsdata,output,stream,stream,hdfs,create,tmp,string,builder,str,new,string,builder,for,int,i,0,i,i,string,line,file,idx,sample,line,i,n,str,append,line,stream,write,line,get,bytes,stream,close,return,new,tuple2,tmp,str,to,string
ContinuousFileProcessingITCase -> private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData( 		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException;1488543889;Create a file and fill it with content.;private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(_		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {__		assert (hdfs != null)___		org.apache.hadoop.fs.Path tmp =_			new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx)___		FSDataOutputStream stream = hdfs.create(tmp)__		StringBuilder str = new StringBuilder()__		for (int i = 0_ i < LINES_PER_FILE_ i++) {_			String line = fileIdx + ": " + sampleLine + " " + i + "\n"__			str.append(line)__			stream.write(line.getBytes(ConfigConstants.DEFAULT_CHARSET))__		}_		stream.close()__		return new Tuple2<>(tmp, str.toString())__	};create,a,file,and,fill,it,with,content;private,tuple2,org,apache,hadoop,fs,path,string,fill,with,data,string,base,string,file,name,int,file,idx,string,sample,line,throws,ioexception,interrupted,exception,assert,hdfs,null,org,apache,hadoop,fs,path,tmp,new,org,apache,hadoop,fs,path,base,file,name,file,idx,fsdata,output,stream,stream,hdfs,create,tmp,string,builder,str,new,string,builder,for,int,i,0,i,i,string,line,file,idx,sample,line,i,n,str,append,line,stream,write,line,get,bytes,config,constants,stream,close,return,new,tuple2,tmp,str,to,string
ContinuousFileProcessingITCase -> private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData( 		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException;1495740632;Create a file and fill it with content.;private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(_		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {__		assert (hdfs != null)___		org.apache.hadoop.fs.Path tmp =_			new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx)___		FSDataOutputStream stream = hdfs.create(tmp)__		StringBuilder str = new StringBuilder()__		for (int i = 0_ i < LINES_PER_FILE_ i++) {_			String line = fileIdx + ": " + sampleLine + " " + i + "\n"__			str.append(line)__			stream.write(line.getBytes(ConfigConstants.DEFAULT_CHARSET))__		}_		stream.close()__		return new Tuple2<>(tmp, str.toString())__	};create,a,file,and,fill,it,with,content;private,tuple2,org,apache,hadoop,fs,path,string,fill,with,data,string,base,string,file,name,int,file,idx,string,sample,line,throws,ioexception,interrupted,exception,assert,hdfs,null,org,apache,hadoop,fs,path,tmp,new,org,apache,hadoop,fs,path,base,file,name,file,idx,fsdata,output,stream,stream,hdfs,create,tmp,string,builder,str,new,string,builder,for,int,i,0,i,i,string,line,file,idx,sample,line,i,n,str,append,line,stream,write,line,get,bytes,config,constants,stream,close,return,new,tuple2,tmp,str,to,string
ContinuousFileProcessingITCase -> private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData( 		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException;1522862573;Create a file and fill it with content.;private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(_		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {__		assert (hdfs != null)___		org.apache.hadoop.fs.Path tmp =_			new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx)___		FSDataOutputStream stream = hdfs.create(tmp)__		StringBuilder str = new StringBuilder()__		for (int i = 0_ i < LINES_PER_FILE_ i++) {_			String line = fileIdx + ": " + sampleLine + " " + i + "\n"__			str.append(line)__			stream.write(line.getBytes(ConfigConstants.DEFAULT_CHARSET))__		}_		stream.close()__		return new Tuple2<>(tmp, str.toString())__	};create,a,file,and,fill,it,with,content;private,tuple2,org,apache,hadoop,fs,path,string,fill,with,data,string,base,string,file,name,int,file,idx,string,sample,line,throws,ioexception,interrupted,exception,assert,hdfs,null,org,apache,hadoop,fs,path,tmp,new,org,apache,hadoop,fs,path,base,file,name,file,idx,fsdata,output,stream,stream,hdfs,create,tmp,string,builder,str,new,string,builder,for,int,i,0,i,i,string,line,file,idx,sample,line,i,n,str,append,line,stream,write,line,get,bytes,config,constants,stream,close,return,new,tuple2,tmp,str,to,string
