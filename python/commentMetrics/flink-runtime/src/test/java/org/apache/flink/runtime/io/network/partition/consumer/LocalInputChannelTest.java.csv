# id;timestamp;commentText;codeText;commentWords;codeWords
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1480688330;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new ExecutionAttemptID(),_			new IntermediateDataSetID(),_			0,_			1,_			mock(TaskActions.class),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,execution,attempt,id,new,intermediate,data,set,id,0,1,mock,task,actions,class,new,unregistered,task,metrics,group,dummy,task,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,new,unregistered,task,metrics,group,dummy,task,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1481127239;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new ExecutionAttemptID(),_			new IntermediateDataSetID(),_			0,_			1,_			mock(TaskActions.class),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,execution,attempt,id,new,intermediate,data,set,id,0,1,mock,task,actions,class,new,unregistered,task,metrics,group,dummy,task,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,new,unregistered,task,metrics,group,dummy,task,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1481560756;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			0,_			1,_			mock(TaskActions.class),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,0,1,mock,task,actions,class,new,unregistered,task,metrics,group,dummy,task,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,new,unregistered,task,metrics,group,dummy,task,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1485269495;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			0,_			1,_			mock(TaskActions.class),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,0,1,mock,task,actions,class,new,unregistered,task,metrics,group,dummy,task,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,new,unregistered,task,metrics,group,dummy,task,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1489060856;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			0,_			1,_			mock(TaskActions.class),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,0,1,mock,task,actions,class,new,unregistered,task,metrics,group,dummy,task,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,new,unregistered,task,metrics,group,dummy,task,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1489149058;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,new,unregistered,task,metrics,group,dummy,task,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,new,unregistered,task,metrics,group,dummy,task,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1493724918;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,new,unregistered,task,metrics,group,dummy,task,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,new,unregistered,task,metrics,group,dummy,task,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1502726910;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,new,unregistered,task,metrics,group,dummy,task,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,new,unregistered,task,metrics,group,dummy,task,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1511516912;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,new,unregistered,task,metrics,group,dummy,task,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,new,unregistered,task,metrics,group,dummy,task,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1513102156;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1516285878;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1519039281;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1519039287;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1519039300;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1519039301;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1519049124;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1525116906;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__- SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock._- If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.__For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),_			true_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,true,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1537385496;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__<ul>_<li>SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock.</li>_<li>If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.</li>_</ul>__<p>For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__<p>The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),_			true_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,ul,li,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,li,li,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,li,ul,p,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,p,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,true,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1540207709;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__<ul>_<li>SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock.</li>_<li>If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.</li>_</ul>__<p>For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__<p>The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),_			true_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,ul,li,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,li,li,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,li,ul,p,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,p,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,true,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception;1547722029;Verifies that concurrent release via the SingleInputGate and re-triggering_of a partition request works smoothly.__<ul>_<li>SingleInputGate acquires its request lock and tries to release all_registered channels. When releasing a channel, it needs to acquire_the channel's shared request-release lock.</li>_<li>If a LocalInputChannel concurrently retriggers a partition request via_a Timer Thread it acquires the channel's request-release lock and calls_the retrigger callback on the SingleInputGate, which again tries to_acquire the gate's request lock.</li>_</ul>__<p>For certain timings this obviously leads to a deadlock. This test reliably_reproduced such a timing (reported in FLINK-5228). This test is pretty much_testing the buggy implementation and has not much more general value. If it_becomes obsolete at some point (future greatness _)), feel free to remove it.__<p>The fix in the end was to to not acquire the channels lock when releasing it_and/or not doing any input gate callbacks while holding the channel's lock._I decided to do both.;@Test_	public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception {_		final SingleInputGate gate = new SingleInputGate(_			"test task name",_			new JobID(),_			new IntermediateDataSetID(),_			ResultPartitionType.PIPELINED,_			0,_			1,_			mock(TaskActions.class),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),_			true_		)___		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)__		when(partitionManager_			.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class)))_			.thenAnswer(new Answer<ResultSubpartitionView>() {_				@Override_				public ResultSubpartitionView answer(InvocationOnMock invocationOnMock) throws Throwable {_					_					_					_					Thread.sleep(100)__					throw new PartitionNotFoundException(new ResultPartitionID())__				}_			})___		final LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			1, 1,_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		gate.setInputChannel(new IntermediateResultPartitionID(), channel)___		Thread releaser = new Thread() {_			@Override_			public void run() {_				try {_					gate.releaseAllResources()__				} catch (IOException ignored) {_				}_			}_		}___		Thread requester = new Thread() {_			@Override_			public void run() {_				try {_					channel.requestSubpartition(0)__				} catch (IOException | InterruptedException ignored) {_				}_			}_		}___		requester.start()__		releaser.start()___		releaser.join()__		requester.join()__	};verifies,that,concurrent,release,via,the,single,input,gate,and,re,triggering,of,a,partition,request,works,smoothly,ul,li,single,input,gate,acquires,its,request,lock,and,tries,to,release,all,registered,channels,when,releasing,a,channel,it,needs,to,acquire,the,channel,s,shared,request,release,lock,li,li,if,a,local,input,channel,concurrently,retriggers,a,partition,request,via,a,timer,thread,it,acquires,the,channel,s,request,release,lock,and,calls,the,retrigger,callback,on,the,single,input,gate,which,again,tries,to,acquire,the,gate,s,request,lock,li,ul,p,for,certain,timings,this,obviously,leads,to,a,deadlock,this,test,reliably,reproduced,such,a,timing,reported,in,flink,5228,this,test,is,pretty,much,testing,the,buggy,implementation,and,has,not,much,more,general,value,if,it,becomes,obsolete,at,some,point,future,greatness,feel,free,to,remove,it,p,the,fix,in,the,end,was,to,to,not,acquire,the,channels,lock,when,releasing,it,and,or,not,doing,any,input,gate,callbacks,while,holding,the,channel,s,lock,i,decided,to,do,both;test,public,void,test,concurrent,release,and,retrigger,partition,request,throws,exception,final,single,input,gate,gate,new,single,input,gate,test,task,name,new,job,id,new,intermediate,data,set,id,result,partition,type,pipelined,0,1,mock,task,actions,class,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,true,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,answer,new,answer,result,subpartition,view,override,public,result,subpartition,view,answer,invocation,on,mock,invocation,on,mock,throws,throwable,thread,sleep,100,throw,new,partition,not,found,exception,new,result,partition,id,final,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,1,1,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,gate,set,input,channel,new,intermediate,result,partition,id,channel,thread,releaser,new,thread,override,public,void,run,try,gate,release,all,resources,catch,ioexception,ignored,thread,requester,new,thread,override,public,void,run,try,channel,request,subpartition,0,catch,ioexception,interrupted,exception,ignored,requester,start,releaser,start,releaser,join,requester,join
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1427367086;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1427784999;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1430123107;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					mock(Environment.class),_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,mock,environment,class,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1431371621;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1432295874;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1432760698;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1433143315;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1441011751;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1441738685;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1452854660;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					false,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,false,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1465991918;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					false,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,false,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1466072697;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					false,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,false,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1472821521;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_					"Test Name",_					jobId,_					partitionIds[i],_					ResultPartitionType.PIPELINED,_					false,_					parallelism,_					partitionManager,_					partitionConsumableNotifier,_					ioManager,_					ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,job,id,partition,ids,i,result,partition,type,pipelined,false,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1475219249;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				false,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,false,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1477389806;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				false,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,false,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1478815184;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				ASYNC)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1478815615;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_				(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_				TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_				mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				ASYNC,_				true)___			_			partition.registerBufferPool(_					networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_					partition,_					false,_					new TestPartitionProducerBufferSource(_							parallelism,_							partition.getBufferProvider(),_							numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_					parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_						new TestLocalInputChannelConsumer(_								i,_								parallelism,_								numberOfBuffersPerChannel,_								networkBuffers.createBufferPool(parallelism, true),_								partitionManager,_								new TaskEventDispatcher(),_								partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,async,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1480624969;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, true),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1480688330;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, true),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1481127239;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, true),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1481560756;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, true),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1485269495;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, true))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, true),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,true,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,true,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1489060856;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1489149058;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1493724918;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1502726910;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE, MemoryType.HEAP)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,memory,type,heap,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1511516912;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1513102156;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1516285878;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			_			for (Future<?> result : results) {_				result.get()__			}_		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,for,future,result,results,result,get,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1519039281;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			waitForAll(60_000L, results)__		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,wait,for,all,results,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1519039287;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			waitForAll(60_000L, results)__		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,wait,for,all,results,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1519039300;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			waitForAll(60_000L, results)__		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,wait,for,all,results,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1519039301;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			waitForAll(60_000L, results)__		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,wait,for,all,results,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1519049124;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			waitForAll(60_000L, results)__		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,wait,for,all,results,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1525116906;Tests the consumption of multiple subpartitions via local input channels.__<p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			waitForAll(60_000L, results)__		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,wait,for,all,results,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1537385496;Tests the consumption of multiple subpartitions via local input channels.__<p>Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier =_			mock(ResultPartitionConsumableNotifier.class)___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			waitForAll(60_000L, results)__		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,mock,result,partition,consumable,notifier,class,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,wait,for,all,results,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1540207709;Tests the consumption of multiple subpartitions via local input channels.__<p>Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier = new NoOpResultPartitionConsumableNotifier()___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<Future<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(partitionProducers[i]))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				results.add(executor.submit(_					new TestLocalInputChannelConsumer(_						i,_						parallelism,_						numberOfBuffersPerChannel,_						networkBuffers.createBufferPool(parallelism, parallelism),_						partitionManager,_						new TaskEventDispatcher(),_						partitionIds)))__			}__			waitForAll(60_000L, results)__		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,new,no,op,result,partition,consumable,notifier,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,executor,submit,partition,producers,i,for,int,i,0,i,parallelism,i,results,add,executor,submit,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,wait,for,all,results,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testConcurrentConsumeMultiplePartitions() throws Exception;1547722029;Tests the consumption of multiple subpartitions via local input channels.__<p>Multiple producer tasks produce pipelined partitions, which are consumed by multiple_tasks via local input channels.;@Test_	public void testConcurrentConsumeMultiplePartitions() throws Exception {_		_		final int parallelism = 32__		final int producerBufferPoolSize = parallelism + 1__		final int numberOfBuffersPerChannel = 1024___		checkArgument(parallelism >= 1)__		checkArgument(producerBufferPoolSize >= parallelism)__		checkArgument(numberOfBuffersPerChannel >= 1)___		_		_		final ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism)___		final NetworkBufferPool networkBuffers = new NetworkBufferPool(_			(parallelism * producerBufferPoolSize) + (parallelism * parallelism),_			TestBufferFactory.BUFFER_SIZE)___		final ResultPartitionConsumableNotifier partitionConsumableNotifier = new NoOpResultPartitionConsumableNotifier()___		final TaskActions taskActions = mock(TaskActions.class)___		final IOManager ioManager = mock(IOManager.class)___		final JobID jobId = new JobID()___		final ResultPartitionManager partitionManager = new ResultPartitionManager()___		final ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism]__		final TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism]___		_		for (int i = 0_ i < parallelism_ i++) {_			partitionIds[i] = new ResultPartitionID()___			final ResultPartition partition = new ResultPartition(_				"Test Name",_				taskActions,_				jobId,_				partitionIds[i],_				ResultPartitionType.PIPELINED,_				parallelism,_				parallelism,_				partitionManager,_				partitionConsumableNotifier,_				ioManager,_				true)___			_			partition.registerBufferPool(_				networkBuffers.createBufferPool(producerBufferPoolSize, producerBufferPoolSize))___			_			partitionProducers[i] = new TestPartitionProducer(_				partition,_				false,_				new TestPartitionProducerBufferSource(_					parallelism,_					partition.getBufferProvider(),_					numberOfBuffersPerChannel)_			)___			_			_			partitionManager.registerResultPartition(partition)__		}__		_		try {_			_			List<CompletableFuture<?>> results = Lists.newArrayListWithCapacity(_				parallelism + 1)___			for (int i = 0_ i < parallelism_ i++) {_				results.add(CompletableFuture.supplyAsync(_					CheckedSupplier.unchecked(partitionProducers[i]::call), executor))__			}__			_			for (int i = 0_ i < parallelism_ i++) {_				final TestLocalInputChannelConsumer consumer = new TestLocalInputChannelConsumer(_					i,_					parallelism,_					numberOfBuffersPerChannel,_					networkBuffers.createBufferPool(parallelism, parallelism),_					partitionManager,_					new TaskEventDispatcher(),_					partitionIds)___				results.add(CompletableFuture.supplyAsync(CheckedSupplier.unchecked(consumer::call), executor))__			}__			FutureUtils.waitForAll(results)_				.get(60_000L, TimeUnit.MILLISECONDS)__		}_		finally {_			networkBuffers.destroyAllBufferPools()__			networkBuffers.destroy()__			executor.shutdown()__		}_	};tests,the,consumption,of,multiple,subpartitions,via,local,input,channels,p,multiple,producer,tasks,produce,pipelined,partitions,which,are,consumed,by,multiple,tasks,via,local,input,channels;test,public,void,test,concurrent,consume,multiple,partitions,throws,exception,final,int,parallelism,32,final,int,producer,buffer,pool,size,parallelism,1,final,int,number,of,buffers,per,channel,1024,check,argument,parallelism,1,check,argument,producer,buffer,pool,size,parallelism,check,argument,number,of,buffers,per,channel,1,final,executor,service,executor,executors,new,fixed,thread,pool,2,parallelism,final,network,buffer,pool,network,buffers,new,network,buffer,pool,parallelism,producer,buffer,pool,size,parallelism,parallelism,test,buffer,factory,final,result,partition,consumable,notifier,partition,consumable,notifier,new,no,op,result,partition,consumable,notifier,final,task,actions,task,actions,mock,task,actions,class,final,iomanager,io,manager,mock,iomanager,class,final,job,id,job,id,new,job,id,final,result,partition,manager,partition,manager,new,result,partition,manager,final,result,partition,id,partition,ids,new,result,partition,id,parallelism,final,test,partition,producer,partition,producers,new,test,partition,producer,parallelism,for,int,i,0,i,parallelism,i,partition,ids,i,new,result,partition,id,final,result,partition,partition,new,result,partition,test,name,task,actions,job,id,partition,ids,i,result,partition,type,pipelined,parallelism,parallelism,partition,manager,partition,consumable,notifier,io,manager,true,partition,register,buffer,pool,network,buffers,create,buffer,pool,producer,buffer,pool,size,producer,buffer,pool,size,partition,producers,i,new,test,partition,producer,partition,false,new,test,partition,producer,buffer,source,parallelism,partition,get,buffer,provider,number,of,buffers,per,channel,partition,manager,register,result,partition,partition,try,list,completable,future,results,lists,new,array,list,with,capacity,parallelism,1,for,int,i,0,i,parallelism,i,results,add,completable,future,supply,async,checked,supplier,unchecked,partition,producers,i,call,executor,for,int,i,0,i,parallelism,i,final,test,local,input,channel,consumer,consumer,new,test,local,input,channel,consumer,i,parallelism,number,of,buffers,per,channel,network,buffers,create,buffer,pool,parallelism,parallelism,partition,manager,new,task,event,dispatcher,partition,ids,results,add,completable,future,supply,async,checked,supplier,unchecked,consumer,call,executor,future,utils,wait,for,all,results,get,time,unit,milliseconds,finally,network,buffers,destroy,all,buffer,pools,network,buffers,destroy,executor,shutdown
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1481127239;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,new,unregistered,task,metrics,group,dummy,task,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1481560756;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,new,unregistered,task,metrics,group,dummy,task,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1485269495;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,new,unregistered,task,metrics,group,dummy,task,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1489060856;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,new,unregistered,task,metrics,group,dummy,task,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1489149058;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferProvider.class),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,provider,class,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,new,unregistered,task,metrics,group,dummy,task,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1493724918;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,new,unregistered,task,metrics,group,dummy,task,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1502726910;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,new,unregistered,task,metrics,group,dummy,task,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1511516912;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			new UnregisteredTaskMetricsGroup.DummyTaskIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,new,unregistered,task,metrics,group,dummy,task,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1513102156;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1516285878;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1519039281;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1519039287;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1519039300;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_				any(ResultPartitionID.class),_				anyInt(),_				any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected IllegalStateException")__		} catch (IllegalStateException ignored) {_		}__		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}_	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,try,channel,get,next,buffer,fail,did,not,throw,expected,illegal,state,exception,catch,illegal,state,exception,ignored,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1519039301;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_			any(ResultPartitionID.class),_			anyInt(),_			any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		assertFalse(channel.getNextBuffer().isPresent())___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}__		channel.releaseAllResources()__		assertFalse(channel.getNextBuffer().isPresent())__	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,assert,false,channel,get,next,buffer,is,present,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored,channel,release,all,resources,assert,false,channel,get,next,buffer,is,present
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1519049124;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_			any(ResultPartitionID.class),_			anyInt(),_			any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		assertFalse(channel.getNextBuffer().isPresent())___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}__		channel.releaseAllResources()__		assertFalse(channel.getNextBuffer().isPresent())__	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,assert,false,channel,get,next,buffer,is,present,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored,channel,release,all,resources,assert,false,channel,get,next,buffer,is,present
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1525116906;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_			any(ResultPartitionID.class),_			anyInt(),_			any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		assertFalse(channel.getNextBuffer().isPresent())___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}__		channel.releaseAllResources()__		assertFalse(channel.getNextBuffer().isPresent())__	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,assert,false,channel,get,next,buffer,is,present,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored,channel,release,all,resources,assert,false,channel,get,next,buffer,is,present
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1537385496;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_			any(ResultPartitionID.class),_			anyInt(),_			any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		assertFalse(channel.getNextBuffer().isPresent())___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}__		channel.releaseAllResources()__		assertFalse(channel.getNextBuffer().isPresent())__	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,assert,false,channel,get,next,buffer,is,present,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored,channel,release,all,resources,assert,false,channel,get,next,buffer,is,present
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1540207709;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_			any(ResultPartitionID.class),_			anyInt(),_			any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		assertFalse(channel.getNextBuffer().isPresent())___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}__		channel.releaseAllResources()__		assertFalse(channel.getNextBuffer().isPresent())__	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,assert,false,channel,get,next,buffer,is,present,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored,channel,release,all,resources,assert,false,channel,get,next,buffer,is,present
LocalInputChannelTest -> @Test 	public void testGetNextAfterPartitionReleased() throws Exception;1547722029;Tests that reading from a channel when after the partition has been_released are handled and don't lead to NPEs.;@Test_	public void testGetNextAfterPartitionReleased() throws Exception {_		ResultSubpartitionView reader = mock(ResultSubpartitionView.class)__		SingleInputGate gate = mock(SingleInputGate.class)__		ResultPartitionManager partitionManager = mock(ResultPartitionManager.class)___		when(partitionManager.createSubpartitionView(_			any(ResultPartitionID.class),_			anyInt(),_			any(BufferAvailabilityListener.class))).thenReturn(reader)___		LocalInputChannel channel = new LocalInputChannel(_			gate,_			0,_			new ResultPartitionID(),_			partitionManager,_			new TaskEventDispatcher(),_			UnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup())___		channel.requestSubpartition(0)___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(false)___		assertFalse(channel.getNextBuffer().isPresent())___		_		when(reader.getNextBuffer()).thenReturn(null)__		when(reader.isReleased()).thenReturn(true)___		try {_			channel.getNextBuffer()__			fail("Did not throw expected CancelTaskException")__		} catch (CancelTaskException ignored) {_		}__		channel.releaseAllResources()__		assertFalse(channel.getNextBuffer().isPresent())__	};tests,that,reading,from,a,channel,when,after,the,partition,has,been,released,are,handled,and,don,t,lead,to,npes;test,public,void,test,get,next,after,partition,released,throws,exception,result,subpartition,view,reader,mock,result,subpartition,view,class,single,input,gate,gate,mock,single,input,gate,class,result,partition,manager,partition,manager,mock,result,partition,manager,class,when,partition,manager,create,subpartition,view,any,result,partition,id,class,any,int,any,buffer,availability,listener,class,then,return,reader,local,input,channel,channel,new,local,input,channel,gate,0,new,result,partition,id,partition,manager,new,task,event,dispatcher,unregistered,metric,groups,create,unregistered,task,metric,group,get,iometric,group,channel,request,subpartition,0,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,false,assert,false,channel,get,next,buffer,is,present,when,reader,get,next,buffer,then,return,null,when,reader,is,released,then,return,true,try,channel,get,next,buffer,fail,did,not,throw,expected,cancel,task,exception,catch,cancel,task,exception,ignored,channel,release,all,resources,assert,false,channel,get,next,buffer,is,present
