commented;modifiers;parameterAmount;loc;comment;code
false;private,static;0;6;;private static Configuration getConfiguration() {     Configuration config = new Configuration().     config.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE, "80m").     config.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 800).     return config. }
false;public;0;16;;@Test public void testSuccessfulProgramAfterFailure() throws Exception {     ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment().     runConnectedComponents(env).     try {         runKMeans(env).         fail("This program execution should have failed.").     } catch (JobExecutionException e) {         assertTrue(e.getCause().getMessage().contains("Insufficient number of network buffers")).     }     runConnectedComponents(env). }
false;private,static;1;40;;private static void runConnectedComponents(ExecutionEnvironment env) throws Exception {     env.setParallelism(PARALLELISM).     env.getConfig().disableSysoutLogging().     // read vertex and edge data     DataSet<Long> vertices = ConnectedComponentsData.getDefaultVertexDataSet(env).rebalance().     DataSet<Tuple2<Long, Long>> edges = ConnectedComponentsData.getDefaultEdgeDataSet(env).rebalance().flatMap(new ConnectedComponents.UndirectEdge()).     // assign the initial components (equal to the vertex id)     DataSet<Tuple2<Long, Long>> verticesWithInitialId = vertices.map(new ConnectedComponents.DuplicateValue<Long>()).     // open a delta iteration     DeltaIteration<Tuple2<Long, Long>, Tuple2<Long, Long>> iteration = verticesWithInitialId.iterateDelta(verticesWithInitialId, 100, 0).     // apply the step logic: join with the edges, select the minimum neighbor,     // update if the component of the candidate is smaller     DataSet<Tuple2<Long, Long>> changes = iteration.getWorkset().join(edges).where(0).equalTo(0).with(new ConnectedComponents.NeighborWithComponentIDJoin()).groupBy(0).aggregate(Aggregations.MIN, 1).join(iteration.getSolutionSet()).where(0).equalTo(0).with(new ConnectedComponents.ComponentIdFilter()).     // close the delta iteration (delta and new workset are identical)     DataSet<Tuple2<Long, Long>> result = iteration.closeWith(changes, changes).     result.output(new DiscardingOutputFormat<Tuple2<Long, Long>>()).     env.execute(). }
false;private,static;1;35;;private static void runKMeans(ExecutionEnvironment env) throws Exception {     env.setParallelism(PARALLELISM).     env.getConfig().disableSysoutLogging().     // get input data     DataSet<KMeans.Point> points = KMeansData.getDefaultPointDataSet(env).rebalance().     DataSet<KMeans.Centroid> centroids = KMeansData.getDefaultCentroidDataSet(env).rebalance().     // set number of bulk iterations for KMeans algorithm     IterativeDataSet<KMeans.Centroid> loop = centroids.iterate(20).     // add some re-partitions to increase network buffer use     DataSet<KMeans.Centroid> newCentroids = points.map(new KMeans.SelectNearestCenter()).withBroadcastSet(loop, "centroids").rebalance().map(new KMeans.CountAppender()).groupBy(0).reduce(new KMeans.CentroidAccumulator()).rebalance().map(new KMeans.CentroidAverager()).     // feed new centroids back into next iteration     DataSet<KMeans.Centroid> finalCentroids = loop.closeWith(newCentroids).     DataSet<Tuple2<Integer, KMeans.Point>> clusteredPoints = points.map(new KMeans.SelectNearestCenter()).withBroadcastSet(finalCentroids, "centroids").     clusteredPoints.output(new DiscardingOutputFormat<Tuple2<Integer, KMeans.Point>>()).     env.execute("KMeans Example"). }
