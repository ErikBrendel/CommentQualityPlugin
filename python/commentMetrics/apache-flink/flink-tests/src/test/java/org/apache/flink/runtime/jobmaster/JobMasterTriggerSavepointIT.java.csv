commented;modifiers;parameterAmount;loc;comment;code
false;private;1;35;;private void setUpWithCheckpointInterval(long checkpointInterval) throws Exception {     invokeLatch = new CountDownLatch(1).     triggerCheckpointLatch = new CountDownLatch(1).     savepointDirectory = temporaryFolder.newFolder().toPath().     Assume.assumeTrue("ClusterClient is not an instance of MiniClusterClient", miniClusterResource.getClusterClient() instanceof MiniClusterClient).     clusterClient = (MiniClusterClient) miniClusterResource.getClusterClient().     clusterClient.setDetached(true).     jobGraph = new JobGraph().     final JobVertex vertex = new JobVertex("testVertex").     vertex.setInvokableClass(NoOpBlockingInvokable.class).     jobGraph.addVertex(vertex).     jobGraph.setSnapshotSettings(new JobCheckpointingSettings(Collections.singletonList(vertex.getID()), Collections.singletonList(vertex.getID()), Collections.singletonList(vertex.getID()), new CheckpointCoordinatorConfiguration(checkpointInterval, 60_000, 10, 1, CheckpointRetentionPolicy.NEVER_RETAIN_AFTER_TERMINATION, true), null)).     clusterClient.submitJob(jobGraph, ClassLoader.getSystemClassLoader()).     invokeLatch.await(60, TimeUnit.SECONDS).     waitForJob(). }
false;public;0;15;;@Test public void testStopJobAfterSavepoint() throws Exception {     setUpWithCheckpointInterval(10L).     final String savepointLocation = cancelWithSavepoint().     final JobStatus jobStatus = clusterClient.getJobStatus(jobGraph.getJobID()).get().     assertThat(jobStatus, isOneOf(JobStatus.CANCELED, JobStatus.CANCELLING)).     final List<Path> savepoints.     try (Stream<Path> savepointFiles = Files.list(savepointDirectory)) {         savepoints = savepointFiles.map(Path::getFileName).collect(Collectors.toList()).     }     assertThat(savepoints, hasItem(Paths.get(savepointLocation).getFileName())). }
false;public;0;16;;@Test public void testStopJobAfterSavepointWithDeactivatedPeriodicCheckpointing() throws Exception {     // set checkpointInterval to Long.MAX_VALUE, which means deactivated checkpointing     setUpWithCheckpointInterval(Long.MAX_VALUE).     final String savepointLocation = cancelWithSavepoint().     final JobStatus jobStatus = clusterClient.getJobStatus(jobGraph.getJobID()).get(60, TimeUnit.SECONDS).     assertThat(jobStatus, isOneOf(JobStatus.CANCELED, JobStatus.CANCELLING)).     final List<Path> savepoints.     try (Stream<Path> savepointFiles = Files.list(savepointDirectory)) {         savepoints = savepointFiles.map(Path::getFileName).collect(Collectors.toList()).     }     assertThat(savepoints, hasItem(Paths.get(savepointLocation).getFileName())). }
false;public;0;23;;@Test public void testDoNotCancelJobIfSavepointFails() throws Exception {     setUpWithCheckpointInterval(10L).     try {         Files.setPosixFilePermissions(savepointDirectory, Collections.emptySet()).     } catch (IOException e) {         Assume.assumeNoException(e).     }     try {         cancelWithSavepoint().     } catch (Exception e) {         assertThat(ExceptionUtils.findThrowable(e, CheckpointTriggerException.class).isPresent(), equalTo(true)).     }     final JobStatus jobStatus = clusterClient.getJobStatus(jobGraph.getJobID()).get(60, TimeUnit.SECONDS).     assertThat(jobStatus, equalTo(JobStatus.RUNNING)).     // assert that checkpoints are continued to be triggered     triggerCheckpointLatch = new CountDownLatch(1).     assertThat(triggerCheckpointLatch.await(60L, TimeUnit.SECONDS), equalTo(true)). }
true;public;0;12;/**  * Tests that cancel with savepoint without a properly configured savepoint  * directory, will fail with a meaningful exception message.  */ ;/**  * Tests that cancel with savepoint without a properly configured savepoint  * directory, will fail with a meaningful exception message.  */ @Test public void testCancelWithSavepointWithoutConfiguredSavepointDirectory() throws Exception {     setUpWithCheckpointInterval(10L).     try {         clusterClient.cancelWithSavepoint(jobGraph.getJobID(), null).     } catch (Exception e) {         if (!ExceptionUtils.findThrowableWithMessage(e, "savepoint directory").isPresent()) {             throw e.         }     } }
false;private;0;15;;private void waitForJob() throws Exception {     for (int i = 0. i < 60. i++) {         try {             final JobStatus jobStatus = clusterClient.getJobStatus(jobGraph.getJobID()).get(60, TimeUnit.SECONDS).             assertThat(jobStatus.isGloballyTerminalState(), equalTo(false)).             if (jobStatus == JobStatus.RUNNING) {                 return.             }         } catch (ExecutionException ignored) {         // JobManagerRunner is not yet registered in Dispatcher         }         Thread.sleep(1000).     }     throw new AssertionError("Job did not become running within timeout."). }
false;public;0;9;;@Override public void invoke() {     invokeLatch.countDown().     try {         Thread.sleep(Long.MAX_VALUE).     } catch (InterruptedException e) {         Thread.currentThread().interrupt().     } }
false;public;2;16;;@Override public boolean triggerCheckpoint(final CheckpointMetaData checkpointMetaData, final CheckpointOptions checkpointOptions) {     final TaskStateSnapshot checkpointStateHandles = new TaskStateSnapshot().     checkpointStateHandles.putSubtaskStateByOperatorID(OperatorID.fromJobVertexID(getEnvironment().getJobVertexId()), new OperatorSubtaskState()).     getEnvironment().acknowledgeCheckpoint(checkpointMetaData.getCheckpointId(), new CheckpointMetrics(), checkpointStateHandles).     triggerCheckpointLatch.countDown().     return true. }
false;public;1;3;;@Override public void notifyCheckpointComplete(final long checkpointId) { }
false;private;0;5;;private String cancelWithSavepoint() throws Exception {     return clusterClient.cancelWithSavepoint(jobGraph.getJobID(), savepointDirectory.toAbsolutePath().toString()). }
