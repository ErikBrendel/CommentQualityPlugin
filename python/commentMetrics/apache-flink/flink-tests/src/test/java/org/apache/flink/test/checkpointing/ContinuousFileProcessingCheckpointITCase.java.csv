commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;16;;@BeforeClass public static void createHDFS() {     try {         baseDir = new File("./target/localfs/fs_tests").getAbsoluteFile().         FileUtil.fullyDelete(baseDir).         org.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration().         localFsURI = "file:///" + baseDir + "/".         localFs = new org.apache.hadoop.fs.Path(localFsURI).getFileSystem(hdConf).     } catch (Throwable e) {         e.printStackTrace().         Assert.fail("Test failed " + e.getMessage()).     } }
false;public,static;0;8;;@AfterClass public static void destroyHDFS() {     try {         FileUtil.fullyDelete(baseDir).     } catch (Throwable t) {         throw new RuntimeException(t).     } }
false;public;2;4;;@Override public void flatMap(String value, Collector<String> out) throws Exception {     out.collect(value). }
false;public;1;27;;@Override public void testProgram(StreamExecutionEnvironment env) {     // set the restart strategy.     env.getConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(NO_OF_RETRIES, 0)).     env.enableCheckpointing(10).     // create and start the file creating thread.     fc = new FileCreator().     fc.start().     // create the monitoring source along with the necessary readers.     TextInputFormat format = new TextInputFormat(new org.apache.flink.core.fs.Path(localFsURI)).     format.setFilesFilter(FilePathFilter.createDefaultFilter()).     DataStream<String> inputStream = env.readFile(format, localFsURI, FileProcessingMode.PROCESS_CONTINUOUSLY, INTERVAL).     TestingSinkFunction sink = new TestingSinkFunction().     inputStream.flatMap(new FlatMapFunction<String, String>() {          @Override         public void flatMap(String value, Collector<String> out) throws Exception {             out.collect(value).         }     }).addSink(sink).setParallelism(1). }
false;public;2;4;;@Override public int compare(String o1, String o2) {     return getLineNo(o1) - getLineNo(o2). }
false;public;0;31;;@Override public void postSubmit() throws Exception {     // be sure that the file creating thread is done.     fc.join().     Map<Integer, Set<String>> collected = actualCollectedContent.     Assert.assertEquals(collected.size(), fc.getFileContent().size()).     for (Integer fileIdx : fc.getFileContent().keySet()) {         Assert.assertTrue(collected.keySet().contains(fileIdx)).         List<String> cntnt = new ArrayList<>(collected.get(fileIdx)).         Collections.sort(cntnt, new Comparator<String>() {              @Override             public int compare(String o1, String o2) {                 return getLineNo(o1) - getLineNo(o2).             }         }).         StringBuilder cntntStr = new StringBuilder().         for (String line : cntnt) {             cntntStr.append(line).         }         Assert.assertEquals(fc.getFileContent().get(fileIdx), cntntStr.toString()).     }     collected.clear().     actualCollectedContent.clear().     fc.clean(). }
false;private;1;4;;private int getLineNo(String line) {     String[] tkns = line.split("\\s").     return Integer.parseInt(tkns[tkns.length - 1]). }
false;public;1;10;;@Override public void open(Configuration parameters) throws Exception {     // this sink can only work with DOP 1     assertEquals(1, getRuntimeContext().getNumberOfParallelSubtasks()).     long failurePosMin = (long) (0.4 * LINES_PER_FILE).     long failurePosMax = (long) (0.7 * LINES_PER_FILE).     elementsToFailure = (new Random().nextLong() % (failurePosMax - failurePosMin)) + failurePosMin. }
false;public;1;34;;@Override public void invoke(String value) throws Exception {     int fileIdx = getFileIdx(value).     Set<String> content = actualContent.get(fileIdx).     if (content == null) {         content = new HashSet<>().         actualContent.put(fileIdx, content).     }     // detect duplicate lines.     if (!content.add(value + "\n")) {         fail("Duplicate line: " + value).         System.exit(0).     }     elementCounter++.     // this is termination     if (elementCounter >= NO_OF_FILES * LINES_PER_FILE) {         actualCollectedContent = actualContent.         throw new SuccessException().     }     // add some latency so that we have at least two checkpoint in     if (!hasRestoredAfterFailure && successfulCheckpoints < 2) {         Thread.sleep(5).     }     // simulate a node failure     if (!hasRestoredAfterFailure && successfulCheckpoints >= 2 && elementCounter >= elementsToFailure) {         throw new Exception("Task Failure @ elem: " + elementCounter + " / " + elementsToFailure).     } }
false;public;0;8;;@Override public void close() {     try {         super.close().     } catch (Exception e) {         e.printStackTrace().     } }
false;public;2;5;;@Override public List<Tuple2<Long, Map<Integer, Set<String>>>> snapshotState(long checkpointId, long checkpointTimestamp) throws Exception {     Tuple2<Long, Map<Integer, Set<String>>> state = new Tuple2<>(elementCounter, actualContent).     return Collections.singletonList(state). }
false;public;1;7;;@Override public void restoreState(List<Tuple2<Long, Map<Integer, Set<String>>>> state) throws Exception {     Tuple2<Long, Map<Integer, Set<String>>> s = state.get(0).     this.elementCounter = s.f0.     this.actualContent = s.f1.     // because now restore is also called at initialization     this.hasRestoredAfterFailure = this.elementCounter != 0. }
false;public;1;4;;@Override public void notifyCheckpointComplete(long checkpointId) throws Exception {     this.successfulCheckpoints++. }
false;private;1;4;;private int getFileIdx(String line) {     String[] tkns = line.split(":").     return Integer.parseInt(tkns[0]). }
false;public;0;34;;public void run() {     try {         for (int i = 0. i < NO_OF_FILES. i++) {             Tuple2<org.apache.hadoop.fs.Path, String> tmpFile.             long modTime.             do {                 // give it some time so that the files have                 // different modification timestamps.                 Thread.sleep(50).                 tmpFile = fillWithData(localFsURI, "file", i, "This is test line.").                 modTime = localFs.getFileStatus(tmpFile.f0).getModificationTime().                 if (modTime <= lastCreatedModTime) {                     // delete the last created file to recreate it with a different timestamp                     localFs.delete(tmpFile.f0, false).                 }             } while (modTime <= lastCreatedModTime).             lastCreatedModTime = modTime.             // rename the file             org.apache.hadoop.fs.Path file = new org.apache.hadoop.fs.Path(localFsURI + "/file" + i).             localFs.rename(tmpFile.f0, file).             Assert.assertTrue(localFs.exists(file)).             filesCreated.add(file).             fileContents.put(i, tmpFile.f1).         }     } catch (IOException | InterruptedException e) {         e.printStackTrace().     } }
false;;0;7;;void clean() throws IOException {     assert (localFs != null).     for (org.apache.hadoop.fs.Path path : filesCreated) {         localFs.delete(path, false).     }     fileContents.clear(). }
false;;0;3;;Map<Integer, String> getFileContent() {     return this.fileContents. }
true;private;4;18;/**  * Fill the file with content and put the content in the {@code hdPathContents} list.  */ ;/**  * Fill the file with content and put the content in the {@code hdPathContents} list.  */ private Tuple2<Path, String> fillWithData(String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {     assert (localFs != null).     org.apache.hadoop.fs.Path tmp = new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx).     FSDataOutputStream stream = localFs.create(tmp).     StringBuilder str = new StringBuilder().     for (int i = 0. i < LINES_PER_FILE. i++) {         String line = fileIdx + ": " + sampleLine + " " + i + "\n".         str.append(line).         stream.write(line.getBytes(ConfigConstants.DEFAULT_CHARSET)).     }     stream.close().     return new Tuple2<>(tmp, str.toString()). }
