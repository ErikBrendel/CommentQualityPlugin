commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;31;;@BeforeClass public static void setup() throws Exception {     zkServer = new TestingServer().     Configuration config = new Configuration().     config.setInteger(ConfigConstants.LOCAL_NUMBER_JOB_MANAGER, NUM_JMS).     config.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TMS).     config.setInteger(TaskManagerOptions.NUM_TASK_SLOTS, NUM_SLOTS_PER_TM).     haStorageDir = TEMPORARY_FOLDER.newFolder().     config.setString(HighAvailabilityOptions.HA_STORAGE_PATH, haStorageDir.toString()).     config.setString(HighAvailabilityOptions.HA_CLUSTER_ID, UUID.randomUUID().toString()).     config.setString(HighAvailabilityOptions.HA_ZOOKEEPER_QUORUM, zkServer.getConnectString()).     config.setString(HighAvailabilityOptions.HA_MODE, "zookeeper").     config.setString(ConfigConstants.METRICS_REPORTER_PREFIX + "restarts." + ConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, RestartReporter.class.getName()).     // we have to manage this manually because we have to create the ZooKeeper server     // ahead of this     miniClusterResource = new MiniClusterWithClientResource(new MiniClusterResourceConfiguration.Builder().setConfiguration(config).setNumberTaskManagers(NUM_TMS).setNumberSlotsPerTaskManager(NUM_SLOTS_PER_TM).build()).     miniClusterResource.before(). }
false;public,static;0;7;;@AfterClass public static void tearDown() throws Exception {     miniClusterResource.after().     zkServer.stop().     zkServer.close(). }
false;public;2;14;;@Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) {     if (file.getFileName().toString().startsWith("completedCheckpoint")) {         log.debug("Moving original checkpoint file {}.", file).         try {             Files.move(file, movedCheckpointLocation.toPath().resolve(file.getFileName())).             numCheckpoints.incrementAndGet().         } catch (IOException ioe) {             // previous checkpoint files may be deleted asynchronously             log.debug("Exception while moving HA files.", ioe).         }     }     return FileVisitResult.CONTINUE. }
false;public;2;5;;@Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {     Files.move(file, haStorageDir.toPath().resolve(file.getFileName())).     return FileVisitResult.CONTINUE. }
true;public;0;116;/**  * Verify that we don't start a job from scratch if we cannot restore any of the  * CompletedCheckpoints.  *  * <p>Synchronization for the different steps and things we want to observe happens via  * latches in the test method and the methods of {@link CheckpointBlockingFunction}.  *  * <p>The test follows these steps:  * <ol>  *     <li>Start job and block on a latch until we have done some checkpoints  *     <li>Block in the special function  *     <li>Move away the contents of the ZooKeeper HA directory to make restoring from  *       checkpoints impossible  *     <li>Unblock the special function, which now induces a failure  *     <li>Make sure that the job does not recover successfully  *     <li>Move back the HA directory  *     <li>Make sure that the job recovers, we use a latch to ensure that the operator  *       restored successfully  * </ol>  */ ;/**  * Verify that we don't start a job from scratch if we cannot restore any of the  * CompletedCheckpoints.  *  * <p>Synchronization for the different steps and things we want to observe happens via  * latches in the test method and the methods of {@link CheckpointBlockingFunction}.  *  * <p>The test follows these steps:  * <ol>  *     <li>Start job and block on a latch until we have done some checkpoints  *     <li>Block in the special function  *     <li>Move away the contents of the ZooKeeper HA directory to make restoring from  *       checkpoints impossible  *     <li>Unblock the special function, which now induces a failure  *     <li>Make sure that the job does not recover successfully  *     <li>Move back the HA directory  *     <li>Make sure that the job recovers, we use a latch to ensure that the operator  *       restored successfully  * </ol>  */ @Test public void testRestoreBehaviourWithFaultyStateHandles() throws Exception {     CheckpointBlockingFunction.allowedInitializeCallsWithoutRestore.set(1).     CheckpointBlockingFunction.successfulRestores.set(0).     CheckpointBlockingFunction.illegalRestores.set(0).     CheckpointBlockingFunction.afterMessWithZooKeeper.set(false).     CheckpointBlockingFunction.failedAlready.set(false).     waitForCheckpointLatch = new OneShotLatch().     failInCheckpointLatch = new OneShotLatch().     ClusterClient<?> clusterClient = miniClusterResource.getClusterClient().     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(1).     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 0)).     // Flink doesn't allow lower than 10 ms     env.enableCheckpointing(10).     File checkpointLocation = TEMPORARY_FOLDER.newFolder().     env.setStateBackend((StateBackend) new FsStateBackend(checkpointLocation.toURI())).     DataStreamSource<String> source = env.addSource(new UnboundedSource()).     source.keyBy((str) -> str).map(new CheckpointBlockingFunction()).     JobGraph jobGraph = env.getStreamGraph().getJobGraph().     JobID jobID = Preconditions.checkNotNull(jobGraph.getJobID()).     clusterClient.setDetached(true).     clusterClient.submitJob(jobGraph, ZooKeeperHighAvailabilityITCase.class.getClassLoader()).     // wait until we did some checkpoints     waitForCheckpointLatch.await().     log.debug("Messing with HA directory").     // mess with the HA directory so that the job cannot restore     File movedCheckpointLocation = TEMPORARY_FOLDER.newFolder().     AtomicInteger numCheckpoints = new AtomicInteger().     Files.walkFileTree(haStorageDir.toPath(), new SimpleFileVisitor<Path>() {          @Override         public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) {             if (file.getFileName().toString().startsWith("completedCheckpoint")) {                 log.debug("Moving original checkpoint file {}.", file).                 try {                     Files.move(file, movedCheckpointLocation.toPath().resolve(file.getFileName())).                     numCheckpoints.incrementAndGet().                 } catch (IOException ioe) {                     // previous checkpoint files may be deleted asynchronously                     log.debug("Exception while moving HA files.", ioe).                 }             }             return FileVisitResult.CONTINUE.         }     }).     // Note to future developers: This will break when we change Flink to not put the     // checkpoint metadata into the HA directory but instead rely on the fact that the     // actual checkpoint directory on DFS contains the checkpoint metadata. In this case,     // ZooKeeper will only contain a "handle" (read: String) that points to the metadata     // in DFS. The likely solution will be that we have to go directly to ZooKeeper, find     // out where the checkpoint is stored and mess with that.     assertTrue(numCheckpoints.get() > 0).     log.debug("Resuming job").     failInCheckpointLatch.trigger().     assertNotNull("fullRestarts metric could not be accessed.", RestartReporter.numRestarts).     while (RestartReporter.numRestarts.getValue() < 5 && deadline.hasTimeLeft()) {         Thread.sleep(50).     }     assertThat(RestartReporter.numRestarts.getValue(), is(greaterThan(4L))).     // move back the HA directory so that the job can restore     CheckpointBlockingFunction.afterMessWithZooKeeper.set(true).     log.debug("Restored zookeeper").     Files.walkFileTree(movedCheckpointLocation.toPath(), new SimpleFileVisitor<Path>() {          @Override         public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {             Files.move(file, haStorageDir.toPath().resolve(file.getFileName())).             return FileVisitResult.CONTINUE.         }     }).     // now the job should be able to go to RUNNING again and then eventually to FINISHED,     // which it only does if it could successfully restore     CompletableFuture<JobStatus> jobStatusFuture = FutureUtils.retrySuccessfulWithDelay(() -> clusterClient.getJobStatus(jobID), Time.milliseconds(50), deadline, JobStatus::isGloballyTerminalState, TestingUtils.defaultScheduledExecutor()).     try {         assertEquals(JobStatus.FINISHED, jobStatusFuture.get()).     } catch (Throwable e) {         // include additional debugging information         StringWriter error = new StringWriter().         try (PrintWriter out = new PrintWriter(error)) {             out.println("The job did not finish in time.").             out.println("allowedInitializeCallsWithoutRestore= " + CheckpointBlockingFunction.allowedInitializeCallsWithoutRestore.get()).             out.println("illegalRestores= " + CheckpointBlockingFunction.illegalRestores.get()).             out.println("successfulRestores= " + CheckpointBlockingFunction.successfulRestores.get()).             out.println("afterMessWithZooKeeper= " + CheckpointBlockingFunction.afterMessWithZooKeeper.get()).             out.println("failedAlready= " + CheckpointBlockingFunction.failedAlready.get()).             out.println("currentJobStatus= " + clusterClient.getJobStatus(jobID).get()).             out.println("numRestarts= " + RestartReporter.numRestarts.getValue()).             out.println("threadDump= " + generateThreadDump()).         }         throw new AssertionError(error.toString(), ExceptionUtils.stripCompletionException(e)).     }     assertThat("We saw illegal restores.", CheckpointBlockingFunction.illegalRestores.get(), is(0)). }
false;private,static;0;23;;private static String generateThreadDump() {     final StringBuilder dump = new StringBuilder().     final ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean().     final ThreadInfo[] threadInfos = threadMXBean.getThreadInfo(threadMXBean.getAllThreadIds(), 100).     for (ThreadInfo threadInfo : threadInfos) {         dump.append('"').         dump.append(threadInfo.getThreadName()).         dump.append('"').         final Thread.State state = threadInfo.getThreadState().         dump.append(System.lineSeparator()).         dump.append("   java.lang.Thread.State: ").         dump.append(state).         final StackTraceElement[] stackTraceElements = threadInfo.getStackTrace().         for (final StackTraceElement stackTraceElement : stackTraceElements) {             dump.append(System.lineSeparator()).             dump.append("        at ").             dump.append(stackTraceElement).         }         dump.append(System.lineSeparator()).         dump.append(System.lineSeparator()).     }     return dump.toString(). }
false;public;1;8;;@Override public void run(SourceContext<String> ctx) throws Exception {     while (running && !CheckpointBlockingFunction.afterMessWithZooKeeper.get()) {         ctx.collect("hello").         // don't overdo it ... .-)         Thread.sleep(50).     } }
false;public;0;4;;@Override public void cancel() {     running = false. }
false;public;1;5;;@Override public String map(String value) throws Exception {     getRuntimeContext().getState(stateDescriptor).update("42").     return value. }
false;public;1;10;;@Override public void snapshotState(FunctionSnapshotContext context) throws Exception {     if (context.getCheckpointId() > 5) {         waitForCheckpointLatch.trigger().         failInCheckpointLatch.await().         if (!failedAlready.getAndSet(true)) {             throw new RuntimeException("Failing on purpose.").         }     } }
false;public;1;17;;@Override public void initializeState(FunctionInitializationContext context) {     if (!context.isRestored()) {         int updatedValue = allowedInitializeCallsWithoutRestore.decrementAndGet().         if (updatedValue < 0) {             illegalRestores.getAndIncrement().             throw new RuntimeException("We are not allowed any more restores.").         }     } else {         if (!afterMessWithZooKeeper.get()) {             illegalRestores.getAndIncrement().         } else if (successfulRestores.getAndIncrement() > 0) {             // already saw the one allowed successful restore             illegalRestores.getAndIncrement().         }     } }
false;public;1;3;;@Override public void open(MetricConfig metricConfig) { }
false;public;0;3;;@Override public void close() { }
false;public;3;6;;@Override public void notifyOfAddedMetric(Metric metric, String s, MetricGroup metricGroup) {     if (metric instanceof NumberOfFullRestartsGauge) {         numRestarts = (NumberOfFullRestartsGauge) metric.     } }
false;public;3;3;;@Override public void notifyOfRemovedMetric(Metric metric, String s, MetricGroup metricGroup) { }
