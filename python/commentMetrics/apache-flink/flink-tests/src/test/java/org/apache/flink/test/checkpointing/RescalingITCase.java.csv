commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;4;;@Parameterized.Parameters(name = "backend = {0}") public static Object[] data() {     return new Object[] { "filesystem", "rocksdb" }. }
false;public;0;26;;@Before public void setup() throws Exception {     // detect parameter change     if (currentBackend != backend) {         shutDownExistingCluster().         currentBackend = backend.         Configuration config = new Configuration().         final File checkpointDir = temporaryFolder.newFolder().         final File savepointDir = temporaryFolder.newFolder().         config.setString(CheckpointingOptions.STATE_BACKEND, currentBackend).         config.setString(CheckpointingOptions.CHECKPOINTS_DIRECTORY, checkpointDir.toURI().toString()).         config.setString(CheckpointingOptions.SAVEPOINT_DIRECTORY, savepointDir.toURI().toString()).         cluster = new MiniClusterWithClientResource(new MiniClusterResourceConfiguration.Builder().setConfiguration(config).setNumberTaskManagers(numTaskManagers).setNumberSlotsPerTaskManager(numSlots).build()).         cluster.before().     } }
false;public,static;0;7;;@AfterClass public static void shutDownExistingCluster() {     if (cluster != null) {         cluster.after().         cluster = null.     } }
false;public;0;4;;@Test public void testSavepointRescalingInKeyedState() throws Exception {     testSavepointRescalingKeyedState(false, false). }
false;public;0;4;;@Test public void testSavepointRescalingOutKeyedState() throws Exception {     testSavepointRescalingKeyedState(true, false). }
false;public;0;4;;@Test public void testSavepointRescalingInKeyedStateDerivedMaxParallelism() throws Exception {     testSavepointRescalingKeyedState(false, true). }
false;public;0;4;;@Test public void testSavepointRescalingOutKeyedStateDerivedMaxParallelism() throws Exception {     testSavepointRescalingKeyedState(true, true). }
true;public;2;76;/**  * Tests that a job with purely keyed state can be restarted from a savepoint  * with a different parallelism.  */ ;/**  * Tests that a job with purely keyed state can be restarted from a savepoint  * with a different parallelism.  */ public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {     final int numberKeys = 42.     final int numberElements = 1000.     final int numberElements2 = 500.     final int parallelism = scaleOut ? numSlots / 2 : numSlots.     final int parallelism2 = scaleOut ? numSlots : numSlots / 2.     final int maxParallelism = 13.     Duration timeout = Duration.ofMinutes(3).     Deadline deadline = Deadline.now().plus(timeout).     ClusterClient<?> client = cluster.getClusterClient().     try {         JobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100).         final JobID jobID = jobGraph.getJobID().         client.setDetached(true).         client.submitJob(jobGraph, RescalingITCase.class.getClassLoader()).         // wait til the sources have emitted numberElements for each key and completed a checkpoint         SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).         // verify the current state         Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet().         Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>().         for (int key = 0. key < numberKeys. key++) {             int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism).             expectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key)).         }         assertEquals(expectedResult, actualResult).         // clear the CollectionSink set for the restarted job         CollectionSink.clearElementsSet().         CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null).         final String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).         client.cancel(jobID).         while (!getRunningJobs(client).isEmpty()) {             Thread.sleep(50).         }         int restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism.         JobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100).         scaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath)).         client.setDetached(false).         client.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader()).         Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet().         Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>().         for (int key = 0. key < numberKeys. key++) {             int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism).             expectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2))).         }         assertEquals(expectedResult2, actualResult2).     } finally {         // clear the CollectionSink set for the restarted job         CollectionSink.clearElementsSet().     } }
true;public;0;49;/**  * Tests that a job cannot be restarted from a savepoint with a different parallelism if the  * rescaled operator has non-partitioned state.  *  * @throws Exception  */ ;/**  * Tests that a job cannot be restarted from a savepoint with a different parallelism if the  * rescaled operator has non-partitioned state.  *  * @throws Exception  */ @Test public void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {     final int parallelism = numSlots / 2.     final int parallelism2 = numSlots.     final int maxParallelism = 13.     Duration timeout = Duration.ofMinutes(3).     Deadline deadline = Deadline.now().plus(timeout).     ClusterClient<?> client = cluster.getClusterClient().     try {         JobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED).         final JobID jobID = jobGraph.getJobID().         client.setDetached(true).         client.submitJob(jobGraph, RescalingITCase.class.getClassLoader()).         // wait until the operator is started         StateSourceBase.workStartedLatch.await().         CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null).         final String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).         client.cancel(jobID).         while (!getRunningJobs(client).isEmpty()) {             Thread.sleep(50).         }         // job successfully removed         JobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED).         scaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath)).         client.setDetached(false).         client.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader()).     } catch (JobExecutionException exception) {         if (exception.getCause() instanceof IllegalStateException) {         // we expect a IllegalStateException wrapped         // in a JobExecutionException, because the job containing non-partitioned state         // is being rescaled         } else {             throw exception.         }     } }
true;public;0;90;/**  * Tests that a job with non partitioned state can be restarted from a savepoint with a  * different parallelism if the operator with non-partitioned state are not rescaled.  *  * @throws Exception  */ ;/**  * Tests that a job with non partitioned state can be restarted from a savepoint with a  * different parallelism if the operator with non-partitioned state are not rescaled.  *  * @throws Exception  */ @Test public void testSavepointRescalingWithKeyedAndNonPartitionedState() throws Exception {     int numberKeys = 42.     int numberElements = 1000.     int numberElements2 = 500.     int parallelism = numSlots / 2.     int parallelism2 = numSlots.     int maxParallelism = 13.     Duration timeout = Duration.ofMinutes(3).     Deadline deadline = Deadline.now().plus(timeout).     ClusterClient<?> client = cluster.getClusterClient().     try {         JobGraph jobGraph = createJobGraphWithKeyedAndNonPartitionedOperatorState(parallelism, maxParallelism, parallelism, numberKeys, numberElements, false, 100).         final JobID jobID = jobGraph.getJobID().         client.setDetached(true).         client.submitJob(jobGraph, RescalingITCase.class.getClassLoader()).         // wait til the sources have emitted numberElements for each key and completed a checkpoint         SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).         // verify the current state         Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet().         Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>().         for (int key = 0. key < numberKeys. key++) {             int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism).             expectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key)).         }         assertEquals(expectedResult, actualResult).         // clear the CollectionSink set for the restarted job         CollectionSink.clearElementsSet().         CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null).         final String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).         client.cancel(jobID).         while (!getRunningJobs(client).isEmpty()) {             Thread.sleep(50).         }         JobGraph scaledJobGraph = createJobGraphWithKeyedAndNonPartitionedOperatorState(parallelism2, maxParallelism, parallelism, numberKeys, numberElements + numberElements2, true, 100).         scaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath)).         client.setDetached(false).         client.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader()).         Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet().         Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>().         for (int key = 0. key < numberKeys. key++) {             int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism).             expectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2))).         }         assertEquals(expectedResult2, actualResult2).     } finally {         // clear the CollectionSink set for the restarted job         CollectionSink.clearElementsSet().     } }
false;public;0;4;;@Test public void testSavepointRescalingInPartitionedOperatorState() throws Exception {     testSavepointRescalingPartitionedOperatorState(false, OperatorCheckpointMethod.CHECKPOINTED_FUNCTION). }
false;public;0;4;;@Test public void testSavepointRescalingOutPartitionedOperatorState() throws Exception {     testSavepointRescalingPartitionedOperatorState(true, OperatorCheckpointMethod.CHECKPOINTED_FUNCTION). }
false;public;0;4;;@Test public void testSavepointRescalingInBroadcastOperatorState() throws Exception {     testSavepointRescalingPartitionedOperatorState(false, OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST). }
false;public;0;4;;@Test public void testSavepointRescalingOutBroadcastOperatorState() throws Exception {     testSavepointRescalingPartitionedOperatorState(true, OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST). }
false;public;0;4;;@Test public void testSavepointRescalingInPartitionedOperatorStateList() throws Exception {     testSavepointRescalingPartitionedOperatorState(false, OperatorCheckpointMethod.LIST_CHECKPOINTED). }
false;public;0;4;;@Test public void testSavepointRescalingOutPartitionedOperatorStateList() throws Exception {     testSavepointRescalingPartitionedOperatorState(true, OperatorCheckpointMethod.LIST_CHECKPOINTED). }
true;public;2;96;/**  * Tests rescaling of partitioned operator state. More specific, we test the mechanism with {@link ListCheckpointed}  * as it subsumes {@link org.apache.flink.streaming.api.checkpoint.CheckpointedFunction}.  */ ;/**  * Tests rescaling of partitioned operator state. More specific, we test the mechanism with {@link ListCheckpointed}  * as it subsumes {@link org.apache.flink.streaming.api.checkpoint.CheckpointedFunction}.  */ public void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {     final int parallelism = scaleOut ? numSlots : numSlots / 2.     final int parallelism2 = scaleOut ? numSlots / 2 : numSlots.     final int maxParallelism = 13.     Duration timeout = Duration.ofMinutes(3).     Deadline deadline = Deadline.now().plus(timeout).     ClusterClient<?> client = cluster.getClusterClient().     int counterSize = Math.max(parallelism, parallelism2).     if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION || checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {         PartitionedStateSource.checkCorrectSnapshot = new int[counterSize].         PartitionedStateSource.checkCorrectRestore = new int[counterSize].     } else {         PartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize].         PartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize].     }     try {         JobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod).         final JobID jobID = jobGraph.getJobID().         client.setDetached(true).         client.submitJob(jobGraph, RescalingITCase.class.getClassLoader()).         // wait until the operator is started         StateSourceBase.workStartedLatch.await().         CompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(() -> {             try {                 return client.triggerSavepoint(jobID, null).             } catch (FlinkException e) {                 return FutureUtils.completedExceptionally(e).             }         }, (int) deadline.timeLeft().getSeconds() / 10, Time.seconds(10), (throwable) -> true, TestingUtils.defaultScheduledExecutor()).         final String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).         client.cancel(jobID).         while (!getRunningJobs(client).isEmpty()) {             Thread.sleep(50).         }         JobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod).         scaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath)).         client.setDetached(false).         client.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader()).         int sumExp = 0.         int sumAct = 0.         if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {             for (int c : PartitionedStateSource.checkCorrectSnapshot) {                 sumExp += c.             }             for (int c : PartitionedStateSource.checkCorrectRestore) {                 sumAct += c.             }         } else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {             for (int c : PartitionedStateSource.checkCorrectSnapshot) {                 sumExp += c.             }             for (int c : PartitionedStateSource.checkCorrectRestore) {                 sumAct += c.             }             sumExp *= parallelism2.         } else {             for (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {                 sumExp += c.             }             for (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {                 sumAct += c.             }         }         assertEquals(sumExp, sumAct).     } finally {     } }
false;private,static;3;36;;// ------------------------------------------------------------------------------------------------------------------ private static JobGraph createJobGraphWithOperatorState(int parallelism, int maxParallelism, OperatorCheckpointMethod checkpointMethod) {     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(parallelism).     env.getConfig().setMaxParallelism(maxParallelism).     env.enableCheckpointing(Long.MAX_VALUE).     env.setRestartStrategy(RestartStrategies.noRestart()).     StateSourceBase.workStartedLatch = new CountDownLatch(parallelism).     SourceFunction<Integer> src.     switch(checkpointMethod) {         case CHECKPOINTED_FUNCTION:             src = new PartitionedStateSource(false).             break.         case CHECKPOINTED_FUNCTION_BROADCAST:             src = new PartitionedStateSource(true).             break.         case LIST_CHECKPOINTED:             src = new PartitionedStateSourceListCheckpointed().             break.         case NON_PARTITIONED:             src = new NonPartitionedStateSource().             break.         default:             throw new IllegalArgumentException().     }     DataStream<Integer> input = env.addSource(src).     input.addSink(new DiscardingSink<Integer>()).     return env.getStreamGraph().getJobGraph(). }
false;public;1;4;;@Override public Integer getKey(Integer value) throws Exception {     return value. }
false;private,static;6;38;;private static JobGraph createJobGraphWithKeyedState(int parallelism, int maxParallelism, int numberKeys, int numberElements, boolean terminateAfterEmission, int checkpointingInterval) {     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(parallelism).     if (0 < maxParallelism) {         env.getConfig().setMaxParallelism(maxParallelism).     }     env.enableCheckpointing(checkpointingInterval).     env.setRestartStrategy(RestartStrategies.noRestart()).     env.getConfig().setUseSnapshotCompression(true).     DataStream<Integer> input = env.addSource(new SubtaskIndexSource(numberKeys, numberElements, terminateAfterEmission)).keyBy(new KeySelector<Integer, Integer>() {          private static final long serialVersionUID = -7952298871120320940L.          @Override         public Integer getKey(Integer value) throws Exception {             return value.         }     }).     SubtaskIndexFlatMapper.workCompletedLatch = new CountDownLatch(numberKeys).     DataStream<Tuple2<Integer, Integer>> result = input.flatMap(new SubtaskIndexFlatMapper(numberElements)).     result.addSink(new CollectionSink<Tuple2<Integer, Integer>>()).     return env.getStreamGraph().getJobGraph(). }
false;public;1;4;;@Override public Integer getKey(Integer value) throws Exception {     return value. }
false;private,static;7;37;;private static JobGraph createJobGraphWithKeyedAndNonPartitionedOperatorState(int parallelism, int maxParallelism, int fixedParallelism, int numberKeys, int numberElements, boolean terminateAfterEmission, int checkpointingInterval) {     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(parallelism).     env.getConfig().setMaxParallelism(maxParallelism).     env.enableCheckpointing(checkpointingInterval).     env.setRestartStrategy(RestartStrategies.noRestart()).     DataStream<Integer> input = env.addSource(new SubtaskIndexNonPartitionedStateSource(numberKeys, numberElements, terminateAfterEmission)).setParallelism(fixedParallelism).keyBy(new KeySelector<Integer, Integer>() {          private static final long serialVersionUID = -7952298871120320940L.          @Override         public Integer getKey(Integer value) throws Exception {             return value.         }     }).     SubtaskIndexFlatMapper.workCompletedLatch = new CountDownLatch(numberKeys).     DataStream<Tuple2<Integer, Integer>> result = input.flatMap(new SubtaskIndexFlatMapper(numberElements)).     result.addSink(new CollectionSink<Tuple2<Integer, Integer>>()).     return env.getStreamGraph().getJobGraph(). }
false;public;1;27;;@Override public void run(SourceContext<Integer> ctx) throws Exception {     final Object lock = ctx.getCheckpointLock().     final int subtaskIndex = getRuntimeContext().getIndexOfThisSubtask().     while (running) {         if (counter < numberElements) {             synchronized (lock) {                 for (int value = subtaskIndex. value < numberKeys. value += getRuntimeContext().getNumberOfParallelSubtasks()) {                     ctx.collect(value).                 }                 counter++.             }         } else {             if (terminateAfterEmission) {                 running = false.             } else {                 Thread.sleep(100).             }         }     } }
false;public;0;4;;@Override public void cancel() {     running = false. }
false;public;2;4;;@Override public List<Integer> snapshotState(long checkpointId, long timestamp) throws Exception {     return Collections.singletonList(this.counter). }
false;public;1;7;;@Override public void restoreState(List<Integer> state) throws Exception {     if (state.isEmpty() || state.size() > 1) {         throw new RuntimeException("Test failed due to unexpected recovered state size " + state.size()).     }     this.counter = state.get(0). }
false;public;2;14;;@Override public void flatMap(Integer value, Collector<Tuple2<Integer, Integer>> out) throws Exception {     int count = counter.value() + 1.     counter.update(count).     int s = sum.value() + value.     sum.update(s).     if (count % numberElements == 0) {         out.collect(Tuple2.of(getRuntimeContext().getIndexOfThisSubtask(), s)).         workCompletedLatch.countDown().     } }
false;public;1;4;;@Override public void snapshotState(FunctionSnapshotContext context) throws Exception { // all managed, nothing to do. }
false;public;1;5;;@Override public void initializeState(FunctionInitializationContext context) throws Exception {     counter = context.getKeyedStateStore().getState(new ValueStateDescriptor<>("counter", Integer.class, 0)).     sum = context.getKeyedStateStore().getState(new ValueStateDescriptor<>("sum", Integer.class, 0)). }
false;public,static;0;3;;public static <IN> Set<IN> getElementsSet() {     return (Set<IN>) elements. }
false;public,static;0;3;;public static void clearElementsSet() {     elements.clear(). }
false;public;1;4;;@Override public void invoke(IN value) throws Exception {     elements.add(value). }
false;public;1;21;;@Override public void run(SourceContext<Integer> ctx) throws Exception {     final Object lock = ctx.getCheckpointLock().     while (running) {         synchronized (lock) {             ++counter.             ctx.collect(1).         }         Thread.sleep(2).         if (counter == 10) {             workStartedLatch.countDown().         }         if (counter >= 500) {             break.         }     } }
false;public;0;4;;@Override public void cancel() {     running = false. }
false;public;2;4;;@Override public List<Integer> snapshotState(long checkpointId, long timestamp) throws Exception {     return Collections.singletonList(this.counter). }
false;public;1;6;;@Override public void restoreState(List<Integer> state) throws Exception {     if (!state.isEmpty()) {         this.counter = state.get(0).     } }
false;public;2;19;;@Override public List<Integer> snapshotState(long checkpointId, long timestamp) throws Exception {     checkCorrectSnapshot[getRuntimeContext().getIndexOfThisSubtask()] = counter.     int div = counter / NUM_PARTITIONS.     int mod = counter % NUM_PARTITIONS.     List<Integer> split = new ArrayList<>().     for (int i = 0. i < NUM_PARTITIONS. ++i) {         int partitionValue = div.         if (mod > 0) {             --mod.             ++partitionValue.         }         split.add(partitionValue).     }     return split. }
false;public;1;7;;@Override public void restoreState(List<Integer> state) throws Exception {     for (Integer v : state) {         counter += v.     }     checkCorrectRestore[getRuntimeContext().getIndexOfThisSubtask()] = counter. }
false;public;1;19;;@Override public void snapshotState(FunctionSnapshotContext context) throws Exception {     counterPartitions.clear().     checkCorrectSnapshot[getRuntimeContext().getIndexOfThisSubtask()] = counter.     int div = counter / NUM_PARTITIONS.     int mod = counter % NUM_PARTITIONS.     for (int i = 0. i < NUM_PARTITIONS. ++i) {         int partitionValue = div.         if (mod > 0) {             --mod.             ++partitionValue.         }         counterPartitions.add(partitionValue).     } }
false;public;1;20;;@Override public void initializeState(FunctionInitializationContext context) throws Exception {     if (broadcast) {         this.counterPartitions = context.getOperatorStateStore().getUnionListState(new ListStateDescriptor<>("counter_partitions", IntSerializer.INSTANCE)).     } else {         this.counterPartitions = context.getOperatorStateStore().getListState(new ListStateDescriptor<>("counter_partitions", IntSerializer.INSTANCE)).     }     if (context.isRestored()) {         for (int v : counterPartitions.get()) {             counter += v.         }         checkCorrectRestore[getRuntimeContext().getIndexOfThisSubtask()] = counter.     } }
false;private,static;1;7;;private static List<JobID> getRunningJobs(ClusterClient<?> client) throws Exception {     Collection<JobStatusMessage> statusMessages = client.listJobs().get().     return statusMessages.stream().filter(status -> !status.getJobState().isGloballyTerminalState()).map(JobStatusMessage::getJobId).collect(Collectors.toList()). }
