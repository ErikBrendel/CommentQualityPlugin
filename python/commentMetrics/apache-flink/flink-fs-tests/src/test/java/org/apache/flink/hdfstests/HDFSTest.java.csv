commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;4;;@BeforeClass public static void verifyOS() {     Assume.assumeTrue("HDFS cluster cannot be started on Windows without extensions.", !OperatingSystem.isWindows()). }
false;public;0;26;;@Before public void createHDFS() {     try {         Configuration hdConf = new Configuration().         File baseDir = new File("./target/hdfs/hdfsTest").getAbsoluteFile().         FileUtil.fullyDelete(baseDir).         hdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath()).         MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf).         hdfsCluster = builder.build().         hdfsURI = "hdfs://" + hdfsCluster.getURI().getHost() + ":" + hdfsCluster.getNameNodePort() + "/".         hdPath = new org.apache.hadoop.fs.Path("/test").         hdfs = hdPath.getFileSystem(hdConf).         FSDataOutputStream stream = hdfs.create(hdPath).         for (int i = 0. i < 10. i++) {             stream.write("Hello HDFS\n".getBytes(ConfigConstants.DEFAULT_CHARSET)).         }         stream.close().     } catch (Throwable e) {         e.printStackTrace().         Assert.fail("Test failed " + e.getMessage()).     } }
false;public;0;10;;@After public void destroyHDFS() {     try {         hdfs.delete(hdPath, false).         hdfsCluster.shutdown().     } catch (IOException e) {         throw new RuntimeException(e).     } }
false;public;0;40;;@Test public void testHDFS() {     Path file = new Path(hdfsURI + hdPath).     org.apache.hadoop.fs.Path result = new org.apache.hadoop.fs.Path(hdfsURI + "/result").     try {         FileSystem fs = file.getFileSystem().         assertTrue("Must be HadoopFileSystem", fs instanceof HadoopFileSystem).         DopOneTestEnvironment.setAsContext().         try {             WordCount.main(new String[] { "--input", file.toString(), "--output", result.toString() }).         } catch (Throwable t) {             t.printStackTrace().             Assert.fail("Test failed with " + t.getMessage()).         } finally {             DopOneTestEnvironment.unsetAsContext().         }         assertTrue("No result file present", hdfs.exists(result)).         // validate output:         org.apache.hadoop.fs.FSDataInputStream inStream = hdfs.open(result).         StringWriter writer = new StringWriter().         IOUtils.copy(inStream, writer).         String resultString = writer.toString().         Assert.assertEquals("hdfs 10\n" + "hello 10\n", resultString).         inStream.close().     } catch (IOException e) {         e.printStackTrace().         Assert.fail("Error in test: " + e.getMessage()).     } }
false;public;0;32;;@Test public void testChangingFileNames() {     org.apache.hadoop.fs.Path hdfsPath = new org.apache.hadoop.fs.Path(hdfsURI + "/hdfsTest").     Path path = new Path(hdfsPath.toString()).     String type = "one".     TextOutputFormat<String> outputFormat = new TextOutputFormat<>(path).     outputFormat.setWriteMode(FileSystem.WriteMode.NO_OVERWRITE).     outputFormat.setOutputDirectoryMode(FileOutputFormat.OutputDirectoryMode.ALWAYS).     try {         outputFormat.open(0, 2).         outputFormat.writeRecord(type).         outputFormat.close().         outputFormat.open(1, 2).         outputFormat.writeRecord(type).         outputFormat.close().         assertTrue("No result file present", hdfs.exists(hdfsPath)).         FileStatus[] files = hdfs.listStatus(hdfsPath).         Assert.assertEquals(2, files.length).         for (FileStatus file : files) {             assertTrue("1".equals(file.getPath().getName()) || "2".equals(file.getPath().getName())).         }     } catch (IOException e) {         e.printStackTrace().         Assert.fail(e.getMessage()).     } }
true;public;0;38;/**  * Test that {@link FileUtils#deletePathIfEmpty(FileSystem, Path)} deletes the path if it is  * empty. A path can only be empty if it is a directory which does not contain any  * files/directories.  */ ;/**  * Test that {@link FileUtils#deletePathIfEmpty(FileSystem, Path)} deletes the path if it is  * empty. A path can only be empty if it is a directory which does not contain any  * files/directories.  */ @Test public void testDeletePathIfEmpty() throws IOException {     final Path basePath = new Path(hdfsURI).     final Path directory = new Path(basePath, UUID.randomUUID().toString()).     final Path directoryFile = new Path(directory, UUID.randomUUID().toString()).     final Path singleFile = new Path(basePath, UUID.randomUUID().toString()).     FileSystem fs = basePath.getFileSystem().     fs.mkdirs(directory).     byte[] data = "HDFSTest#testDeletePathIfEmpty".getBytes(ConfigConstants.DEFAULT_CHARSET).     for (Path file : Arrays.asList(singleFile, directoryFile)) {         org.apache.flink.core.fs.FSDataOutputStream outputStream = fs.create(file, FileSystem.WriteMode.OVERWRITE).         outputStream.write(data).         outputStream.close().     }     // verify that the files have been created     assertTrue(fs.exists(singleFile)).     assertTrue(fs.exists(directoryFile)).     // delete the single file     assertFalse(FileUtils.deletePathIfEmpty(fs, singleFile)).     assertTrue(fs.exists(singleFile)).     // try to delete the non-empty directory     assertFalse(FileUtils.deletePathIfEmpty(fs, directory)).     assertTrue(fs.exists(directory)).     // delete the file contained in the directory     assertTrue(fs.delete(directoryFile, false)).     // now the deletion should work     assertTrue(FileUtils.deletePathIfEmpty(fs, directory)).     assertFalse(fs.exists(directory)). }
true;public;0;17;/**  * Tests that with {@link HighAvailabilityMode#ZOOKEEPER} distributed JARs are recoverable from any  * participating BlobServer when talking to the {@link org.apache.flink.runtime.blob.BlobServer} directly.  */ ;/**  * Tests that with {@link HighAvailabilityMode#ZOOKEEPER} distributed JARs are recoverable from any  * participating BlobServer when talking to the {@link org.apache.flink.runtime.blob.BlobServer} directly.  */ @Test public void testBlobServerRecovery() throws Exception {     org.apache.flink.configuration.Configuration config = new org.apache.flink.configuration.Configuration().     config.setString(HighAvailabilityOptions.HA_MODE, "ZOOKEEPER").     config.setString(BlobServerOptions.STORAGE_DIRECTORY, temporaryFolder.newFolder().getAbsolutePath()).     config.setString(HighAvailabilityOptions.HA_STORAGE_PATH, hdfsURI).     BlobStoreService blobStoreService = BlobUtils.createBlobStoreFromConfig(config).     try {         BlobServerRecoveryTest.testBlobServerRecovery(config, blobStoreService).     } finally {         blobStoreService.closeAndCleanupAllData().     } }
true;public;0;17;/**  * Tests that with {@link HighAvailabilityMode#ZOOKEEPER} distributed corrupted JARs are  * recognised during the download via a {@link org.apache.flink.runtime.blob.BlobServer}.  */ ;/**  * Tests that with {@link HighAvailabilityMode#ZOOKEEPER} distributed corrupted JARs are  * recognised during the download via a {@link org.apache.flink.runtime.blob.BlobServer}.  */ @Test public void testBlobServerCorruptedFile() throws Exception {     org.apache.flink.configuration.Configuration config = new org.apache.flink.configuration.Configuration().     config.setString(HighAvailabilityOptions.HA_MODE, "ZOOKEEPER").     config.setString(BlobServerOptions.STORAGE_DIRECTORY, temporaryFolder.newFolder().getAbsolutePath()).     config.setString(HighAvailabilityOptions.HA_STORAGE_PATH, hdfsURI).     BlobStoreService blobStoreService = BlobUtils.createBlobStoreFromConfig(config).     try {         BlobServerCorruptionTest.testGetFailsFromCorruptFile(config, blobStoreService, exception).     } finally {         blobStoreService.closeAndCleanupAllData().     } }
true;public;0;17;/**  * Tests that with {@link HighAvailabilityMode#ZOOKEEPER} distributed JARs are recoverable from any  * participating BlobServer when uploaded via a BLOB cache.  */ ;/**  * Tests that with {@link HighAvailabilityMode#ZOOKEEPER} distributed JARs are recoverable from any  * participating BlobServer when uploaded via a BLOB cache.  */ @Test public void testBlobCacheRecovery() throws Exception {     org.apache.flink.configuration.Configuration config = new org.apache.flink.configuration.Configuration().     config.setString(HighAvailabilityOptions.HA_MODE, "ZOOKEEPER").     config.setString(BlobServerOptions.STORAGE_DIRECTORY, temporaryFolder.newFolder().getAbsolutePath()).     config.setString(HighAvailabilityOptions.HA_STORAGE_PATH, hdfsURI).     BlobStoreService blobStoreService = BlobUtils.createBlobStoreFromConfig(config).     try {         BlobCacheRecoveryTest.testBlobCacheRecovery(config, blobStoreService).     } finally {         blobStoreService.closeAndCleanupAllData().     } }
true;public;0;18;/**  * Tests that with {@link HighAvailabilityMode#ZOOKEEPER} distributed corrupted JARs are  * recognised during the download via a BLOB cache.  */ ;/**  * Tests that with {@link HighAvailabilityMode#ZOOKEEPER} distributed corrupted JARs are  * recognised during the download via a BLOB cache.  */ @Test public void testBlobCacheCorruptedFile() throws Exception {     org.apache.flink.configuration.Configuration config = new org.apache.flink.configuration.Configuration().     config.setString(HighAvailabilityOptions.HA_MODE, "ZOOKEEPER").     config.setString(BlobServerOptions.STORAGE_DIRECTORY, temporaryFolder.newFolder().getAbsolutePath()).     config.setString(HighAvailabilityOptions.HA_STORAGE_PATH, hdfsURI).     BlobStoreService blobStoreService = BlobUtils.createBlobStoreFromConfig(config).     try {         BlobCacheCorruptionTest.testGetFailsFromCorruptFile(new JobID(), config, blobStoreService, exception).     } finally {         blobStoreService.closeAndCleanupAllData().     } }
false;public;0;4;;@Override public ExecutionEnvironment createExecutionEnvironment() {     return le. }
false;public,static;0;12;;public static void setAsContext() {     final LocalEnvironment le = new LocalEnvironment().     le.setParallelism(1).     initializeContextEnvironment(new ExecutionEnvironmentFactory() {          @Override         public ExecutionEnvironment createExecutionEnvironment() {             return le.         }     }). }
false;public,static;0;3;;public static void unsetAsContext() {     resetContextEnvironment(). }
