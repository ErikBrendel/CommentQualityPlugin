commented;modifiers;parameterAmount;loc;comment;code
false;public;0;21;;// PREPARING FOR THE TESTS @Before public void createHDFS() {     try {         baseDir = new File("./target/hdfs/hdfsTesting").getAbsoluteFile().         FileUtil.fullyDelete(baseDir).         org.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration().         hdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath()).         // this is the minimum we can set.         hdConf.set("dfs.block.size", String.valueOf(1048576)).         MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf).         hdfsCluster = builder.build().         hdfsURI = "hdfs://" + hdfsCluster.getURI().getHost() + ":" + hdfsCluster.getNameNodePort() + "/".         hdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf).     } catch (Throwable e) {         e.printStackTrace().         Assert.fail("Test failed " + e.getMessage()).     } }
false;public;0;9;;@After public void destroyHDFS() {     try {         FileUtil.fullyDelete(baseDir).         hdfsCluster.shutdown().     } catch (Throwable t) {         throw new RuntimeException(t).     } }
false;public;0;19;;@Override public void run() {     try {         env.execute("ContinuousFileProcessingITCase Job.").     } catch (Exception e) {         Throwable th = e.         for (int depth = 0. depth < 20. depth++) {             if (th instanceof SuccessException) {                 return.             } else if (th.getCause() != null) {                 th = th.getCause().             } else {                 break.             }         }         e.printStackTrace().         Assert.fail(e.getMessage()).     } }
false;public;0;99;;// END OF PREPARATIONS @Test public void testProgram() throws Exception {     /* 		* This test checks the interplay between the monitor and the reader 		* and also the failExternally() functionality. To test the latter we 		* set the parallelism to 1 so that we have the chaining between the sink, 		* which throws the SuccessException to signal the end of the test, and the 		* reader. 		* */     TextInputFormat format = new TextInputFormat(new Path(hdfsURI)).     format.setFilePath(hdfsURI).     format.setFilesFilter(FilePathFilter.createDefaultFilter()).     // create the stream execution environment with a parallelism > 1 to test     final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(PARALLELISM).     ContinuousFileMonitoringFunction<String> monitoringFunction = new ContinuousFileMonitoringFunction<>(format, FileProcessingMode.PROCESS_CONTINUOUSLY, env.getParallelism(), INTERVAL).     // the monitor has always DOP 1     DataStream<TimestampedFileInputSplit> splits = env.addSource(monitoringFunction).     Assert.assertEquals(1, splits.getParallelism()).     ContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format).     TypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format).     // the readers can be multiple     DataStream<String> content = splits.transform("FileSplitReader", typeInfo, reader).     Assert.assertEquals(PARALLELISM, content.getParallelism()).     // finally for the sink we set the parallelism to 1 so that we can verify the output     TestingSinkFunction sink = new TestingSinkFunction().     content.addSink(sink).setParallelism(1).     Thread job = new Thread() {          @Override         public void run() {             try {                 env.execute("ContinuousFileProcessingITCase Job.").             } catch (Exception e) {                 Throwable th = e.                 for (int depth = 0. depth < 20. depth++) {                     if (th instanceof SuccessException) {                         return.                     } else if (th.getCause() != null) {                         th = th.getCause().                     } else {                         break.                     }                 }                 e.printStackTrace().                 Assert.fail(e.getMessage()).             }         }     }.     job.start().     // The modification time of the last created file.     long lastCreatedModTime = Long.MIN_VALUE.     // create the files to be read     for (int i = 0. i < NO_OF_FILES. i++) {         Tuple2<org.apache.hadoop.fs.Path, String> tmpFile.         long modTime.         do {             // give it some time so that the files have             // different modification timestamps.             Thread.sleep(50).             tmpFile = fillWithData(hdfsURI, "file", i, "This is test line.").             modTime = hdfs.getFileStatus(tmpFile.f0).getModificationTime().             if (modTime <= lastCreatedModTime) {                 // delete the last created file to recreate it with a different timestamp                 hdfs.delete(tmpFile.f0, false).             }         } while (modTime <= lastCreatedModTime).         lastCreatedModTime = modTime.         // put the contents in the expected results list before the reader picks them         // this is to guarantee that they are in before the reader finishes (avoid race conditions)         expectedContents.put(i, tmpFile.f1).         org.apache.hadoop.fs.Path file = new org.apache.hadoop.fs.Path(hdfsURI + "/file" + i).         hdfs.rename(tmpFile.f0, file).         Assert.assertTrue(hdfs.exists(file)).     }     // wait for the job to finish.     job.join(). }
false;public;2;4;;@Override public int compare(String o1, String o2) {     return getLineNo(o1) - getLineNo(o2). }
false;public;1;12;;@Override public void open(Configuration parameters) throws Exception {     // this sink can only work with DOP 1     assertEquals(1, getRuntimeContext().getNumberOfParallelSubtasks()).     comparator = new Comparator<String>() {          @Override         public int compare(String o1, String o2) {             return getLineNo(o1) - getLineNo(o2).         }     }. }
false;public;1;20;;@Override public void invoke(String value) throws Exception {     int fileIdx = getFileIdx(value).     Set<String> content = actualContent.get(fileIdx).     if (content == null) {         content = new HashSet<>().         actualContent.put(fileIdx, content).     }     if (!content.add(value + "\n")) {         Assert.fail("Duplicate line: " + value).         System.exit(0).     }     elementCounter++.     if (elementCounter == NO_OF_FILES * LINES_PER_FILE) {         throw new SuccessException().     } }
false;public;0;18;;@Override public void close() {     // check if the data that we collected are the ones they are supposed to be.     Assert.assertEquals(expectedContents.size(), actualContent.size()).     for (Integer fileIdx : expectedContents.keySet()) {         Assert.assertTrue(actualContent.keySet().contains(fileIdx)).         List<String> cntnt = new ArrayList<>(actualContent.get(fileIdx)).         Collections.sort(cntnt, comparator).         StringBuilder cntntStr = new StringBuilder().         for (String line : cntnt) {             cntntStr.append(line).         }         Assert.assertEquals(expectedContents.get(fileIdx), cntntStr.toString()).     }     expectedContents.clear(). }
false;private;1;4;;private int getLineNo(String line) {     String[] tkns = line.split("\\s").     return Integer.parseInt(tkns[tkns.length - 1]). }
false;private;1;4;;private int getFileIdx(String line) {     String[] tkns = line.split(":").     return Integer.parseInt(tkns[0]). }
true;private;4;18;/**  * Create a file and fill it with content.  */ ;/**  * Create a file and fill it with content.  */ private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {     assert (hdfs != null).     org.apache.hadoop.fs.Path tmp = new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx).     FSDataOutputStream stream = hdfs.create(tmp).     StringBuilder str = new StringBuilder().     for (int i = 0. i < LINES_PER_FILE. i++) {         String line = fileIdx + ": " + sampleLine + " " + i + "\n".         str.append(line).         stream.write(line.getBytes(ConfigConstants.DEFAULT_CHARSET)).     }     stream.close().     return new Tuple2<>(tmp, str.toString()). }
