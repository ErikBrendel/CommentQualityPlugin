commented;modifiers;parameterAmount;loc;comment;code
false;public;3;48;;@Override public List<List<OperatorStateHandle>> repartitionState(List<List<OperatorStateHandle>> previousParallelSubtaskStates, int oldParallelism, int newParallelism) {     Preconditions.checkNotNull(previousParallelSubtaskStates).     Preconditions.checkArgument(newParallelism > 0).     Preconditions.checkArgument(previousParallelSubtaskStates.size() == oldParallelism, "This method still depends on the order of the new and old operators").     // Assemble result from all merge maps     List<List<OperatorStateHandle>> result = new ArrayList<>(newParallelism).     List<Map<StreamStateHandle, OperatorStateHandle>> mergeMapList.     // We only round-robin repartition UNION state if new parallelism equals to the old one.     if (newParallelism == oldParallelism) {         Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> unionStates = collectUnionStates(previousParallelSubtaskStates).         if (unionStates.isEmpty()) {             return previousParallelSubtaskStates.         }         // Initialize         mergeMapList = initMergeMapList(previousParallelSubtaskStates).         repartitionUnionState(unionStates, mergeMapList).     } else {         // Reorganize: group by (State Name -> StreamStateHandle + Offsets)         GroupByStateNameResults nameToStateByMode = groupByStateMode(previousParallelSubtaskStates).         if (OPTIMIZE_MEMORY_USE) {             // free for GC at to cost that old handles are no longer available             previousParallelSubtaskStates.clear().         }         // Do the actual repartitioning for all named states         mergeMapList = repartition(nameToStateByMode, newParallelism).     }     for (int i = 0. i < mergeMapList.size(). ++i) {         result.add(i, new ArrayList<>(mergeMapList.get(i).values())).     }     return result. }
true;private;1;13;/**  * Init the the list of StreamStateHandle -> OperatorStateHandle map with given parallelSubtaskStates when parallelism not changed.  */ ;/**  * Init the the list of StreamStateHandle -> OperatorStateHandle map with given parallelSubtaskStates when parallelism not changed.  */ private List<Map<StreamStateHandle, OperatorStateHandle>> initMergeMapList(List<List<OperatorStateHandle>> parallelSubtaskStates) {     int parallelism = parallelSubtaskStates.size().     final List<Map<StreamStateHandle, OperatorStateHandle>> mergeMapList = new ArrayList<>(parallelism).     for (List<OperatorStateHandle> previousParallelSubtaskState : parallelSubtaskStates) {         mergeMapList.add(previousParallelSubtaskState.stream().collect(Collectors.toMap(OperatorStateHandle::getDelegateStateHandle, Function.identity()))).     }     return mergeMapList. }
true;private;1;28;/**  * Collect union states from given parallelSubtaskStates.  */ ;/**  * Collect union states from given parallelSubtaskStates.  */ private Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> collectUnionStates(List<List<OperatorStateHandle>> parallelSubtaskStates) {     Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> unionStates = new HashMap<>(parallelSubtaskStates.size()).     for (List<OperatorStateHandle> subTaskState : parallelSubtaskStates) {         for (OperatorStateHandle operatorStateHandle : subTaskState) {             if (operatorStateHandle == null) {                 continue.             }             final Set<Map.Entry<String, OperatorStateHandle.StateMetaInfo>> partitionOffsetEntries = operatorStateHandle.getStateNameToPartitionOffsets().entrySet().             partitionOffsetEntries.stream().filter(entry -> entry.getValue().getDistributionMode().equals(OperatorStateHandle.Mode.UNION)).forEach(entry -> {                 List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>> stateLocations = unionStates.computeIfAbsent(entry.getKey(), k -> new ArrayList<>(parallelSubtaskStates.size() * partitionOffsetEntries.size())).                 stateLocations.add(Tuple2.of(operatorStateHandle.getDelegateStateHandle(), entry.getValue())).             }).         }     }     return unionStates. }
true;private;1;43;/**  * Group by the different named states.  */ ;/**  * Group by the different named states.  */ @SuppressWarnings("unchecked, rawtype") private GroupByStateNameResults groupByStateMode(List<List<OperatorStateHandle>> previousParallelSubtaskStates) {     // Reorganize: group by (State Name -> StreamStateHandle + StateMetaInfo)     EnumMap<OperatorStateHandle.Mode, Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>>> nameToStateByMode = new EnumMap<>(OperatorStateHandle.Mode.class).     for (OperatorStateHandle.Mode mode : OperatorStateHandle.Mode.values()) {         nameToStateByMode.put(mode, new HashMap<>()).     }     for (List<OperatorStateHandle> previousParallelSubtaskState : previousParallelSubtaskStates) {         for (OperatorStateHandle operatorStateHandle : previousParallelSubtaskState) {             if (operatorStateHandle == null) {                 continue.             }             final Set<Map.Entry<String, OperatorStateHandle.StateMetaInfo>> partitionOffsetEntries = operatorStateHandle.getStateNameToPartitionOffsets().entrySet().             for (Map.Entry<String, OperatorStateHandle.StateMetaInfo> e : partitionOffsetEntries) {                 OperatorStateHandle.StateMetaInfo metaInfo = e.getValue().                 Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> nameToState = nameToStateByMode.get(metaInfo.getDistributionMode()).                 List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>> stateLocations = nameToState.computeIfAbsent(e.getKey(), k -> new ArrayList<>(previousParallelSubtaskStates.size() * partitionOffsetEntries.size())).                 stateLocations.add(Tuple2.of(operatorStateHandle.getDelegateStateHandle(), e.getValue())).             }         }     }     return new GroupByStateNameResults(nameToStateByMode). }
true;private;2;32;/**  * Repartition all named states.  */ ;/**  * Repartition all named states.  */ private List<Map<StreamStateHandle, OperatorStateHandle>> repartition(GroupByStateNameResults nameToStateByMode, int newParallelism) {     // We will use this to merge w.r.t. StreamStateHandles for each parallel subtask inside the maps     List<Map<StreamStateHandle, OperatorStateHandle>> mergeMapList = new ArrayList<>(newParallelism).     // Initialize     for (int i = 0. i < newParallelism. ++i) {         mergeMapList.add(new HashMap<>()).     }     // Start with the state handles we distribute round robin by splitting by offsets     Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> nameToDistributeState = nameToStateByMode.getByMode(OperatorStateHandle.Mode.SPLIT_DISTRIBUTE).     repartitionSplitState(nameToDistributeState, newParallelism, mergeMapList).     // Now we also add the state handles marked for union to all parallel instances     Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> nameToUnionState = nameToStateByMode.getByMode(OperatorStateHandle.Mode.UNION).     repartitionUnionState(nameToUnionState, mergeMapList).     // Now we also add the state handles marked for uniform broadcast to all parallel instances     Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> nameToBroadcastState = nameToStateByMode.getByMode(OperatorStateHandle.Mode.BROADCAST).     repartitionBroadcastState(nameToBroadcastState, mergeMapList).     return mergeMapList. }
true;private;3;88;/**  * Repartition SPLIT_DISTRIBUTE state.  */ ;/**  * Repartition SPLIT_DISTRIBUTE state.  */ private void repartitionSplitState(Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> nameToDistributeState, int newParallelism, List<Map<StreamStateHandle, OperatorStateHandle>> mergeMapList) {     int startParallelOp = 0.     // Iterate all named states and repartition one named state at a time per iteration     for (Map.Entry<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> e : nameToDistributeState.entrySet()) {         List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>> current = e.getValue().         // Determine actual number of partitions for this named state         int totalPartitions = 0.         for (Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo> offsets : current) {             totalPartitions += offsets.f1.getOffsets().length.         }         // Repartition the state across the parallel operator instances         int lstIdx = 0.         int offsetIdx = 0.         int baseFraction = totalPartitions / newParallelism.         int remainder = totalPartitions % newParallelism.         int newStartParallelOp = startParallelOp.         for (int i = 0. i < newParallelism. ++i) {             // Preparation: calculate the actual index considering wrap around             int parallelOpIdx = (i + startParallelOp) % newParallelism.             // Now calculate the number of partitions we will assign to the parallel instance in this round ...             int numberOfPartitionsToAssign = baseFraction.             // ... and distribute odd partitions while we still have some, one at a time             if (remainder > 0) {                 ++numberOfPartitionsToAssign.                 --remainder.             } else if (remainder == 0) {                 // We are out of odd partitions now and begin our next redistribution round with the current                 // parallel operator to ensure fair load balance                 newStartParallelOp = parallelOpIdx.                 --remainder.             }             while (numberOfPartitionsToAssign > 0) {                 Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo> handleWithOffsets = current.get(lstIdx).                 long[] offsets = handleWithOffsets.f1.getOffsets().                 int remaining = offsets.length - offsetIdx.                 // Repartition offsets                 long[] offs.                 if (remaining > numberOfPartitionsToAssign) {                     offs = Arrays.copyOfRange(offsets, offsetIdx, offsetIdx + numberOfPartitionsToAssign).                     offsetIdx += numberOfPartitionsToAssign.                 } else {                     if (OPTIMIZE_MEMORY_USE) {                         // GC                         handleWithOffsets.f1 = null.                     }                     offs = Arrays.copyOfRange(offsets, offsetIdx, offsets.length).                     offsetIdx = 0.                     ++lstIdx.                 }                 numberOfPartitionsToAssign -= remaining.                 // As a last step we merge partitions that use the same StreamStateHandle in a single                 // OperatorStateHandle                 Map<StreamStateHandle, OperatorStateHandle> mergeMap = mergeMapList.get(parallelOpIdx).                 OperatorStateHandle operatorStateHandle = mergeMap.get(handleWithOffsets.f0).                 if (operatorStateHandle == null) {                     operatorStateHandle = new OperatorStreamStateHandle(new HashMap<>(nameToDistributeState.size()), handleWithOffsets.f0).                     mergeMap.put(handleWithOffsets.f0, operatorStateHandle).                 }                 operatorStateHandle.getStateNameToPartitionOffsets().put(e.getKey(), new OperatorStateHandle.StateMetaInfo(offs, OperatorStateHandle.Mode.SPLIT_DISTRIBUTE)).             }         }         startParallelOp = newStartParallelOp.         e.setValue(null).     } }
true;private;2;21;/**  * Repartition UNION state.  */ ;/**  * Repartition UNION state.  */ private void repartitionUnionState(Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> unionState, List<Map<StreamStateHandle, OperatorStateHandle>> mergeMapList) {     for (Map<StreamStateHandle, OperatorStateHandle> mergeMap : mergeMapList) {         for (Map.Entry<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> e : unionState.entrySet()) {             for (Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo> handleWithMetaInfo : e.getValue()) {                 OperatorStateHandle operatorStateHandle = mergeMap.get(handleWithMetaInfo.f0).                 if (operatorStateHandle == null) {                     operatorStateHandle = new OperatorStreamStateHandle(new HashMap<>(unionState.size()), handleWithMetaInfo.f0).                     mergeMap.put(handleWithMetaInfo.f0, operatorStateHandle).                 }                 operatorStateHandle.getStateNameToPartitionOffsets().put(e.getKey(), handleWithMetaInfo.f1).             }         }     } }
true;private;2;29;/**  * Repartition BROADCAST state.  */ ;/**  * Repartition BROADCAST state.  */ private void repartitionBroadcastState(Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> broadcastState, List<Map<StreamStateHandle, OperatorStateHandle>> mergeMapList) {     int newParallelism = mergeMapList.size().     for (int i = 0. i < newParallelism. ++i) {         final Map<StreamStateHandle, OperatorStateHandle> mergeMap = mergeMapList.get(i).         // for each name, pick the i-th entry         for (Map.Entry<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> e : broadcastState.entrySet()) {             int previousParallelism = e.getValue().size().             Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo> handleWithMetaInfo = e.getValue().get(i % previousParallelism).             OperatorStateHandle operatorStateHandle = mergeMap.get(handleWithMetaInfo.f0).             if (operatorStateHandle == null) {                 operatorStateHandle = new OperatorStreamStateHandle(new HashMap<>(broadcastState.size()), handleWithMetaInfo.f0).                 mergeMap.put(handleWithMetaInfo.f0, operatorStateHandle).             }             operatorStateHandle.getStateNameToPartitionOffsets().put(e.getKey(), handleWithMetaInfo.f1).         }     } }
false;public;1;4;;public Map<String, List<Tuple2<StreamStateHandle, OperatorStateHandle.StateMetaInfo>>> getByMode(OperatorStateHandle.Mode mode) {     return byMode.get(mode). }
