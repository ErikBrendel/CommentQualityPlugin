commented;modifiers;parameterAmount;loc;comment;code
true;public;0;3;/**  * Gets the total capacity of this hash table, in bytes.  *  * @return The hash table's total capacity.  */ ;/**  * Gets the total capacity of this hash table, in bytes.  *  * @return The hash table's total capacity.  */ public long getCapacity() {     return numAllMemorySegments * (long) segmentSize. }
true;public;0;3;/**  * Gets the number of bytes currently occupied in this hash table.  *  * @return The number of bytes occupied.  */ ;/**  * Gets the number of bytes currently occupied in this hash table.  *  * @return The number of bytes occupied.  */ public long getOccupancy() {     return numAllMemorySegments * segmentSize - freeMemorySegments.size() * segmentSize. }
false;private;1;14;;private void open(int numBucketSegments) {     synchronized (stateLock) {         if (!closed) {             throw new IllegalStateException("currently not closed.").         }         closed = false.     }     allocateBucketSegments(numBucketSegments).     stagingSegments.add(forcedAllocateSegment()).     reuse = buildSideSerializer.createInstance(). }
true;public;0;4;/**  * Initialize the hash table  */ ;/**  * Initialize the hash table  */ @Override public void open() {     open(calcInitialNumBucketSegments()). }
false;public;0;29;;@Override public void close() {     // make sure that we close only once     synchronized (stateLock) {         if (closed) {             // We have to do this here, because the ctor already allocates a segment to the record area and             // the staging area, even before we are opened. So we might have segments to free, even if we             // are closed.             recordArea.giveBackSegments().             freeMemorySegments.addAll(stagingSegments).             stagingSegments.clear().             return.         }         closed = true.     }     LOG.debug("Closing InPlaceMutableHashTable and releasing resources.").     releaseBucketSegments().     recordArea.giveBackSegments().     freeMemorySegments.addAll(stagingSegments).     stagingSegments.clear().     numElements = 0.     holes = 0. }
false;public;0;5;;@Override public void abort() {     LOG.debug("Aborting InPlaceMutableHashTable.").     close(). }
false;public;0;8;;@Override public List<MemorySegment> getFreeMemory() {     if (!this.closed) {         throw new IllegalStateException("Cannot return memory while InPlaceMutableHashTable is open.").     }     return freeMemorySegments. }
false;private;0;28;;private int calcInitialNumBucketSegments() {     int recordLength = buildSideSerializer.getLength().     // fraction of memory to use for the buckets     double fraction.     if (recordLength == -1) {         // We don't know the record length, so we start with a small number of buckets, and do resizes if         // necessary.         // It seems that resizing is quite efficient, so we can err here on the too few bucket segments side.         // Even with small records, we lose only ~15% speed.         fraction = 0.1.     } else {         // We know the record length, so we can find a good value for the number of buckets right away, and         // won't need any resizes later. (enableResize is false in this case, so no resizing will happen.)         // Reasoning behind the formula:         // We are aiming for one bucket per record, and one bucket contains one 8 byte pointer. The total         // memory overhead of an element will be approximately 8+8 bytes, as the record in the record area         // is preceded by a pointer (for the linked list).         fraction = 8.0 / (16 + recordLength).     }     // We make the number of buckets a power of 2 so that taking modulo is efficient.     int ret = Math.max(1, MathUtils.roundDownToPowerOf2((int) (numAllMemorySegments * fraction))).     // We can't handle more than Integer.MAX_VALUE buckets (eg. because hash functions return int)     if ((long) ret * numBucketsPerSegment > Integer.MAX_VALUE) {         ret = MathUtils.roundDownToPowerOf2(Integer.MAX_VALUE / numBucketsPerSegment).     }     return ret. }
false;private;1;16;;private void allocateBucketSegments(int numBucketSegments) {     if (numBucketSegments < 1) {         throw new RuntimeException("Bug in InPlaceMutableHashTable").     }     bucketSegments = new MemorySegment[numBucketSegments].     for (int i = 0. i < bucketSegments.length. i++) {         bucketSegments[i] = forcedAllocateSegment().         // Init all pointers in all buckets to END_OF_LIST         for (int j = 0. j < numBucketsPerSegment. j++) {             bucketSegments[i].putLong(j << bucketSizeBits, END_OF_LIST).         }     }     numBuckets = numBucketSegments * numBucketsPerSegment.     numBucketsMask = (1 << MathUtils.log2strict(numBuckets)) - 1. }
false;private;0;4;;private void releaseBucketSegments() {     freeMemorySegments.addAll(Arrays.asList(bucketSegments)).     bucketSegments = null. }
false;private;0;8;;private MemorySegment allocateSegment() {     int s = freeMemorySegments.size().     if (s > 0) {         return freeMemorySegments.remove(s - 1).     } else {         return null.     } }
false;private;0;7;;private MemorySegment forcedAllocateSegment() {     MemorySegment segment = allocateSegment().     if (segment == null) {         throw new RuntimeException("Bug in InPlaceMutableHashTable: A free segment should have been available.").     }     return segment. }
true;public;1;13;/**  * Searches the hash table for a record with the given key.  * If it is found, then it is overridden with the specified record.  * Otherwise, the specified record is inserted.  * @param record The record to insert or to replace with.  * @throws IOException (EOFException specifically, if memory ran out)  */ ;/**  * Searches the hash table for a record with the given key.  * If it is found, then it is overridden with the specified record.  * Otherwise, the specified record is inserted.  * @param record The record to insert or to replace with.  * @throws IOException (EOFException specifically, if memory ran out)  */ @Override public void insertOrReplaceRecord(T record) throws IOException {     if (closed) {         return.     }     T match = prober.getMatchFor(record, reuse).     if (match == null) {         prober.insertAfterNoMatch(record).     } else {         prober.updateMatch(record).     } }
true;public;1;25;/**  * Inserts the given record into the hash table.  * Note: this method doesn't care about whether a record with the same key is already present.  * @param record The record to insert.  * @throws IOException (EOFException specifically, if memory ran out)  */ ;/**  * Inserts the given record into the hash table.  * Note: this method doesn't care about whether a record with the same key is already present.  * @param record The record to insert.  * @throws IOException (EOFException specifically, if memory ran out)  */ @Override public void insert(T record) throws IOException {     if (closed) {         return.     }     final int hashCode = MathUtils.jenkinsHash(buildSideComparator.hash(record)).     final int bucket = hashCode & numBucketsMask.     // which segment contains the bucket     final int bucketSegmentIndex = bucket >>> numBucketsPerSegmentBits.     final MemorySegment bucketSegment = bucketSegments[bucketSegmentIndex].     // offset of the bucket in the segment     final int bucketOffset = (bucket & numBucketsPerSegmentMask) << bucketSizeBits.     final long firstPointer = bucketSegment.getLong(bucketOffset).     try {         final long newFirstPointer = recordArea.appendPointerAndRecord(firstPointer, record).         bucketSegment.putLong(bucketOffset, newFirstPointer).     } catch (EOFException ex) {         compactOrThrow().         insert(record).         return.     }     numElements++.     resizeTableIfNecessary(). }
false;private;0;15;;private void resizeTableIfNecessary() throws IOException {     if (enableResize && numElements > numBuckets) {         final long newNumBucketSegments = 2L * bucketSegments.length.         // - the buckets shouldn't occupy more than half of all our memory         if (newNumBucketSegments * numBucketsPerSegment < Integer.MAX_VALUE && newNumBucketSegments - bucketSegments.length < freeMemorySegments.size() && newNumBucketSegments < numAllMemorySegments / 2) {             // do the resize             rebuild(newNumBucketSegments).         }     } }
true;public;0;4;/**  * Returns an iterator that can be used to iterate over all the elements in the table.  * WARNING: Doing any other operation on the table invalidates the iterator! (Even  * using getMatchFor of a prober!)  * @return the iterator  */ ;/**  * Returns an iterator that can be used to iterate over all the elements in the table.  * WARNING: Doing any other operation on the table invalidates the iterator! (Even  * using getMatchFor of a prober!)  * @return the iterator  */ @Override public EntryIterator getEntryIterator() {     return new EntryIterator(). }
false;public;2;3;;public <PT> HashTableProber<PT> getProber(TypeComparator<PT> probeTypeComparator, TypePairComparator<PT, T> pairComparator) {     return new HashTableProber<>(probeTypeComparator, pairComparator). }
true;private;0;3;/**  * This function reinitializes the bucket segments,  * reads all records from the record segments (sequentially, without using the pointers or the buckets),  * and rebuilds the hash table.  */ ;/**  * This function reinitializes the bucket segments,  * reads all records from the record segments (sequentially, without using the pointers or the buckets),  * and rebuilds the hash table.  */ private void rebuild() throws IOException {     rebuild(bucketSegments.length). }
true;private;1;29;/**  * Same as above, but the number of bucket segments of the new table can be specified.  */ ;/**  * Same as above, but the number of bucket segments of the new table can be specified.  */ private void rebuild(long newNumBucketSegments) throws IOException {     // Get new bucket segments     releaseBucketSegments().     allocateBucketSegments((int) newNumBucketSegments).     T record = buildSideSerializer.createInstance().     try {         EntryIterator iter = getEntryIterator().         recordArea.resetAppendPosition().         recordArea.setWritePosition(0).         while ((record = iter.next(record)) != null && !closed) {             final int hashCode = MathUtils.jenkinsHash(buildSideComparator.hash(record)).             final int bucket = hashCode & numBucketsMask.             // which segment contains the bucket             final int bucketSegmentIndex = bucket >>> numBucketsPerSegmentBits.             final MemorySegment bucketSegment = bucketSegments[bucketSegmentIndex].             // offset of the bucket in the segment             final int bucketOffset = (bucket & numBucketsPerSegmentMask) << bucketSizeBits.             final long firstPointer = bucketSegment.getLong(bucketOffset).             long ptrToAppended = recordArea.noSeekAppendPointerAndRecord(firstPointer, record).             bucketSegment.putLong(bucketOffset, ptrToAppended).         }         recordArea.freeSegmentsAfterAppendPosition().         holes = 0.     } catch (EOFException ex) {         throw new RuntimeException("Bug in InPlaceMutableHashTable: we shouldn't get out of memory during a rebuild, " + "because we aren't allocating any new memory.").     } }
true;private;0;7;/**  * If there is wasted space (due to updated records not fitting in their old places), then do a compaction.  * Else, throw EOFException to indicate that memory ran out.  * @throws IOException  */ ;/**  * If there is wasted space (due to updated records not fitting in their old places), then do a compaction.  * Else, throw EOFException to indicate that memory ran out.  * @throws IOException  */ private void compactOrThrow() throws IOException {     if (holes > (double) recordArea.getTotalSize() * 0.05) {         rebuild().     } else {         throw new EOFException("InPlaceMutableHashTable memory ran out. " + getMemoryConsumptionString()).     } }
true;private;0;10;/**  * @return String containing a summary of the memory consumption for error messages  */ ;/**  * @return String containing a summary of the memory consumption for error messages  */ private String getMemoryConsumptionString() {     return "InPlaceMutableHashTable memory stats:\n" + "Total memory:     " + numAllMemorySegments * segmentSize + "\n" + "Free memory:      " + freeMemorySegments.size() * segmentSize + "\n" + "Bucket area:      " + numBuckets * 8 + "\n" + "Record area:      " + recordArea.getTotalSize() + "\n" + "Staging area:     " + stagingSegments.size() * segmentSize + "\n" + "Num of elements:  " + numElements + "\n" + "Holes total size: " + holes. }
false;private;0;7;;private void addSegment() throws EOFException {     MemorySegment m = allocateSegment().     if (m == null) {         throw new EOFException().     }     segments.add(m). }
true;public;0;6;/**  * Moves all its memory segments to freeMemorySegments.  * Warning: this will leave the RecordArea in an unwritable state: you have to  * call setWritePosition before writing again.  */ ;/**  * Moves all its memory segments to freeMemorySegments.  * Warning: this will leave the RecordArea in an unwritable state: you have to  * call setWritePosition before writing again.  */ public void giveBackSegments() {     freeMemorySegments.addAll(segments).     segments.clear().     resetAppendPosition(). }
false;public;0;3;;public long getTotalSize() {     return segments.size() * (long) segmentSize. }
false;private;1;17;;// ----------------------- Output ----------------------- private void setWritePosition(long position) throws EOFException {     if (position > appendPosition) {         throw new IndexOutOfBoundsException().     }     final int segmentIndex = (int) (position >>> segmentSizeBits).     final int offset = (int) (position & segmentSizeMask).     // then we will be seeking to the beginning of a new segment     if (segmentIndex == segments.size()) {         addSegment().     }     outView.currentSegmentIndex = segmentIndex.     outView.seekOutput(segments.get(segmentIndex), offset). }
true;public;0;8;/**  * Sets appendPosition and the write position to 0, so that appending starts  * overwriting elements from the beginning. (This is used in rebuild.)  *  * Note: if data was written to the area after the current appendPosition  * before a call to resetAppendPosition, it should still be readable. To  * release the segments after the current append position, call  * freeSegmentsAfterAppendPosition()  */ ;/**  * Sets appendPosition and the write position to 0, so that appending starts  * overwriting elements from the beginning. (This is used in rebuild.)  *  * Note: if data was written to the area after the current appendPosition  * before a call to resetAppendPosition, it should still be readable. To  * release the segments after the current append position, call  * freeSegmentsAfterAppendPosition()  */ public void resetAppendPosition() {     appendPosition = 0.     // this is just for safety (making sure that we fail immediately     // if a write happens without calling setWritePosition)     outView.currentSegmentIndex = -1.     outView.seekOutput(null, -1). }
true;public;0;7;/**  * Releases the memory segments that are after the current append position.  * Note: The situation that there are segments after the current append position  * can arise from a call to resetAppendPosition().  */ ;/**  * Releases the memory segments that are after the current append position.  * Note: The situation that there are segments after the current append position  * can arise from a call to resetAppendPosition().  */ public void freeSegmentsAfterAppendPosition() {     final int appendSegmentIndex = (int) (appendPosition >>> segmentSizeBits).     while (segments.size() > appendSegmentIndex + 1 && !closed) {         freeMemorySegments.add(segments.get(segments.size() - 1)).         segments.remove(segments.size() - 1).     } }
true;public;2;4;/**  * Overwrites the long value at the specified position.  * @param pointer Points to the position to overwrite.  * @param value The value to write.  * @throws IOException  */ ;/**  * Overwrites the long value at the specified position.  * @param pointer Points to the position to overwrite.  * @param value The value to write.  * @throws IOException  */ public void overwritePointerAt(long pointer, long value) throws IOException {     setWritePosition(pointer).     outView.writeLong(value). }
true;public;3;4;/**  * Overwrites a record at the specified position. The record is read from a DataInputView  (this will be the staging area).  * WARNING: The record must not be larger than the original record.  * @param pointer Points to the position to overwrite.  * @param input The DataInputView to read the record from  * @param size The size of the record  * @throws IOException  */ ;/**  * Overwrites a record at the specified position. The record is read from a DataInputView  (this will be the staging area).  * WARNING: The record must not be larger than the original record.  * @param pointer Points to the position to overwrite.  * @param input The DataInputView to read the record from  * @param size The size of the record  * @throws IOException  */ public void overwriteRecordAt(long pointer, DataInputView input, int size) throws IOException {     setWritePosition(pointer).     outView.write(input, size). }
true;public;3;8;/**  * Appends a pointer and a record. The record is read from a DataInputView (this will be the staging area).  * @param pointer The pointer to write (Note: this is NOT the position to write to!)  * @param input The DataInputView to read the record from  * @param recordSize The size of the record  * @return A pointer to the written data  * @throws IOException (EOFException specifically, if memory ran out)  */ ;/**  * Appends a pointer and a record. The record is read from a DataInputView (this will be the staging area).  * @param pointer The pointer to write (Note: this is NOT the position to write to!)  * @param input The DataInputView to read the record from  * @param recordSize The size of the record  * @return A pointer to the written data  * @throws IOException (EOFException specifically, if memory ran out)  */ public long appendPointerAndCopyRecord(long pointer, DataInputView input, int recordSize) throws IOException {     setWritePosition(appendPosition).     final long oldLastPosition = appendPosition.     outView.writeLong(pointer).     outView.write(input, recordSize).     appendPosition += 8 + recordSize.     return oldLastPosition. }
true;public;2;4;/**  * Appends a pointer and a record.  * @param pointer The pointer to write (Note: this is NOT the position to write to!)  * @param record The record to write  * @return A pointer to the written data  * @throws IOException (EOFException specifically, if memory ran out)  */ ;/**  * Appends a pointer and a record.  * @param pointer The pointer to write (Note: this is NOT the position to write to!)  * @param record The record to write  * @return A pointer to the written data  * @throws IOException (EOFException specifically, if memory ran out)  */ public long appendPointerAndRecord(long pointer, T record) throws IOException {     setWritePosition(appendPosition).     return noSeekAppendPointerAndRecord(pointer, record). }
true;public;2;10;/**  * Appends a pointer and a record. Call this function only if the write position is at the end!  * @param pointer The pointer to write (Note: this is NOT the position to write to!)  * @param record The record to write  * @return A pointer to the written data  * @throws IOException (EOFException specifically, if memory ran out)  */ ;/**  * Appends a pointer and a record. Call this function only if the write position is at the end!  * @param pointer The pointer to write (Note: this is NOT the position to write to!)  * @param record The record to write  * @return A pointer to the written data  * @throws IOException (EOFException specifically, if memory ran out)  */ public long noSeekAppendPointerAndRecord(long pointer, T record) throws IOException {     final long oldLastPosition = appendPosition.     final long oldPositionInSegment = outView.getCurrentPositionInSegment().     final long oldSegmentIndex = outView.currentSegmentIndex.     outView.writeLong(pointer).     buildSideSerializer.serialize(record, outView).     appendPosition += outView.getCurrentPositionInSegment() - oldPositionInSegment + outView.getSegmentSize() * (outView.currentSegmentIndex - oldSegmentIndex).     return oldLastPosition. }
false;public;0;3;;public long getAppendPosition() {     return appendPosition. }
false;public;1;3;;// ----------------------- Input ----------------------- public void setReadPosition(long position) {     inView.setReadPosition(position). }
false;public;0;3;;public long getReadPosition() {     return inView.getReadPosition(). }
true;public;0;3;/**  * Note: this is sometimes a negated length instead of a pointer (see HashTableProber.updateMatch).  */ ;/**  * Note: this is sometimes a negated length instead of a pointer (see HashTableProber.updateMatch).  */ public long readPointer() throws IOException {     return inView.readLong(). }
false;public;1;3;;public T readRecord(T reuse) throws IOException {     return buildSideSerializer.deserialize(reuse, inView). }
false;public;1;3;;public void skipBytesToRead(int numBytes) throws IOException {     inView.skipBytesToRead(numBytes). }
false;protected;2;8;;@Override protected MemorySegment nextSegment(MemorySegment current, int positionInCurrent) throws EOFException {     currentSegmentIndex++.     if (currentSegmentIndex == segments.size()) {         addSegment().     }     return segments.get(currentSegmentIndex). }
false;public;2;4;;@Override public void seekOutput(MemorySegment seg, int position) {     super.seekOutput(seg, position). }
true;public;0;4;/**  * Seeks to the beginning.  */ ;/**  * Seeks to the beginning.  */ public void reset() {     seekOutput(segments.get(0), 0).     currentSegmentIndex = 0. }
false;protected;2;12;;@Override protected MemorySegment nextSegment(MemorySegment current, int positionInCurrent) throws EOFException {     currentSegmentIndex++.     if (currentSegmentIndex == segments.size()) {         MemorySegment m = allocateSegment().         if (m == null) {             throw new EOFException().         }         segments.add(m).     }     return segments.get(currentSegmentIndex). }
false;public;0;3;;public long getWritePosition() {     return (((long) currentSegmentIndex) << segmentSizeBits) + getCurrentPositionInSegment(). }
true;public;2;39;/**  * Searches the hash table for the record with the given key.  * (If there would be multiple matches, only one is returned.)  * @param record The record whose key we are searching for  * @param targetForMatch If a match is found, it will be written here  * @return targetForMatch if a match is found, otherwise null.  */ ;/**  * Searches the hash table for the record with the given key.  * (If there would be multiple matches, only one is returned.)  * @param record The record whose key we are searching for  * @param targetForMatch If a match is found, it will be written here  * @return targetForMatch if a match is found, otherwise null.  */ @Override public T getMatchFor(PT record, T targetForMatch) {     if (closed) {         return null.     }     final int hashCode = MathUtils.jenkinsHash(probeTypeComparator.hash(record)).     final int bucket = hashCode & numBucketsMask.     // which segment contains the bucket     bucketSegmentIndex = bucket >>> numBucketsPerSegmentBits.     final MemorySegment bucketSegment = bucketSegments[bucketSegmentIndex].     // offset of the bucket in the segment     bucketOffset = (bucket & numBucketsPerSegmentMask) << bucketSizeBits.     curElemPtr = bucketSegment.getLong(bucketOffset).     pairComparator.setReference(record).     T currentRecordInList = targetForMatch.     prevElemPtr = INVALID_PREV_POINTER.     try {         while (curElemPtr != END_OF_LIST && !closed) {             recordArea.setReadPosition(curElemPtr).             nextPtr = recordArea.readPointer().             currentRecordInList = recordArea.readRecord(currentRecordInList).             recordEnd = recordArea.getReadPosition().             if (pairComparator.equalToReference(currentRecordInList)) {                 // we found an element with a matching key, and not just a hash collision                 return currentRecordInList.             }             prevElemPtr = curElemPtr.             curElemPtr = nextPtr.         }     } catch (IOException ex) {         throw new RuntimeException("Error deserializing record from the hashtable: " + ex.getMessage(), ex).     }     return null. }
false;public;1;4;;@Override public T getMatchFor(PT probeSideRecord) {     return getMatchFor(probeSideRecord, buildSideSerializer.createInstance()). }
true;public;1;53;/**  * This method can be called after getMatchFor returned a match.  * It will overwrite the record that was found by getMatchFor.  * Warning: The new record should have the same key as the old!  * WARNING. Don't do any modifications to the table between  * getMatchFor and updateMatch!  * @param newRecord The record to override the old record with.  * @throws IOException (EOFException specifically, if memory ran out)  */ ;/**  * This method can be called after getMatchFor returned a match.  * It will overwrite the record that was found by getMatchFor.  * Warning: The new record should have the same key as the old!  * WARNING. Don't do any modifications to the table between  * getMatchFor and updateMatch!  * @param newRecord The record to override the old record with.  * @throws IOException (EOFException specifically, if memory ran out)  */ @Override public void updateMatch(T newRecord) throws IOException {     if (closed) {         return.     }     if (curElemPtr == END_OF_LIST) {         throw new RuntimeException("updateMatch was called after getMatchFor returned no match").     }     try {         // determine the new size         stagingSegmentsOutView.reset().         buildSideSerializer.serialize(newRecord, stagingSegmentsOutView).         final int newRecordSize = (int) stagingSegmentsOutView.getWritePosition().         stagingSegmentsInView.setReadPosition(0).         // Determine the size of the place of the old record.         final int oldRecordSize = (int) (recordEnd - (curElemPtr + RECORD_OFFSET_IN_LINK)).         if (newRecordSize == oldRecordSize) {             // overwrite record at its original place             recordArea.overwriteRecordAt(curElemPtr + RECORD_OFFSET_IN_LINK, stagingSegmentsInView, newRecordSize).         } else {             // new record has a different size than the old one, append new at the end of the record area.             // Note: we have to do this, even if the new record is smaller, because otherwise EntryIterator             // wouldn't know the size of this place, and wouldn't know where does the next record start.             final long pointerToAppended = recordArea.appendPointerAndCopyRecord(nextPtr, stagingSegmentsInView, newRecordSize).             // modify the pointer in the previous link             if (prevElemPtr == INVALID_PREV_POINTER) {                 // list had only one element, so prev is in the bucketSegments                 bucketSegments[bucketSegmentIndex].putLong(bucketOffset, pointerToAppended).             } else {                 recordArea.overwritePointerAt(prevElemPtr, pointerToAppended).             }             // write the negated size of the hole to the place where the next pointer was, so that EntryIterator             // will know the size of the place without reading the old record.             // The negative sign will mean that the record is abandoned, and the             // the -1 is for avoiding trouble in case of a record having 0 size. (though I think this should             // never actually happen)             // Note: the last record in the record area can't be abandoned. (EntryIterator makes use of this fact.)             recordArea.overwritePointerAt(curElemPtr, -oldRecordSize - 1).             holes += oldRecordSize.         }     } catch (EOFException ex) {         compactOrThrow().         insertOrReplaceRecord(newRecord).     } }
true;public;1;27;/**  * This method can be called after getMatchFor returned null.  * It inserts the given record to the hash table.  * Important: The given record should have the same key as the record  * that was given to getMatchFor!  * WARNING. Don't do any modifications to the table between  * getMatchFor and insertAfterNoMatch!  * @throws IOException (EOFException specifically, if memory ran out)  */ ;/**  * This method can be called after getMatchFor returned null.  * It inserts the given record to the hash table.  * Important: The given record should have the same key as the record  * that was given to getMatchFor!  * WARNING. Don't do any modifications to the table between  * getMatchFor and insertAfterNoMatch!  * @throws IOException (EOFException specifically, if memory ran out)  */ public void insertAfterNoMatch(T record) throws IOException {     if (closed) {         return.     }     // create new link     long pointerToAppended.     try {         pointerToAppended = recordArea.appendPointerAndRecord(END_OF_LIST, record).     } catch (EOFException ex) {         compactOrThrow().         insert(record).         return.     }     // add new link to the end of the list     if (prevElemPtr == INVALID_PREV_POINTER) {         // list was empty         bucketSegments[bucketSegmentIndex].putLong(bucketOffset, pointerToAppended).     } else {         // update the pointer of the last element of the list.         recordArea.overwritePointerAt(prevElemPtr, pointerToAppended).     }     numElements++.     resizeTableIfNecessary(). }
false;public;1;21;;@Override public T next(T reuse) throws IOException {     if (endPosition != 0 && recordArea.getReadPosition() < endPosition) {         // Note: the last record in the record area can't be abandoned.         while (!closed) {             final long pointerOrNegatedLength = recordArea.readPointer().             final boolean isAbandoned = pointerOrNegatedLength < 0.             if (!isAbandoned) {                 reuse = recordArea.readRecord(reuse).                 return reuse.             } else {                 // pointerOrNegatedLength is storing a length, because the record was abandoned.                 recordArea.skipBytesToRead((int) -(pointerOrNegatedLength + 1)).             }         }         // (we were closed)         return null.     } else {         return null.     } }
false;public;0;4;;@Override public T next() throws IOException {     return next(buildSideSerializer.createInstance()). }
true;public;1;16;/**  * Looks up the table entry that has the same key as the given record, and updates it by performing  * a reduce step.  * @param record The record to update.  * @throws Exception  */ ;/**  * Looks up the table entry that has the same key as the given record, and updates it by performing  * a reduce step.  * @param record The record to update.  * @throws Exception  */ public void updateTableEntryWithReduce(T record) throws Exception {     T match = prober.getMatchFor(record, reuse).     if (match == null) {         prober.insertAfterNoMatch(record).     } else {         // do the reduce step         T res = reducer.reduce(match, record).         // We have given reuse to the reducer UDF, so create new one if object reuse is disabled         if (!objectReuseEnabled) {             reuse = buildSideSerializer.createInstance().         }         prober.updateMatch(res).     } }
true;public;0;10;/**  * Emits all elements currently held by the table to the collector.  */ ;/**  * Emits all elements currently held by the table to the collector.  */ public void emit() throws IOException {     T record = buildSideSerializer.createInstance().     EntryIterator iter = getEntryIterator().     while ((record = iter.next(record)) != null && !closed) {         outputCollector.collect(record).         if (!objectReuseEnabled) {             record = buildSideSerializer.createInstance().         }     } }
true;public;0;6;/**  * Emits all elements currently held by the table to the collector,  * and resets the table. The table will have the same number of buckets  * as before the reset, to avoid doing resizes again.  */ ;/**  * Emits all elements currently held by the table to the collector,  * and resets the table. The table will have the same number of buckets  * as before the reset, to avoid doing resizes again.  */ public void emitAndReset() throws IOException {     final int oldNumBucketSegments = bucketSegments.length.     emit().     close().     open(oldNumBucketSegments). }
