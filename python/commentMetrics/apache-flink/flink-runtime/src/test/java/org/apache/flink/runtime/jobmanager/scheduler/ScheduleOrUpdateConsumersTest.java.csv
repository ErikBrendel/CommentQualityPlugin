commented;modifiers;parameterAmount;loc;comment;code
false;private,static;0;6;;private static Configuration getFlinkConfiguration() {     final Configuration config = new Configuration().     config.setString(AkkaOptions.ASK_TIMEOUT, TestingUtils.DEFAULT_AKKA_ASK_TIMEOUT()).     return config. }
true;public;0;41;/**  * Tests notifications of multiple receivers when a task produces both a pipelined and blocking  * result.  *  * <pre>  *                             +----------+  *            +-- pipelined -> | Receiver |  * +--------+ |                +----------+  * | Sender |-|  * +--------+ |                +----------+  *            +-- blocking --> | Receiver |  *                             +----------+  * </pre>  *  * <p>The pipelined receiver gets deployed after the first buffer is available and the blocking  * one after all subtasks are finished.  */ ;/**  * Tests notifications of multiple receivers when a task produces both a pipelined and blocking  * result.  *  * <pre>  *                             +----------+  *            +-- pipelined -> | Receiver |  * +--------+ |                +----------+  * | Sender |-|  * +--------+ |                +----------+  *            +-- blocking --> | Receiver |  *                             +----------+  * </pre>  *  * <p>The pipelined receiver gets deployed after the first buffer is available and the blocking  * one after all subtasks are finished.  */ @Test public void testMixedPipelinedAndBlockingResults() throws Exception {     final JobVertex sender = new JobVertex("Sender").     sender.setInvokableClass(BinaryRoundRobinSubtaskIndexSender.class).     sender.getConfiguration().setInteger(BinaryRoundRobinSubtaskIndexSender.CONFIG_KEY, PARALLELISM).     sender.setParallelism(PARALLELISM).     final JobVertex pipelinedReceiver = new JobVertex("Pipelined Receiver").     pipelinedReceiver.setInvokableClass(SlotCountExceedingParallelismTest.SubtaskIndexReceiver.class).     pipelinedReceiver.getConfiguration().setInteger(CONFIG_KEY, PARALLELISM).     pipelinedReceiver.setParallelism(PARALLELISM).     pipelinedReceiver.connectNewDataSetAsInput(sender, DistributionPattern.ALL_TO_ALL, ResultPartitionType.PIPELINED).     final JobVertex blockingReceiver = new JobVertex("Blocking Receiver").     blockingReceiver.setInvokableClass(SlotCountExceedingParallelismTest.SubtaskIndexReceiver.class).     blockingReceiver.getConfiguration().setInteger(CONFIG_KEY, PARALLELISM).     blockingReceiver.setParallelism(PARALLELISM).     blockingReceiver.connectNewDataSetAsInput(sender, DistributionPattern.ALL_TO_ALL, ResultPartitionType.BLOCKING).     SlotSharingGroup slotSharingGroup = new SlotSharingGroup(sender.getID(), pipelinedReceiver.getID(), blockingReceiver.getID()).     sender.setSlotSharingGroup(slotSharingGroup).     pipelinedReceiver.setSlotSharingGroup(slotSharingGroup).     blockingReceiver.setSlotSharingGroup(slotSharingGroup).     final JobGraph jobGraph = new JobGraph("Mixed pipelined and blocking result", sender, pipelinedReceiver, blockingReceiver).     MINI_CLUSTER_RESOURCE.getMiniCluster().executeJobBlocking(jobGraph). }
false;public;0;33;;@Override public void invoke() throws Exception {     List<RecordWriter<IntValue>> writers = Lists.newArrayListWithCapacity(2).     // The order of intermediate result creation in the job graph specifies which produced     // result partition is pipelined/blocking.     final RecordWriter<IntValue> pipelinedWriter = new RecordWriter<>(getEnvironment().getWriter(0)).     final RecordWriter<IntValue> blockingWriter = new RecordWriter<>(getEnvironment().getWriter(1)).     writers.add(pipelinedWriter).     writers.add(blockingWriter).     final int numberOfTimesToSend = getTaskConfiguration().getInteger(CONFIG_KEY, 0).     final IntValue subtaskIndex = new IntValue(getEnvironment().getTaskInfo().getIndexOfThisSubtask()).     // Produce the first intermediate result and then the second in a serial fashion.     for (RecordWriter<IntValue> writer : writers) {         try {             for (int i = 0. i < numberOfTimesToSend. i++) {                 writer.emit(subtaskIndex).             }             writer.flushAll().         } finally {             writer.clearBuffers().         }     } }
