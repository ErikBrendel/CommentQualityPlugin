commented;modifiers;parameterAmount;loc;comment;code
false;protected;3;4;;@Override protected <T> AbstractMutableHashTable<T> getHashTable(TypeSerializer<T> serializer, TypeComparator<T> comparator, List<MemorySegment> memory) {     return new InPlaceMutableHashTable<>(serializer, comparator, memory). }
false;public;1;4;;@Override public void setReference(Long reference) {     ref = reference. }
false;public;1;5;;@Override public boolean equalToReference(Tuple2<Long, String> candidate) {     // noinspection UnnecessaryUnboxing     return candidate.f0.longValue() == ref. }
false;public;1;6;;@Override public int compareToReference(Tuple2<Long, String> candidate) {     long x = ref.     long y = candidate.f0.     return (x < y) ? -1 : ((x == y) ? 0 : 1). }
true;public;0;51;/**  * This has to be duplicated in InPlaceMutableHashTableTest and CompactingHashTableTest  * because of the different constructor calls.  */ ;/**  * This has to be duplicated in InPlaceMutableHashTableTest and CompactingHashTableTest  * because of the different constructor calls.  */ @Test public void testHashTableGrowthWithInsert() {     try {         final int numElements = 1000000.         List<MemorySegment> memory = getMemory(10000, 32 * 1024).         InPlaceMutableHashTable<Tuple2<Long, String>> table = new InPlaceMutableHashTable<Tuple2<Long, String>>(serializer, comparator, memory).         table.open().         for (long i = 0. i < numElements. i++) {             table.insert(new Tuple2<Long, String>(i, String.valueOf(i))).         }         // make sure that all elements are contained via the entry iterator         {             BitSet bitSet = new BitSet(numElements).             MutableObjectIterator<Tuple2<Long, String>> iter = table.getEntryIterator().             Tuple2<Long, String> next.             while ((next = iter.next()) != null) {                 assertNotNull(next.f0).                 assertNotNull(next.f1).                 assertEquals(next.f0.longValue(), Long.parseLong(next.f1)).                 bitSet.set(next.f0.intValue()).             }             assertEquals(numElements, bitSet.cardinality()).         }         // make sure all entries are contained via the prober         {             InPlaceMutableHashTable<Tuple2<Long, String>>.HashTableProber<Long> proper = table.getProber(probeComparator, pairComparator).             Tuple2<Long, String> reuse = new Tuple2<>().             for (long i = 0. i < numElements. i++) {                 assertNotNull(proper.getMatchFor(i, reuse)).                 assertNull(proper.getMatchFor(i + numElements, reuse)).             }         }         table.close().     } catch (Exception e) {         e.printStackTrace().         fail(e.getMessage()).     } }
true;public;0;51;/**  * This test validates that records are not lost via "insertOrReplace()" as in bug [FLINK-2361]  *  * This has to be duplicated in InPlaceMutableHashTableTest and CompactingHashTableTest  * because of the different constructor calls.  */ ;/**  * This test validates that records are not lost via "insertOrReplace()" as in bug [FLINK-2361]  *  * This has to be duplicated in InPlaceMutableHashTableTest and CompactingHashTableTest  * because of the different constructor calls.  */ @Test public void testHashTableGrowthWithInsertOrReplace() {     try {         final int numElements = 1000000.         List<MemorySegment> memory = getMemory(1000, 32 * 1024).         InPlaceMutableHashTable<Tuple2<Long, String>> table = new InPlaceMutableHashTable<Tuple2<Long, String>>(serializer, comparator, memory).         table.open().         for (long i = 0. i < numElements. i++) {             table.insertOrReplaceRecord(Tuple2.of(i, String.valueOf(i))).         }         // make sure that all elements are contained via the entry iterator         {             BitSet bitSet = new BitSet(numElements).             MutableObjectIterator<Tuple2<Long, String>> iter = table.getEntryIterator().             Tuple2<Long, String> next.             while ((next = iter.next()) != null) {                 assertNotNull(next.f0).                 assertNotNull(next.f1).                 assertEquals(next.f0.longValue(), Long.parseLong(next.f1)).                 bitSet.set(next.f0.intValue()).             }             assertEquals(numElements, bitSet.cardinality()).         }         // make sure all entries are contained via the prober         {             InPlaceMutableHashTable<Tuple2<Long, String>>.HashTableProber<Long> proper = table.getProber(probeComparator, pairComparator).             Tuple2<Long, String> reuse = new Tuple2<>().             for (long i = 0. i < numElements. i++) {                 assertNotNull(proper.getMatchFor(i, reuse)).                 assertNull(proper.getMatchFor(i + numElements, reuse)).             }         }         table.close().     } catch (Exception e) {         e.printStackTrace().         fail(e.getMessage()).     } }
false;public;2;11;;public void updateTableEntryWithReduce(T record, K key) throws Exception {     record = serializer.copy(record).     if (!map.containsKey(key)) {         map.put(key, record).     } else {         T x = map.get(key).         x = reducer.reduce(x, record).         map.put(key, x).     } }
false;public;0;6;;public void emitAndReset() {     for (T record : map.values()) {         outputCollector.collect(record).     }     map.clear(). }
false;public;0;73;;@Test public void testWithIntPair() throws Exception {     Random rnd = new Random(RANDOM_SEED).     // varying the keyRange between 1000 and 1000000 can make a 5x speed difference     // (because of cache misses (also in the segment arrays))     final int keyRange = 1000000.     final int valueRange = 10.     final int numRecords = 1000000.     final IntPairSerializer serializer = new IntPairSerializer().     final TypeComparator<IntPair> comparator = new IntPairComparator().     final ReduceFunction<IntPair> reducer = new SumReducer().     // Create the InPlaceMutableHashTableWithJavaHashMap, which will provide the correct output.     List<IntPair> expectedOutput = new ArrayList<>().     InPlaceMutableHashTableWithJavaHashMap<IntPair, Integer> reference = new InPlaceMutableHashTableWithJavaHashMap<>(serializer, comparator, reducer, new CopyingListCollector<>(expectedOutput, serializer)).     // Create the InPlaceMutableHashTable to test     // memory use is proportional to the number of different keys     final int numMemPages = keyRange * 32 / PAGE_SIZE.     List<IntPair> actualOutput = new ArrayList<>().     InPlaceMutableHashTable<IntPair> table = new InPlaceMutableHashTable<>(serializer, comparator, getMemory(numMemPages, PAGE_SIZE)).     InPlaceMutableHashTable<IntPair>.ReduceFacade reduceFacade = table.new ReduceFacade(reducer, new CopyingListCollector<>(actualOutput, serializer), true).     table.open().     // Generate some input     final List<IntPair> input = new ArrayList<>().     for (int i = 0. i < numRecords. i++) {         input.add(new IntPair(rnd.nextInt(keyRange), rnd.nextInt(valueRange))).     }     // System.out.println("start").     // long start = System.currentTimeMillis().     // Process the generated input     final int numIntermingledEmits = 5.     for (IntPair record : input) {         reduceFacade.updateTableEntryWithReduce(serializer.copy(record)).         reference.updateTableEntryWithReduce(serializer.copy(record), record.getKey()).         if (rnd.nextDouble() < 1.0 / ((double) numRecords / numIntermingledEmits)) {             // this will fire approx. numIntermingledEmits times             reference.emitAndReset().             reduceFacade.emitAndReset().         }     }     reference.emitAndReset().     reduceFacade.emit().     table.close().     // long end = System.currentTimeMillis().     // System.out.println("stop, time: " + (end - start)).     // Check results     assertEquals(expectedOutput.size(), actualOutput.size()).     Integer[] expectedValues = new Integer[expectedOutput.size()].     for (int i = 0. i < expectedOutput.size(). i++) {         expectedValues[i] = expectedOutput.get(i).getValue().     }     Integer[] actualValues = new Integer[actualOutput.size()].     for (int i = 0. i < actualOutput.size(). i++) {         actualValues[i] = actualOutput.get(i).getValue().     }     Arrays.sort(expectedValues, Ordering.<Integer>natural()).     Arrays.sort(actualValues, Ordering.<Integer>natural()).     assertArrayEquals(expectedValues, actualValues). }
false;public;2;8;;@Override public IntPair reduce(IntPair a, IntPair b) throws Exception {     if (a.getKey() != b.getKey()) {         throw new RuntimeException("SumReducer was called with two records that have differing keys.").     }     a.setValue(a.getValue() + b.getValue()).     return a. }
false;public;0;93;;@Test public void testWithLengthChangingReduceFunction() throws Exception {     Random rnd = new Random(RANDOM_SEED).     final int numKeys = 10000.     final int numVals = 10.     final int numRecords = numKeys * numVals.     StringPairSerializer serializer = new StringPairSerializer().     StringPairComparator comparator = new StringPairComparator().     ReduceFunction<StringPair> reducer = new ConcatReducer().     // Create the InPlaceMutableHashTableWithJavaHashMap, which will provide the correct output.     List<StringPair> expectedOutput = new ArrayList<>().     InPlaceMutableHashTableWithJavaHashMap<StringPair, String> reference = new InPlaceMutableHashTableWithJavaHashMap<>(serializer, comparator, reducer, new CopyingListCollector<>(expectedOutput, serializer)).     // Create the InPlaceMutableHashTable to test     final int numMemPages = numRecords * 10 / PAGE_SIZE.     List<StringPair> actualOutput = new ArrayList<>().     InPlaceMutableHashTable<StringPair> table = new InPlaceMutableHashTable<>(serializer, comparator, getMemory(numMemPages, PAGE_SIZE)).     InPlaceMutableHashTable<StringPair>.ReduceFacade reduceFacade = table.new ReduceFacade(reducer, new CopyingListCollector<>(actualOutput, serializer), true).     // The loop is for checking the feature that multiple open / close are possible.     for (int j = 0. j < 3. j++) {         table.open().         // Test emit when table is empty         reduceFacade.emit().         // Process some manual stuff         reference.updateTableEntryWithReduce(serializer.copy(new StringPair("foo", "bar")), "foo").         reference.updateTableEntryWithReduce(serializer.copy(new StringPair("foo", "baz")), "foo").         reference.updateTableEntryWithReduce(serializer.copy(new StringPair("alma", "xyz")), "alma").         reduceFacade.updateTableEntryWithReduce(serializer.copy(new StringPair("foo", "bar"))).         reduceFacade.updateTableEntryWithReduce(serializer.copy(new StringPair("foo", "baz"))).         reduceFacade.updateTableEntryWithReduce(serializer.copy(new StringPair("alma", "xyz"))).         for (int i = 0. i < 5. i++) {             reduceFacade.updateTableEntryWithReduce(serializer.copy(new StringPair("korte", "abc"))).             reference.updateTableEntryWithReduce(serializer.copy(new StringPair("korte", "abc")), "korte").         }         reference.emitAndReset().         reduceFacade.emitAndReset().         // Generate some input         UniformStringPairGenerator gen = new UniformStringPairGenerator(numKeys, numVals, true).         List<StringPair> input = new ArrayList<>().         StringPair cur = new StringPair().         while (gen.next(cur) != null) {             input.add(serializer.copy(cur)).         }         Collections.shuffle(input, rnd).         // Process the generated input         final int numIntermingledEmits = 5.         for (StringPair record : input) {             reference.updateTableEntryWithReduce(serializer.copy(record), record.getKey()).             reduceFacade.updateTableEntryWithReduce(serializer.copy(record)).             if (rnd.nextDouble() < 1.0 / ((double) numRecords / numIntermingledEmits)) {                 // this will fire approx. numIntermingledEmits times                 reference.emitAndReset().                 reduceFacade.emitAndReset().             }         }         reference.emitAndReset().         reduceFacade.emit().         table.close().         // Check results         assertEquals(expectedOutput.size(), actualOutput.size()).         String[] expectedValues = new String[expectedOutput.size()].         for (int i = 0. i < expectedOutput.size(). i++) {             expectedValues[i] = expectedOutput.get(i).getValue().         }         String[] actualValues = new String[actualOutput.size()].         for (int i = 0. i < actualOutput.size(). i++) {             actualValues[i] = actualOutput.get(i).getValue().         }         Arrays.sort(expectedValues, Ordering.<String>natural()).         Arrays.sort(actualValues, Ordering.<String>natural()).         assertArrayEquals(expectedValues, actualValues).         expectedOutput.clear().         actualOutput.clear().     } }
false;public;2;7;;@Override public StringPair reduce(StringPair a, StringPair b) throws Exception {     if (a.getKey().compareTo(b.getKey()) != 0) {         throw new RuntimeException("ConcatReducer was called with two records that have differing keys.").     }     return new StringPair(a.getKey(), a.getValue().concat(b.getValue())). }
false;private,static;2;9;;private static List<MemorySegment> getMemory(int numPages, int pageSize) {     List<MemorySegment> memory = new ArrayList<>().     for (int i = 0. i < numPages. i++) {         memory.add(MemorySegmentFactory.allocateUnpooledSegment(pageSize)).     }     return memory. }
true;public;0;37;/**  * The records are larger than one segment. Additionally, there is just barely enough memory,  * so lots of compactions will happen.  */ ;/**  * The records are larger than one segment. Additionally, there is just barely enough memory,  * so lots of compactions will happen.  */ @Test public void testLargeRecordsWithManyCompactions() {     try {         final int numElements = 1000.         final String longString1 = getLongString(100000), longString2 = getLongString(110000).         List<MemorySegment> memory = getMemory(3800, 32 * 1024).         InPlaceMutableHashTable<Tuple2<Long, String>> table = new InPlaceMutableHashTable<>(serializer, comparator, memory).         table.open().         // first, we insert some elements         for (long i = 0. i < numElements. i++) {             table.insertOrReplaceRecord(Tuple2.of(i, longString1)).         }         // now, we replace the same elements with larger ones, causing fragmentation         for (long i = 0. i < numElements. i++) {             table.insertOrReplaceRecord(Tuple2.of(i, longString2)).         }         // check the results         InPlaceMutableHashTable<Tuple2<Long, String>>.HashTableProber<Tuple2<Long, String>> prober = table.getProber(comparator, new SameTypePairComparator<>(comparator)).         Tuple2<Long, String> reuse = new Tuple2<>().         for (long i = 0. i < numElements. i++) {             assertNotNull(prober.getMatchFor(Tuple2.of(i, longString2), reuse)).         }         table.close().     } catch (Exception e) {         e.printStackTrace().         fail(e.getMessage()).     } }
false;private,static;1;7;;private static String getLongString(int length) {     StringBuilder bld = new StringBuilder(length).     for (int i = 0. i < length. i++) {         bld.append('a').     }     return bld.toString(). }
false;public;0;28;;@Test public void testOutOfMemory() {     try {         List<MemorySegment> memory = getMemory(100, 1024).         InPlaceMutableHashTable<Tuple2<Long, String>> table = new InPlaceMutableHashTable<>(serializer, comparator, memory).         try {             final int numElements = 100000.             table.open().             // Insert too many elements             for (long i = 0. i < numElements. i++) {                 table.insertOrReplaceRecord(Tuple2.of(i, "alma")).             }             fail("We should have got out of memory (EOFException)").         } catch (EOFException e) {             // OK             table.close().         }     } catch (Exception e) {         e.printStackTrace().         fail(e.getMessage()).     } }
