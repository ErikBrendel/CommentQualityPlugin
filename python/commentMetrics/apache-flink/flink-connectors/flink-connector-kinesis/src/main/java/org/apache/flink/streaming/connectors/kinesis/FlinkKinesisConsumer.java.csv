commented;modifiers;parameterAmount;loc;comment;code
false;public;0;3;;public KinesisShardAssigner getShardAssigner() {     return shardAssigner. }
true;public;1;4;/**  * Provide a custom assigner to influence how shards are distributed over subtasks.  * @param shardAssigner shard assigner  */ ;/**  * Provide a custom assigner to influence how shards are distributed over subtasks.  * @param shardAssigner shard assigner  */ public void setShardAssigner(KinesisShardAssigner shardAssigner) {     this.shardAssigner = checkNotNull(shardAssigner, "function can not be null").     ClosureCleaner.clean(shardAssigner, true). }
false;public;0;3;;public AssignerWithPeriodicWatermarks<T> getPeriodicWatermarkAssigner() {     return periodicWatermarkAssigner. }
true;public;1;5;/**  * Set the assigner that will extract the timestamp from {@link T} and calculate the  * watermark.  * @param periodicWatermarkAssigner periodic watermark assigner  */ ;/**  * Set the assigner that will extract the timestamp from {@link T} and calculate the  * watermark.  * @param periodicWatermarkAssigner periodic watermark assigner  */ public void setPeriodicWatermarkAssigner(AssignerWithPeriodicWatermarks<T> periodicWatermarkAssigner) {     this.periodicWatermarkAssigner = periodicWatermarkAssigner.     ClosureCleaner.clean(this.periodicWatermarkAssigner, true). }
false;public;1;73;;// ------------------------------------------------------------------------ // Source life cycle // ------------------------------------------------------------------------ @Override public void run(SourceContext<T> sourceContext) throws Exception {     // all subtasks will run a fetcher, regardless of whether or not the subtask will initially have     // shards to subscribe to. fetchers will continuously poll for changes in the shard list, so all subtasks     // can potentially have new shards to subscribe to later on     KinesisDataFetcher<T> fetcher = createFetcher(streams, sourceContext, getRuntimeContext(), configProps, deserializer).     // initial discovery     List<StreamShardHandle> allShards = fetcher.discoverNewShardsToSubscribe().     for (StreamShardHandle shard : allShards) {         StreamShardMetadata.EquivalenceWrapper kinesisStreamShard = new StreamShardMetadata.EquivalenceWrapper(KinesisDataFetcher.convertToStreamShardMetadata(shard)).         if (sequenceNumsToRestore != null) {             if (sequenceNumsToRestore.containsKey(kinesisStreamShard)) {                 // if the shard was already seen and is contained in the state,                 // just use the sequence number stored in the state                 fetcher.registerNewSubscribedShardState(new KinesisStreamShardState(kinesisStreamShard.getShardMetadata(), shard, sequenceNumsToRestore.get(kinesisStreamShard))).                 if (LOG.isInfoEnabled()) {                     LOG.info("Subtask {} is seeding the fetcher with restored shard {}," + " starting state set to the restored sequence number {}", getRuntimeContext().getIndexOfThisSubtask(), shard.toString(), sequenceNumsToRestore.get(kinesisStreamShard)).                 }             } else {                 // the shard wasn't discovered in the previous run, therefore should be consumed from the beginning                 fetcher.registerNewSubscribedShardState(new KinesisStreamShardState(kinesisStreamShard.getShardMetadata(), shard, SentinelSequenceNumber.SENTINEL_EARLIEST_SEQUENCE_NUM.get())).                 if (LOG.isInfoEnabled()) {                     LOG.info("Subtask {} is seeding the fetcher with new discovered shard {}," + " starting state set to the SENTINEL_EARLIEST_SEQUENCE_NUM", getRuntimeContext().getIndexOfThisSubtask(), shard.toString()).                 }             }         } else {             // we're starting fresh. use the configured start position as initial state             SentinelSequenceNumber startingSeqNum = InitialPosition.valueOf(configProps.getProperty(ConsumerConfigConstants.STREAM_INITIAL_POSITION, ConsumerConfigConstants.DEFAULT_STREAM_INITIAL_POSITION)).toSentinelSequenceNumber().             fetcher.registerNewSubscribedShardState(new KinesisStreamShardState(kinesisStreamShard.getShardMetadata(), shard, startingSeqNum.get())).             if (LOG.isInfoEnabled()) {                 LOG.info("Subtask {} will be seeded with initial shard {}, starting state set as sequence number {}", getRuntimeContext().getIndexOfThisSubtask(), shard.toString(), startingSeqNum.get()).             }         }     }     // check that we are running before starting the fetcher     if (!running) {         return.     }     // expose the fetcher from this point, so that state     // snapshots can be taken from the fetcher's state holders     this.fetcher = fetcher.     // start the fetcher loop. The fetcher will stop running only when cancel() or     // close() is called, or an error is thrown by threads created by the fetcher     fetcher.runFetcher().     // check that the fetcher has terminated before fully closing     fetcher.awaitTermination().     sourceContext.close(). }
false;public;0;19;;@Override public void cancel() {     running = false.     KinesisDataFetcher fetcher = this.fetcher.     this.fetcher = null.     // so we must check if the fetcher is actually created     if (fetcher != null) {         try {             // interrupt the fetcher of any work             fetcher.shutdownFetcher().             fetcher.awaitTermination().         } catch (Exception e) {             LOG.warn("Error while closing Kinesis data fetcher", e).         }     } }
false;public;0;5;;@Override public void close() throws Exception {     cancel().     super.close(). }
false;public;0;4;;@Override public TypeInformation<T> getProducedType() {     return deserializer.getProducedType(). }
false;public;1;29;;// ------------------------------------------------------------------------ // State Snapshot & Restore // ------------------------------------------------------------------------ @Override public void initializeState(FunctionInitializationContext context) throws Exception {     TypeInformation<Tuple2<StreamShardMetadata, SequenceNumber>> shardsStateTypeInfo = new TupleTypeInfo<>(TypeInformation.of(StreamShardMetadata.class), TypeInformation.of(SequenceNumber.class)).     sequenceNumsStateForCheckpoint = context.getOperatorStateStore().getUnionListState(new ListStateDescriptor<>(sequenceNumsStateStoreName, shardsStateTypeInfo)).     if (context.isRestored()) {         if (sequenceNumsToRestore == null) {             sequenceNumsToRestore = new HashMap<>().             for (Tuple2<StreamShardMetadata, SequenceNumber> kinesisSequenceNumber : sequenceNumsStateForCheckpoint.get()) {                 sequenceNumsToRestore.put(// we will still be able to match it in sequenceNumsToRestore. Please see FLINK-8484 for details.                 new StreamShardMetadata.EquivalenceWrapper(kinesisSequenceNumber.f0), kinesisSequenceNumber.f1).             }             LOG.info("Setting restore state in the FlinkKinesisConsumer. Using the following offsets: {}", sequenceNumsToRestore).         }     } else {         LOG.info("No restore state for FlinkKinesisConsumer.").     } }
false;public;1;42;;@Override public void snapshotState(FunctionSnapshotContext context) throws Exception {     if (!running) {         LOG.debug("snapshotState() called on closed source. returning null.").     } else {         if (LOG.isDebugEnabled()) {             LOG.debug("Snapshotting state ...").         }         sequenceNumsStateForCheckpoint.clear().         if (fetcher == null) {             if (sequenceNumsToRestore != null) {                 for (Map.Entry<StreamShardMetadata.EquivalenceWrapper, SequenceNumber> entry : sequenceNumsToRestore.entrySet()) {                     // sequenceNumsToRestore is the restored global union state.                     // should only snapshot shards that actually belong to us                     int hashCode = shardAssigner.assign(KinesisDataFetcher.convertToStreamShardHandle(entry.getKey().getShardMetadata()), getRuntimeContext().getNumberOfParallelSubtasks()).                     if (KinesisDataFetcher.isThisSubtaskShouldSubscribeTo(hashCode, getRuntimeContext().getNumberOfParallelSubtasks(), getRuntimeContext().getIndexOfThisSubtask())) {                         sequenceNumsStateForCheckpoint.add(Tuple2.of(entry.getKey().getShardMetadata(), entry.getValue())).                     }                 }             }         } else {             HashMap<StreamShardMetadata, SequenceNumber> lastStateSnapshot = fetcher.snapshotState().             if (LOG.isDebugEnabled()) {                 LOG.debug("Snapshotted state, last processed sequence numbers: {}, checkpoint id: {}, timestamp: {}", lastStateSnapshot, context.getCheckpointId(), context.getCheckpointTimestamp()).             }             for (Map.Entry<StreamShardMetadata, SequenceNumber> entry : lastStateSnapshot.entrySet()) {                 sequenceNumsStateForCheckpoint.add(Tuple2.of(entry.getKey(), entry.getValue())).             }         }     } }
true;protected;5;9;/**  * This method is exposed for tests that need to mock the KinesisDataFetcher in the consumer.  */ ;/**  * This method is exposed for tests that need to mock the KinesisDataFetcher in the consumer.  */ protected KinesisDataFetcher<T> createFetcher(List<String> streams, SourceFunction.SourceContext<T> sourceContext, RuntimeContext runtimeContext, Properties configProps, KinesisDeserializationSchema<T> deserializationSchema) {     return new KinesisDataFetcher<>(streams, sourceContext, runtimeContext, configProps, deserializationSchema, shardAssigner, periodicWatermarkAssigner). }
false;;0;4;;@VisibleForTesting HashMap<StreamShardMetadata.EquivalenceWrapper, SequenceNumber> getRestoredState() {     return sequenceNumsToRestore. }
