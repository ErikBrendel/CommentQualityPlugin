commented;modifiers;parameterAmount;loc;comment;code
true;public;1;14;/**  * Specifies the fields which are returned by the InputFormat and their order.  *  * @param fields The fields and their order which are returned by the InputFormat.  * @return This InputFormat with specified return fields.  * @throws java.io.IOException  */ ;/**  * Specifies the fields which are returned by the InputFormat and their order.  *  * @param fields The fields and their order which are returned by the InputFormat.  * @return This InputFormat with specified return fields.  * @throws java.io.IOException  */ public HCatInputFormatBase<T> getFields(String... fields) throws IOException {     // build output schema     ArrayList<HCatFieldSchema> fieldSchemas = new ArrayList<HCatFieldSchema>(fields.length).     for (String field : fields) {         fieldSchemas.add(this.outputSchema.get(field)).     }     this.outputSchema = new HCatSchema(fieldSchemas).     // update output schema configuration     configuration.set("mapreduce.lib.hcat.output.schema", HCatUtil.serialize(outputSchema)).     return this. }
true;public;1;7;/**  * Specifies a SQL-like filter condition on the table's partition columns.  * Filter conditions on non-partition columns are invalid.  * A partition filter can significantly reduce the amount of data to be read.  *  * @param filter A SQL-like filter condition on the table's partition columns.  * @return This InputFormat with specified partition filter.  * @throws java.io.IOException  */ ;/**  * Specifies a SQL-like filter condition on the table's partition columns.  * Filter conditions on non-partition columns are invalid.  * A partition filter can significantly reduce the amount of data to be read.  *  * @param filter A SQL-like filter condition on the table's partition columns.  * @return This InputFormat with specified partition filter.  * @throws java.io.IOException  */ public HCatInputFormatBase<T> withFilter(String filter) throws IOException {     // set filter     this.hCatInputFormat.setFilter(filter).     return this. }
true;public;0;25;/**  * Specifies that the InputFormat returns Flink tuples instead of  * {@link org.apache.hive.hcatalog.data.HCatRecord}.  *  * <p>Note: Flink tuples might only support a limited number of fields (depending on the API).  *  * @return This InputFormat.  * @throws org.apache.hive.hcatalog.common.HCatException  */ ;/**  * Specifies that the InputFormat returns Flink tuples instead of  * {@link org.apache.hive.hcatalog.data.HCatRecord}.  *  * <p>Note: Flink tuples might only support a limited number of fields (depending on the API).  *  * @return This InputFormat.  * @throws org.apache.hive.hcatalog.common.HCatException  */ public HCatInputFormatBase<T> asFlinkTuples() throws HCatException {     // build type information     int numFields = outputSchema.getFields().size().     if (numFields > this.getMaxFlinkTupleSize()) {         throw new IllegalArgumentException("Only up to " + this.getMaxFlinkTupleSize() + " fields can be returned as Flink tuples.").     }     TypeInformation[] fieldTypes = new TypeInformation[numFields].     fieldNames = new String[numFields].     for (String fieldName : outputSchema.getFieldNames()) {         HCatFieldSchema field = outputSchema.get(fieldName).         int fieldPos = outputSchema.getPosition(fieldName).         TypeInformation fieldType = getFieldType(field).         fieldTypes[fieldPos] = fieldType.         fieldNames[fieldPos] = fieldName.     }     this.resultType = new TupleTypeInfo(fieldTypes).     return this. }
false;protected,abstract;0;1;;protected abstract int getMaxFlinkTupleSize().
false;private;1;31;;private TypeInformation getFieldType(HCatFieldSchema fieldSchema) {     switch(fieldSchema.getType()) {         case INT:             return BasicTypeInfo.INT_TYPE_INFO.         case TINYINT:             return BasicTypeInfo.BYTE_TYPE_INFO.         case SMALLINT:             return BasicTypeInfo.SHORT_TYPE_INFO.         case BIGINT:             return BasicTypeInfo.LONG_TYPE_INFO.         case BOOLEAN:             return BasicTypeInfo.BOOLEAN_TYPE_INFO.         case FLOAT:             return BasicTypeInfo.FLOAT_TYPE_INFO.         case DOUBLE:             return BasicTypeInfo.DOUBLE_TYPE_INFO.         case STRING:             return BasicTypeInfo.STRING_TYPE_INFO.         case BINARY:             return PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO.         case ARRAY:             return new GenericTypeInfo(List.class).         case MAP:             return new GenericTypeInfo(Map.class).         case STRUCT:             return new GenericTypeInfo(List.class).         default:             throw new IllegalArgumentException("Unknown data type \"" + fieldSchema.getType() + "\" encountered.").     } }
true;public;0;3;/**  * Returns the {@link org.apache.hadoop.conf.Configuration} of the HCatInputFormat.  *  * @return The Configuration of the HCatInputFormat.  */ ;/**  * Returns the {@link org.apache.hadoop.conf.Configuration} of the HCatInputFormat.  *  * @return The Configuration of the HCatInputFormat.  */ public Configuration getConfiguration() {     return this.configuration. }
true;public;0;3;/**  * Returns the {@link org.apache.hive.hcatalog.data.schema.HCatSchema} of the {@link org.apache.hive.hcatalog.data.HCatRecord}  * returned by this InputFormat.  *  * @return The HCatSchema of the HCatRecords returned by this InputFormat.  */ ;/**  * Returns the {@link org.apache.hive.hcatalog.data.schema.HCatSchema} of the {@link org.apache.hive.hcatalog.data.HCatRecord}  * returned by this InputFormat.  *  * @return The HCatSchema of the HCatRecords returned by this InputFormat.  */ public HCatSchema getOutputSchema() {     return this.outputSchema. }
false;public;1;4;;// -------------------------------------------------------------------------------------------- // InputFormat // -------------------------------------------------------------------------------------------- @Override public void configure(org.apache.flink.configuration.Configuration parameters) { // nothing to do }
false;public;1;5;;@Override public BaseStatistics getStatistics(BaseStatistics cachedStats) throws IOException {     // no statistics provided at the moment     return null. }
false;public;1;20;;@Override public HadoopInputSplit[] createInputSplits(int minNumSplits) throws IOException {     configuration.setInt("mapreduce.input.fileinputformat.split.minsize", minNumSplits).     JobContext jobContext = new JobContextImpl(configuration, new JobID()).     List<InputSplit> splits.     try {         splits = this.hCatInputFormat.getSplits(jobContext).     } catch (InterruptedException e) {         throw new IOException("Could not get Splits.", e).     }     HadoopInputSplit[] hadoopInputSplits = new HadoopInputSplit[splits.size()].     for (int i = 0. i < hadoopInputSplits.length. i++) {         hadoopInputSplits[i] = new HadoopInputSplit(i, splits.get(i), jobContext).     }     return hadoopInputSplits. }
false;public;1;4;;@Override public InputSplitAssigner getInputSplitAssigner(HadoopInputSplit[] inputSplits) {     return new LocatableInputSplitAssigner(inputSplits). }
false;public;1;14;;@Override public void open(HadoopInputSplit split) throws IOException {     TaskAttemptContext context = new TaskAttemptContextImpl(configuration, new TaskAttemptID()).     try {         this.recordReader = this.hCatInputFormat.createRecordReader(split.getHadoopInputSplit(), context).         this.recordReader.initialize(split.getHadoopInputSplit(), context).     } catch (InterruptedException e) {         throw new IOException("Could not create RecordReader.", e).     } finally {         this.fetched = false.     } }
false;public;0;7;;@Override public boolean reachedEnd() throws IOException {     if (!this.fetched) {         fetchNext().     }     return !this.hasNext. }
false;private;0;9;;private void fetchNext() throws IOException {     try {         this.hasNext = this.recordReader.nextKeyValue().     } catch (InterruptedException e) {         throw new IOException("Could not fetch next KeyValue pair.", e).     } finally {         this.fetched = true.     } }
false;public;1;28;;@Override public T nextRecord(T record) throws IOException {     if (!this.fetched) {         // first record         fetchNext().     }     if (!this.hasNext) {         return null.     }     try {         // get next HCatRecord         HCatRecord v = this.recordReader.getCurrentValue().         this.fetched = false.         if (this.fieldNames.length > 0) {             // return as Flink tuple             return this.buildFlinkTuple(record, v).         } else {             // return as HCatRecord             return (T) v.         }     } catch (InterruptedException e) {         throw new IOException("Could not get next record.", e).     } }
false;protected,abstract;2;1;;protected abstract T buildFlinkTuple(T t, HCatRecord record) throws HCatException.
false;public;0;4;;@Override public void close() throws IOException {     this.recordReader.close(). }
false;private;1;7;;// -------------------------------------------------------------------------------------------- // Custom de/serialization methods // -------------------------------------------------------------------------------------------- private void writeObject(ObjectOutputStream out) throws IOException {     out.writeInt(this.fieldNames.length).     for (String fieldName : this.fieldNames) {         out.writeUTF(fieldName).     }     this.configuration.write(out). }
false;private;1;17;;@SuppressWarnings("unchecked") private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {     this.fieldNames = new String[in.readInt()].     for (int i = 0. i < this.fieldNames.length. i++) {         this.fieldNames[i] = in.readUTF().     }     Configuration configuration = new Configuration().     configuration.readFields(in).     if (this.configuration == null) {         this.configuration = configuration.     }     this.hCatInputFormat = new org.apache.hive.hcatalog.mapreduce.HCatInputFormat().     this.outputSchema = (HCatSchema) HCatUtil.deserialize(this.configuration.get("mapreduce.lib.hcat.output.schema")). }
false;public;0;4;;// -------------------------------------------------------------------------------------------- // Result type business // -------------------------------------------------------------------------------------------- @Override public TypeInformation<T> getProducedType() {     return this.resultType. }
