commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;4;;// ------------------------------------------------------------------------ // Setup and teardown of the mini clusters // ------------------------------------------------------------------------ @BeforeClass public static void prepare() throws ClassNotFoundException {     prepare(true). }
false;public,static;1;7;;public static void prepare(boolean hideKafkaBehindProxy) throws ClassNotFoundException {     LOG.info("-------------------------------------------------------------------------").     LOG.info("    Starting KafkaTestBase ").     LOG.info("-------------------------------------------------------------------------").     startClusters(false, hideKafkaBehindProxy). }
false;public,static;0;15;;@AfterClass public static void shutDownServices() throws Exception {     LOG.info("-------------------------------------------------------------------------").     LOG.info("    Shut down KafkaTestBase ").     LOG.info("-------------------------------------------------------------------------").     TestStreamEnvironment.unsetAsContext().     shutdownClusters().     LOG.info("-------------------------------------------------------------------------").     LOG.info("    KafkaTestBase finished").     LOG.info("-------------------------------------------------------------------------"). }
false;protected,static;0;9;;protected static Configuration getFlinkConfiguration() {     Configuration flinkConfig = new Configuration().     flinkConfig.setString(AkkaOptions.WATCH_HEARTBEAT_PAUSE, "5 s").     flinkConfig.setString(AkkaOptions.WATCH_HEARTBEAT_INTERVAL, "1 s").     flinkConfig.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE, "16m").     flinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, "0 s").     flinkConfig.setString(ConfigConstants.METRICS_REPORTER_PREFIX + "my_reporter." + ConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, JMXReporter.class.getName()).     return flinkConfig. }
false;protected,static;2;25;;protected static void startClusters(boolean secureMode, boolean hideKafkaBehindProxy) throws ClassNotFoundException {     // dynamically load the implementation for the test     Class<?> clazz = Class.forName("org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl").     kafkaServer = (KafkaTestEnvironment) InstantiationUtil.instantiate(clazz).     LOG.info("Starting KafkaTestBase.prepare() for Kafka " + kafkaServer.getVersion()).     kafkaServer.prepare(kafkaServer.createConfig().setKafkaServersNumber(NUMBER_OF_KAFKA_SERVERS).setSecureMode(secureMode).setHideKafkaBehindProxy(hideKafkaBehindProxy)).     standardProps = kafkaServer.getStandardProperties().     brokerConnectionStrings = kafkaServer.getBrokerConnectionString().     if (secureMode) {         if (!kafkaServer.isSecureRunSupported()) {             throw new IllegalStateException("Attempting to test in secure mode but secure mode not supported by the KafkaTestEnvironment.").         }         secureProps = kafkaServer.getSecureProperties().     } }
false;protected,static;0;7;;protected static void shutdownClusters() throws Exception {     if (secureProps != null) {         secureProps.clear().     }     kafkaServer.shutdown(). }
false;protected,static;2;19;;// ------------------------------------------------------------------------ // Execution utilities // ------------------------------------------------------------------------ protected static void tryExecutePropagateExceptions(StreamExecutionEnvironment see, String name) throws Exception {     try {         see.execute(name).     } catch (ProgramInvocationException | JobExecutionException root) {         Throwable cause = root.getCause().         // search for nested SuccessExceptions         int depth = 0.         while (!(cause instanceof SuccessException)) {             if (cause == null || depth++ == 20) {                 throw root.             } else {                 cause = cause.getCause().             }         }     } }
false;protected,static;3;3;;protected static void createTestTopic(String topic, int numberOfPartitions, int replicationFactor) {     kafkaServer.createTestTopic(topic, numberOfPartitions, replicationFactor). }
false;protected,static;1;3;;protected static void deleteTestTopic(String topic) {     kafkaServer.deleteTestTopic(topic). }
true;protected;5;30;/**  * We manually handle the timeout instead of using JUnit's timeout to return failure instead of timeout error.  * After timeout we assume that there are missing records and there is a bug, not that the test has run out of time.  */ ;/**  * We manually handle the timeout instead of using JUnit's timeout to return failure instead of timeout error.  * After timeout we assume that there are missing records and there is a bug, not that the test has run out of time.  */ protected void assertAtLeastOnceForTopic(Properties properties, String topic, int partition, Set<Integer> expectedElements, long timeoutMillis) throws Exception {     long startMillis = System.currentTimeMillis().     Set<Integer> actualElements = new HashSet<>().     // until we timeout...     while (System.currentTimeMillis() < startMillis + timeoutMillis) {         properties.put("key.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer").         properties.put("value.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer").         // query kafka for new records ...         Collection<ConsumerRecord<Integer, Integer>> records = kafkaServer.getAllRecordsFromTopic(properties, topic, partition, 100).         for (ConsumerRecord<Integer, Integer> record : records) {             actualElements.add(record.value()).         }         // succeed if we got all expectedElements         if (actualElements.containsAll(expectedElements)) {             return.         }     }     fail(String.format("Expected to contain all of: <%s>, but was: <%s>", expectedElements, actualElements)). }
false;protected;4;7;;protected void assertExactlyOnceForTopic(Properties properties, String topic, int partition, List<Integer> expectedElements) {     assertExactlyOnceForTopic(properties, topic, partition, expectedElements, 30_000L). }
true;protected;5;37;/**  * We manually handle the timeout instead of using JUnit's timeout to return failure instead of timeout error.  * After timeout we assume that there are missing records and there is a bug, not that the test has run out of time.  */ ;/**  * We manually handle the timeout instead of using JUnit's timeout to return failure instead of timeout error.  * After timeout we assume that there are missing records and there is a bug, not that the test has run out of time.  */ protected void assertExactlyOnceForTopic(Properties properties, String topic, int partition, List<Integer> expectedElements, long timeoutMillis) {     long startMillis = System.currentTimeMillis().     List<Integer> actualElements = new ArrayList<>().     Properties consumerProperties = new Properties().     consumerProperties.putAll(properties).     consumerProperties.put("key.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer").     consumerProperties.put("value.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer").     consumerProperties.put("isolation.level", "read_committed").     // until we timeout...     while (System.currentTimeMillis() < startMillis + timeoutMillis) {         // query kafka for new records ...         Collection<ConsumerRecord<Integer, Integer>> records = kafkaServer.getAllRecordsFromTopic(consumerProperties, topic, partition, 1000).         for (ConsumerRecord<Integer, Integer> record : records) {             actualElements.add(record.value()).         }         // succeed if we got all expectedElements         if (actualElements.equals(expectedElements)) {             return.         }         // fail early if we already have too many elements         if (actualElements.size() > expectedElements.size()) {             break.         }     }     fail(String.format("Expected number of elements: <%s>, but was: <%s>", expectedElements.size(), actualElements.size())). }
