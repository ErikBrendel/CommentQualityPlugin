commented;modifiers;parameterAmount;loc;comment;code
false;public;1;29;;@Override public void run(SourceContext<Integer> ctx) {     // create a sequence     int[] elements = new int[numElements].     for (int i = 0, val = getRuntimeContext().getIndexOfThisSubtask(). i < numElements. i++, val += getRuntimeContext().getNumberOfParallelSubtasks()) {         elements[i] = val.     }     // scramble the sequence     if (randomizeOrder) {         Random rnd = new Random().         for (int i = 0. i < elements.length. i++) {             int otherPos = rnd.nextInt(elements.length).             int tmp = elements[i].             elements[i] = elements[otherPos].             elements[otherPos] = tmp.         }     }     // emit the sequence     int pos = 0.     while (running && pos < elements.length) {         ctx.collect(elements[pos++]).     } }
false;public;0;4;;@Override public void cancel() {     running = false. }
false;public;5;4;;@Override public int partition(Integer next, byte[] serializedKey, byte[] serializedValue, String topic, int[] partitions) {     return next % partitions.length. }
false;public,static;6;71;;public static void generateRandomizedIntegerSequence(StreamExecutionEnvironment env, KafkaTestEnvironment testServer, String topic, final int numPartitions, final int numElements, final boolean randomizeOrder) throws Exception {     env.setParallelism(numPartitions).     env.getConfig().disableSysoutLogging().     env.setRestartStrategy(RestartStrategies.noRestart()).     DataStream<Integer> stream = env.addSource(new RichParallelSourceFunction<Integer>() {          private volatile boolean running = true.          @Override         public void run(SourceContext<Integer> ctx) {             // create a sequence             int[] elements = new int[numElements].             for (int i = 0, val = getRuntimeContext().getIndexOfThisSubtask(). i < numElements. i++, val += getRuntimeContext().getNumberOfParallelSubtasks()) {                 elements[i] = val.             }             // scramble the sequence             if (randomizeOrder) {                 Random rnd = new Random().                 for (int i = 0. i < elements.length. i++) {                     int otherPos = rnd.nextInt(elements.length).                     int tmp = elements[i].                     elements[i] = elements[otherPos].                     elements[otherPos] = tmp.                 }             }             // emit the sequence             int pos = 0.             while (running && pos < elements.length) {                 ctx.collect(elements[pos++]).             }         }          @Override         public void cancel() {             running = false.         }     }).     Properties props = new Properties().     props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(testServer.getBrokerConnectionString())).     Properties secureProps = testServer.getSecureProperties().     if (secureProps != null) {         props.putAll(testServer.getSecureProperties()).     }     stream = stream.rebalance().     testServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, env.getConfig())), props, new FlinkKafkaPartitioner<Integer>() {          @Override         public int partition(Integer next, byte[] serializedKey, byte[] serializedValue, String topic, int[] partitions) {             return next % partitions.length.         }     }).     env.execute("Scrambles int sequence generator"). }
false;public;0;50;;@Override public void run() {     // we manually feed data into the Kafka sink     RichFunction producer = null.     try {         Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(server.getBrokerConnectionString()).         producerProperties.setProperty("retries", "3").         StreamTransformation<String> mockTransform = new MockStreamTransformation().         DataStream<String> stream = new DataStream<>(new DummyStreamExecutionEnvironment(), mockTransform).         StreamSink<String> sink = server.getProducerSink(topic, new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()), producerProperties, new FlinkFixedPartitioner<String>()).         OneInputStreamOperatorTestHarness<String, Object> testHarness = new OneInputStreamOperatorTestHarness<>(sink).         testHarness.open().         final StringBuilder bld = new StringBuilder().         final Random rnd = new Random().         while (running) {             bld.setLength(0).             int len = rnd.nextInt(100) + 1.             for (int i = 0. i < len. i++) {                 bld.append((char) (rnd.nextInt(20) + 'a')).             }             String next = bld.toString().             testHarness.processElement(new StreamRecord<>(next)).         }     } catch (Throwable t) {         this.error = t.     } finally {         if (producer != null) {             try {                 producer.close().             } catch (Throwable t) {             // ignore             }         }     } }
false;public;0;4;;public void shutdown() {     this.running = false.     this.interrupt(). }
false;public;0;3;;public Throwable getError() {     return this.error. }
false;public;1;4;;@Override public void setChainingStrategy(ChainingStrategy strategy) { }
false;public;0;4;;@Override public Collection<StreamTransformation<?>> getTransitivePredecessors() {     return null. }
false;public;1;4;;@Override public JobExecutionResult execute(String jobName) throws Exception {     return null. }
