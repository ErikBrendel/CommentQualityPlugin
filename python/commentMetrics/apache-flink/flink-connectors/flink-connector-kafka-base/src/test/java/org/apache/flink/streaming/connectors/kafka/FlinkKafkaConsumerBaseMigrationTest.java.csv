# id;timestamp;commentText;codeText;commentWords;codeWords
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromFlink11() throws Exception;1487173364;Test restoring from a non-empty state taken using Flink 1.1, when some partitions could be found for topics.;@Test_	public void testRestoreFromFlink11() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>()__		partitions.add(new KafkaTopicPartition("abc", 13))__		partitions.add(new KafkaTopicPartition("def", 7))___		final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(partitions)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()__		_		testHarness.initializeStateFromLegacyCheckpoint(_			getResourceFilename("kafka-consumer-migration-test-flink1.1-snapshot"))__		testHarness.open()___		_		Assert.assertTrue(consumerFunction.getSubscribedPartitions() != null)__		Assert.assertTrue(!consumerFunction.getSubscribedPartitions().isEmpty())__		Assert.assertEquals(partitions, consumerFunction.getSubscribedPartitions())___		_		final HashMap<KafkaTopicPartition, Long> expectedState = new HashMap<>()__		expectedState.put(new KafkaTopicPartition("abc", 13), 16768L)__		expectedState.put(new KafkaTopicPartition("def", 7), 987654321L)___		_		Assert.assertTrue(consumerFunction.getRestoredState() != null)__		Assert.assertEquals(expectedState, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,flink,1,1,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,flink11,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,partitions,add,new,kafka,topic,partition,abc,13,partitions,add,new,kafka,topic,partition,def,7,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,from,legacy,checkpoint,get,resource,filename,kafka,consumer,migration,test,flink1,1,snapshot,test,harness,open,assert,assert,true,consumer,function,get,subscribed,partitions,null,assert,assert,true,consumer,function,get,subscribed,partitions,is,empty,assert,assert,equals,partitions,consumer,function,get,subscribed,partitions,final,hash,map,kafka,topic,partition,long,expected,state,new,hash,map,expected,state,put,new,kafka,topic,partition,abc,13,16768l,expected,state,put,new,kafka,topic,partition,def,7,987654321l,assert,assert,true,consumer,function,get,restored,state,null,assert,assert,equals,expected,state,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromFlink11() throws Exception;1488214488;Test restoring from a non-empty state taken using Flink 1.1, when some partitions could be found for topics.;@Test_	public void testRestoreFromFlink11() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>()__		partitions.add(new KafkaTopicPartition("abc", 13))__		partitions.add(new KafkaTopicPartition("def", 7))___		final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(partitions)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()__		_		testHarness.initializeStateFromLegacyCheckpoint(_			getResourceFilename("kafka-consumer-migration-test-flink1.1-snapshot"))__		testHarness.open()___		_		final HashMap<KafkaTopicPartition, Long> expectedState = new HashMap<>()__		expectedState.put(new KafkaTopicPartition("abc", 13), 16768L)__		expectedState.put(new KafkaTopicPartition("def", 7), 987654321L)___		_		Assert.assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		Assert.assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		Assert.assertEquals(expectedState, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		Assert.assertTrue(consumerFunction.getRestoredState() != null)__		Assert.assertEquals(expectedState, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,flink,1,1,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,flink11,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,partitions,add,new,kafka,topic,partition,abc,13,partitions,add,new,kafka,topic,partition,def,7,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,from,legacy,checkpoint,get,resource,filename,kafka,consumer,migration,test,flink1,1,snapshot,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,state,new,hash,map,expected,state,put,new,kafka,topic,partition,abc,13,16768l,expected,state,put,new,kafka,topic,partition,def,7,987654321l,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,assert,equals,expected,state,consumer,function,get,subscribed,partitions,to,start,offsets,assert,assert,true,consumer,function,get,restored,state,null,assert,assert,equals,expected,state,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromFlink11() throws Exception;1489510697;Test restoring from a non-empty state taken using Flink 1.1, when some partitions could be found for topics.;@Test_	public void testRestoreFromFlink11() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>()__		partitions.add(new KafkaTopicPartition("abc", 13))__		partitions.add(new KafkaTopicPartition("def", 7))___		final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(partitions)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()__		_		testHarness.initializeStateFromLegacyCheckpoint(_			getResourceFilename("kafka-consumer-migration-test-flink1.1-snapshot"))__		testHarness.open()___		_		final HashMap<KafkaTopicPartition, Long> expectedState = new HashMap<>()__		expectedState.put(new KafkaTopicPartition("abc", 13), 16768L)__		expectedState.put(new KafkaTopicPartition("def", 7), 987654321L)___		_		Assert.assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		Assert.assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		Assert.assertEquals(expectedState, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		Assert.assertTrue(consumerFunction.getRestoredState() != null)__		Assert.assertEquals(expectedState, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,flink,1,1,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,flink11,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,partitions,add,new,kafka,topic,partition,abc,13,partitions,add,new,kafka,topic,partition,def,7,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,from,legacy,checkpoint,get,resource,filename,kafka,consumer,migration,test,flink1,1,snapshot,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,state,new,hash,map,expected,state,put,new,kafka,topic,partition,abc,13,16768l,expected,state,put,new,kafka,topic,partition,def,7,987654321l,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,assert,equals,expected,state,consumer,function,get,subscribed,partitions,to,start,offsets,assert,assert,true,consumer,function,get,restored,state,null,assert,assert,equals,expected,state,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromFlink11WithEmptyStateNoPartitions() throws Exception;1487173364;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromFlink11WithEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(Collections.<KafkaTopicPartition>emptyList())___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()__		_		testHarness.initializeStateFromLegacyCheckpoint(_			getResourceFilename("kafka-consumer-migration-test-flink1.1-snapshot-empty-state"))__		testHarness.open()___		_		Assert.assertTrue(consumerFunction.getSubscribedPartitions() != null)__		Assert.assertTrue(consumerFunction.getSubscribedPartitions().isEmpty())___		_		Assert.assertTrue(consumerFunction.getRestoredState() == null)___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,flink11with,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,from,legacy,checkpoint,get,resource,filename,kafka,consumer,migration,test,flink1,1,snapshot,empty,state,test,harness,open,assert,assert,true,consumer,function,get,subscribed,partitions,null,assert,assert,true,consumer,function,get,subscribed,partitions,is,empty,assert,assert,true,consumer,function,get,restored,state,null,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromFlink11WithEmptyStateNoPartitions() throws Exception;1488214488;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromFlink11WithEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(Collections.<KafkaTopicPartition>emptyList())___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()__		_		testHarness.initializeStateFromLegacyCheckpoint(_			getResourceFilename("kafka-consumer-migration-test-flink1.1-snapshot-empty-state"))__		testHarness.open()___		_		Assert.assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		Assert.assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		Assert.assertTrue(consumerFunction.getRestoredState() == null)___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,flink11with,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,from,legacy,checkpoint,get,resource,filename,kafka,consumer,migration,test,flink1,1,snapshot,empty,state,test,harness,open,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,assert,true,consumer,function,get,restored,state,null,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromFlink11WithEmptyStateNoPartitions() throws Exception;1489510697;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromFlink11WithEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(Collections.<KafkaTopicPartition>emptyList())___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()__		_		testHarness.initializeStateFromLegacyCheckpoint(_			getResourceFilename("kafka-consumer-migration-test-flink1.1-snapshot-empty-state"))__		testHarness.open()___		_		Assert.assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		Assert.assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		Assert.assertTrue(consumerFunction.getRestoredState() == null)___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,flink11with,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,from,legacy,checkpoint,get,resource,filename,kafka,consumer,migration,test,flink1,1,snapshot,empty,state,test,harness,open,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,assert,true,consumer,function,get,restored,state,null,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1496852938;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(Collections.<KafkaTopicPartition>emptyList())___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState() == null)___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,null,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1498894422;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1503598628;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1517943538;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1517943539;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1517943539;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1519567828;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1519973085;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1534233743;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1534779482;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1545142782;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1547026204;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1547725934;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		testHarness.initializeState(_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"))___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1547725946;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		testHarness.initializeState(_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"))___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateNoPartitions() throws Exception;1550834396;Test restoring from an legacy empty state, when no partitions could be found for topics.;@Test_	public void testRestoreFromEmptyStateNoPartitions() throws Exception {_		final DummyFlinkKafkaConsumer<String> consumerFunction =_				new DummyFlinkKafkaConsumer<>(_					Collections.<KafkaTopicPartition>emptyList(),_					FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		testHarness.initializeState(_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"))___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertTrue(consumerFunction.getRestoredState().isEmpty())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,legacy,empty,state,when,no,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,no,partitions,throws,exception,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,collections,kafka,topic,partition,empty,list,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,true,consumer,function,get,restored,state,is,empty,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1496852938;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(partitions)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		Assert.assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		assertTrue(consumerFunction.getRestoredState() == null)___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1498894422;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1503598628;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1517943538;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1517943539;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1517943539;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1519567828;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1519973085;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1534233743;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1534779482;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1545142782;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1547026204;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,migrate,version,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1547725934;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		testHarness.initializeState(_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"))___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1547725946;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		testHarness.initializeState(_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"))___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromEmptyStateWithPartitions() throws Exception;1550834396;Test restoring from an empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestoreFromEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		testHarness.initializeState(_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"))___		testHarness.open()___		_		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {_			expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET)__		}__		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {_			assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey()))__		}__		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,empty,state,snapshot,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,for,kafka,topic,partition,partition,key,set,expected,subscribed,partitions,with,start,offsets,put,partition,kafka,topic,partition,state,sentinel,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,for,map,entry,kafka,topic,partition,long,expected,entry,expected,subscribed,partitions,with,start,offsets,entry,set,assert,equals,expected,entry,get,value,consumer,function,get,restored,state,get,expected,entry,get,key,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromFlink11WithEmptyStateWithPartitions() throws Exception;1487173364;Test restoring from an empty state taken using Flink 1.1, when some partitions could be found for topics.;@Test_	public void testRestoreFromFlink11WithEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>()__		partitions.add(new KafkaTopicPartition("abc", 13))__		partitions.add(new KafkaTopicPartition("def", 7))___		final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(partitions)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()__		_		testHarness.initializeStateFromLegacyCheckpoint(_			getResourceFilename("kafka-consumer-migration-test-flink1.1-snapshot-empty-state"))__		testHarness.open()___		_		Assert.assertTrue(consumerFunction.getSubscribedPartitions() != null)__		Assert.assertTrue(!consumerFunction.getSubscribedPartitions().isEmpty())__		Assert.assertTrue(consumerFunction.getSubscribedPartitions().equals(partitions))___		_		Assert.assertTrue(consumerFunction.getRestoredState() == null)___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,flink,1,1,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,flink11with,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,partitions,add,new,kafka,topic,partition,abc,13,partitions,add,new,kafka,topic,partition,def,7,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,from,legacy,checkpoint,get,resource,filename,kafka,consumer,migration,test,flink1,1,snapshot,empty,state,test,harness,open,assert,assert,true,consumer,function,get,subscribed,partitions,null,assert,assert,true,consumer,function,get,subscribed,partitions,is,empty,assert,assert,true,consumer,function,get,subscribed,partitions,equals,partitions,assert,assert,true,consumer,function,get,restored,state,null,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromFlink11WithEmptyStateWithPartitions() throws Exception;1488214488;Test restoring from an empty state taken using Flink 1.1, when some partitions could be found for topics.;@Test_	public void testRestoreFromFlink11WithEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>()__		partitions.add(new KafkaTopicPartition("abc", 13))__		partitions.add(new KafkaTopicPartition("def", 7))___		final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(partitions)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()__		_		testHarness.initializeStateFromLegacyCheckpoint(_			getResourceFilename("kafka-consumer-migration-test-flink1.1-snapshot-empty-state"))__		testHarness.open()___		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		expectedSubscribedPartitionsWithStartOffsets.put(new KafkaTopicPartition("abc", 13), KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__		expectedSubscribedPartitionsWithStartOffsets.put(new KafkaTopicPartition("def", 7), KafkaTopicPartitionStateSentinel.GROUP_OFFSET)___		_		Assert.assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		Assert.assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		Assert.assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		Assert.assertTrue(consumerFunction.getRestoredState() == null)___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,flink,1,1,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,flink11with,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,partitions,add,new,kafka,topic,partition,abc,13,partitions,add,new,kafka,topic,partition,def,7,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,from,legacy,checkpoint,get,resource,filename,kafka,consumer,migration,test,flink1,1,snapshot,empty,state,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,expected,subscribed,partitions,with,start,offsets,put,new,kafka,topic,partition,abc,13,kafka,topic,partition,state,sentinel,expected,subscribed,partitions,with,start,offsets,put,new,kafka,topic,partition,def,7,kafka,topic,partition,state,sentinel,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,assert,true,consumer,function,get,restored,state,null,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFromFlink11WithEmptyStateWithPartitions() throws Exception;1489510697;Test restoring from an empty state taken using Flink 1.1, when some partitions could be found for topics.;@Test_	public void testRestoreFromFlink11WithEmptyStateWithPartitions() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>()__		partitions.add(new KafkaTopicPartition("abc", 13))__		partitions.add(new KafkaTopicPartition("def", 7))___		final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(partitions)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()__		_		testHarness.initializeStateFromLegacyCheckpoint(_			getResourceFilename("kafka-consumer-migration-test-flink1.1-snapshot-empty-state"))__		testHarness.open()___		_		_		final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>()__		expectedSubscribedPartitionsWithStartOffsets.put(new KafkaTopicPartition("abc", 13), KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__		expectedSubscribedPartitionsWithStartOffsets.put(new KafkaTopicPartition("def", 7), KafkaTopicPartitionStateSentinel.GROUP_OFFSET)___		_		Assert.assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		Assert.assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())__		Assert.assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		Assert.assertTrue(consumerFunction.getRestoredState() == null)___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,an,empty,state,taken,using,flink,1,1,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,from,flink11with,empty,state,with,partitions,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,partitions,add,new,kafka,topic,partition,abc,13,partitions,add,new,kafka,topic,partition,def,7,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,from,legacy,checkpoint,get,resource,filename,kafka,consumer,migration,test,flink1,1,snapshot,empty,state,test,harness,open,final,hash,map,kafka,topic,partition,long,expected,subscribed,partitions,with,start,offsets,new,hash,map,expected,subscribed,partitions,with,start,offsets,put,new,kafka,topic,partition,abc,13,kafka,topic,partition,state,sentinel,expected,subscribed,partitions,with,start,offsets,put,new,kafka,topic,partition,def,7,kafka,topic,partition,state,sentinel,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,assert,equals,expected,subscribed,partitions,with,start,offsets,consumer,function,get,subscribed,partitions,to,start,offsets,assert,assert,true,consumer,function,get,restored,state,null,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1496852938;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1498894422;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1503598628;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1517943538;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1517943539;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1517943539;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1519567828;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1519973085;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1534233743;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1534779482;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1545142782;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1547026204;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1547725934;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1547725946;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Ignore 	@Test 	public void writeSnapshot() throws Exception;1550834396;Manually run this to write binary snapshot data.;@Ignore_	@Test_	public void writeSnapshot() throws Exception {_		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE)___		final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>()__		writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState)__	};manually,run,this,to,write,binary,snapshot,data;ignore,test,public,void,write,snapshot,throws,exception,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,snapshot,final,hash,map,kafka,topic,partition,long,empty,state,new,hash,map,write,snapshot,src,test,resources,kafka,consumer,migration,test,flink,flink,generate,savepoint,version,empty,state,snapshot,empty,state
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1496852938;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(partitions)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		Assert.assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		Assert.assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1498894422;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1503598628;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1517943538;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1517943539;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1517943539;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1519567828;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1519973085;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1534233743;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1534779482;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1545142782;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1547026204;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		MigrationTestUtil.restoreFromSnapshot(_			testHarness,_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_			testMigrateVersion)___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1547725934;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		testHarness.initializeState(_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"))___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1547725946;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		testHarness.initializeState(_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"))___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestore() throws Exception;1550834396;Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be_found for topics.;@Test_	public void testRestore() throws Exception {_		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)___		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_				new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_				new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		testHarness.initializeState(_			OperatorSnapshotUtil.getResourceFilename(_				"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"))___		testHarness.open()___		_		assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null)__		assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty())___		_		assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets())___		_		assertTrue(consumerFunction.getRestoredState() != null)__		assertEquals(PARTITION_STATE, consumerFunction.getRestoredState())___		consumerOperator.close()__		consumerOperator.cancel()__	};test,restoring,from,a,non,empty,state,taken,using,a,previous,flink,version,when,some,partitions,could,be,found,for,topics;test,public,void,test,restore,throws,exception,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,flink,kafka,consumer,base,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,harness,open,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,null,assert,true,consumer,function,get,subscribed,partitions,to,start,offsets,is,empty,assert,equals,consumer,function,get,subscribed,partitions,to,start,offsets,assert,true,consumer,function,get,restored,state,null,assert,equals,consumer,function,get,restored,state,consumer,operator,close,consumer,operator,cancel
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1498894422;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_1 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			if (testMigrateVersion == MigrationVersion.v1_1) {_				Assert.assertTrue(e.getCause() instanceof IllegalArgumentException)__			} else {_				Assert.assertTrue(e instanceof IllegalArgumentException)__			}_		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,if,test,migrate,version,migration,version,assert,assert,true,e,get,cause,instanceof,illegal,argument,exception,else,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1503598628;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			if (testMigrateVersion == MigrationVersion.v1_1) {_				Assert.assertTrue(e.getCause() instanceof IllegalArgumentException)__			} else {_				Assert.assertTrue(e instanceof IllegalArgumentException)__			}_		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,if,test,migrate,version,migration,version,assert,assert,true,e,get,cause,instanceof,illegal,argument,exception,else,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1517943538;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			if (testMigrateVersion == MigrationVersion.v1_1) {_				Assert.assertTrue(e.getCause() instanceof IllegalArgumentException)__			} else {_				Assert.assertTrue(e instanceof IllegalArgumentException)__			}_		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,if,test,migrate,version,migration,version,assert,assert,true,e,get,cause,instanceof,illegal,argument,exception,else,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1517943539;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			if (testMigrateVersion == MigrationVersion.v1_1) {_				Assert.assertTrue(e.getCause() instanceof IllegalArgumentException)__			} else {_				Assert.assertTrue(e instanceof IllegalArgumentException)__			}_		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,if,test,migrate,version,migration,version,assert,assert,true,e,get,cause,instanceof,illegal,argument,exception,else,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1517943539;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1519567828;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1519973085;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1534233743;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1534779482;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1545142782;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1547026204;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			MigrationTestUtil.restoreFromSnapshot(_				testHarness,_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"),_				testMigrateVersion)___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,migration,test,util,restore,from,snapshot,test,harness,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,test,migrate,version,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1547725934;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			testHarness.initializeState(_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"))___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1547725946;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			testHarness.initializeState(_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"))___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
FlinkKafkaConsumerBaseMigrationTest -> @Test 	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception;1550834396;Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.;@Test_	public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {_		assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2)___		final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet())___		final DummyFlinkKafkaConsumer<String> consumerFunction =_			new DummyFlinkKafkaConsumer<>(partitions, 1000L)_ __		StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator =_			new StreamSource<>(consumerFunction)___		final AbstractStreamOperatorTestHarness<String> testHarness =_			new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0)___		testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime)___		testHarness.setup()___		_		try {_			testHarness.initializeState(_				OperatorSnapshotUtil.getResourceFilename(_					"kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot"))___			fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.")__		} catch (Exception e) {_			Assert.assertTrue(e instanceof IllegalArgumentException)__		}_	};test,restoring,from,savepoints,before,version,flink,1,3,should,fail,if,discovery,is,enabled;test,public,void,test,restore,fails,with,non,empty,pre,flink13states,if,discovery,enabled,throws,exception,assume,true,test,migrate,version,migration,version,test,migrate,version,migration,version,final,list,kafka,topic,partition,partitions,new,array,list,key,set,final,dummy,flink,kafka,consumer,string,consumer,function,new,dummy,flink,kafka,consumer,partitions,1000l,stream,source,string,dummy,flink,kafka,consumer,string,consumer,operator,new,stream,source,consumer,function,final,abstract,stream,operator,test,harness,string,test,harness,new,abstract,stream,operator,test,harness,consumer,operator,1,1,0,test,harness,set,time,characteristic,time,characteristic,processing,time,test,harness,setup,try,test,harness,initialize,state,operator,snapshot,util,get,resource,filename,kafka,consumer,migration,test,flink,test,migrate,version,snapshot,fail,restore,from,savepoints,from,version,before,flink,1,3,x,should,have,failed,if,discovery,is,enabled,catch,exception,e,assert,assert,true,e,instanceof,illegal,argument,exception
