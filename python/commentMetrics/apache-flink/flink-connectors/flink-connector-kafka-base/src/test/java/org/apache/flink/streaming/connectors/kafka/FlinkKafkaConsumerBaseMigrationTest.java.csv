commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;10;;@Parameterized.Parameters(name = "Migration Savepoint: {0}") public static Collection<MigrationVersion> parameters() {     return Arrays.asList(MigrationVersion.v1_2, MigrationVersion.v1_3, MigrationVersion.v1_4, MigrationVersion.v1_5, MigrationVersion.v1_6, MigrationVersion.v1_7). }
true;public;0;8;/**  * Manually run this to write binary snapshot data.  */ ;/**  * Manually run this to write binary snapshot data.  */ @Ignore @Test public void writeSnapshot() throws Exception {     writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot", PARTITION_STATE).     final HashMap<KafkaTopicPartition, Long> emptyState = new HashMap<>().     writeSnapshot("src/test/resources/kafka-consumer-migration-test-flink" + flinkGenerateSavepointVersion + "-empty-state-snapshot", emptyState). }
false;public;1;5;;@Override public Void answer(InvocationOnMock invocation) throws Throwable {     latch.trigger().     return null. }
false;public;1;4;;@Override public void collect(String element) { }
false;public;0;15;;@Override public void run() {     try {         consumerFunction.run(new DummySourceContext() {              @Override             public void collect(String element) {             }         }).     } catch (Throwable t) {         t.printStackTrace().         error[0] = t.     } }
false;private;2;67;;private void writeSnapshot(String path, HashMap<KafkaTopicPartition, Long> state) throws Exception {     final OneShotLatch latch = new OneShotLatch().     final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class).     doAnswer(new Answer<Void>() {          @Override         public Void answer(InvocationOnMock invocation) throws Throwable {             latch.trigger().             return null.         }     }).when(fetcher).runFetchLoop().     when(fetcher.snapshotCurrentState()).thenReturn(state).     final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet()).     final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(fetcher, TOPICS, partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED).     StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction).     final AbstractStreamOperatorTestHarness<String> testHarness = new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0).     testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime).     testHarness.setup().     testHarness.open().     final Throwable[] error = new Throwable[1].     // run the source asynchronously     Thread runner = new Thread() {          @Override         public void run() {             try {                 consumerFunction.run(new DummySourceContext() {                      @Override                     public void collect(String element) {                     }                 }).             } catch (Throwable t) {                 t.printStackTrace().                 error[0] = t.             }         }     }.     runner.start().     if (!latch.isTriggered()) {         latch.await().     }     final OperatorSubtaskState snapshot.     synchronized (testHarness.getCheckpointLock()) {         snapshot = testHarness.snapshot(0L, 0L).     }     OperatorSnapshotUtil.writeStateHandle(snapshot, path).     consumerOperator.close().     runner.join(). }
true;public;0;34;/**  * Test restoring from an legacy empty state, when no partitions could be found for topics.  */ ;/**  * Test restoring from an legacy empty state, when no partitions could be found for topics.  */ @Test public void testRestoreFromEmptyStateNoPartitions() throws Exception {     final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(Collections.singletonList("dummy-topic"), Collections.<KafkaTopicPartition>emptyList(), FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED).     StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction).     final AbstractStreamOperatorTestHarness<String> testHarness = new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0).     testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime).     testHarness.setup().     // restore state from binary snapshot file     testHarness.initializeState(OperatorSnapshotUtil.getResourceFilename("kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot")).     testHarness.open().     // assert that no partitions were found and is empty     assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null).     assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty()).     // assert that no state was restored     assertTrue(consumerFunction.getRestoredState().isEmpty()).     consumerOperator.close().     consumerOperator.cancel(). }
true;public;0;47;/**  * Test restoring from an empty state taken using a previous Flink version, when some partitions could be  * found for topics.  */ ;/**  * Test restoring from an empty state taken using a previous Flink version, when some partitions could be  * found for topics.  */ @Test public void testRestoreFromEmptyStateWithPartitions() throws Exception {     final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet()).     final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(TOPICS, partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED).     StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction).     final AbstractStreamOperatorTestHarness<String> testHarness = new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0).     testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime).     testHarness.setup().     // restore state from binary snapshot file     testHarness.initializeState(OperatorSnapshotUtil.getResourceFilename("kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot")).     testHarness.open().     // the expected state in "kafka-consumer-migration-test-flink1.2-snapshot-empty-state".     // all new partitions after the snapshot are considered as partitions that were created while the     // consumer wasn't running, and should start from the earliest offset.     final HashMap<KafkaTopicPartition, Long> expectedSubscribedPartitionsWithStartOffsets = new HashMap<>().     for (KafkaTopicPartition partition : PARTITION_STATE.keySet()) {         expectedSubscribedPartitionsWithStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET).     }     // assert that there are partitions and is identical to expected list     assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null).     assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty()).     assertEquals(expectedSubscribedPartitionsWithStartOffsets, consumerFunction.getSubscribedPartitionsToStartOffsets()).     // the new partitions should have been considered as restored state     assertTrue(consumerFunction.getRestoredState() != null).     assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty()).     for (Map.Entry<KafkaTopicPartition, Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {         assertEquals(expectedEntry.getValue(), consumerFunction.getRestoredState().get(expectedEntry.getKey())).     }     consumerOperator.close().     consumerOperator.cancel(). }
true;public;0;38;/**  * Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be  * found for topics.  */ ;/**  * Test restoring from a non-empty state taken using a previous Flink version, when some partitions could be  * found for topics.  */ @Test public void testRestore() throws Exception {     final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet()).     final DummyFlinkKafkaConsumer<String> consumerFunction = new DummyFlinkKafkaConsumer<>(TOPICS, partitions, FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED).     StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction).     final AbstractStreamOperatorTestHarness<String> testHarness = new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0).     testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime).     testHarness.setup().     // restore state from binary snapshot file     testHarness.initializeState(OperatorSnapshotUtil.getResourceFilename("kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot")).     testHarness.open().     // assert that there are partitions and is identical to expected list     assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null).     assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty()).     // on restore, subscribedPartitionsToStartOffsets should be identical to the restored state     assertEquals(PARTITION_STATE, consumerFunction.getSubscribedPartitionsToStartOffsets()).     // assert that state is correctly restored from legacy checkpoint     assertTrue(consumerFunction.getRestoredState() != null).     assertEquals(PARTITION_STATE, consumerFunction.getRestoredState()).     consumerOperator.close().     consumerOperator.cancel(). }
true;public;0;30;/**  * Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.  */ ;/**  * Test restoring from savepoints before version Flink 1.3 should fail if discovery is enabled.  */ @Test public void testRestoreFailsWithNonEmptyPreFlink13StatesIfDiscoveryEnabled() throws Exception {     assumeTrue(testMigrateVersion == MigrationVersion.v1_3 || testMigrateVersion == MigrationVersion.v1_2).     final List<KafkaTopicPartition> partitions = new ArrayList<>(PARTITION_STATE.keySet()).     final DummyFlinkKafkaConsumer<String> consumerFunction = // discovery enabled     new DummyFlinkKafkaConsumer<>(TOPICS, partitions, 1000L).     StreamSource<String, DummyFlinkKafkaConsumer<String>> consumerOperator = new StreamSource<>(consumerFunction).     final AbstractStreamOperatorTestHarness<String> testHarness = new AbstractStreamOperatorTestHarness<>(consumerOperator, 1, 1, 0).     testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime).     testHarness.setup().     // restore state from binary snapshot file. should fail since discovery is enabled     try {         testHarness.initializeState(OperatorSnapshotUtil.getResourceFilename("kafka-consumer-migration-test-flink" + testMigrateVersion + "-snapshot")).         fail("Restore from savepoints from version before Flink 1.3.x should have failed if discovery is enabled.").     } catch (Exception e) {         Assert.assertTrue(e instanceof IllegalArgumentException).     } }
false;protected;8;12;;@Override protected AbstractFetcher<T, ?> createFetcher(SourceContext<T> sourceContext, Map<KafkaTopicPartition, Long> thisSubtaskPartitionsWithStartOffsets, SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, StreamingRuntimeContext runtimeContext, OffsetCommitMode offsetCommitMode, MetricGroup consumerMetricGroup, boolean useMetrics) throws Exception {     return fetcher. }
false;protected;3;17;;@Override protected AbstractPartitionDiscoverer createPartitionDiscoverer(KafkaTopicsDescriptor topicsDescriptor, int indexOfThisSubtask, int numParallelSubtasks) {     AbstractPartitionDiscoverer mockPartitionDiscoverer = mock(AbstractPartitionDiscoverer.class).     try {         when(mockPartitionDiscoverer.discoverPartitions()).thenReturn(partitions).     } catch (Exception e) {     // ignore     }     when(mockPartitionDiscoverer.setAndCheckDiscoveredPartition(any(KafkaTopicPartition.class))).thenReturn(true).     return mockPartitionDiscoverer. }
false;protected;0;4;;@Override protected boolean getIsAutoCommitEnabled() {     return false. }
false;protected;2;6;;@Override protected Map<KafkaTopicPartition, Long> fetchOffsetsWithTimestamp(Collection<KafkaTopicPartition> partitions, long timestamp) {     throw new UnsupportedOperationException(). }
false;public;2;3;;@Override public void collectWithTimestamp(String element, long timestamp) { }
false;public;1;3;;@Override public void emitWatermark(Watermark mark) { }
false;public;0;4;;@Override public Object getCheckpointLock() {     return lock. }
false;public;0;3;;@Override public void close() { }
false;public;0;4;;@Override public void markAsTemporarilyIdle() { }
