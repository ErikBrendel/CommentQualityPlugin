commented;modifiers;parameterAmount;loc;comment;code
false;private,static;0;6;;private static Configuration getConfiguration() {     Configuration flinkConfig = new Configuration().     flinkConfig.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE, "16m").     flinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, "0 s").     return flinkConfig. }
false;public,static;0;25;;@BeforeClass public static void prepare() throws ClassNotFoundException {     LOG.info("-------------------------------------------------------------------------").     LOG.info("    Starting KafkaShortRetentionTestBase ").     LOG.info("-------------------------------------------------------------------------").     // dynamically load the implementation for the test     Class<?> clazz = Class.forName("org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl").     kafkaServer = (KafkaTestEnvironment) InstantiationUtil.instantiate(clazz).     LOG.info("Starting KafkaTestBase.prepare() for Kafka " + kafkaServer.getVersion()).     if (kafkaServer.isSecureRunSupported()) {         secureProps = kafkaServer.getSecureProperties().     }     Properties specificProperties = new Properties().     specificProperties.setProperty("log.retention.hours", "0").     specificProperties.setProperty("log.retention.minutes", "0").     specificProperties.setProperty("log.retention.ms", "250").     specificProperties.setProperty("log.retention.check.interval.ms", "100").     kafkaServer.prepare(kafkaServer.createConfig().setKafkaServerProperties(specificProperties)).     standardProps = kafkaServer.getStandardProperties(). }
false;public,static;0;6;;@AfterClass public static void shutDownServices() throws Exception {     kafkaServer.shutdown().     secureProps.clear(). }
false;public;1;12;;@Override public void run(SourceContext<String> ctx) throws InterruptedException {     int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition.     int limit = cnt + elementsPerPartition.     while (running && !stopProducer && cnt < limit) {         ctx.collect("element-" + cnt).         cnt++.         Thread.sleep(10).     }     LOG.info("Stopping producer"). }
false;public;0;4;;@Override public void cancel() {     running = false. }
false;public;0;56;;public void runAutoOffsetResetTest() throws Exception {     final String topic = "auto-offset-reset-test".     final int parallelism = 1.     final int elementsPerPartition = 50000.     Properties tprops = new Properties().     tprops.setProperty("retention.ms", "250").     kafkaServer.createTestTopic(topic, parallelism, 1, tprops).     final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(parallelism).     // fail immediately     env.setRestartStrategy(RestartStrategies.noRestart()).     env.getConfig().disableSysoutLogging().     // ----------- add producer dataflow ----------     DataStream<String> stream = env.addSource(new RichParallelSourceFunction<String>() {          private boolean running = true.          @Override         public void run(SourceContext<String> ctx) throws InterruptedException {             int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition.             int limit = cnt + elementsPerPartition.             while (running && !stopProducer && cnt < limit) {                 ctx.collect("element-" + cnt).                 cnt++.                 Thread.sleep(10).             }             LOG.info("Stopping producer").         }          @Override         public void cancel() {             running = false.         }     }).     Properties props = new Properties().     props.putAll(standardProps).     props.putAll(secureProps).     kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()), props, null).     // ----------- add consumer dataflow ----------     NonContinousOffsetsDeserializationSchema deserSchema = new NonContinousOffsetsDeserializationSchema().     FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, deserSchema, props).     DataStreamSource<String> consuming = env.addSource(source).     consuming.addSink(new DiscardingSink<String>()).     tryExecute(env, "run auto offset reset test").     kafkaServer.deleteTestTopic(topic). }
false;public;1;16;;@Override public String deserialize(ConsumerRecord<byte[], byte[]> record) {     final long offset = record.offset().     if (offset != nextExpected) {         numJumps++.         nextExpected = offset.         LOG.info("Registered now jump at offset {}", offset).     }     nextExpected++.     try {         // slow down data consumption to trigger log eviction         Thread.sleep(10).     } catch (InterruptedException e) {         throw new RuntimeException("Stopping it").     }     return "". }
false;public;1;9;;@Override public boolean isEndOfStream(String nextElement) {     if (numJumps >= 5) {         // we saw 5 jumps and no failures --> consumer can handle auto.offset.reset         stopProducer = true.         return true.     }     return false. }
false;public;0;4;;@Override public TypeInformation<String> getProducedType() {     return Types.STRING. }
true;public;0;35;/**  * Ensure that the consumer is properly failing if "auto.offset.reset" is set to "none".  */ ;/**  * Ensure that the consumer is properly failing if "auto.offset.reset" is set to "none".  */ public void runFailOnAutoOffsetResetNone() throws Exception {     final String topic = "auto-offset-reset-none-test".     final int parallelism = 1.     kafkaServer.createTestTopic(topic, parallelism, 1).     final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(parallelism).     // fail immediately     env.setRestartStrategy(RestartStrategies.noRestart()).     env.getConfig().disableSysoutLogging().     // ----------- add consumer ----------     Properties customProps = new Properties().     customProps.putAll(standardProps).     customProps.putAll(secureProps).     // test that "none" leads to an exception     customProps.setProperty("auto.offset.reset", "none").     FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), customProps).     DataStreamSource<String> consuming = env.addSource(source).     consuming.addSink(new DiscardingSink<String>()).     try {         env.execute("Test auto offset reset none").     } catch (Throwable e) {         // check if correct exception has been thrown         if (// kafka 0.8         !e.getCause().getCause().getMessage().contains("Unable to find previous offset") && // kafka 0.9         !e.getCause().getCause().getMessage().contains("Undefined offset with no reset policy for partition")) {             throw e.         }     }     kafkaServer.deleteTestTopic(topic). }
false;public;0;24;;public void runFailOnAutoOffsetResetNoneEager() throws Exception {     final String topic = "auto-offset-reset-none-test".     final int parallelism = 1.     kafkaServer.createTestTopic(topic, parallelism, 1).     // ----------- add consumer ----------     Properties customProps = new Properties().     customProps.putAll(standardProps).     customProps.putAll(secureProps).     // test that "none" leads to an exception     customProps.setProperty("auto.offset.reset", "none").     try {         kafkaServer.getConsumer(topic, new SimpleStringSchema(), customProps).         fail("should fail with an exception").     } catch (IllegalArgumentException e) {         // expected         assertTrue(e.getMessage().contains("none")).     }     kafkaServer.deleteTestTopic(topic). }
