commented;modifiers;parameterAmount;loc;comment;code
false;public;0;79;;@Test @SuppressWarnings("unchecked") public void testTableSource() {     // prepare parameters for Kafka table source     final TableSchema schema = TableSchema.builder().field(FRUIT_NAME, Types.STRING()).field(COUNT, Types.DECIMAL()).field(EVENT_TIME, Types.SQL_TIMESTAMP()).field(PROC_TIME, Types.SQL_TIMESTAMP()).build().     final List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors = Collections.singletonList(new RowtimeAttributeDescriptor(EVENT_TIME, new ExistingField(TIME), new AscendingTimestamps())).     final Map<String, String> fieldMapping = new HashMap<>().     fieldMapping.put(FRUIT_NAME, NAME).     fieldMapping.put(NAME, NAME).     fieldMapping.put(COUNT, COUNT).     fieldMapping.put(TIME, TIME).     final Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>().     specificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_0), OFFSET_0).     specificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_1), OFFSET_1).     final TestDeserializationSchema deserializationSchema = new TestDeserializationSchema(TableSchema.builder().field(NAME, Types.STRING()).field(COUNT, Types.DECIMAL()).field(TIME, Types.SQL_TIMESTAMP()).build().toRowType()).     final KafkaTableSourceBase expected = getExpectedKafkaTableSource(schema, Optional.of(PROC_TIME), rowtimeAttributeDescriptors, fieldMapping, TOPIC, KAFKA_PROPERTIES, deserializationSchema, StartupMode.SPECIFIC_OFFSETS, specificOffsets).     TableSourceUtil.validateTableSource(expected).     // construct table source using descriptors and table source factory     final TestTableDescriptor testDesc = new TestTableDescriptor(new Kafka().version(getKafkaVersion()).topic(TOPIC).properties(KAFKA_PROPERTIES).sinkPartitionerRoundRobin().startFromSpecificOffsets(OFFSETS)).withFormat(new TestTableFormat()).withSchema(new Schema().field(FRUIT_NAME, Types.STRING()).from(NAME).field(COUNT, // no from so it must match with the input     Types.DECIMAL()).field(EVENT_TIME, Types.SQL_TIMESTAMP()).rowtime(new Rowtime().timestampsFromField(TIME).watermarksPeriodicAscending()).field(PROC_TIME, Types.SQL_TIMESTAMP()).proctime()).inAppendMode().     final Map<String, String> propertiesMap = testDesc.toProperties().     final TableSource<?> actualSource = TableFactoryService.find(StreamTableSourceFactory.class, propertiesMap).createStreamTableSource(propertiesMap).     assertEquals(expected, actualSource).     // test Kafka consumer     final KafkaTableSourceBase actualKafkaSource = (KafkaTableSourceBase) actualSource.     final StreamExecutionEnvironmentMock mock = new StreamExecutionEnvironmentMock().     actualKafkaSource.getDataStream(mock).     assertTrue(getExpectedFlinkKafkaConsumer().isAssignableFrom(mock.sourceFunction.getClass())). }
true;public;0;46;/**  * This test can be unified with the corresponding source test once we have fixed FLINK-9870.  */ ;/**  * This test can be unified with the corresponding source test once we have fixed FLINK-9870.  */ @Test public void testTableSink() {     // prepare parameters for Kafka table sink     final TableSchema schema = TableSchema.builder().field(FRUIT_NAME, Types.STRING()).field(COUNT, Types.DECIMAL()).field(EVENT_TIME, Types.SQL_TIMESTAMP()).build().     final KafkaTableSinkBase expected = getExpectedKafkaTableSink(schema, TOPIC, KAFKA_PROPERTIES, Optional.of(new FlinkFixedPartitioner<>()), new TestSerializationSchema(schema.toRowType())).     // construct table sink using descriptors and table sink factory     final TestTableDescriptor testDesc = new TestTableDescriptor(new Kafka().version(getKafkaVersion()).topic(TOPIC).properties(KAFKA_PROPERTIES).sinkPartitionerFixed().startFromSpecificOffsets(// test if they accepted although not needed     OFFSETS)).withFormat(new TestTableFormat()).withSchema(new Schema().field(FRUIT_NAME, Types.STRING()).field(COUNT, Types.DECIMAL()).field(EVENT_TIME, Types.SQL_TIMESTAMP())).inAppendMode().     final Map<String, String> propertiesMap = testDesc.toProperties().     final TableSink<?> actualSink = TableFactoryService.find(StreamTableSinkFactory.class, propertiesMap).createStreamTableSink(propertiesMap).     assertEquals(expected, actualSink).     // test Kafka producer     final KafkaTableSinkBase actualKafkaSink = (KafkaTableSinkBase) actualSink.     final DataStreamMock streamMock = new DataStreamMock(new StreamExecutionEnvironmentMock(), schema.toRowType()).     actualKafkaSink.emitDataStream(streamMock).     assertTrue(getExpectedFlinkKafkaProducer().isAssignableFrom(streamMock.sinkFunction.getClass())). }
false;public;1;5;;@Override public <OUT> DataStreamSource<OUT> addSource(SourceFunction<OUT> sourceFunction) {     this.sourceFunction = sourceFunction.     return super.addSource(sourceFunction). }
false;public;1;4;;@Override public JobExecutionResult execute(String jobName) {     throw new UnsupportedOperationException(). }
false;public;1;5;;@Override public DataStreamSink<Row> addSink(SinkFunction<Row> sinkFunction) {     this.sinkFunction = sinkFunction.     return super.addSink(sinkFunction). }
false;public;1;4;;@Override public void setChainingStrategy(ChainingStrategy strategy) { // do nothing }
false;public;0;4;;@Override public Collection<StreamTransformation<?>> getTransitivePredecessors() {     return null. }
false;protected,abstract;0;1;;// -------------------------------------------------------------------------------------------- // For version-specific tests // -------------------------------------------------------------------------------------------- protected abstract String getKafkaVersion().
false;protected,abstract;0;1;;protected abstract Class<FlinkKafkaConsumerBase<Row>> getExpectedFlinkKafkaConsumer().
false;protected,abstract;0;1;;protected abstract Class<?> getExpectedFlinkKafkaProducer().
false;protected,abstract;9;10;;protected abstract KafkaTableSourceBase getExpectedKafkaTableSource(TableSchema schema, Optional<String> proctimeAttribute, List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors, Map<String, String> fieldMapping, String topic, Properties properties, DeserializationSchema<Row> deserializationSchema, StartupMode startupMode, Map<KafkaTopicPartition, Long> specificStartupOffsets).
false;protected,abstract;5;6;;protected abstract KafkaTableSinkBase getExpectedKafkaTableSink(TableSchema schema, String topic, Properties properties, Optional<FlinkKafkaPartitioner<Row>> partitioner, SerializationSchema<Row> serializationSchema).
