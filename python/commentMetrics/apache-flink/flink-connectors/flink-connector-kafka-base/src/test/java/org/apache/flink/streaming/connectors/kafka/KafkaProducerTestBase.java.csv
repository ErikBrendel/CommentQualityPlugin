commented;modifiers;parameterAmount;loc;comment;code
false;public;1;11;;@Override public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {     long cnt = 0.     while (running) {         ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt)).         cnt++.         if (cnt % 100 == 0) {             Thread.sleep(1).         }     } }
false;public;0;4;;@Override public void cancel() {     running = false. }
true;public;0;100;/**  * This tests verifies that custom partitioning works correctly, with a default topic  * and dynamic topic. The number of partitions for each topic is deliberately different.  *  * <p>Test topology:  *  * <pre>  *             +------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+  *            /                  |                             |          |        |  *           |                   |                             |          |  ------+--> (sink)  *             +------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+  *            /                  |  *           |                   |  * (source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+  *           |                   |                             |          |        |  *            \                  |                             |          |        |  *             +------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)  *           |                   |                             |          |        |  *            \                  |                             |          |        |  *             +------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+  * </pre>  *  * <p>Each topic has an independent mapper that validates the values come consistently from  * the correct Kafka partition of the topic is is responsible of.  *  * <p>Each topic also has a final sink that validates that there are no duplicates and that all  * partitions are present.  */ ;/**  * This tests verifies that custom partitioning works correctly, with a default topic  * and dynamic topic. The number of partitions for each topic is deliberately different.  *  * <p>Test topology:  *  * <pre>  *             +------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+  *            /                  |                             |          |        |  *           |                   |                             |          |  ------+--> (sink)  *             +------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+  *            /                  |  *           |                   |  * (source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+  *           |                   |                             |          |        |  *            \                  |                             |          |        |  *             +------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)  *           |                   |                             |          |        |  *            \                  |                             |          |        |  *             +------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+  * </pre>  *  * <p>Each topic has an independent mapper that validates the values come consistently from  * the correct Kafka partition of the topic is is responsible of.  *  * <p>Each topic also has a final sink that validates that there are no duplicates and that all  * partitions are present.  */ @Test public void testCustomPartitioning() {     try {         LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()").         final String defaultTopic = "defaultTopic".         final int defaultTopicPartitions = 2.         final String dynamicTopic = "dynamicTopic".         final int dynamicTopicPartitions = 3.         createTestTopic(defaultTopic, defaultTopicPartitions, 1).         createTestTopic(dynamicTopic, dynamicTopicPartitions, 1).         Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2).         expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions).         expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions).         TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInformation.of(new TypeHint<Tuple2<Long, String>>() {         }).         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().         env.setRestartStrategy(RestartStrategies.noRestart()).         env.getConfig().disableSysoutLogging().         TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema = new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig()).         TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema = new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig()).         // ------ producing topology ---------         // source has DOP 1 to make sure it generates no duplicates         DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {              private boolean running = true.              @Override             public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {                 long cnt = 0.                 while (running) {                     ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt)).                     cnt++.                     if (cnt % 100 == 0) {                         Thread.sleep(1).                     }                 }             }              @Override             public void cancel() {                 running = false.             }         }).setParallelism(1).         Properties props = new Properties().         props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)).         props.putAll(secureProps).         // sink partitions into         kafkaServer.produceIntoKafka(stream, defaultTopic, // this serialization schema will route between the default topic and dynamic topic         new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic), props, new CustomPartitioner(expectedTopicsToNumPartitions)).setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions)).         // ------ consuming topology ---------         Properties consumerProps = new Properties().         consumerProps.putAll(standardProps).         consumerProps.putAll(secureProps).         FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource = kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps).         FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource = kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps).         env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions).map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions).addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1).         env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions).map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions).addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1).         tryExecute(env, "custom partitioning test").         deleteTestTopic(defaultTopic).         deleteTestTopic(dynamicTopic).         LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()").     } catch (Exception e) {         e.printStackTrace().         fail(e.getMessage()).     } }
true;public;0;4;/**  * Tests the at-least-once semantic for the simple writes into Kafka.  */ ;/**  * Tests the at-least-once semantic for the simple writes into Kafka.  */ @Test public void testOneToOneAtLeastOnceRegularSink() throws Exception {     testOneToOneAtLeastOnce(true). }
true;public;0;4;/**  * Tests the at-least-once semantic for the simple writes into Kafka.  */ ;/**  * Tests the at-least-once semantic for the simple writes into Kafka.  */ @Test public void testOneToOneAtLeastOnceCustomOperator() throws Exception {     testOneToOneAtLeastOnce(false). }
false;public;5;4;;@Override public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {     return partition. }
false;public;5;4;;@Override public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {     return partition. }
true;protected;1;74;/**  * This test sets KafkaProducer so that it will not automatically flush the data and  * simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer  * flushed records manually on snapshotState.  *  * <p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The  * parameter controls which method is used.  */ ;/**  * This test sets KafkaProducer so that it will not automatically flush the data and  * simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer  * flushed records manually on snapshotState.  *  * <p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The  * parameter controls which method is used.  */ protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {     final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator".     final int partition = 0.     final int numElements = 1000.     final int failAfterElements = 333.     createTestTopic(topic, 1, 1).     TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig()).     KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema).     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.enableCheckpointing(500).     env.setParallelism(1).     env.setRestartStrategy(RestartStrategies.noRestart()).     env.getConfig().disableSysoutLogging().     Properties properties = new Properties().     properties.putAll(standardProps).     properties.putAll(secureProps).     // decrease timeout and block time from 60s down to 10s - this is how long KafkaProducer will try send pending (not flushed) data on close()     properties.setProperty("timeout.ms", "10000").     properties.setProperty("max.block.ms", "10000").     // increase batch.size and linger.ms - this tells KafkaProducer to batch produced events instead of flushing them immediately     properties.setProperty("batch.size", "10240000").     properties.setProperty("linger.ms", "10000").     BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic).     // process exactly failAfterElements number of elements and then shutdown Kafka broker and fail application     DataStream<Integer> inputStream = env.fromCollection(getIntegersSequence(numElements)).map(new BrokerRestartingMapper<>(failAfterElements)).     StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {          @Override         public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {             return partition.         }     }).     if (regularSink) {         inputStream.addSink(kafkaSink.getUserFunction()).     } else {         kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {              @Override             public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {                 return partition.             }         }).     }     FailingIdentityMapper.failedBefore = false.     try {         env.execute("One-to-one at least once test").         fail("Job should fail!").     } catch (JobExecutionException ex) {     // ignore error, it can be one of many errors so it would be hard to check the exception message/cause     }     kafkaServer.unblockProxyTraffic().     // assert that before failure we successfully snapshot/flushed all expected elements     assertAtLeastOnceForTopic(properties, topic, partition, Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.lastSnapshotedElementBeforeShutdown))), KAFKA_READ_TIMEOUT).     deleteTestTopic(topic). }
true;public;0;4;/**  * Tests the exactly-once semantic for the simple writes into Kafka.  */ ;/**  * Tests the exactly-once semantic for the simple writes into Kafka.  */ @Test public void testExactlyOnceRegularSink() throws Exception {     testExactlyOnce(true, 1). }
true;public;0;4;/**  * Tests the exactly-once semantic for the simple writes into Kafka.  */ ;/**  * Tests the exactly-once semantic for the simple writes into Kafka.  */ @Test public void testExactlyOnceCustomOperator() throws Exception {     testExactlyOnce(false, 1). }
false;public;5;4;;@Override public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {     return partition. }
true;protected;2;60;/**  * This test sets KafkaProducer so that it will  automatically flush the data and  * and fails the broker to check whether flushed records since last checkpoint were not duplicated.  */ ;/**  * This test sets KafkaProducer so that it will  automatically flush the data and  * and fails the broker to check whether flushed records since last checkpoint were not duplicated.  */ protected void testExactlyOnce(boolean regularSink, int sinksCount) throws Exception {     final String topic = (regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator") + sinksCount.     final int partition = 0.     final int numElements = 1000.     final int failAfterElements = 333.     for (int i = 0. i < sinksCount. i++) {         createTestTopic(topic + i, 1, 1).     }     TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig()).     KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper<>(schema).     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.enableCheckpointing(500).     env.setParallelism(1).     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0)).     env.getConfig().disableSysoutLogging().     Properties properties = new Properties().     properties.putAll(standardProps).     properties.putAll(secureProps).     // process exactly failAfterElements number of elements and then shutdown Kafka broker and fail application     List<Integer> expectedElements = getIntegersSequence(numElements).     DataStream<Integer> inputStream = env.addSource(new IntegerSource(numElements)).map(new FailingIdentityMapper<Integer>(failAfterElements)).     for (int i = 0. i < sinksCount. i++) {         FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {              @Override             public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {                 return partition.             }         }.         if (regularSink) {             StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic + i, keyedSerializationSchema, properties, partitioner).             inputStream.addSink(kafkaSink.getUserFunction()).         } else {             kafkaServer.produceIntoKafka(inputStream, topic + i, keyedSerializationSchema, properties, partitioner).         }     }     FailingIdentityMapper.failedBefore = false.     TestUtils.tryExecute(env, "Exactly once test").     for (int i = 0. i < sinksCount. i++) {         // assert that before failure we successfully snapshot/flushed all expected elements         assertExactlyOnceForTopic(properties, topic + i, partition, expectedElements, KAFKA_READ_TIMEOUT).         deleteTestTopic(topic + i).     } }
false;private;1;7;;private List<Integer> getIntegersSequence(int size) {     List<Integer> result = new ArrayList<>(size).     for (int i = 0. i < size. i++) {         result.add(i).     }     return result. }
false;public;5;6;;@Override public int partition(Tuple2<Long, String> next, byte[] serializedKey, byte[] serializedValue, String topic, int[] partitions) {     assertEquals(expectedTopicsToNumPartitions.get(topic).intValue(), partitions.length).     return (int) (next.f0 % partitions.length). }
false;public;1;4;;@Override public String getTargetTopic(Tuple2<Long, String> element) {     return (element.f0 % 2 == 0) ? defaultTopic : dynamicTopic. }
false;public;1;10;;@Override public Integer map(Tuple2<Long, String> value) throws Exception {     int partition = value.f0.intValue() % numPartitions.     if (ourPartition != -1) {         assertEquals("inconsistent partitioning", ourPartition, partition).     } else {         ourPartition = partition.     }     return partition. }
false;public;1;15;;@Override public void invoke(Integer value) throws Exception {     valuesPerPartition[value]++.     boolean missing = false.     for (int i : valuesPerPartition) {         if (i < 100) {             missing = true.             break.         }     }     if (!missing) {         throw new SuccessException().     } }
false;public,static;1;5;;public static void resetState(Runnable shutdownAction) {     triggeredShutdown = false.     lastSnapshotedElementBeforeShutdown = 0.     BrokerRestartingMapper.shutdownAction = shutdownAction. }
false;public;1;4;;@Override public void open(Configuration parameters) {     failer = getRuntimeContext().getIndexOfThisSubtask() == 0. }
false;public;1;12;;@Override public T map(T value) throws Exception {     numElementsTotal++.     Thread.sleep(10).     if (!triggeredShutdown && failer && numElementsTotal >= failCount) {         // shut down a Kafka broker         triggeredShutdown = true.         shutdownAction.run().     }     return value. }
false;public;1;3;;@Override public void notifyCheckpointComplete(long checkpointId) { }
false;public;1;6;;@Override public void snapshotState(FunctionSnapshotContext context) throws Exception {     if (!triggeredShutdown) {         lastSnapshotedElementBeforeShutdown = numElementsTotal.     } }
false;public;1;3;;@Override public void initializeState(FunctionInitializationContext context) throws Exception { }
