# id;timestamp;commentText;codeText;commentWords;codeWords
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1480685315;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1487173364;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1488214488;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1488217628;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1489510697;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1492569128;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1494830990;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1495923077;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1498894422;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception;1501249949;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredNullCheckpointWhenFetcherNotReady() throws Exception {_		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		StreamingRuntimeContext runtimeContext = mock(StreamingRuntimeContext.class)__		when(runtimeContext.getIndexOfThisSubtask()).thenReturn(0)__		when(runtimeContext.getNumberOfParallelSubtasks()).thenReturn(1)__		consumer.setRuntimeContext(runtimeContext)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(false)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,null,checkpoint,when,fetcher,not,ready,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,streaming,runtime,context,runtime,context,mock,streaming,runtime,context,class,when,runtime,context,get,index,of,this,subtask,then,return,0,when,runtime,context,get,number,of,parallel,subtasks,then,return,1,consumer,set,runtime,context,runtime,context,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	@Test 	public void checkUseFetcherWhenNoCheckpoint() throws Exception;1489510697;Tests that on snapshots, states and offsets to commit to Kafka are correct;@SuppressWarnings("unchecked")_	@Test_	public void checkUseFetcherWhenNoCheckpoint() throws Exception {__		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		List<KafkaTopicPartition> partitionList = new ArrayList<>(1)__		partitionList.add(new KafkaTopicPartition("test", 0))__		consumer.setSubscribedPartitions(partitionList)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)___		_		when(initializationContext.isRestored()).thenReturn(false)__		consumer.initializeState(initializationContext)__		consumer.run(mock(SourceFunction.SourceContext.class))__	};tests,that,on,snapshots,states,and,offsets,to,commit,to,kafka,are,correct;suppress,warnings,unchecked,test,public,void,check,use,fetcher,when,no,checkpoint,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,list,kafka,topic,partition,partition,list,new,array,list,1,partition,list,add,new,kafka,topic,partition,test,0,consumer,set,subscribed,partitions,partition,list,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,run,mock,source,function,source,context,class
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	@Test 	public void checkUseFetcherWhenNoCheckpoint() throws Exception;1492569128;Tests that on snapshots, states and offsets to commit to Kafka are correct;@SuppressWarnings("unchecked")_	@Test_	public void checkUseFetcherWhenNoCheckpoint() throws Exception {__		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		List<KafkaTopicPartition> partitionList = new ArrayList<>(1)__		partitionList.add(new KafkaTopicPartition("test", 0))__		consumer.setSubscribedPartitions(partitionList)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)___		_		when(initializationContext.isRestored()).thenReturn(false)__		consumer.initializeState(initializationContext)__		consumer.run(mock(SourceFunction.SourceContext.class))__	};tests,that,on,snapshots,states,and,offsets,to,commit,to,kafka,are,correct;suppress,warnings,unchecked,test,public,void,check,use,fetcher,when,no,checkpoint,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,list,kafka,topic,partition,partition,list,new,array,list,1,partition,list,add,new,kafka,topic,partition,test,0,consumer,set,subscribed,partitions,partition,list,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,run,mock,source,function,source,context,class
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	@Test 	public void checkUseFetcherWhenNoCheckpoint() throws Exception;1494830990;Tests that on snapshots, states and offsets to commit to Kafka are correct;@SuppressWarnings("unchecked")_	@Test_	public void checkUseFetcherWhenNoCheckpoint() throws Exception {__		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		List<KafkaTopicPartition> partitionList = new ArrayList<>(1)__		partitionList.add(new KafkaTopicPartition("test", 0))__		consumer.setSubscribedPartitions(partitionList)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)___		_		when(initializationContext.isRestored()).thenReturn(false)__		consumer.initializeState(initializationContext)__		consumer.run(mock(SourceFunction.SourceContext.class))__	};tests,that,on,snapshots,states,and,offsets,to,commit,to,kafka,are,correct;suppress,warnings,unchecked,test,public,void,check,use,fetcher,when,no,checkpoint,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,list,kafka,topic,partition,partition,list,new,array,list,1,partition,list,add,new,kafka,topic,partition,test,0,consumer,set,subscribed,partitions,partition,list,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,run,mock,source,function,source,context,class
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	@Test 	public void checkUseFetcherWhenNoCheckpoint() throws Exception;1495923077;Tests that on snapshots, states and offsets to commit to Kafka are correct.;@SuppressWarnings("unchecked")_	@Test_	public void checkUseFetcherWhenNoCheckpoint() throws Exception {__		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		List<KafkaTopicPartition> partitionList = new ArrayList<>(1)__		partitionList.add(new KafkaTopicPartition("test", 0))__		consumer.setSubscribedPartitions(partitionList)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)___		_		when(initializationContext.isRestored()).thenReturn(false)__		consumer.initializeState(initializationContext)__		consumer.run(mock(SourceFunction.SourceContext.class))__	};tests,that,on,snapshots,states,and,offsets,to,commit,to,kafka,are,correct;suppress,warnings,unchecked,test,public,void,check,use,fetcher,when,no,checkpoint,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,list,kafka,topic,partition,partition,list,new,array,list,1,partition,list,add,new,kafka,topic,partition,test,0,consumer,set,subscribed,partitions,partition,list,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,run,mock,source,function,source,context,class
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1515757408;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1515757408;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1516359916;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1516626397;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1517943538;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1518008821;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1518852834;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1519567828;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1519973085;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1526978549;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1526978550;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1534244814;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1547725934;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeEmptyState()__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,empty,state,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1548931626;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeEmptyState()__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,empty,state,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1548931627;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeEmptyState()__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,empty,state,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1548931627;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeEmptyState()__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,empty,state,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1548931628;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeEmptyState()__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,empty,state,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1550834396;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeEmptyState()__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,empty,state,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	private void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1550863152;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	private void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				initialParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup))___			consumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeEmptyState()__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			OperatorSubtaskState initState = AbstractStreamOperatorTestHarness.repartitionOperatorState(_				mergedState, maxParallelism, initialParallelism, restoredParallelism, i)___			TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(_				new KafkaTopicsDescriptor(Collections.singletonList("test-topic"), null),_				i,_				restoredParallelism,_				TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(Collections.singletonList("test-topic")),_				TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore))___			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(mock(AbstractFetcher.class), partitionDiscoverer, false)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(initState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,private,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,initial,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,on,startup,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,empty,state,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,subtask,state,state,new,operator,subtask,state,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,subtask,state,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,operator,subtask,state,init,state,abstract,stream,operator,test,harness,repartition,operator,state,merged,state,max,parallelism,initial,parallelism,restored,parallelism,i,test,partition,discoverer,partition,discoverer,new,test,partition,discoverer,new,kafka,topics,descriptor,collections,singleton,list,test,topic,null,i,restored,parallelism,test,partition,discoverer,create,mock,get,all,topics,sequence,from,fixed,return,collections,singleton,list,test,topic,test,partition,discoverer,create,mock,get,all,partitions,from,topics,sequence,from,fixed,return,mock,fetched,partitions,after,restore,restored,consumers,i,new,dummy,flink,kafka,consumer,mock,abstract,fetcher,class,partition,discoverer,false,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,init,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1480685315;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}_		_		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)__		_		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1487173364;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}_		_		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)__		_		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1488214488;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}_		_		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)__		_		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1488217628;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}_		_		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)__		_		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1489510697;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}_		_		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)__		_		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1492569128;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}_		_		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)__		_		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1494830990;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}_		_		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)__		_		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1495923077;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1498894422;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1501249949;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1501249950;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1501249950;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1501249950;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1511347989;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1515213257;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void testEitherWatermarkExtractor();1515598528;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<Object>) null)__			fail()__		} catch (NullPointerException ignored) {}__		@SuppressWarnings("unchecked")_		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		@SuppressWarnings("unchecked")_		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,object,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,object,null,fail,catch,null,pointer,exception,ignored,suppress,warnings,unchecked,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,suppress,warnings,unchecked,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1480685315;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1487173364;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1488214488;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1488217628;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1489510697;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1492569128;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1494830990;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1495923077;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)___		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1498894422;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class)__		when(context.getNumberOfParallelSubtasks()).thenReturn(1)__		when(context.getIndexOfThisSubtask()).thenReturn(0)__		consumer.setRuntimeContext(context)___		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,streaming,runtime,context,context,mock,streaming,runtime,context,class,when,context,get,number,of,parallel,subtasks,then,return,1,when,context,get,index,of,this,subtask,then,return,0,consumer,set,runtime,context,context,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1501249949;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> listState = new TestingListState<>()__		listState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		listState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class)__		when(context.getNumberOfParallelSubtasks()).thenReturn(1)__		when(context.getIndexOfThisSubtask()).thenReturn(0)__		consumer.setRuntimeContext(context)___		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(listState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : listState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : listState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,streaming,runtime,context,context,mock,streaming,runtime,context,class,when,context,get,number,of,parallel,subtasks,then,return,1,when,context,get,index,of,this,subtask,then,return,0,consumer,set,runtime,context,context,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1501249950;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> restoredListState = new TestingListState<>()__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class)__		when(context.getNumberOfParallelSubtasks()).thenReturn(1)__		when(context.getIndexOfThisSubtask()).thenReturn(0)__		consumer.setRuntimeContext(context)___		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(restoredListState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,restored,list,state,new,testing,list,state,restored,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,restored,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,streaming,runtime,context,context,mock,streaming,runtime,context,class,when,context,get,number,of,parallel,subtasks,then,return,1,when,context,get,index,of,this,subtask,then,return,0,consumer,set,runtime,context,context,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,restored,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1501249950;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> restoredListState = new TestingListState<>()__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class)__		when(context.getNumberOfParallelSubtasks()).thenReturn(1)__		when(context.getIndexOfThisSubtask()).thenReturn(0)__		consumer.setRuntimeContext(context)___		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(restoredListState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,restored,list,state,new,testing,list,state,restored,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,restored,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,streaming,runtime,context,context,mock,streaming,runtime,context,class,when,context,get,number,of,parallel,subtasks,then,return,1,when,context,get,index,of,this,subtask,then,return,0,consumer,set,runtime,context,context,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,restored,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1501249950;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> restoredListState = new TestingListState<>()__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class)__		when(context.getNumberOfParallelSubtasks()).thenReturn(1)__		when(context.getIndexOfThisSubtask()).thenReturn(0)__		consumer.setRuntimeContext(context)___		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(restoredListState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,restored,list,state,new,testing,list,state,restored,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,restored,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,streaming,runtime,context,context,mock,streaming,runtime,context,class,when,context,get,number,of,parallel,subtasks,then,return,1,when,context,get,index,of,this,subtask,then,return,0,consumer,set,runtime,context,context,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,restored,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1511347989;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> restoredListState = new TestingListState<>()__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class)__		when(context.getNumberOfParallelSubtasks()).thenReturn(1)__		when(context.getIndexOfThisSubtask()).thenReturn(0)__		consumer.setRuntimeContext(context)___		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(restoredListState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,restored,list,state,new,testing,list,state,restored,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,restored,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,streaming,runtime,context,context,mock,streaming,runtime,context,class,when,context,get,number,of,parallel,subtasks,then,return,1,when,context,get,index,of,this,subtask,then,return,0,consumer,set,runtime,context,context,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,restored,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1515213257;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> restoredListState = new TestingListState<>()__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class)__		when(context.getNumberOfParallelSubtasks()).thenReturn(1)__		when(context.getIndexOfThisSubtask()).thenReturn(0)__		consumer.setRuntimeContext(context)___		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(restoredListState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,restored,list,state,new,testing,list,state,restored,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,restored,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,streaming,runtime,context,context,mock,streaming,runtime,context,class,when,context,get,number,of,parallel,subtasks,then,return,1,when,context,get,index,of,this,subtask,then,return,0,consumer,set,runtime,context,context,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,restored,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1515598528;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)___		TestingListState<Serializable> restoredListState = new TestingListState<>()__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("abc", 13), 16768L))__		restoredListState.add(Tuple2.of(new KafkaTopicPartition("def", 7), 987654321L))___		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class)__		when(context.getNumberOfParallelSubtasks()).thenReturn(1)__		when(context.getIndexOfThisSubtask()).thenReturn(0)__		consumer.setRuntimeContext(context)___		_		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(new TestingListState<Serializable>())__		_		when(operatorStateStore.getUnionListState(Matchers.any(ListStateDescriptor.class))).thenReturn(restoredListState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)__		when(initializationContext.isRestored()).thenReturn(true)___		consumer.initializeState(initializationContext)___		consumer.open(new Configuration())___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,restored,list,state,new,testing,list,state,restored,list,state,add,tuple2,of,new,kafka,topic,partition,abc,13,16768l,restored,list,state,add,tuple2,of,new,kafka,topic,partition,def,7,987654321l,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,streaming,runtime,context,context,mock,streaming,runtime,context,class,when,context,get,number,of,parallel,subtasks,then,return,1,when,context,get,index,of,this,subtask,then,return,0,consumer,set,runtime,context,context,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,new,testing,list,state,serializable,when,operator,state,store,get,union,list,state,matchers,any,list,state,descriptor,class,then,return,restored,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,true,consumer,initialize,state,initialization,context,consumer,open,new,configuration,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1515757408;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1515757408;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1516359916;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1516626397;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1517943538;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1518008821;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1518852834;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1519567828;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1519973085;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1526978549;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1526978550;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1534244814;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1547725934;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1548931626;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1548931627;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1548931627;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1548931628;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1550834396;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception;1550863152;Tests that when taking a checkpoint when the fetcher is not running yet,_the checkpoint correctly contains the restored state instead.;@Test_	public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>()__		setupConsumer(consumer, true, restoredListState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17))___		_		_		Assert.assertTrue(restoredListState.isClearCalled())___		Set<Serializable> expected = new HashSet<>()___		for (Serializable serializable : restoredListState.get()) {_			expected.add(serializable)__		}__		int counter = 0___		for (Serializable serializable : restoredListState.get()) {_			assertTrue(expected.contains(serializable))__			counter++__		}__		assertEquals(expected.size(), counter)__	};tests,that,when,taking,a,checkpoint,when,the,fetcher,is,not,running,yet,the,checkpoint,correctly,contains,the,restored,state,instead;test,public,void,check,restored,checkpoint,when,fetcher,not,ready,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,restored,list,state,new,testing,list,state,setup,consumer,consumer,true,restored,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,17,17,assert,assert,true,restored,list,state,is,clear,called,set,serializable,expected,new,hash,set,for,serializable,serializable,restored,list,state,get,expected,add,serializable,int,counter,0,for,serializable,serializable,restored,list,state,get,assert,true,expected,contains,serializable,counter,assert,equals,expected,size,counter
FlinkKafkaConsumerBaseTest -> @Test 	public void checkUseFetcherWhenNoCheckpoint() throws Exception;1480685315;Tests that on snapshots, states and offsets to commit to Kafka are correct;@Test_	public void checkUseFetcherWhenNoCheckpoint() throws Exception {__		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		List<KafkaTopicPartition> partitionList = new ArrayList<>(1)__		partitionList.add(new KafkaTopicPartition("test", 0))__		consumer.setSubscribedPartitions(partitionList)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)___		_		when(initializationContext.isRestored()).thenReturn(false)__		consumer.initializeState(initializationContext)__		consumer.run(mock(SourceFunction.SourceContext.class))__	};tests,that,on,snapshots,states,and,offsets,to,commit,to,kafka,are,correct;test,public,void,check,use,fetcher,when,no,checkpoint,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,list,kafka,topic,partition,partition,list,new,array,list,1,partition,list,add,new,kafka,topic,partition,test,0,consumer,set,subscribed,partitions,partition,list,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,run,mock,source,function,source,context,class
FlinkKafkaConsumerBaseTest -> @Test 	public void checkUseFetcherWhenNoCheckpoint() throws Exception;1487173364;Tests that on snapshots, states and offsets to commit to Kafka are correct;@Test_	public void checkUseFetcherWhenNoCheckpoint() throws Exception {__		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		List<KafkaTopicPartition> partitionList = new ArrayList<>(1)__		partitionList.add(new KafkaTopicPartition("test", 0))__		consumer.setSubscribedPartitions(partitionList)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)___		_		when(initializationContext.isRestored()).thenReturn(false)__		consumer.initializeState(initializationContext)__		consumer.run(mock(SourceFunction.SourceContext.class))__	};tests,that,on,snapshots,states,and,offsets,to,commit,to,kafka,are,correct;test,public,void,check,use,fetcher,when,no,checkpoint,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,list,kafka,topic,partition,partition,list,new,array,list,1,partition,list,add,new,kafka,topic,partition,test,0,consumer,set,subscribed,partitions,partition,list,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,run,mock,source,function,source,context,class
FlinkKafkaConsumerBaseTest -> @Test 	public void checkUseFetcherWhenNoCheckpoint() throws Exception;1488214488;Tests that on snapshots, states and offsets to commit to Kafka are correct;@Test_	public void checkUseFetcherWhenNoCheckpoint() throws Exception {__		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		List<KafkaTopicPartition> partitionList = new ArrayList<>(1)__		partitionList.add(new KafkaTopicPartition("test", 0))__		consumer.setSubscribedPartitions(partitionList)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)___		_		when(initializationContext.isRestored()).thenReturn(false)__		consumer.initializeState(initializationContext)__		consumer.run(mock(SourceFunction.SourceContext.class))__	};tests,that,on,snapshots,states,and,offsets,to,commit,to,kafka,are,correct;test,public,void,check,use,fetcher,when,no,checkpoint,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,list,kafka,topic,partition,partition,list,new,array,list,1,partition,list,add,new,kafka,topic,partition,test,0,consumer,set,subscribed,partitions,partition,list,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,run,mock,source,function,source,context,class
FlinkKafkaConsumerBaseTest -> @Test 	public void checkUseFetcherWhenNoCheckpoint() throws Exception;1488217628;Tests that on snapshots, states and offsets to commit to Kafka are correct;@Test_	public void checkUseFetcherWhenNoCheckpoint() throws Exception {__		FlinkKafkaConsumerBase<String> consumer = getConsumer(null, new LinkedMap(), true)__		List<KafkaTopicPartition> partitionList = new ArrayList<>(1)__		partitionList.add(new KafkaTopicPartition("test", 0))__		consumer.setSubscribedPartitions(partitionList)___		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Serializable> listState = new TestingListState<>()__		when(operatorStateStore.getSerializableListState(Matchers.any(String.class))).thenReturn(listState)___		StateInitializationContext initializationContext = mock(StateInitializationContext.class)___		when(initializationContext.getOperatorStateStore()).thenReturn(operatorStateStore)___		_		when(initializationContext.isRestored()).thenReturn(false)__		consumer.initializeState(initializationContext)__		consumer.run(mock(SourceFunction.SourceContext.class))__	};tests,that,on,snapshots,states,and,offsets,to,commit,to,kafka,are,correct;test,public,void,check,use,fetcher,when,no,checkpoint,throws,exception,flink,kafka,consumer,base,string,consumer,get,consumer,null,new,linked,map,true,list,kafka,topic,partition,partition,list,new,array,list,1,partition,list,add,new,kafka,topic,partition,test,0,consumer,set,subscribed,partitions,partition,list,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,serializable,list,state,new,testing,list,state,when,operator,state,store,get,serializable,list,state,matchers,any,string,class,then,return,list,state,state,initialization,context,initialization,context,mock,state,initialization,context,class,when,initialization,context,get,operator,state,store,then,return,operator,state,store,when,initialization,context,is,restored,then,return,false,consumer,initialize,state,initialization,context,consumer,run,mock,source,function,source,context,class
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1501249949;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			consumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsOnStartup)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsAfterRestore)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,on,startup,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,restored,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,after,restore,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1501249950;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			consumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsOnStartup)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsAfterRestore)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,on,startup,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,restored,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,after,restore,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1501249950;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			consumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsOnStartup)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsAfterRestore)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,on,startup,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,restored,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,after,restore,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1501249950;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			consumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsOnStartup)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsAfterRestore)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,on,startup,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,restored,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,after,restore,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1511347989;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			consumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsOnStartup)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsAfterRestore)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,on,startup,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,restored,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,after,restore,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1515213257;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			consumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsOnStartup)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsAfterRestore)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,on,startup,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,restored,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,after,restore,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @SuppressWarnings("unchecked") 	void testRescaling( 		final int initialParallelism, 		final int numPartitions, 		final int restoredParallelism, 		final int restoredNumPartitions) throws Exception;1515598528;Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,_which means that operator state is being reshuffled.__<p>This also verifies that a restoring source is always impervious to changes in the list_of topics fetched from Kafka.;@SuppressWarnings("unchecked")_	void testRescaling(_		final int initialParallelism,_		final int numPartitions,_		final int restoredParallelism,_		final int restoredNumPartitions) throws Exception {__		Preconditions.checkArgument(_			restoredNumPartitions >= numPartitions,_			"invalid test case for Kafka repartitioning_ Kafka only allows increasing partitions.")___		List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>()__		for (int i = 0_ i < numPartitions_ i++) {_			mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] consumers =_			new DummyFlinkKafkaConsumer[initialParallelism]___		AbstractStreamOperatorTestHarness<String>[] testHarnesses =_			new AbstractStreamOperatorTestHarness[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			consumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsOnStartup)__			testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i)___			_			testHarnesses[i].initializeState(null)__			testHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < initialParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				consumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(globalSubscribedPartitions, not(hasKey(partition)))__			}_			globalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet())))___		OperatorStateHandles[] state = new OperatorStateHandles[initialParallelism]___		for (int i = 0_ i < initialParallelism_ i++) {_			state[i] = testHarnesses[i].snapshot(0, 0)__		}__		OperatorStateHandles mergedState = AbstractStreamOperatorTestHarness.repackageState(state)___		_		__		List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>()__		for (int i = 0_ i < restoredNumPartitions_ i++) {_			mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i))__		}__		DummyFlinkKafkaConsumer<String>[] restoredConsumers =_			new DummyFlinkKafkaConsumer[restoredParallelism]___		AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses =_			new AbstractStreamOperatorTestHarness[restoredParallelism]___		for (int i = 0_ i < restoredParallelism_ i++) {_			restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(_				Collections.singletonList("test-topic"), mockFetchedPartitionsAfterRestore)__			restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i)___			_			restoredTestHarnesses[i].initializeState(mergedState)__			restoredTestHarnesses[i].open()__		}__		Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>()___		for (int i = 0_ i < restoredParallelism_ i++) {_			Map<KafkaTopicPartition, Long> subscribedPartitions =_				restoredConsumers[i].getSubscribedPartitionsToStartOffsets()___			_			for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {_				assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition)))__			}_			restoredGlobalSubscribedPartitions.putAll(subscribedPartitions)__		}__		assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions))__		assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet())))__	};tests,whether,the,kafka,consumer,behaves,correctly,when,scaling,the,parallelism,up,down,which,means,that,operator,state,is,being,reshuffled,p,this,also,verifies,that,a,restoring,source,is,always,impervious,to,changes,in,the,list,of,topics,fetched,from,kafka;suppress,warnings,unchecked,void,test,rescaling,final,int,initial,parallelism,final,int,num,partitions,final,int,restored,parallelism,final,int,restored,num,partitions,throws,exception,preconditions,check,argument,restored,num,partitions,num,partitions,invalid,test,case,for,kafka,repartitioning,kafka,only,allows,increasing,partitions,list,kafka,topic,partition,mock,fetched,partitions,on,startup,new,array,list,for,int,i,0,i,num,partitions,i,mock,fetched,partitions,on,startup,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,consumers,new,dummy,flink,kafka,consumer,initial,parallelism,abstract,stream,operator,test,harness,string,test,harnesses,new,abstract,stream,operator,test,harness,initial,parallelism,for,int,i,0,i,initial,parallelism,i,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,on,startup,test,harnesses,i,create,test,harness,consumers,i,initial,parallelism,i,test,harnesses,i,initialize,state,null,test,harnesses,i,open,map,kafka,topic,partition,long,global,subscribed,partitions,new,hash,map,for,int,i,0,i,initial,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,global,subscribed,partitions,not,has,key,partition,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,global,subscribed,partitions,values,has,size,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,global,subscribed,partitions,key,set,operator,state,handles,state,new,operator,state,handles,initial,parallelism,for,int,i,0,i,initial,parallelism,i,state,i,test,harnesses,i,snapshot,0,0,operator,state,handles,merged,state,abstract,stream,operator,test,harness,repackage,state,state,list,kafka,topic,partition,mock,fetched,partitions,after,restore,new,array,list,for,int,i,0,i,restored,num,partitions,i,mock,fetched,partitions,after,restore,add,new,kafka,topic,partition,test,topic,i,dummy,flink,kafka,consumer,string,restored,consumers,new,dummy,flink,kafka,consumer,restored,parallelism,abstract,stream,operator,test,harness,string,restored,test,harnesses,new,abstract,stream,operator,test,harness,restored,parallelism,for,int,i,0,i,restored,parallelism,i,restored,consumers,i,new,dummy,flink,kafka,consumer,collections,singleton,list,test,topic,mock,fetched,partitions,after,restore,restored,test,harnesses,i,create,test,harness,restored,consumers,i,restored,parallelism,i,restored,test,harnesses,i,initialize,state,merged,state,restored,test,harnesses,i,open,map,kafka,topic,partition,long,restored,global,subscribed,partitions,new,hash,map,for,int,i,0,i,restored,parallelism,i,map,kafka,topic,partition,long,subscribed,partitions,restored,consumers,i,get,subscribed,partitions,to,start,offsets,for,kafka,topic,partition,partition,subscribed,partitions,key,set,assert,that,restored,global,subscribed,partitions,not,has,key,partition,restored,global,subscribed,partitions,put,all,subscribed,partitions,assert,that,restored,global,subscribed,partitions,values,has,size,restored,num,partitions,assert,that,mock,fetched,partitions,on,startup,every,item,is,in,restored,global,subscribed,partitions,key,set
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1480685315;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getOperatorState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,operator,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1487173364;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getOperatorState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,operator,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1488214488;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getOperatorState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,operator,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1488217628;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getOperatorState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,operator,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1489510697;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getOperatorState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,operator,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1492569128;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1494830990;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1495923077;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1498894422;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1501249949;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1501249950;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1501249950;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1501249950;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1511347989;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1515213257;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1515598528;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final AbstractFetcher<String, ?> fetcher = mock(AbstractFetcher.class)___		FlinkKafkaConsumerBase<String> consumer = getConsumer(fetcher, new LinkedMap(), false)__		OperatorStateStore operatorStateStore = mock(OperatorStateStore.class)__		TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		when(operatorStateStore.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState)___		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		assertFalse(listState.get().iterator().hasNext())__		consumer.notifyCheckpointComplete(66L)__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,abstract,fetcher,string,fetcher,mock,abstract,fetcher,class,flink,kafka,consumer,base,string,consumer,get,consumer,fetcher,new,linked,map,false,operator,state,store,operator,state,store,mock,operator,state,store,class,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,when,operator,state,store,get,list,state,matchers,any,list,state,descriptor,class,then,return,list,state,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,66l
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1515757408;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>()___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1515757408;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1516359916;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1516626397;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1517943538;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1518008821;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1518852834;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1519567828;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1519973085;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1526978549;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1526978550;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1534244814;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1547725934;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1548931626;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1548931627;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1548931627;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1548931628;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1550834396;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	public void ignoreCheckpointWhenNotRunning() throws Exception;1550863152;Tests that no checkpoints happen when the fetcher is not running.;@Test_	public void ignoreCheckpointWhenNotRunning() throws Exception {_		@SuppressWarnings("unchecked")_		final MockFetcher<String> fetcher = new MockFetcher<>()__		final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(_				fetcher,_				mock(AbstractPartitionDiscoverer.class),_				false)___		final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>()__		setupConsumer(consumer, false, listState, true, 0, 1)___		_		consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1))___		_		assertFalse(listState.get().iterator().hasNext())___		_		consumer.notifyCheckpointComplete(1L)__		assertNull(fetcher.getAndClearLastCommittedOffsets())__		assertEquals(0, fetcher.getCommitCount())__	};tests,that,no,checkpoints,happen,when,the,fetcher,is,not,running;test,public,void,ignore,checkpoint,when,not,running,throws,exception,suppress,warnings,unchecked,final,mock,fetcher,string,fetcher,new,mock,fetcher,final,flink,kafka,consumer,base,string,consumer,new,dummy,flink,kafka,consumer,fetcher,mock,abstract,partition,discoverer,class,false,final,testing,list,state,tuple2,kafka,topic,partition,long,list,state,new,testing,list,state,setup,consumer,consumer,false,list,state,true,0,1,consumer,snapshot,state,new,state,snapshot,context,synchronous,impl,1,1,assert,false,list,state,get,iterator,has,next,consumer,notify,checkpoint,complete,1l,assert,null,fetcher,get,and,clear,last,committed,offsets,assert,equals,0,fetcher,get,commit,count
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1515757408;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1515757408;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1516359916;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1516626397;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1517943538;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1518008821;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1518852834;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1519567828;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1519973085;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1526978549;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1526978550;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1534244814;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1547725934;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1548931626;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1548931627;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1548931627;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1548931628;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1550834396;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
FlinkKafkaConsumerBaseTest -> @Test 	@SuppressWarnings("unchecked") 	public void testEitherWatermarkExtractor();1550863152;Tests that not both types of timestamp extractors / watermark generators can be used.;@Test_	@SuppressWarnings("unchecked")_	public void testEitherWatermarkExtractor() {_		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		try {_			new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null)__			fail()__		} catch (NullPointerException ignored) {}__		final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class)__		final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class)___		DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>()__		c1.assignTimestampsAndWatermarks(periodicAssigner)__		try {_			c1.assignTimestampsAndWatermarks(punctuatedAssigner)__			fail()__		} catch (IllegalStateException ignored) {}__		DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>()__		c2.assignTimestampsAndWatermarks(punctuatedAssigner)__		try {_			c2.assignTimestampsAndWatermarks(periodicAssigner)__			fail()__		} catch (IllegalStateException ignored) {}_	};tests,that,not,both,types,of,timestamp,extractors,watermark,generators,can,be,used;test,suppress,warnings,unchecked,public,void,test,either,watermark,extractor,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,string,null,fail,catch,null,pointer,exception,ignored,try,new,dummy,flink,kafka,consumer,string,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,string,null,fail,catch,null,pointer,exception,ignored,final,assigner,with,periodic,watermarks,string,periodic,assigner,mock,assigner,with,periodic,watermarks,class,final,assigner,with,punctuated,watermarks,string,punctuated,assigner,mock,assigner,with,punctuated,watermarks,class,dummy,flink,kafka,consumer,string,c1,new,dummy,flink,kafka,consumer,c1,assign,timestamps,and,watermarks,periodic,assigner,try,c1,assign,timestamps,and,watermarks,punctuated,assigner,fail,catch,illegal,state,exception,ignored,dummy,flink,kafka,consumer,string,c2,new,dummy,flink,kafka,consumer,c2,assign,timestamps,and,watermarks,punctuated,assigner,try,c2,assign,timestamps,and,watermarks,periodic,assigner,fail,catch,illegal,state,exception,ignored
