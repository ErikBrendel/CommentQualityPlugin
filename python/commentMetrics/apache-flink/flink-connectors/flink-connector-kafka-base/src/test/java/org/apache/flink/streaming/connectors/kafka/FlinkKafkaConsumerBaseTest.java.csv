commented;modifiers;parameterAmount;loc;comment;code
true;public;0;30;/**  * Tests that not both types of timestamp extractors / watermark generators can be used.  */ ;/**  * Tests that not both types of timestamp extractors / watermark generators can be used.  */ @Test @SuppressWarnings("unchecked") public void testEitherWatermarkExtractor() {     try {         new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPeriodicWatermarks<String>) null).         fail().     } catch (NullPointerException ignored) {     }     try {         new DummyFlinkKafkaConsumer<String>().assignTimestampsAndWatermarks((AssignerWithPunctuatedWatermarks<String>) null).         fail().     } catch (NullPointerException ignored) {     }     final AssignerWithPeriodicWatermarks<String> periodicAssigner = mock(AssignerWithPeriodicWatermarks.class).     final AssignerWithPunctuatedWatermarks<String> punctuatedAssigner = mock(AssignerWithPunctuatedWatermarks.class).     DummyFlinkKafkaConsumer<String> c1 = new DummyFlinkKafkaConsumer<>().     c1.assignTimestampsAndWatermarks(periodicAssigner).     try {         c1.assignTimestampsAndWatermarks(punctuatedAssigner).         fail().     } catch (IllegalStateException ignored) {     }     DummyFlinkKafkaConsumer<String> c2 = new DummyFlinkKafkaConsumer<>().     c2.assignTimestampsAndWatermarks(punctuatedAssigner).     try {         c2.assignTimestampsAndWatermarks(periodicAssigner).         fail().     } catch (IllegalStateException ignored) {     } }
true;public;0;23;/**  * Tests that no checkpoints happen when the fetcher is not running.  */ ;/**  * Tests that no checkpoints happen when the fetcher is not running.  */ @Test public void ignoreCheckpointWhenNotRunning() throws Exception {     @SuppressWarnings("unchecked")     final MockFetcher<String> fetcher = new MockFetcher<>().     final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(fetcher, mock(AbstractPartitionDiscoverer.class), false).     final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>().     setupConsumer(consumer, false, listState, true, 0, 1).     // snapshot before the fetcher starts running     consumer.snapshotState(new StateSnapshotContextSynchronousImpl(1, 1)).     // no state should have been checkpointed     assertFalse(listState.get().iterator().hasNext()).     // acknowledgement of the checkpoint should also not result in any offset commits     consumer.notifyCheckpointComplete(1L).     assertNull(fetcher.getAndClearLastCommittedOffsets()).     assertEquals(0, fetcher.getCommitCount()). }
true;public;0;30;/**  * Tests that when taking a checkpoint when the fetcher is not running yet,  * the checkpoint correctly contains the restored state instead.  */ ;/**  * Tests that when taking a checkpoint when the fetcher is not running yet,  * the checkpoint correctly contains the restored state instead.  */ @Test public void checkRestoredCheckpointWhenFetcherNotReady() throws Exception {     @SuppressWarnings("unchecked")     final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>().     final TestingListState<Tuple2<KafkaTopicPartition, Long>> restoredListState = new TestingListState<>().     setupConsumer(consumer, true, restoredListState, true, 0, 1).     // snapshot before the fetcher starts running     consumer.snapshotState(new StateSnapshotContextSynchronousImpl(17, 17)).     // ensure that the list was cleared and refilled. while this is an implementation detail, we use it here     // to figure out that snapshotState() actually did something.     Assert.assertTrue(restoredListState.isClearCalled()).     Set<Serializable> expected = new HashSet<>().     for (Serializable serializable : restoredListState.get()) {         expected.add(serializable).     }     int counter = 0.     for (Serializable serializable : restoredListState.get()) {         assertTrue(expected.contains(serializable)).         counter++.     }     assertEquals(expected.size(), counter). }
false;public;0;16;;@Test public void testConfigureOnCheckpointsCommitMode() throws Exception {     @SuppressWarnings("unchecked")     final DummyFlinkKafkaConsumer<String> // auto-commit enabled. this should be ignored in this case     consumer = new DummyFlinkKafkaConsumer<>(true).     setupConsumer(consumer, false, null, // enable checkpointing. auto commit should be ignored     true, 0, 1).     assertEquals(OffsetCommitMode.ON_CHECKPOINTS, consumer.getOffsetCommitMode()). }
false;public;0;9;;@Test public void testConfigureAutoCommitMode() throws Exception {     @SuppressWarnings("unchecked")     final DummyFlinkKafkaConsumer<String> consumer = new DummyFlinkKafkaConsumer<>(true).     setupConsumer(consumer).     assertEquals(OffsetCommitMode.KAFKA_PERIODIC, consumer.getOffsetCommitMode()). }
false;public;0;17;;@Test public void testConfigureDisableOffsetCommitWithCheckpointing() throws Exception {     @SuppressWarnings("unchecked")     final DummyFlinkKafkaConsumer<String> // auto-commit enabled. this should be ignored in this case     consumer = new DummyFlinkKafkaConsumer<>(true).     // disabling offset committing should override everything     consumer.setCommitOffsetsOnCheckpoints(false).     setupConsumer(consumer, false, null, // enable checkpointing. auto commit should be ignored     true, 0, 1).     assertEquals(OffsetCommitMode.DISABLED, consumer.getOffsetCommitMode()). }
false;public;0;9;;@Test public void testConfigureDisableOffsetCommitWithoutCheckpointing() throws Exception {     @SuppressWarnings("unchecked")     final DummyFlinkKafkaConsumer<String> consumer = new DummyFlinkKafkaConsumer<>(false).     setupConsumer(consumer).     assertEquals(OffsetCommitMode.DISABLED, consumer.getOffsetCommitMode()). }
true;public;0;8;/**  * Tests that subscribed partitions didn't change when there's no change  * on the intial topics. (filterRestoredPartitionsWithDiscovered is active)  */ ;/**  * Tests that subscribed partitions didn't change when there's no change  * on the intial topics. (filterRestoredPartitionsWithDiscovered is active)  */ @Test public void testSetFilterRestoredParitionsNoChange() throws Exception {     checkFilterRestoredPartitionsWithDisovered(Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), false). }
true;public;0;8;/**  * Tests that removed partitions will be removed from subscribed partitions  * Even if it's still in restored partitions.  * (filterRestoredPartitionsWithDiscovered is active)  */ ;/**  * Tests that removed partitions will be removed from subscribed partitions  * Even if it's still in restored partitions.  * (filterRestoredPartitionsWithDiscovered is active)  */ @Test public void testSetFilterRestoredParitionsWithRemovedTopic() throws Exception {     checkFilterRestoredPartitionsWithDisovered(Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), Arrays.asList(new String[] { "kafka_topic_1" }), Arrays.asList(new String[] { "kafka_topic_1" }), false). }
true;public;0;8;/**  * Tests that newly added partitions will be added to subscribed partitions.  * (filterRestoredPartitionsWithDiscovered is active)  */ ;/**  * Tests that newly added partitions will be added to subscribed partitions.  * (filterRestoredPartitionsWithDiscovered is active)  */ @Test public void testSetFilterRestoredParitionsWithAddedTopic() throws Exception {     checkFilterRestoredPartitionsWithDisovered(Arrays.asList(new String[] { "kafka_topic_1" }), Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), false). }
true;public;0;8;/**  * Tests that subscribed partitions are the same when there's no  * change on the intial topics.  * (filterRestoredPartitionsWithDiscovered is disabled)  */ ;/**  * Tests that subscribed partitions are the same when there's no  * change on the intial topics.  * (filterRestoredPartitionsWithDiscovered is disabled)  */ @Test public void testDisableFilterRestoredParitionsNoChange() throws Exception {     checkFilterRestoredPartitionsWithDisovered(Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), true). }
true;public;0;8;/**  * Tests that removed partitions will not be removed from subscribed partitions  * Even if it's still in restored partitions.  * (filterRestoredPartitionsWithDiscovered is disabled)  */ ;/**  * Tests that removed partitions will not be removed from subscribed partitions  * Even if it's still in restored partitions.  * (filterRestoredPartitionsWithDiscovered is disabled)  */ @Test public void testDisableFilterRestoredParitionsWithRemovedTopic() throws Exception {     checkFilterRestoredPartitionsWithDisovered(Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), Arrays.asList(new String[] { "kafka_topic_1" }), Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), true). }
true;public;0;8;/**  * Tests that newly added partitions will be added to subscribed partitions.  * (filterRestoredPartitionsWithDiscovered is disabled)  */ ;/**  * Tests that newly added partitions will be added to subscribed partitions.  * (filterRestoredPartitionsWithDiscovered is disabled)  */ @Test public void testDisableFilterRestoredParitionsWithAddedTopic() throws Exception {     checkFilterRestoredPartitionsWithDisovered(Arrays.asList(new String[] { "kafka_topic_1" }), Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), Arrays.asList(new String[] { "kafka_topic_1", "kafka_topic_2" }), true). }
false;private;4;36;;private void checkFilterRestoredPartitionsWithDisovered(List<String> restoredKafkaTopics, List<String> initKafkaTopics, List<String> expectedSubscribedPartitions, Boolean disableFiltering) throws Exception {     final AbstractPartitionDiscoverer discoverer = new TestPartitionDiscoverer(new KafkaTopicsDescriptor(initKafkaTopics, null), 0, 1, TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(initKafkaTopics), TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(initKafkaTopics.stream().map(topic -> new KafkaTopicPartition(topic, 0)).collect(Collectors.toList()))).     final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(initKafkaTopics, discoverer).     if (disableFiltering) {         consumer.disableFilterRestoredPartitionsWithSubscribedTopics().     }     final TestingListState<Tuple2<KafkaTopicPartition, Long>> listState = new TestingListState<>().     for (int i = 0. i < restoredKafkaTopics.size(). i++) {         listState.add(new Tuple2<>(new KafkaTopicPartition(restoredKafkaTopics.get(i), 0), 12345L)).     }     setupConsumer(consumer, true, listState, true, 0, 1).     Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets = consumer.getSubscribedPartitionsToStartOffsets().     assertEquals(new HashSet<>(expectedSubscribedPartitions), subscribedPartitionsToStartOffsets.keySet().stream().map(partition -> partition.getTopic()).collect(Collectors.toSet())). }
false;public;0;4;;@Override public void go() throws Exception {     consumer.run(new TestSourceContext<>()). }
false;public;0;108;;@Test @SuppressWarnings("unchecked") public void testSnapshotStateWithCommitOnCheckpointsEnabled() throws Exception {     // --------------------------------------------------------------------     // prepare fake states     // --------------------------------------------------------------------     final HashMap<KafkaTopicPartition, Long> state1 = new HashMap<>().     state1.put(new KafkaTopicPartition("abc", 13), 16768L).     state1.put(new KafkaTopicPartition("def", 7), 987654321L).     final HashMap<KafkaTopicPartition, Long> state2 = new HashMap<>().     state2.put(new KafkaTopicPartition("abc", 13), 16770L).     state2.put(new KafkaTopicPartition("def", 7), 987654329L).     final HashMap<KafkaTopicPartition, Long> state3 = new HashMap<>().     state3.put(new KafkaTopicPartition("abc", 13), 16780L).     state3.put(new KafkaTopicPartition("def", 7), 987654377L).     // --------------------------------------------------------------------     final MockFetcher<String> fetcher = new MockFetcher<>(state1, state2, state3).     final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(fetcher, mock(AbstractPartitionDiscoverer.class), false).     final TestingListState<Serializable> listState = new TestingListState<>().     // setup and run the consumer. wait until the consumer reaches the main fetch loop before continuing test     setupConsumer(consumer, false, listState, true, 0, 1).     final CheckedThread runThread = new CheckedThread() {          @Override         public void go() throws Exception {             consumer.run(new TestSourceContext<>()).         }     }.     runThread.start().     fetcher.waitUntilRun().     assertEquals(0, consumer.getPendingOffsetsToCommit().size()).     // checkpoint 1     consumer.snapshotState(new StateSnapshotContextSynchronousImpl(138, 138)).     HashMap<KafkaTopicPartition, Long> snapshot1 = new HashMap<>().     for (Serializable serializable : listState.get()) {         Tuple2<KafkaTopicPartition, Long> kafkaTopicPartitionLongTuple2 = (Tuple2<KafkaTopicPartition, Long>) serializable.         snapshot1.put(kafkaTopicPartitionLongTuple2.f0, kafkaTopicPartitionLongTuple2.f1).     }     assertEquals(state1, snapshot1).     assertEquals(1, consumer.getPendingOffsetsToCommit().size()).     assertEquals(state1, consumer.getPendingOffsetsToCommit().get(138L)).     // checkpoint 2     consumer.snapshotState(new StateSnapshotContextSynchronousImpl(140, 140)).     HashMap<KafkaTopicPartition, Long> snapshot2 = new HashMap<>().     for (Serializable serializable : listState.get()) {         Tuple2<KafkaTopicPartition, Long> kafkaTopicPartitionLongTuple2 = (Tuple2<KafkaTopicPartition, Long>) serializable.         snapshot2.put(kafkaTopicPartitionLongTuple2.f0, kafkaTopicPartitionLongTuple2.f1).     }     assertEquals(state2, snapshot2).     assertEquals(2, consumer.getPendingOffsetsToCommit().size()).     assertEquals(state2, consumer.getPendingOffsetsToCommit().get(140L)).     // ack checkpoint 1     consumer.notifyCheckpointComplete(138L).     assertEquals(1, consumer.getPendingOffsetsToCommit().size()).     assertTrue(consumer.getPendingOffsetsToCommit().containsKey(140L)).     assertEquals(state1, fetcher.getAndClearLastCommittedOffsets()).     assertEquals(1, fetcher.getCommitCount()).     // checkpoint 3     consumer.snapshotState(new StateSnapshotContextSynchronousImpl(141, 141)).     HashMap<KafkaTopicPartition, Long> snapshot3 = new HashMap<>().     for (Serializable serializable : listState.get()) {         Tuple2<KafkaTopicPartition, Long> kafkaTopicPartitionLongTuple2 = (Tuple2<KafkaTopicPartition, Long>) serializable.         snapshot3.put(kafkaTopicPartitionLongTuple2.f0, kafkaTopicPartitionLongTuple2.f1).     }     assertEquals(state3, snapshot3).     assertEquals(2, consumer.getPendingOffsetsToCommit().size()).     assertEquals(state3, consumer.getPendingOffsetsToCommit().get(141L)).     // ack checkpoint 3, subsumes number 2     consumer.notifyCheckpointComplete(141L).     assertEquals(0, consumer.getPendingOffsetsToCommit().size()).     assertEquals(state3, fetcher.getAndClearLastCommittedOffsets()).     assertEquals(2, fetcher.getCommitCount()).     // invalid checkpoint     consumer.notifyCheckpointComplete(666).     assertEquals(0, consumer.getPendingOffsetsToCommit().size()).     assertNull(fetcher.getAndClearLastCommittedOffsets()).     assertEquals(2, fetcher.getCommitCount()).     consumer.cancel().     runThread.sync(). }
false;public;0;4;;@Override public void go() throws Exception {     consumer.run(new TestSourceContext<>()). }
false;public;0;101;;@Test @SuppressWarnings("unchecked") public void testSnapshotStateWithCommitOnCheckpointsDisabled() throws Exception {     // --------------------------------------------------------------------     // prepare fake states     // --------------------------------------------------------------------     final HashMap<KafkaTopicPartition, Long> state1 = new HashMap<>().     state1.put(new KafkaTopicPartition("abc", 13), 16768L).     state1.put(new KafkaTopicPartition("def", 7), 987654321L).     final HashMap<KafkaTopicPartition, Long> state2 = new HashMap<>().     state2.put(new KafkaTopicPartition("abc", 13), 16770L).     state2.put(new KafkaTopicPartition("def", 7), 987654329L).     final HashMap<KafkaTopicPartition, Long> state3 = new HashMap<>().     state3.put(new KafkaTopicPartition("abc", 13), 16780L).     state3.put(new KafkaTopicPartition("def", 7), 987654377L).     // --------------------------------------------------------------------     final MockFetcher<String> fetcher = new MockFetcher<>(state1, state2, state3).     final FlinkKafkaConsumerBase<String> consumer = new DummyFlinkKafkaConsumer<>(fetcher, mock(AbstractPartitionDiscoverer.class), false).     // disable offset committing     consumer.setCommitOffsetsOnCheckpoints(false).     final TestingListState<Serializable> listState = new TestingListState<>().     // setup and run the consumer. wait until the consumer reaches the main fetch loop before continuing test     setupConsumer(consumer, false, listState, true, 0, 1).     final CheckedThread runThread = new CheckedThread() {          @Override         public void go() throws Exception {             consumer.run(new TestSourceContext<>()).         }     }.     runThread.start().     fetcher.waitUntilRun().     assertEquals(0, consumer.getPendingOffsetsToCommit().size()).     // checkpoint 1     consumer.snapshotState(new StateSnapshotContextSynchronousImpl(138, 138)).     HashMap<KafkaTopicPartition, Long> snapshot1 = new HashMap<>().     for (Serializable serializable : listState.get()) {         Tuple2<KafkaTopicPartition, Long> kafkaTopicPartitionLongTuple2 = (Tuple2<KafkaTopicPartition, Long>) serializable.         snapshot1.put(kafkaTopicPartitionLongTuple2.f0, kafkaTopicPartitionLongTuple2.f1).     }     assertEquals(state1, snapshot1).     // pending offsets to commit should not be updated     assertEquals(0, consumer.getPendingOffsetsToCommit().size()).     // checkpoint 2     consumer.snapshotState(new StateSnapshotContextSynchronousImpl(140, 140)).     HashMap<KafkaTopicPartition, Long> snapshot2 = new HashMap<>().     for (Serializable serializable : listState.get()) {         Tuple2<KafkaTopicPartition, Long> kafkaTopicPartitionLongTuple2 = (Tuple2<KafkaTopicPartition, Long>) serializable.         snapshot2.put(kafkaTopicPartitionLongTuple2.f0, kafkaTopicPartitionLongTuple2.f1).     }     assertEquals(state2, snapshot2).     // pending offsets to commit should not be updated     assertEquals(0, consumer.getPendingOffsetsToCommit().size()).     // ack checkpoint 1     consumer.notifyCheckpointComplete(138L).     assertEquals(0, fetcher.getCommitCount()).     // no offsets should be committed     assertNull(fetcher.getAndClearLastCommittedOffsets()).     // checkpoint 3     consumer.snapshotState(new StateSnapshotContextSynchronousImpl(141, 141)).     HashMap<KafkaTopicPartition, Long> snapshot3 = new HashMap<>().     for (Serializable serializable : listState.get()) {         Tuple2<KafkaTopicPartition, Long> kafkaTopicPartitionLongTuple2 = (Tuple2<KafkaTopicPartition, Long>) serializable.         snapshot3.put(kafkaTopicPartitionLongTuple2.f0, kafkaTopicPartitionLongTuple2.f1).     }     assertEquals(state3, snapshot3).     // pending offsets to commit should not be updated     assertEquals(0, consumer.getPendingOffsetsToCommit().size()).     // ack checkpoint 3, subsumes number 2     consumer.notifyCheckpointComplete(141L).     assertEquals(0, fetcher.getCommitCount()).     // no offsets should be committed     assertNull(fetcher.getAndClearLastCommittedOffsets()).     // invalid checkpoint     consumer.notifyCheckpointComplete(666).     assertEquals(0, fetcher.getCommitCount()).     // no offsets should be committed     assertNull(fetcher.getAndClearLastCommittedOffsets()).     consumer.cancel().     runThread.sync(). }
false;public;0;10;;@Test public void testClosePartitionDiscovererWhenOpenThrowException() throws Exception {     final RuntimeException failureCause = new RuntimeException(new FlinkException("Test partition discoverer exception")).     final FailingPartitionDiscoverer failingPartitionDiscoverer = new FailingPartitionDiscoverer(failureCause).     final DummyFlinkKafkaConsumer<String> consumer = new DummyFlinkKafkaConsumer<>(failingPartitionDiscoverer).     testFailingConsumerLifecycle(consumer, failureCause).     assertTrue("partitionDiscoverer should be closed when consumer is closed", failingPartitionDiscoverer.isClosed()). }
false;public;0;15;;@Test public void testClosePartitionDiscovererWhenCreateKafkaFetcherFails() throws Exception {     final FlinkException failureCause = new FlinkException("Create Kafka fetcher failure.").     final DummyPartitionDiscoverer testPartitionDiscoverer = new DummyPartitionDiscoverer().     final DummyFlinkKafkaConsumer<String> consumer = new DummyFlinkKafkaConsumer<>(() -> {         throw failureCause.     }, testPartitionDiscoverer, 100L).     testFailingConsumerLifecycle(consumer, failureCause).     assertTrue("partitionDiscoverer should be closed when consumer is closed", testPartitionDiscoverer.isClosed()). }
false;public;0;17;;@Test public void testClosePartitionDiscovererWhenKafkaFetcherFails() throws Exception {     final FlinkException failureCause = new FlinkException("Run Kafka fetcher failure.").     // in this scenario, the partition discoverer will be concurrently accessed.     // use the WakeupBeforeCloseTestingPartitionDiscoverer to verify that we always call     // wakeup() before closing the discoverer     final WakeupBeforeCloseTestingPartitionDiscoverer testPartitionDiscoverer = new WakeupBeforeCloseTestingPartitionDiscoverer().     final AbstractFetcher<String, ?> mock = (AbstractFetcher<String, ?>) mock(AbstractFetcher.class).     doThrow(failureCause).when(mock).runFetchLoop().     final DummyFlinkKafkaConsumer<String> consumer = new DummyFlinkKafkaConsumer<>(() -> mock, testPartitionDiscoverer, 100L).     testFailingConsumerLifecycle(consumer, failureCause).     assertTrue("partitionDiscoverer should be closed when consumer is closed", testPartitionDiscoverer.isClosed()). }
false;private;2;11;;private void testFailingConsumerLifecycle(FlinkKafkaConsumerBase<String> testKafkaConsumer, @Nonnull Exception expectedException) throws Exception {     try {         setupConsumer(testKafkaConsumer).         testKafkaConsumer.run(new TestSourceContext<>()).         fail("Exception should have been thrown from open / run method of FlinkKafkaConsumerBase.").     } catch (Exception e) {         assertThat(ExceptionUtils.findThrowable(e, throwable -> throwable.equals(expectedException)).isPresent(), is(true)).     }     testKafkaConsumer.close(). }
false;public;0;9;;@Test public void testClosePartitionDiscovererWithCancellation() throws Exception {     final DummyPartitionDiscoverer testPartitionDiscoverer = new DummyPartitionDiscoverer().     final TestingFlinkKafkaConsumer<String> consumer = new TestingFlinkKafkaConsumer<>(testPartitionDiscoverer, 100L).     testNormalConsumerLifecycle(consumer).     assertTrue("partitionDiscoverer should be closed when consumer is closed", testPartitionDiscoverer.isClosed()). }
false;private;1;6;;private void testNormalConsumerLifecycle(FlinkKafkaConsumerBase<String> testKafkaConsumer) throws Exception {     setupConsumer(testKafkaConsumer).     final CompletableFuture<Void> runFuture = CompletableFuture.runAsync(ThrowingRunnable.unchecked(() -> testKafkaConsumer.run(new TestSourceContext<>()))).     testKafkaConsumer.close().     runFuture.get(). }
false;private;1;9;;private void setupConsumer(FlinkKafkaConsumerBase<String> consumer) throws Exception {     setupConsumer(consumer, false, null, false, 0, 1). }
false;public;0;4;;@Test public void testScaleUp() throws Exception {     testRescaling(5, 2, 8, 30). }
false;public;0;4;;@Test public void testScaleDown() throws Exception {     testRescaling(5, 10, 2, 100). }
true;private;4;113;/**  * Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,  * which means that operator state is being reshuffled.  *  * <p>This also verifies that a restoring source is always impervious to changes in the list  * of topics fetched from Kafka.  */ ;/**  * Tests whether the Kafka consumer behaves correctly when scaling the parallelism up/down,  * which means that operator state is being reshuffled.  *  * <p>This also verifies that a restoring source is always impervious to changes in the list  * of topics fetched from Kafka.  */ @SuppressWarnings("unchecked") private void testRescaling(final int initialParallelism, final int numPartitions, final int restoredParallelism, final int restoredNumPartitions) throws Exception {     Preconditions.checkArgument(restoredNumPartitions >= numPartitions, "invalid test case for Kafka repartitioning. Kafka only allows increasing partitions.").     List<KafkaTopicPartition> mockFetchedPartitionsOnStartup = new ArrayList<>().     for (int i = 0. i < numPartitions. i++) {         mockFetchedPartitionsOnStartup.add(new KafkaTopicPartition("test-topic", i)).     }     DummyFlinkKafkaConsumer<String>[] consumers = new DummyFlinkKafkaConsumer[initialParallelism].     AbstractStreamOperatorTestHarness<String>[] testHarnesses = new AbstractStreamOperatorTestHarness[initialParallelism].     List<String> testTopics = Collections.singletonList("test-topic").     for (int i = 0. i < initialParallelism. i++) {         TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(new KafkaTopicsDescriptor(testTopics, null), i, initialParallelism, TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(testTopics), TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsOnStartup)).         consumers[i] = new DummyFlinkKafkaConsumer<>(testTopics, partitionDiscoverer).         testHarnesses[i] = createTestHarness(consumers[i], initialParallelism, i).         // initializeState() is always called, null signals that we didn't restore         testHarnesses[i].initializeEmptyState().         testHarnesses[i].open().     }     Map<KafkaTopicPartition, Long> globalSubscribedPartitions = new HashMap<>().     for (int i = 0. i < initialParallelism. i++) {         Map<KafkaTopicPartition, Long> subscribedPartitions = consumers[i].getSubscribedPartitionsToStartOffsets().         // make sure that no one else is subscribed to these partitions         for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {             assertThat(globalSubscribedPartitions, not(hasKey(partition))).         }         globalSubscribedPartitions.putAll(subscribedPartitions).     }     assertThat(globalSubscribedPartitions.values(), hasSize(numPartitions)).     assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(globalSubscribedPartitions.keySet()))).     OperatorSubtaskState[] state = new OperatorSubtaskState[initialParallelism].     for (int i = 0. i < initialParallelism. i++) {         state[i] = testHarnesses[i].snapshot(0, 0).     }     OperatorSubtaskState mergedState = AbstractStreamOperatorTestHarness.repackageState(state).     // -----------------------------------------------------------------------------------------     // restore     List<KafkaTopicPartition> mockFetchedPartitionsAfterRestore = new ArrayList<>().     for (int i = 0. i < restoredNumPartitions. i++) {         mockFetchedPartitionsAfterRestore.add(new KafkaTopicPartition("test-topic", i)).     }     DummyFlinkKafkaConsumer<String>[] restoredConsumers = new DummyFlinkKafkaConsumer[restoredParallelism].     AbstractStreamOperatorTestHarness<String>[] restoredTestHarnesses = new AbstractStreamOperatorTestHarness[restoredParallelism].     for (int i = 0. i < restoredParallelism. i++) {         OperatorSubtaskState initState = AbstractStreamOperatorTestHarness.repartitionOperatorState(mergedState, maxParallelism, initialParallelism, restoredParallelism, i).         TestPartitionDiscoverer partitionDiscoverer = new TestPartitionDiscoverer(new KafkaTopicsDescriptor(testTopics, null), i, restoredParallelism, TestPartitionDiscoverer.createMockGetAllTopicsSequenceFromFixedReturn(testTopics), TestPartitionDiscoverer.createMockGetAllPartitionsFromTopicsSequenceFromFixedReturn(mockFetchedPartitionsAfterRestore)).         restoredConsumers[i] = new DummyFlinkKafkaConsumer<>(testTopics, partitionDiscoverer).         restoredTestHarnesses[i] = createTestHarness(restoredConsumers[i], restoredParallelism, i).         // initializeState() is always called, null signals that we didn't restore         restoredTestHarnesses[i].initializeState(initState).         restoredTestHarnesses[i].open().     }     Map<KafkaTopicPartition, Long> restoredGlobalSubscribedPartitions = new HashMap<>().     for (int i = 0. i < restoredParallelism. i++) {         Map<KafkaTopicPartition, Long> subscribedPartitions = restoredConsumers[i].getSubscribedPartitionsToStartOffsets().         // make sure that no one else is subscribed to these partitions         for (KafkaTopicPartition partition : subscribedPartitions.keySet()) {             assertThat(restoredGlobalSubscribedPartitions, not(hasKey(partition))).         }         restoredGlobalSubscribedPartitions.putAll(subscribedPartitions).     }     assertThat(restoredGlobalSubscribedPartitions.values(), hasSize(restoredNumPartitions)).     assertThat(mockFetchedPartitionsOnStartup, everyItem(isIn(restoredGlobalSubscribedPartitions.keySet()))). }
false;private,static;3;11;;// ------------------------------------------------------------------------ private static <T> AbstractStreamOperatorTestHarness<T> createTestHarness(SourceFunction<T> source, int numSubtasks, int subtaskIndex) throws Exception {     AbstractStreamOperatorTestHarness<T> testHarness = new AbstractStreamOperatorTestHarness<>(new StreamSource<>(source), maxParallelism, numSubtasks, subtaskIndex).     testHarness.setTimeCharacteristic(TimeCharacteristic.EventTime).     return testHarness. }
false;protected;0;4;;@Override protected void initializeConnections() throws Exception {     closed = false. }
false;protected;0;4;;@Override protected void wakeupConnections() { }
false;protected;0;4;;@Override protected void closeConnections() throws Exception {     closed = true. }
false;protected;0;4;;@Override protected List<String> getAllTopics() throws WakeupException {     return null. }
false;protected;1;4;;@Override protected List<KafkaTopicPartition> getAllPartitionsForTopics(List<String> topics) throws WakeupException {     return null. }
false;public;0;3;;@Override public List<KafkaTopicPartition> discoverPartitions() throws WakeupException, ClosedException {     throw failureCause. }
false;public;0;3;;public boolean isClosed() {     return closed. }
false;protected;0;8;;@Override protected void closeConnections() {     if (!isWakedUp()) {         fail("Partition discoverer should have been waked up first before closing.").     }     super.closeConnections(). }
false;protected;0;4;;@Override protected void initializeConnections() { // noop }
false;protected;0;4;;@Override protected void wakeupConnections() {     wakedUp = true. }
false;protected;0;4;;@Override protected void closeConnections() {     closed = true. }
false;protected;0;6;;@Override protected List<String> getAllTopics() throws WakeupException {     checkState().     return allTopics. }
false;protected;1;5;;@Override protected List<KafkaTopicPartition> getAllPartitionsForTopics(List<String> topics) throws WakeupException {     checkState().     return allPartitions. }
false;private;0;5;;private void checkState() throws WakeupException {     if (wakedUp || closed) {         throw new WakeupException().     } }
false;;0;3;;boolean isClosed() {     return closed. }
false;public;0;3;;public boolean isWakedUp() {     return wakedUp. }
false;public;0;6;;@Override public void runFetchLoop() throws Exception {     while (isRunning) {         Thread.sleep(10L).     } }
false;public;0;4;;@Override public void cancel() {     isRunning = false. }
false;protected;2;4;;@Override protected void doCommitInternalOffsetsToKafka(Map<KafkaTopicPartition, Long> offsets, @Nonnull KafkaCommitCallback commitCallback) throws Exception { }
false;protected;1;4;;@Override protected KPH createKafkaPartitionHandle(KafkaTopicPartition partition) {     return null. }
false;protected;8;13;;@Override @SuppressWarnings("unchecked") protected AbstractFetcher<T, ?> createFetcher(SourceContext<T> sourceContext, Map<KafkaTopicPartition, Long> thisSubtaskPartitionsWithStartOffsets, SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, StreamingRuntimeContext runtimeContext, OffsetCommitMode offsetCommitMode, MetricGroup consumerMetricGroup, boolean useMetrics) throws Exception {     return testFetcherSupplier.get(). }
false;protected;3;7;;@Override protected AbstractPartitionDiscoverer createPartitionDiscoverer(KafkaTopicsDescriptor topicsDescriptor, int indexOfThisSubtask, int numParallelSubtasks) {     return this.testPartitionDiscoverer. }
false;protected;0;4;;@Override protected boolean getIsAutoCommitEnabled() {     return isAutoCommitEnabled. }
false;protected;2;6;;@Override protected Map<KafkaTopicPartition, Long> fetchOffsetsWithTimestamp(Collection<KafkaTopicPartition> partitions, long timestamp) {     throw new UnsupportedOperationException(). }
false;protected;8;5;;@Override protected AbstractFetcher<T, ?> createFetcher(SourceContext<T> sourceContext, Map<KafkaTopicPartition, Long> thisSubtaskPartitionsWithStartOffsets, SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, StreamingRuntimeContext runtimeContext, OffsetCommitMode offsetCommitMode, MetricGroup consumerMetricGroup, boolean useMetrics) throws Exception {     return new TestingFetcher<T, String>(sourceContext, thisSubtaskPartitionsWithStartOffsets, watermarksPeriodic, watermarksPunctuated, runtimeContext.getProcessingTimeService(), 0L, getClass().getClassLoader(), consumerMetricGroup, useMetrics). }
false;protected;3;4;;@Override protected AbstractPartitionDiscoverer createPartitionDiscoverer(KafkaTopicsDescriptor topicsDescriptor, int indexOfThisSubtask, int numParallelSubtasks) {     return partitionDiscoverer. }
false;protected;0;4;;@Override protected boolean getIsAutoCommitEnabled() {     return false. }
false;protected;2;4;;@Override protected Map<KafkaTopicPartition, Long> fetchOffsetsWithTimestamp(Collection<KafkaTopicPartition> partitions, long timestamp) {     throw new UnsupportedOperationException("fetchOffsetsWithTimestamp is not supported"). }
false;public;0;5;;@Override public void clear() {     list.clear().     clearCalled = true. }
false;public;0;4;;@Override public Iterable<T> get() throws Exception {     return list. }
false;public;1;5;;@Override public void add(T value) throws Exception {     Preconditions.checkNotNull(value, "You cannot add null to a ListState.").     list.add(value). }
false;public;0;3;;public List<T> getList() {     return list. }
false;;0;3;;boolean isClearCalled() {     return clearCalled. }
false;public;1;6;;@Override public void update(List<T> values) throws Exception {     clear().     addAll(values). }
false;public;1;8;;@Override public void addAll(List<T> values) throws Exception {     if (values != null) {         values.forEach(v -> Preconditions.checkNotNull(v, "You cannot add null to a ListState.")).         list.addAll(values).     } }
false;private,static;6;14;;@SuppressWarnings("unchecked") private static <T, S> void setupConsumer(FlinkKafkaConsumerBase<T> consumer, boolean isRestored, ListState<S> restoredListState, boolean isCheckpointingEnabled, int subtaskIndex, int totalNumSubtasks) throws Exception {     // run setup procedure in operator life cycle     consumer.setRuntimeContext(new MockStreamingRuntimeContext(isCheckpointingEnabled, totalNumSubtasks, subtaskIndex)).     consumer.initializeState(new MockFunctionInitializationContext(isRestored, new MockOperatorStateStore(restoredListState))).     consumer.open(new Configuration()). }
false;protected;2;8;;@Override protected void doCommitInternalOffsetsToKafka(Map<KafkaTopicPartition, Long> offsets, @Nonnull KafkaCommitCallback commitCallback) throws Exception {     this.lastCommittedOffsets = offsets.     this.commitCount++.     commitCallback.onSuccess(). }
false;public;0;5;;@Override public void runFetchLoop() throws Exception {     runLatch.trigger().     stopLatch.await(). }
false;public;0;5;;@Override public HashMap<KafkaTopicPartition, Long> snapshotCurrentState() {     checkState(!stateSnapshotsToReturn.isEmpty()).     return stateSnapshotsToReturn.poll(). }
false;protected;1;4;;@Override protected Object createKafkaPartitionHandle(KafkaTopicPartition partition) {     throw new UnsupportedOperationException(). }
false;public;0;4;;@Override public void cancel() {     stopLatch.trigger(). }
false;private;0;3;;private void waitUntilRun() throws InterruptedException {     runLatch.await(). }
false;private;0;5;;private Map<KafkaTopicPartition, Long> getAndClearLastCommittedOffsets() {     Map<KafkaTopicPartition, Long> offsets = this.lastCommittedOffsets.     this.lastCommittedOffsets = null.     return offsets. }
false;private;0;3;;private int getCommitCount() {     return commitCount. }
false;public;1;5;;@Override @SuppressWarnings("unchecked") public <S> ListState<S> getUnionListState(ListStateDescriptor<S> stateDescriptor) throws Exception {     return (ListState<S>) mockRestoredUnionListState. }
false;public;1;5;;@Override public <T extends Serializable> ListState<T> getSerializableListState(String stateName) throws Exception {     // return empty state for the legacy 1.2 Kafka consumer state     return new TestingListState<>(). }
false;public;1;4;;// ------------------------------------------------------------------------ @Override public <S> ListState<S> getOperatorState(ListStateDescriptor<S> stateDescriptor) throws Exception {     throw new UnsupportedOperationException(). }
false;public;1;4;;@Override public <K, V> BroadcastState<K, V> getBroadcastState(MapStateDescriptor<K, V> stateDescriptor) throws Exception {     throw new UnsupportedOperationException(). }
false;public;1;4;;@Override public <S> ListState<S> getListState(ListStateDescriptor<S> stateDescriptor) throws Exception {     throw new UnsupportedOperationException(). }
false;public;0;4;;@Override public Set<String> getRegisteredStateNames() {     throw new UnsupportedOperationException(). }
false;public;0;4;;@Override public Set<String> getRegisteredBroadcastStateNames() {     throw new UnsupportedOperationException(). }
false;public;0;4;;@Override public boolean isRestored() {     return isRestored. }
false;public;0;4;;@Override public OperatorStateStore getOperatorStateStore() {     return operatorStateStore. }
false;public;0;4;;@Override public KeyedStateStore getKeyedStateStore() {     throw new UnsupportedOperationException(). }
