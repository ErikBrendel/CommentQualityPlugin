commented;modifiers;parameterAmount;loc;comment;code
true;static;2;5;/**  * Make sure that auto commit is disabled when our offset commit mode is ON_CHECKPOINTS.  * This overwrites whatever setting the user configured in the properties.  * @param properties - Kafka configuration properties to be adjusted  * @param offsetCommitMode offset commit mode  */ ;/**  * Make sure that auto commit is disabled when our offset commit mode is ON_CHECKPOINTS.  * This overwrites whatever setting the user configured in the properties.  * @param properties - Kafka configuration properties to be adjusted  * @param offsetCommitMode offset commit mode  */ static void adjustAutoCommitConfig(Properties properties, OffsetCommitMode offsetCommitMode) {     if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS || offsetCommitMode == OffsetCommitMode.DISABLED) {         properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false").     } }
true;public;1;14;/**  * Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner.  * The watermark extractor will run per Kafka partition, watermarks will be merged across partitions  * in the same way as in the Flink runtime, when streams are merged.  *  * <p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,  * the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition  * characteristics are usually lost that way. For example, if the timestamps are strictly ascending  * per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the  * parallel source subtask reads more that one partition.  *  * <p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka  * partition, allows users to let them exploit the per-partition characteristics.  *  * <p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an  * {@link AssignerWithPeriodicWatermarks}, not both at the same time.  *  * @param assigner The timestamp assigner / watermark generator to use.  * @return The consumer object, to allow function chaining.  */ ;// ------------------------------------------------------------------------ // Configuration // ------------------------------------------------------------------------ /**  * Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner.  * The watermark extractor will run per Kafka partition, watermarks will be merged across partitions  * in the same way as in the Flink runtime, when streams are merged.  *  * <p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,  * the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition  * characteristics are usually lost that way. For example, if the timestamps are strictly ascending  * per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the  * parallel source subtask reads more that one partition.  *  * <p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka  * partition, allows users to let them exploit the per-partition characteristics.  *  * <p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an  * {@link AssignerWithPeriodicWatermarks}, not both at the same time.  *  * @param assigner The timestamp assigner / watermark generator to use.  * @return The consumer object, to allow function chaining.  */ public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {     checkNotNull(assigner).     if (this.periodicWatermarkAssigner != null) {         throw new IllegalStateException("A periodic watermark emitter has already been set.").     }     try {         ClosureCleaner.clean(assigner, true).         this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner).         return this.     } catch (Exception e) {         throw new IllegalArgumentException("The given assigner is not serializable", e).     } }
true;public;1;14;/**  * Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner.  * The watermark extractor will run per Kafka partition, watermarks will be merged across partitions  * in the same way as in the Flink runtime, when streams are merged.  *  * <p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,  * the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition  * characteristics are usually lost that way. For example, if the timestamps are strictly ascending  * per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the  * parallel source subtask reads more that one partition.  *  * <p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka  * partition, allows users to let them exploit the per-partition characteristics.  *  * <p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an  * {@link AssignerWithPeriodicWatermarks}, not both at the same time.  *  * @param assigner The timestamp assigner / watermark generator to use.  * @return The consumer object, to allow function chaining.  */ ;/**  * Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner.  * The watermark extractor will run per Kafka partition, watermarks will be merged across partitions  * in the same way as in the Flink runtime, when streams are merged.  *  * <p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,  * the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition  * characteristics are usually lost that way. For example, if the timestamps are strictly ascending  * per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the  * parallel source subtask reads more that one partition.  *  * <p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka  * partition, allows users to let them exploit the per-partition characteristics.  *  * <p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an  * {@link AssignerWithPeriodicWatermarks}, not both at the same time.  *  * @param assigner The timestamp assigner / watermark generator to use.  * @return The consumer object, to allow function chaining.  */ public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {     checkNotNull(assigner).     if (this.punctuatedWatermarkAssigner != null) {         throw new IllegalStateException("A punctuated watermark emitter has already been set.").     }     try {         ClosureCleaner.clean(assigner, true).         this.periodicWatermarkAssigner = new SerializedValue<>(assigner).         return this.     } catch (Exception e) {         throw new IllegalArgumentException("The given assigner is not serializable", e).     } }
true;public;1;4;/**  * Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.  *  * <p>This setting will only have effect if checkpointing is enabled for the job.  * If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)  * property settings will be used.  *  * @return The consumer object, to allow function chaining.  */ ;/**  * Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.  *  * <p>This setting will only have effect if checkpointing is enabled for the job.  * If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)  * property settings will be used.  *  * @return The consumer object, to allow function chaining.  */ public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {     this.enableCommitOnCheckpoints = commitOnCheckpoints.     return this. }
true;public;0;6;/**  * Specifies the consumer to start reading from the earliest offset for all partitions.  * This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.  *  * <p>This method does not affect where partitions are read from when the consumer is restored  * from a checkpoint or savepoint. When the consumer is restored from a checkpoint or  * savepoint, only the offsets in the restored state will be used.  *  * @return The consumer object, to allow function chaining.  */ ;/**  * Specifies the consumer to start reading from the earliest offset for all partitions.  * This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.  *  * <p>This method does not affect where partitions are read from when the consumer is restored  * from a checkpoint or savepoint. When the consumer is restored from a checkpoint or  * savepoint, only the offsets in the restored state will be used.  *  * @return The consumer object, to allow function chaining.  */ public FlinkKafkaConsumerBase<T> setStartFromEarliest() {     this.startupMode = StartupMode.EARLIEST.     this.startupOffsetsTimestamp = null.     this.specificStartupOffsets = null.     return this. }
true;public;0;6;/**  * Specifies the consumer to start reading from the latest offset for all partitions.  * This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.  *  * <p>This method does not affect where partitions are read from when the consumer is restored  * from a checkpoint or savepoint. When the consumer is restored from a checkpoint or  * savepoint, only the offsets in the restored state will be used.  *  * @return The consumer object, to allow function chaining.  */ ;/**  * Specifies the consumer to start reading from the latest offset for all partitions.  * This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.  *  * <p>This method does not affect where partitions are read from when the consumer is restored  * from a checkpoint or savepoint. When the consumer is restored from a checkpoint or  * savepoint, only the offsets in the restored state will be used.  *  * @return The consumer object, to allow function chaining.  */ public FlinkKafkaConsumerBase<T> setStartFromLatest() {     this.startupMode = StartupMode.LATEST.     this.startupOffsetsTimestamp = null.     this.specificStartupOffsets = null.     return this. }
true;protected;1;12;// Version-specific subclasses which can expose the functionality should override and allow public access. ;/**  * Specifies the consumer to start reading partitions from a specified timestamp.  * The specified timestamp must be before the current timestamp.  * This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.  *  * <p>The consumer will look up the earliest offset whose timestamp is greater than or equal  * to the specific timestamp from Kafka. If there's no such offset, the consumer will use the  * latest offset to read data from kafka.  *  * <p>This method does not affect where partitions are read from when the consumer is restored  * from a checkpoint or savepoint. When the consumer is restored from a checkpoint or  * savepoint, only the offsets in the restored state will be used.  *  * @param startupOffsetsTimestamp timestamp for the startup offsets, as milliseconds from epoch.  *  * @return The consumer object, to allow function chaining.  */ // NOTE - // This method is implemented in the base class because this is where the startup logging and verifications live. // However, it is not publicly exposed since only newer Kafka versions support the functionality. // Version-specific subclasses which can expose the functionality should override and allow public access. protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp) {     checkArgument(startupOffsetsTimestamp >= 0, "The provided value for the startup offsets timestamp is invalid.").     long currentTimestamp = System.currentTimeMillis().     checkArgument(startupOffsetsTimestamp <= currentTimestamp, "Startup time[%s] must be before current time[%s].", startupOffsetsTimestamp, currentTimestamp).     this.startupMode = StartupMode.TIMESTAMP.     this.startupOffsetsTimestamp = startupOffsetsTimestamp.     this.specificStartupOffsets = null.     return this. }
true;public;0;6;/**  * Specifies the consumer to start reading from any committed group offsets found  * in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration  * properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"  * set in the configuration properties will be used for the partition.  *  * <p>This method does not affect where partitions are read from when the consumer is restored  * from a checkpoint or savepoint. When the consumer is restored from a checkpoint or  * savepoint, only the offsets in the restored state will be used.  *  * @return The consumer object, to allow function chaining.  */ ;/**  * Specifies the consumer to start reading from any committed group offsets found  * in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration  * properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"  * set in the configuration properties will be used for the partition.  *  * <p>This method does not affect where partitions are read from when the consumer is restored  * from a checkpoint or savepoint. When the consumer is restored from a checkpoint or  * savepoint, only the offsets in the restored state will be used.  *  * @return The consumer object, to allow function chaining.  */ public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {     this.startupMode = StartupMode.GROUP_OFFSETS.     this.startupOffsetsTimestamp = null.     this.specificStartupOffsets = null.     return this. }
true;public;1;6;/**  * Specifies the consumer to start reading partitions from specific offsets, set independently for each partition.  * The specified offset should be the offset of the next record that will be read from partitions.  * This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.  *  * <p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the  * consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided  * map of offsets, the consumer will fallback to the default group offset behaviour (see  * {@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.  *  * <p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group  * offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the  * configuration properties will be used for the partition  *  * <p>This method does not affect where partitions are read from when the consumer is restored  * from a checkpoint or savepoint. When the consumer is restored from a checkpoint or  * savepoint, only the offsets in the restored state will be used.  *  * @return The consumer object, to allow function chaining.  */ ;/**  * Specifies the consumer to start reading partitions from specific offsets, set independently for each partition.  * The specified offset should be the offset of the next record that will be read from partitions.  * This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.  *  * <p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the  * consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided  * map of offsets, the consumer will fallback to the default group offset behaviour (see  * {@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.  *  * <p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group  * offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the  * configuration properties will be used for the partition  *  * <p>This method does not affect where partitions are read from when the consumer is restored  * from a checkpoint or savepoint. When the consumer is restored from a checkpoint or  * savepoint, only the offsets in the restored state will be used.  *  * @return The consumer object, to allow function chaining.  */ public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {     this.startupMode = StartupMode.SPECIFIC_OFFSETS.     this.startupOffsetsTimestamp = null.     this.specificStartupOffsets = checkNotNull(specificStartupOffsets).     return this. }
true;public;0;4;/**  * By default, when restoring from a checkpoint / savepoint, the consumer always  * ignores restored partitions that are no longer associated with the current specified topics or  * topic pattern to subscribe to.  *  * <p>This method configures the consumer to not filter the restored partitions,  * therefore always attempting to consume whatever partition was present in the  * previous execution regardless of the specified topics to subscribe to in the  * current execution.  *  * @return The consumer object, to allow function chaining.  */ ;/**  * By default, when restoring from a checkpoint / savepoint, the consumer always  * ignores restored partitions that are no longer associated with the current specified topics or  * topic pattern to subscribe to.  *  * <p>This method configures the consumer to not filter the restored partitions,  * therefore always attempting to consume whatever partition was present in the  * previous execution regardless of the specified topics to subscribe to in the  * current execution.  *  * @return The consumer object, to allow function chaining.  */ public FlinkKafkaConsumerBase<T> disableFilterRestoredPartitionsWithSubscribedTopics() {     this.filterRestoredPartitionsWithCurrentTopicsDescriptor = false.     return this. }
false;public;1;164;;// ------------------------------------------------------------------------ // Work methods // ------------------------------------------------------------------------ @Override public void open(Configuration configuration) throws Exception {     // determine the offset commit mode     this.offsetCommitMode = OffsetCommitModes.fromConfiguration(getIsAutoCommitEnabled(), enableCommitOnCheckpoints, ((StreamingRuntimeContext) getRuntimeContext()).isCheckpointingEnabled()).     // create the partition discoverer     this.partitionDiscoverer = createPartitionDiscoverer(topicsDescriptor, getRuntimeContext().getIndexOfThisSubtask(), getRuntimeContext().getNumberOfParallelSubtasks()).     this.partitionDiscoverer.open().     subscribedPartitionsToStartOffsets = new HashMap<>().     final List<KafkaTopicPartition> allPartitions = partitionDiscoverer.discoverPartitions().     if (restoredState != null) {         for (KafkaTopicPartition partition : allPartitions) {             if (!restoredState.containsKey(partition)) {                 restoredState.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET).             }         }         for (Map.Entry<KafkaTopicPartition, Long> restoredStateEntry : restoredState.entrySet()) {             if (!restoredFromOldState) {                 // restored partitions that should not be subscribed by this subtask                 if (KafkaTopicPartitionAssigner.assign(restoredStateEntry.getKey(), getRuntimeContext().getNumberOfParallelSubtasks()) == getRuntimeContext().getIndexOfThisSubtask()) {                     subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue()).                 }             } else {                 // when restoring from older 1.1 / 1.2 state, the restored state would not be the union state.                 // in this case, just use the restored state as the subscribed partitions                 subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue()).             }         }         if (filterRestoredPartitionsWithCurrentTopicsDescriptor) {             subscribedPartitionsToStartOffsets.entrySet().removeIf(entry -> {                 if (!topicsDescriptor.isMatchingTopic(entry.getKey().getTopic())) {                     LOG.warn("{} is removed from subscribed partitions since it is no longer associated with topics descriptor of current execution.", entry.getKey()).                     return true.                 }                 return false.             }).         }         LOG.info("Consumer subtask {} will start reading {} partitions with offsets in restored state: {}", getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets).     } else {         // when the partition is actually read.         switch(startupMode) {             case SPECIFIC_OFFSETS:                 if (specificStartupOffsets == null) {                     throw new IllegalStateException("Startup mode for the consumer set to " + StartupMode.SPECIFIC_OFFSETS + ", but no specific offsets were specified.").                 }                 for (KafkaTopicPartition seedPartition : allPartitions) {                     Long specificOffset = specificStartupOffsets.get(seedPartition).                     if (specificOffset != null) {                         // since the specified offsets represent the next record to read, we subtract                         // it by one so that the initial state of the consumer will be correct                         subscribedPartitionsToStartOffsets.put(seedPartition, specificOffset - 1).                     } else {                         // default to group offset behaviour if the user-provided specific offsets                         // do not contain a value for this partition                         subscribedPartitionsToStartOffsets.put(seedPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET).                     }                 }                 break.             case TIMESTAMP:                 if (startupOffsetsTimestamp == null) {                     throw new IllegalStateException("Startup mode for the consumer set to " + StartupMode.TIMESTAMP + ", but no startup timestamp was specified.").                 }                 for (Map.Entry<KafkaTopicPartition, Long> partitionToOffset : fetchOffsetsWithTimestamp(allPartitions, startupOffsetsTimestamp).entrySet()) {                     subscribedPartitionsToStartOffsets.put(partitionToOffset.getKey(), (partitionToOffset.getValue() == null) ? // we default to using the latest offset for the partition                     KafkaTopicPartitionStateSentinel.LATEST_OFFSET : // it by one so that the initial state of the consumer will be correct                     partitionToOffset.getValue() - 1).                 }                 break.             default:                 for (KafkaTopicPartition seedPartition : allPartitions) {                     subscribedPartitionsToStartOffsets.put(seedPartition, startupMode.getStateSentinel()).                 }         }         if (!subscribedPartitionsToStartOffsets.isEmpty()) {             switch(startupMode) {                 case EARLIEST:                     LOG.info("Consumer subtask {} will start reading the following {} partitions from the earliest offsets: {}", getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets.keySet()).                     break.                 case LATEST:                     LOG.info("Consumer subtask {} will start reading the following {} partitions from the latest offsets: {}", getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets.keySet()).                     break.                 case TIMESTAMP:                     LOG.info("Consumer subtask {} will start reading the following {} partitions from timestamp {}: {}", getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), startupOffsetsTimestamp, subscribedPartitionsToStartOffsets.keySet()).                     break.                 case SPECIFIC_OFFSETS:                     LOG.info("Consumer subtask {} will start reading the following {} partitions from the specified startup offsets {}: {}", getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), specificStartupOffsets, subscribedPartitionsToStartOffsets.keySet()).                     List<KafkaTopicPartition> partitionsDefaultedToGroupOffsets = new ArrayList<>(subscribedPartitionsToStartOffsets.size()).                     for (Map.Entry<KafkaTopicPartition, Long> subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) {                         if (subscribedPartition.getValue() == KafkaTopicPartitionStateSentinel.GROUP_OFFSET) {                             partitionsDefaultedToGroupOffsets.add(subscribedPartition.getKey()).                         }                     }                     if (partitionsDefaultedToGroupOffsets.size() > 0) {                         LOG.warn("Consumer subtask {} cannot find offsets for the following {} partitions in the specified startup offsets: {}" + ". their startup offsets will be defaulted to their committed group offsets in Kafka.", getRuntimeContext().getIndexOfThisSubtask(), partitionsDefaultedToGroupOffsets.size(), partitionsDefaultedToGroupOffsets).                     }                     break.                 case GROUP_OFFSETS:                     LOG.info("Consumer subtask {} will start reading the following {} partitions from the committed group offsets in Kafka: {}", getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets.keySet()).             }         } else {             LOG.info("Consumer subtask {} initially has no partitions to read from.", getRuntimeContext().getIndexOfThisSubtask()).         }     } }
false;public;0;4;;@Override public void onSuccess() {     successfulCommits.inc(). }
false;public;1;5;;@Override public void onException(Throwable cause) {     LOG.warn("Async Kafka commit failed.", cause).     failedCommits.inc(). }
false;public;1;60;;@Override public void run(SourceContext<T> sourceContext) throws Exception {     if (subscribedPartitionsToStartOffsets == null) {         throw new Exception("The partitions were not set for the consumer").     }     // initialize commit metrics and default offset callback method     this.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER).     this.failedCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER).     this.offsetCommitCallback = new KafkaCommitCallback() {          @Override         public void onSuccess() {             successfulCommits.inc().         }          @Override         public void onException(Throwable cause) {             LOG.warn("Async Kafka commit failed.", cause).             failedCommits.inc().         }     }.     // status will automatically be triggered back to be active.     if (subscribedPartitionsToStartOffsets.isEmpty()) {         sourceContext.markAsTemporarilyIdle().     }     // from this point forward:     // - 'snapshotState' will draw offsets from the fetcher,     // instead of being built from `subscribedPartitionsToStartOffsets`     // - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to     // Kafka through the fetcher, if configured to do so)     this.kafkaFetcher = createFetcher(sourceContext, subscribedPartitionsToStartOffsets, periodicWatermarkAssigner, punctuatedWatermarkAssigner, (StreamingRuntimeContext) getRuntimeContext(), offsetCommitMode, getRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP), useMetrics).     if (!running) {         return.     }     // 2) Old state - partition discovery is disabled and only the main fetcher loop is executed     if (discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED) {         kafkaFetcher.runFetchLoop().     } else {         runWithPartitionDiscovery().     } }
false;private;0;17;;private void runWithPartitionDiscovery() throws Exception {     final AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>().     createAndStartDiscoveryLoop(discoveryLoopErrorRef).     kafkaFetcher.runFetchLoop().     // make sure that the partition discoverer is waked up so that     // the discoveryLoopThread exits     partitionDiscoverer.wakeup().     joinDiscoveryLoopThread().     // rethrow any fetcher errors     final Exception discoveryLoopError = discoveryLoopErrorRef.get().     if (discoveryLoopError != null) {         throw new RuntimeException(discoveryLoopError).     } }
false;;0;6;;@VisibleForTesting void joinDiscoveryLoopThread() throws InterruptedException {     if (discoveryLoopThread != null) {         discoveryLoopThread.join().     } }
false;private;1;50;;private void createAndStartDiscoveryLoop(AtomicReference<Exception> discoveryLoopErrorRef) {     discoveryLoopThread = new Thread(() -> {         try {             while (running) {                 if (LOG.isDebugEnabled()) {                     LOG.debug("Consumer subtask {} is trying to discover new partitions ...", getRuntimeContext().getIndexOfThisSubtask()).                 }                 final List<KafkaTopicPartition> discoveredPartitions.                 try {                     discoveredPartitions = partitionDiscoverer.discoverPartitions().                 } catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {                     // this would only happen if the consumer was canceled. simply escape the loop                     break.                 }                 // no need to add the discovered partitions if we were closed during the meantime                 if (running && !discoveredPartitions.isEmpty()) {                     kafkaFetcher.addDiscoveredPartitions(discoveredPartitions).                 }                 // do not waste any time sleeping if we're not running anymore                 if (running && discoveryIntervalMillis != 0) {                     try {                         Thread.sleep(discoveryIntervalMillis).                     } catch (InterruptedException iex) {                         // may be interrupted if the consumer was canceled midway. simply escape the loop                         break.                     }                 }             }         } catch (Exception e) {             discoveryLoopErrorRef.set(e).         } finally {             // (if not running, cancel() was already called)             if (running) {                 cancel().             }         }     }, "Kafka Partition Discovery for " + getRuntimeContext().getTaskNameWithSubtasks()).     discoveryLoopThread.start(). }
false;public;0;24;;@Override public void cancel() {     // set ourselves as not running.     // this would let the main discovery loop escape as soon as possible     running = false.     if (discoveryLoopThread != null) {         if (partitionDiscoverer != null) {             // we cannot close the discoverer here, as it is error-prone to concurrent access.             // only wakeup the discoverer, the discovery loop will clean itself up after it escapes             partitionDiscoverer.wakeup().         }         // the discovery loop may currently be sleeping in-between         // consecutive discoveries. interrupt to shutdown faster         discoveryLoopThread.interrupt().     }     // abort the fetcher, if there is one     if (kafkaFetcher != null) {         kafkaFetcher.cancel().     } }
false;public;0;25;;@Override public void close() throws Exception {     cancel().     joinDiscoveryLoopThread().     Exception exception = null.     if (partitionDiscoverer != null) {         try {             partitionDiscoverer.close().         } catch (Exception e) {             exception = e.         }     }     try {         super.close().     } catch (Exception e) {         exception = ExceptionUtils.firstOrSuppressed(e, exception).     }     if (exception != null) {         throw exception.     } }
false;public,final;1;37;;// ------------------------------------------------------------------------ // Checkpoint and restore // ------------------------------------------------------------------------ @Override public final void initializeState(FunctionInitializationContext context) throws Exception {     OperatorStateStore stateStore = context.getOperatorStateStore().     ListState<Tuple2<KafkaTopicPartition, Long>> oldRoundRobinListState = stateStore.getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME).     this.unionOffsetStates = stateStore.getUnionListState(new ListStateDescriptor<>(OFFSETS_STATE_NAME, TypeInformation.of(new TypeHint<Tuple2<KafkaTopicPartition, Long>>() {     }))).     if (context.isRestored() && !restoredFromOldState) {         restoredState = new TreeMap<>(new KafkaTopicPartition.Comparator()).         // migrate from 1.2 state, if there is any         for (Tuple2<KafkaTopicPartition, Long> kafkaOffset : oldRoundRobinListState.get()) {             restoredFromOldState = true.             unionOffsetStates.add(kafkaOffset).         }         oldRoundRobinListState.clear().         if (restoredFromOldState && discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {             throw new IllegalArgumentException("Topic / partition discovery cannot be enabled if the job is restored from a savepoint from Flink 1.2.x.").         }         // populate actual holder for restored state         for (Tuple2<KafkaTopicPartition, Long> kafkaOffset : unionOffsetStates.get()) {             restoredState.put(kafkaOffset.f0, kafkaOffset.f1).         }         LOG.info("Setting restore state in the FlinkKafkaConsumer: {}", restoredState).     } else {         LOG.info("No restore state for FlinkKafkaConsumer.").     } }
false;public,final;1;43;;@Override public final void snapshotState(FunctionSnapshotContext context) throws Exception {     if (!running) {         LOG.debug("snapshotState() called on closed source").     } else {         unionOffsetStates.clear().         final AbstractFetcher<?, ?> fetcher = this.kafkaFetcher.         if (fetcher == null) {             // originally restored offsets or the assigned partitions             for (Map.Entry<KafkaTopicPartition, Long> subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) {                 unionOffsetStates.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue())).             }             if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) {                 // the map cannot be asynchronously updated, because only one checkpoint call can happen                 // on this function at a time: either snapshotState() or notifyCheckpointComplete()                 pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState).             }         } else {             HashMap<KafkaTopicPartition, Long> currentOffsets = fetcher.snapshotCurrentState().             if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) {                 // the map cannot be asynchronously updated, because only one checkpoint call can happen                 // on this function at a time: either snapshotState() or notifyCheckpointComplete()                 pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets).             }             for (Map.Entry<KafkaTopicPartition, Long> kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) {                 unionOffsetStates.add(Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue())).             }         }         if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) {             // truncate the map of pending offsets to commit, to prevent infinite growth             while (pendingOffsetsToCommit.size() > MAX_NUM_PENDING_CHECKPOINTS) {                 pendingOffsetsToCommit.remove(0).             }         }     } }
false;public,final;1;49;;@Override public final void notifyCheckpointComplete(long checkpointId) throws Exception {     if (!running) {         LOG.debug("notifyCheckpointComplete() called on closed source").         return.     }     final AbstractFetcher<?, ?> fetcher = this.kafkaFetcher.     if (fetcher == null) {         LOG.debug("notifyCheckpointComplete() called on uninitialized source").         return.     }     if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) {         // only one commit operation must be in progress         if (LOG.isDebugEnabled()) {             LOG.debug("Committing offsets to Kafka/ZooKeeper for checkpoint " + checkpointId).         }         try {             final int posInMap = pendingOffsetsToCommit.indexOf(checkpointId).             if (posInMap == -1) {                 LOG.warn("Received confirmation for unknown checkpoint id {}", checkpointId).                 return.             }             @SuppressWarnings("unchecked")             Map<KafkaTopicPartition, Long> offsets = (Map<KafkaTopicPartition, Long>) pendingOffsetsToCommit.remove(posInMap).             // remove older checkpoints in map             for (int i = 0. i < posInMap. i++) {                 pendingOffsetsToCommit.remove(0).             }             if (offsets == null || offsets.size() == 0) {                 LOG.debug("Checkpoint state was empty.").                 return.             }             fetcher.commitInternalOffsetsToKafka(offsets, offsetCommitCallback).         } catch (Exception e) {             if (running) {                 throw e.             }         // else ignore exception if we are no longer running         }     } }
true;protected,abstract;8;9;/**  * Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the  * data, and emits it into the data streams.  *  * @param sourceContext The source context to emit data to.  * @param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets.  * @param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator.  * @param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator.  * @param runtimeContext The task's runtime context.  *  * @return The instantiated fetcher  *  * @throws Exception The method should forward exceptions  */ ;// ------------------------------------------------------------------------ // Kafka Consumer specific methods // ------------------------------------------------------------------------ /**  * Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the  * data, and emits it into the data streams.  *  * @param sourceContext The source context to emit data to.  * @param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets.  * @param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator.  * @param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator.  * @param runtimeContext The task's runtime context.  *  * @return The instantiated fetcher  *  * @throws Exception The method should forward exceptions  */ protected abstract AbstractFetcher<T, ?> createFetcher(SourceContext<T> sourceContext, Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, StreamingRuntimeContext runtimeContext, OffsetCommitMode offsetCommitMode, MetricGroup kafkaMetricGroup, boolean useMetrics) throws Exception.
true;protected,abstract;3;4;/**  * Creates the partition discoverer that is used to find new partitions for this subtask.  *  * @param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern.  * @param indexOfThisSubtask The index of this consumer subtask.  * @param numParallelSubtasks The total number of parallel consumer subtasks.  *  * @return The instantiated partition discoverer  */ ;/**  * Creates the partition discoverer that is used to find new partitions for this subtask.  *  * @param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern.  * @param indexOfThisSubtask The index of this consumer subtask.  * @param numParallelSubtasks The total number of parallel consumer subtasks.  *  * @return The instantiated partition discoverer  */ protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(KafkaTopicsDescriptor topicsDescriptor, int indexOfThisSubtask, int numParallelSubtasks).
false;protected,abstract;0;1;;protected abstract boolean getIsAutoCommitEnabled().
false;protected,abstract;2;3;;protected abstract Map<KafkaTopicPartition, Long> fetchOffsetsWithTimestamp(Collection<KafkaTopicPartition> partitions, long timestamp).
false;public;0;4;;// ------------------------------------------------------------------------ // ResultTypeQueryable methods // ------------------------------------------------------------------------ @Override public TypeInformation<T> getProducedType() {     return deserializer.getProducedType(). }
false;;0;4;;// ------------------------------------------------------------------------ // Test utilities // ------------------------------------------------------------------------ @VisibleForTesting Map<KafkaTopicPartition, Long> getSubscribedPartitionsToStartOffsets() {     return subscribedPartitionsToStartOffsets. }
false;;0;4;;@VisibleForTesting TreeMap<KafkaTopicPartition, Long> getRestoredState() {     return restoredState. }
false;;0;4;;@VisibleForTesting OffsetCommitMode getOffsetCommitMode() {     return offsetCommitMode. }
false;;0;4;;@VisibleForTesting LinkedMap getPendingOffsetsToCommit() {     return pendingOffsetsToCommit. }
