commented;modifiers;parameterAmount;loc;comment;code
true;public;1;5;/**  * Sets the Kafka version to be used.  *  * @param version Kafka version. E.g., "0.8", "0.11", etc.  */ ;/**  * Sets the Kafka version to be used.  *  * @param version Kafka version. E.g., "0.8", "0.11", etc.  */ public Kafka version(String version) {     Preconditions.checkNotNull(version).     this.version = version.     return this. }
true;public;1;5;/**  * Sets the topic from which the table is read.  *  * @param topic The topic from which the table is read.  */ ;/**  * Sets the topic from which the table is read.  *  * @param topic The topic from which the table is read.  */ public Kafka topic(String topic) {     Preconditions.checkNotNull(topic).     this.topic = topic.     return this. }
true;public;1;9;/**  * Sets the configuration properties for the Kafka consumer. Resets previously set properties.  *  * @param properties The configuration properties for the Kafka consumer.  */ ;/**  * Sets the configuration properties for the Kafka consumer. Resets previously set properties.  *  * @param properties The configuration properties for the Kafka consumer.  */ public Kafka properties(Properties properties) {     Preconditions.checkNotNull(properties).     if (this.kafkaProperties == null) {         this.kafkaProperties = new HashMap<>().     }     this.kafkaProperties.clear().     properties.forEach((k, v) -> this.kafkaProperties.put((String) k, (String) v)).     return this. }
true;public;2;9;/**  * Adds a configuration properties for the Kafka consumer.  *  * @param key property key for the Kafka consumer  * @param value property value for the Kafka consumer  */ ;/**  * Adds a configuration properties for the Kafka consumer.  *  * @param key property key for the Kafka consumer  * @param value property value for the Kafka consumer  */ public Kafka property(String key, String value) {     Preconditions.checkNotNull(key).     Preconditions.checkNotNull(value).     if (this.kafkaProperties == null) {         this.kafkaProperties = new HashMap<>().     }     kafkaProperties.put(key, value).     return this. }
true;public;0;5;/**  * Configures to start reading from the earliest offset for all partitions.  *  * @see FlinkKafkaConsumerBase#setStartFromEarliest()  */ ;/**  * Configures to start reading from the earliest offset for all partitions.  *  * @see FlinkKafkaConsumerBase#setStartFromEarliest()  */ public Kafka startFromEarliest() {     this.startupMode = StartupMode.EARLIEST.     this.specificOffsets = null.     return this. }
true;public;0;5;/**  * Configures to start reading from the latest offset for all partitions.  *  * @see FlinkKafkaConsumerBase#setStartFromLatest()  */ ;/**  * Configures to start reading from the latest offset for all partitions.  *  * @see FlinkKafkaConsumerBase#setStartFromLatest()  */ public Kafka startFromLatest() {     this.startupMode = StartupMode.LATEST.     this.specificOffsets = null.     return this. }
true;public;0;5;/**  * Configures to start reading from any committed group offsets found in Zookeeper / Kafka brokers.  *  * @see FlinkKafkaConsumerBase#setStartFromGroupOffsets()  */ ;/**  * Configures to start reading from any committed group offsets found in Zookeeper / Kafka brokers.  *  * @see FlinkKafkaConsumerBase#setStartFromGroupOffsets()  */ public Kafka startFromGroupOffsets() {     this.startupMode = StartupMode.GROUP_OFFSETS.     this.specificOffsets = null.     return this. }
true;public;1;5;/**  * Configures to start reading partitions from specific offsets, set independently for each partition.  * Resets previously set offsets.  *  * @param specificOffsets the specified offsets for partitions  * @see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map)  */ ;/**  * Configures to start reading partitions from specific offsets, set independently for each partition.  * Resets previously set offsets.  *  * @param specificOffsets the specified offsets for partitions  * @see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map)  */ public Kafka startFromSpecificOffsets(Map<Integer, Long> specificOffsets) {     this.startupMode = StartupMode.SPECIFIC_OFFSETS.     this.specificOffsets = Preconditions.checkNotNull(specificOffsets).     return this. }
true;public;2;8;/**  * Configures to start reading partitions from specific offsets and specifies the given offset for  * the given partition.  *  * @param partition partition index  * @param specificOffset partition offset to start reading from  * @see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map)  */ ;/**  * Configures to start reading partitions from specific offsets and specifies the given offset for  * the given partition.  *  * @param partition partition index  * @param specificOffset partition offset to start reading from  * @see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map)  */ public Kafka startFromSpecificOffset(int partition, long specificOffset) {     this.startupMode = StartupMode.SPECIFIC_OFFSETS.     if (this.specificOffsets == null) {         this.specificOffsets = new HashMap<>().     }     this.specificOffsets.put(partition, specificOffset).     return this. }
true;public;0;5;/**  * Configures how to partition records from Flink's partitions into Kafka's partitions.  *  * <p>This strategy ensures that each Flink partition ends up in one Kafka partition.  *  * <p>Note: One Kafka partition can contain multiple Flink partitions. Examples:  *  * <p>More Flink partitions than Kafka partitions. Some (or all) Kafka partitions contain  * the output of more than one flink partition:  * <pre>  *     Flink Sinks            Kafka Partitions  *         1    ----------------&gt.    1  *         2    --------------/  *         3    -------------/  *         4    ------------/  * </pre>  *  * <p>Fewer Flink partitions than Kafka partitions:  * <pre>  *     Flink Sinks            Kafka Partitions  *         1    ----------------&gt.    1  *         2    ----------------&gt.    2  *                                      3  *                                      4  *                                      5  * </pre>  *  * @see org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner  */ ;/**  * Configures how to partition records from Flink's partitions into Kafka's partitions.  *  * <p>This strategy ensures that each Flink partition ends up in one Kafka partition.  *  * <p>Note: One Kafka partition can contain multiple Flink partitions. Examples:  *  * <p>More Flink partitions than Kafka partitions. Some (or all) Kafka partitions contain  * the output of more than one flink partition:  * <pre>  *     Flink Sinks            Kafka Partitions  *         1    ----------------&gt.    1  *         2    --------------/  *         3    -------------/  *         4    ------------/  * </pre>  *  * <p>Fewer Flink partitions than Kafka partitions:  * <pre>  *     Flink Sinks            Kafka Partitions  *         1    ----------------&gt.    1  *         2    ----------------&gt.    2  *                                      3  *                                      4  *                                      5  * </pre>  *  * @see org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner  */ public Kafka sinkPartitionerFixed() {     sinkPartitionerType = CONNECTOR_SINK_PARTITIONER_VALUE_FIXED.     sinkPartitionerClass = null.     return this. }
true;public;0;5;/**  * Configures how to partition records from Flink's partitions into Kafka's partitions.  *  * <p>This strategy ensures that records will be distributed to Kafka partitions in a  * round-robin fashion.  *  * <p>Note: This strategy is useful to avoid an unbalanced partitioning. However, it will  * cause a lot of network connections between all the Flink instances and all the Kafka brokers.  */ ;/**  * Configures how to partition records from Flink's partitions into Kafka's partitions.  *  * <p>This strategy ensures that records will be distributed to Kafka partitions in a  * round-robin fashion.  *  * <p>Note: This strategy is useful to avoid an unbalanced partitioning. However, it will  * cause a lot of network connections between all the Flink instances and all the Kafka brokers.  */ public Kafka sinkPartitionerRoundRobin() {     sinkPartitionerType = CONNECTOR_SINK_PARTITIONER_VALUE_ROUND_ROBIN.     sinkPartitionerClass = null.     return this. }
true;public;1;5;/**  * Configures how to partition records from Flink's partitions into Kafka's partitions.  *  * <p>This strategy allows for a custom partitioner by providing an implementation  * of {@link FlinkKafkaPartitioner}.  */ ;/**  * Configures how to partition records from Flink's partitions into Kafka's partitions.  *  * <p>This strategy allows for a custom partitioner by providing an implementation  * of {@link FlinkKafkaPartitioner}.  */ public Kafka sinkPartitionerCustom(Class<? extends FlinkKafkaPartitioner> partitionerClass) {     sinkPartitionerType = CONNECTOR_SINK_PARTITIONER_VALUE_CUSTOM.     sinkPartitionerClass = Preconditions.checkNotNull(partitionerClass).     return this. }
false;protected;0;46;;@Override protected Map<String, String> toConnectorProperties() {     final DescriptorProperties properties = new DescriptorProperties().     if (version != null) {         properties.putString(CONNECTOR_VERSION, version).     }     if (topic != null) {         properties.putString(CONNECTOR_TOPIC, topic).     }     if (startupMode != null) {         properties.putString(CONNECTOR_STARTUP_MODE, KafkaValidator.normalizeStartupMode(startupMode)).     }     if (specificOffsets != null) {         final List<List<String>> values = new ArrayList<>().         for (Map.Entry<Integer, Long> specificOffset : specificOffsets.entrySet()) {             values.add(Arrays.asList(specificOffset.getKey().toString(), specificOffset.getValue().toString())).         }         properties.putIndexedFixedProperties(CONNECTOR_SPECIFIC_OFFSETS, Arrays.asList(CONNECTOR_SPECIFIC_OFFSETS_PARTITION, CONNECTOR_SPECIFIC_OFFSETS_OFFSET), values).     }     if (kafkaProperties != null) {         properties.putIndexedFixedProperties(CONNECTOR_PROPERTIES, Arrays.asList(CONNECTOR_PROPERTIES_KEY, CONNECTOR_PROPERTIES_VALUE), this.kafkaProperties.entrySet().stream().map(e -> Arrays.asList(e.getKey(), e.getValue())).collect(Collectors.toList())).     }     if (sinkPartitionerType != null) {         properties.putString(CONNECTOR_SINK_PARTITIONER, sinkPartitionerType).         if (sinkPartitionerClass != null) {             properties.putClass(CONNECTOR_SINK_PARTITIONER_CLASS, sinkPartitionerClass).         }     }     return properties.asMap(). }
