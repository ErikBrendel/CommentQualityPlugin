commented;modifiers;parameterAmount;loc;comment;code
true;public;1;3;/**  * Defines whether the producer should fail on errors, or only log them.  * If this is set to true, then exceptions will be only logged, if set to false,  * exceptions will be eventually thrown and cause the streaming program to  * fail (and enter recovery).  *  * @param logFailuresOnly The flag to indicate logging-only on exceptions.  */ ;// ---------------------------------- Properties -------------------------- /**  * Defines whether the producer should fail on errors, or only log them.  * If this is set to true, then exceptions will be only logged, if set to false,  * exceptions will be eventually thrown and cause the streaming program to  * fail (and enter recovery).  *  * @param logFailuresOnly The flag to indicate logging-only on exceptions.  */ public void setLogFailuresOnly(boolean logFailuresOnly) {     this.logFailuresOnly = logFailuresOnly. }
true;public;1;3;/**  * If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers  * to be acknowledged by the Kafka producer on a checkpoint.  * This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.  *  * @param flush Flag indicating the flushing mode (true = flush on checkpoint)  */ ;/**  * If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers  * to be acknowledged by the Kafka producer on a checkpoint.  * This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.  *  * @param flush Flag indicating the flushing mode (true = flush on checkpoint)  */ public void setFlushOnCheckpoint(boolean flush) {     this.flushOnCheckpoint = flush. }
true;protected;1;4;/**  * Used for testing only.  */ ;/**  * Used for testing only.  */ @VisibleForTesting protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props) {     return new KafkaProducer<>(props). }
false;public;2;7;;@Override public void onCompletion(RecordMetadata metadata, Exception exception) {     if (exception != null && asyncException == null) {         asyncException = exception.     }     acknowledgeMessage(). }
false;public;2;7;;@Override public void onCompletion(RecordMetadata metadata, Exception e) {     if (e != null) {         LOG.error("Error while sending record to Kafka: " + e.getMessage(), e).     }     acknowledgeMessage(). }
true;public;1;60;/**  * Initializes the connection to Kafka.  */ ;// ----------------------------------- Utilities -------------------------- /**  * Initializes the connection to Kafka.  */ @Override public void open(Configuration configuration) {     producer = getKafkaProducer(this.producerConfig).     RuntimeContext ctx = getRuntimeContext().     if (null != flinkKafkaPartitioner) {         if (flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {             ((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(getPartitionsByTopic(this.defaultTopicId, this.producer)).         }         flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks()).     }     LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}", ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId).     // register Kafka metrics to Flink accumulators     if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {         Map<MetricName, ? extends Metric> metrics = this.producer.metrics().         if (metrics == null) {             // MapR's Kafka implementation returns null here.             LOG.info("Producer implementation does not support metrics").         } else {             final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer").             for (Map.Entry<MetricName, ? extends Metric> metric : metrics.entrySet()) {                 kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue())).             }         }     }     if (flushOnCheckpoint && !((StreamingRuntimeContext) this.getRuntimeContext()).isCheckpointingEnabled()) {         LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.").         flushOnCheckpoint = false.     }     if (logFailuresOnly) {         callback = new Callback() {              @Override             public void onCompletion(RecordMetadata metadata, Exception e) {                 if (e != null) {                     LOG.error("Error while sending record to Kafka: " + e.getMessage(), e).                 }                 acknowledgeMessage().             }         }.     } else {         callback = new Callback() {              @Override             public void onCompletion(RecordMetadata metadata, Exception exception) {                 if (exception != null && asyncException == null) {                     asyncException = exception.                 }                 acknowledgeMessage().             }         }.     } }
true;public;2;35;/**  * Called when new data arrives to the sink, and forwards it to Kafka.  *  * @param next  * 		The incoming data  */ ;/**  * Called when new data arrives to the sink, and forwards it to Kafka.  *  * @param next  * 		The incoming data  */ @Override public void invoke(IN next, Context context) throws Exception {     // propagate asynchronous errors     checkErroneous().     byte[] serializedKey = schema.serializeKey(next).     byte[] serializedValue = schema.serializeValue(next).     String targetTopic = schema.getTargetTopic(next).     if (targetTopic == null) {         targetTopic = defaultTopicId.     }     int[] partitions = this.topicPartitionsMap.get(targetTopic).     if (null == partitions) {         partitions = getPartitionsByTopic(targetTopic, producer).         this.topicPartitionsMap.put(targetTopic, partitions).     }     ProducerRecord<byte[], byte[]> record.     if (flinkKafkaPartitioner == null) {         record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue).     } else {         record = new ProducerRecord<>(targetTopic, flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions), serializedKey, serializedValue).     }     if (flushOnCheckpoint) {         synchronized (pendingRecordsLock) {             pendingRecords++.         }     }     producer.send(record, callback). }
false;public;0;9;;@Override public void close() throws Exception {     if (producer != null) {         producer.close().     }     // make sure we propagate pending errors     checkErroneous(). }
false;private;0;10;;// ------------------- Logic for handling checkpoint flushing -------------------------- // private void acknowledgeMessage() {     if (flushOnCheckpoint) {         synchronized (pendingRecordsLock) {             pendingRecords--.             if (pendingRecords == 0) {                 pendingRecordsLock.notifyAll().             }         }     } }
true;protected,abstract;0;1;/**  * Flush pending records.  */ ;/**  * Flush pending records.  */ protected abstract void flush().
false;public;1;4;;@Override public void initializeState(FunctionInitializationContext context) throws Exception { // nothing to do }
false;public;1;18;;@Override public void snapshotState(FunctionSnapshotContext ctx) throws Exception {     // check for asynchronous errors and fail the checkpoint if necessary     checkErroneous().     if (flushOnCheckpoint) {         // flushing is activated: We need to wait until pendingRecords is 0         flush().         synchronized (pendingRecordsLock) {             if (pendingRecords != 0) {                 throw new IllegalStateException("Pending record count must be zero at this point: " + pendingRecords).             }             // if the flushed requests has errors, we should propagate it also and fail the checkpoint             checkErroneous().         }     } }
false;protected;0;8;;// ----------------------------------- Utilities -------------------------- protected void checkErroneous() throws Exception {     Exception e = asyncException.     if (e != null) {         // prevent double throwing         asyncException = null.         throw new Exception("Failed to send data to Kafka: " + e.getMessage(), e).     } }
false;public,static;1;12;;public static Properties getPropertiesFromBrokerList(String brokerList) {     String[] elements = brokerList.split(",").     // validate the broker addresses     for (String broker : elements) {         NetUtils.getCorrectHostnamePort(broker).     }     Properties props = new Properties().     props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList).     return props. }
false;public;2;4;;@Override public int compare(PartitionInfo o1, PartitionInfo o2) {     return Integer.compare(o1.partition(), o2.partition()). }
false;protected,static;2;19;;protected static int[] getPartitionsByTopic(String topic, KafkaProducer<byte[], byte[]> producer) {     // the fetched list is immutable, so we're creating a mutable copy in order to sort it     List<PartitionInfo> partitionsList = new ArrayList<>(producer.partitionsFor(topic)).     // sort the partitions by partition id to make sure the fetched partition list is the same across subtasks     Collections.sort(partitionsList, new Comparator<PartitionInfo>() {          @Override         public int compare(PartitionInfo o1, PartitionInfo o2) {             return Integer.compare(o1.partition(), o2.partition()).         }     }).     int[] partitions = new int[partitionsList.size()].     for (int i = 0. i < partitions.length. i++) {         partitions[i] = partitionsList.get(i).partition().     }     return partitions. }
false;protected;0;6;;@VisibleForTesting protected long numPendingRecords() {     synchronized (pendingRecordsLock) {         return pendingRecords.     } }
