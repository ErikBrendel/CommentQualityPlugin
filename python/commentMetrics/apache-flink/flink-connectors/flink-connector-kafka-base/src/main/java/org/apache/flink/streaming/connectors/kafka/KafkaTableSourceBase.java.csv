commented;modifiers;parameterAmount;loc;comment;code
true;public;1;8;/**  * NOTE: This method is for internal use only for defining a TableSource.  *       Do not use it in Table API programs.  */ ;/**  * NOTE: This method is for internal use only for defining a TableSource.  *       Do not use it in Table API programs.  */ @Override public DataStream<Row> getDataStream(StreamExecutionEnvironment env) {     DeserializationSchema<Row> deserializationSchema = getDeserializationSchema().     // Version-specific Kafka consumer     FlinkKafkaConsumerBase<Row> kafkaConsumer = getKafkaConsumer(topic, properties, deserializationSchema).     return env.addSource(kafkaConsumer).name(explainSource()). }
false;public;0;4;;@Override public TypeInformation<Row> getReturnType() {     return deserializationSchema.getProducedType(). }
false;public;0;4;;@Override public TableSchema getTableSchema() {     return schema. }
false;public;0;4;;@Override public String getProctimeAttribute() {     return proctimeAttribute.orElse(null). }
false;public;0;4;;@Override public List<RowtimeAttributeDescriptor> getRowtimeAttributeDescriptors() {     return rowtimeAttributeDescriptors. }
false;public;0;4;;@Override public Map<String, String> getFieldMapping() {     return fieldMapping.orElse(null). }
false;public;0;4;;@Override public String explainSource() {     return TableConnectorUtils.generateRuntimeName(this.getClass(), schema.getFieldNames()). }
true;public;0;3;/**  * Returns the properties for the Kafka consumer.  *  * @return properties for the Kafka consumer.  */ ;/**  * Returns the properties for the Kafka consumer.  *  * @return properties for the Kafka consumer.  */ public Properties getProperties() {     return properties. }
true;public;0;3;/**  * Returns the deserialization schema.  *  * @return The deserialization schema  */ ;/**  * Returns the deserialization schema.  *  * @return The deserialization schema  */ public DeserializationSchema<Row> getDeserializationSchema() {     return deserializationSchema. }
false;public;1;19;;@Override public boolean equals(Object o) {     if (this == o) {         return true.     }     if (o == null || getClass() != o.getClass()) {         return false.     }     final KafkaTableSourceBase that = (KafkaTableSourceBase) o.     return Objects.equals(schema, that.schema) && Objects.equals(proctimeAttribute, that.proctimeAttribute) && Objects.equals(rowtimeAttributeDescriptors, that.rowtimeAttributeDescriptors) && Objects.equals(fieldMapping, that.fieldMapping) && Objects.equals(topic, that.topic) && Objects.equals(properties, that.properties) && Objects.equals(deserializationSchema, that.deserializationSchema) && startupMode == that.startupMode && Objects.equals(specificStartupOffsets, that.specificStartupOffsets). }
false;public;0;13;;@Override public int hashCode() {     return Objects.hash(schema, proctimeAttribute, rowtimeAttributeDescriptors, fieldMapping, topic, properties, deserializationSchema, startupMode, specificStartupOffsets). }
true;protected;3;22;/**  * Returns a version-specific Kafka consumer with the start position configured.  *  * @param topic                 Kafka topic to consume.  * @param properties            Properties for the Kafka consumer.  * @param deserializationSchema Deserialization schema to use for Kafka records.  * @return The version-specific Kafka consumer  */ ;/**  * Returns a version-specific Kafka consumer with the start position configured.  *  * @param topic                 Kafka topic to consume.  * @param properties            Properties for the Kafka consumer.  * @param deserializationSchema Deserialization schema to use for Kafka records.  * @return The version-specific Kafka consumer  */ protected FlinkKafkaConsumerBase<Row> getKafkaConsumer(String topic, Properties properties, DeserializationSchema<Row> deserializationSchema) {     FlinkKafkaConsumerBase<Row> kafkaConsumer = createKafkaConsumer(topic, properties, deserializationSchema).     switch(startupMode) {         case EARLIEST:             kafkaConsumer.setStartFromEarliest().             break.         case LATEST:             kafkaConsumer.setStartFromLatest().             break.         case GROUP_OFFSETS:             kafkaConsumer.setStartFromGroupOffsets().             break.         case SPECIFIC_OFFSETS:             kafkaConsumer.setStartFromSpecificOffsets(specificStartupOffsets).             break.     }     return kafkaConsumer. }
true;private;1;12;/**  * Validates a field of the schema to be the processing time attribute.  *  * @param proctimeAttribute The name of the field that becomes the processing time field.  */ ;// ////// VALIDATION FOR PARAMETERS /**  * Validates a field of the schema to be the processing time attribute.  *  * @param proctimeAttribute The name of the field that becomes the processing time field.  */ private Optional<String> validateProctimeAttribute(Optional<String> proctimeAttribute) {     return proctimeAttribute.map((attribute) -> {         // validate that field exists and is of correct type         Optional<TypeInformation<?>> tpe = schema.getFieldType(attribute).         if (!tpe.isPresent()) {             throw new ValidationException("Processing time attribute '" + attribute + "' is not present in TableSchema.").         } else if (tpe.get() != Types.SQL_TIMESTAMP()) {             throw new ValidationException("Processing time attribute '" + attribute + "' is not of type SQL_TIMESTAMP.").         }         return attribute.     }). }
true;private;1;14;/**  * Validates a list of fields to be rowtime attributes.  *  * @param rowtimeAttributeDescriptors The descriptors of the rowtime attributes.  */ ;/**  * Validates a list of fields to be rowtime attributes.  *  * @param rowtimeAttributeDescriptors The descriptors of the rowtime attributes.  */ private List<RowtimeAttributeDescriptor> validateRowtimeAttributeDescriptors(List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors) {     Preconditions.checkNotNull(rowtimeAttributeDescriptors, "List of rowtime attributes must not be null.").     // validate that all declared fields exist and are of correct type     for (RowtimeAttributeDescriptor desc : rowtimeAttributeDescriptors) {         String rowtimeAttribute = desc.getAttributeName().         Optional<TypeInformation<?>> tpe = schema.getFieldType(rowtimeAttribute).         if (!tpe.isPresent()) {             throw new ValidationException("Rowtime attribute '" + rowtimeAttribute + "' is not present in TableSchema.").         } else if (tpe.get() != Types.SQL_TIMESTAMP()) {             throw new ValidationException("Rowtime attribute '" + rowtimeAttribute + "' is not of type SQL_TIMESTAMP.").         }     }     return rowtimeAttributeDescriptors. }
true;protected,abstract;3;4;/**  * Creates a version-specific Kafka consumer.  *  * @param topic                 Kafka topic to consume.  * @param properties            Properties for the Kafka consumer.  * @param deserializationSchema Deserialization schema to use for Kafka records.  * @return The version-specific Kafka consumer  */ ;// ////// ABSTRACT METHODS FOR SUBCLASSES /**  * Creates a version-specific Kafka consumer.  *  * @param topic                 Kafka topic to consume.  * @param properties            Properties for the Kafka consumer.  * @param deserializationSchema Deserialization schema to use for Kafka records.  * @return The version-specific Kafka consumer  */ protected abstract FlinkKafkaConsumerBase<Row> createKafkaConsumer(String topic, Properties properties, DeserializationSchema<Row> deserializationSchema).
