commented;modifiers;parameterAmount;loc;comment;code
true;public;1;20;/**  * Adds a list of newly discovered partitions to the fetcher for consuming.  *  * <p>This method creates the partition state holder for each new partition, using  * {@link KafkaTopicPartitionStateSentinel#EARLIEST_OFFSET} as the starting offset.  * It uses the earliest offset because there may be delay in discovering a partition  * after it was created and started receiving records.  *  * <p>After the state representation for a partition is created, it is added to the  * unassigned partitions queue to await to be consumed.  *  * @param newPartitions discovered partitions to add  */ ;/**  * Adds a list of newly discovered partitions to the fetcher for consuming.  *  * <p>This method creates the partition state holder for each new partition, using  * {@link KafkaTopicPartitionStateSentinel#EARLIEST_OFFSET} as the starting offset.  * It uses the earliest offset because there may be delay in discovering a partition  * after it was created and started receiving records.  *  * <p>After the state representation for a partition is created, it is added to the  * unassigned partitions queue to await to be consumed.  *  * @param newPartitions discovered partitions to add  */ public void addDiscoveredPartitions(List<KafkaTopicPartition> newPartitions) throws IOException, ClassNotFoundException {     List<KafkaTopicPartitionState<KPH>> newPartitionStates = createPartitionStateHolders(newPartitions, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET, timestampWatermarkMode, watermarksPeriodic, watermarksPunctuated, userCodeClassLoader).     if (useMetrics) {         registerOffsetMetrics(consumerMetricGroup, newPartitionStates).     }     for (KafkaTopicPartitionState<KPH> newPartitionState : newPartitionStates) {         // the ordering is crucial here. first register the state holder, then         // push it to the partitions queue to be read         subscribedPartitionStates.add(newPartitionState).         unassignedPartitionsQueue.add(newPartitionState).     } }
true;protected,final;0;3;/**  * Gets all partitions (with partition state) that this fetcher is subscribed to.  *  * @return All subscribed partitions.  */ ;// ------------------------------------------------------------------------ // Properties // ------------------------------------------------------------------------ /**  * Gets all partitions (with partition state) that this fetcher is subscribed to.  *  * @return All subscribed partitions.  */ protected final List<KafkaTopicPartitionState<KPH>> subscribedPartitionStates() {     return subscribedPartitionStates. }
false;public,abstract;0;1;;// ------------------------------------------------------------------------ // Core fetcher work methods // ------------------------------------------------------------------------ public abstract void runFetchLoop() throws Exception.
false;public,abstract;0;1;;public abstract void cancel().
true;public,final;2;7;/**  * Commits the given partition offsets to the Kafka brokers (or to ZooKeeper for  * older Kafka versions). This method is only ever called when the offset commit mode of  * the consumer is {@link OffsetCommitMode#ON_CHECKPOINTS}.  *  * <p>The given offsets are the internal checkpointed offsets, representing  * the last processed record of each partition. Version-specific implementations of this method  * need to hold the contract that the given offsets must be incremented by 1 before  * committing them, so that committed offsets to Kafka represent "the next record to process".  *  * @param offsets The offsets to commit to Kafka (implementations must increment offsets by 1 before committing).  * @param commitCallback The callback that the user should trigger when a commit request completes or fails.  * @throws Exception This method forwards exceptions.  */ ;// ------------------------------------------------------------------------ // Kafka version specifics // ------------------------------------------------------------------------ /**  * Commits the given partition offsets to the Kafka brokers (or to ZooKeeper for  * older Kafka versions). This method is only ever called when the offset commit mode of  * the consumer is {@link OffsetCommitMode#ON_CHECKPOINTS}.  *  * <p>The given offsets are the internal checkpointed offsets, representing  * the last processed record of each partition. Version-specific implementations of this method  * need to hold the contract that the given offsets must be incremented by 1 before  * committing them, so that committed offsets to Kafka represent "the next record to process".  *  * @param offsets The offsets to commit to Kafka (implementations must increment offsets by 1 before committing).  * @param commitCallback The callback that the user should trigger when a commit request completes or fails.  * @throws Exception This method forwards exceptions.  */ public final void commitInternalOffsetsToKafka(Map<KafkaTopicPartition, Long> offsets, @Nonnull KafkaCommitCallback commitCallback) throws Exception {     // Ignore sentinels. They might appear here if snapshot has started before actual offsets values     // replaced sentinels     doCommitInternalOffsetsToKafka(filterOutSentinels(offsets), commitCallback). }
false;protected,abstract;2;3;;protected abstract void doCommitInternalOffsetsToKafka(Map<KafkaTopicPartition, Long> offsets, @Nonnull KafkaCommitCallback commitCallback) throws Exception.
false;private;1;6;;private Map<KafkaTopicPartition, Long> filterOutSentinels(Map<KafkaTopicPartition, Long> offsets) {     return offsets.entrySet().stream().filter(entry -> !KafkaTopicPartitionStateSentinel.isSentinel(entry.getValue())).collect(Collectors.toMap(entry -> entry.getKey(), entry -> entry.getValue())). }
true;protected,abstract;1;1;/**  * Creates the Kafka version specific representation of the given  * topic partition.  *  * @param partition The Flink representation of the Kafka topic partition.  * @return The version-specific Kafka representation of the Kafka topic partition.  */ ;/**  * Creates the Kafka version specific representation of the given  * topic partition.  *  * @param partition The Flink representation of the Kafka topic partition.  * @return The version-specific Kafka representation of the Kafka topic partition.  */ protected abstract KPH createKafkaPartitionHandle(KafkaTopicPartition partition).
true;public;0;10;/**  * Takes a snapshot of the partition offsets.  *  * <p>Important: This method must be called under the checkpoint lock.  *  * @return A map from partition to current offset.  */ ;// ------------------------------------------------------------------------ // snapshot and restore the state // ------------------------------------------------------------------------ /**  * Takes a snapshot of the partition offsets.  *  * <p>Important: This method must be called under the checkpoint lock.  *  * @return A map from partition to current offset.  */ public HashMap<KafkaTopicPartition, Long> snapshotCurrentState() {     // this method assumes that the checkpoint lock is held     assert Thread.holdsLock(checkpointLock).     HashMap<KafkaTopicPartition, Long> state = new HashMap<>(subscribedPartitionStates.size()).     for (KafkaTopicPartitionState<KPH> partition : subscribedPartitionStates) {         state.put(partition.getKafkaTopicPartition(), partition.getOffset()).     }     return state. }
true;protected;3;24;/**  * Emits a record without attaching an existing timestamp to it.  *  * <p>Implementation Note: This method is kept brief to be JIT inlining friendly.  * That makes the fast path efficient, the extended paths are called as separate methods.  *  * @param record The record to emit  * @param partitionState The state of the Kafka partition from which the record was fetched  * @param offset The offset of the record  */ ;// ------------------------------------------------------------------------ // emitting records // ------------------------------------------------------------------------ /**  * Emits a record without attaching an existing timestamp to it.  *  * <p>Implementation Note: This method is kept brief to be JIT inlining friendly.  * That makes the fast path efficient, the extended paths are called as separate methods.  *  * @param record The record to emit  * @param partitionState The state of the Kafka partition from which the record was fetched  * @param offset The offset of the record  */ protected void emitRecord(T record, KafkaTopicPartitionState<KPH> partitionState, long offset) throws Exception {     if (record != null) {         if (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) {             // atomicity of record emission and offset state update             synchronized (checkpointLock) {                 sourceContext.collect(record).                 partitionState.setOffset(offset).             }         } else if (timestampWatermarkMode == PERIODIC_WATERMARKS) {             emitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, Long.MIN_VALUE).         } else {             emitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, Long.MIN_VALUE).         }     } else {         // if the record is null, simply just update the offset state for partition         synchronized (checkpointLock) {             partitionState.setOffset(offset).         }     } }
true;protected;4;25;/**  * Emits a record attaching a timestamp to it.  *  * <p>Implementation Note: This method is kept brief to be JIT inlining friendly.  * That makes the fast path efficient, the extended paths are called as separate methods.  *  * @param record The record to emit  * @param partitionState The state of the Kafka partition from which the record was fetched  * @param offset The offset of the record  */ ;/**  * Emits a record attaching a timestamp to it.  *  * <p>Implementation Note: This method is kept brief to be JIT inlining friendly.  * That makes the fast path efficient, the extended paths are called as separate methods.  *  * @param record The record to emit  * @param partitionState The state of the Kafka partition from which the record was fetched  * @param offset The offset of the record  */ protected void emitRecordWithTimestamp(T record, KafkaTopicPartitionState<KPH> partitionState, long offset, long timestamp) throws Exception {     if (record != null) {         if (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) {             // atomicity of record emission and offset state update             synchronized (checkpointLock) {                 sourceContext.collectWithTimestamp(record, timestamp).                 partitionState.setOffset(offset).             }         } else if (timestampWatermarkMode == PERIODIC_WATERMARKS) {             emitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, timestamp).         } else {             emitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, timestamp).         }     } else {         // if the record is null, simply just update the offset state for partition         synchronized (checkpointLock) {             partitionState.setOffset(offset).         }     } }
true;private;4;22;/**  * Record emission, if a timestamp will be attached from an assigner that is  * also a periodic watermark generator.  */ ;/**  * Record emission, if a timestamp will be attached from an assigner that is  * also a periodic watermark generator.  */ private void emitRecordWithTimestampAndPeriodicWatermark(T record, KafkaTopicPartitionState<KPH> partitionState, long offset, long kafkaEventTimestamp) {     @SuppressWarnings("unchecked")     final KafkaTopicPartitionStateWithPeriodicWatermarks<T, KPH> withWatermarksState = (KafkaTopicPartitionStateWithPeriodicWatermarks<T, KPH>) partitionState.     // extract timestamp - this accesses/modifies the per-partition state inside the     // watermark generator instance, so we need to lock the access on the     // partition state. concurrent access can happen from the periodic emitter     final long timestamp.     // noinspection SynchronizationOnLocalVariableOrMethodParameter     synchronized (withWatermarksState) {         timestamp = withWatermarksState.getTimestampForRecord(record, kafkaEventTimestamp).     }     // atomicity of record emission and offset state update     synchronized (checkpointLock) {         sourceContext.collectWithTimestamp(record, timestamp).         partitionState.setOffset(offset).     } }
true;private;4;24;/**  * Record emission, if a timestamp will be attached from an assigner that is  * also a punctuated watermark generator.  */ ;/**  * Record emission, if a timestamp will be attached from an assigner that is  * also a punctuated watermark generator.  */ private void emitRecordWithTimestampAndPunctuatedWatermark(T record, KafkaTopicPartitionState<KPH> partitionState, long offset, long kafkaEventTimestamp) {     @SuppressWarnings("unchecked")     final KafkaTopicPartitionStateWithPunctuatedWatermarks<T, KPH> withWatermarksState = (KafkaTopicPartitionStateWithPunctuatedWatermarks<T, KPH>) partitionState.     // only one thread ever works on accessing timestamps and watermarks     // from the punctuated extractor     final long timestamp = withWatermarksState.getTimestampForRecord(record, kafkaEventTimestamp).     final Watermark newWatermark = withWatermarksState.checkAndGetNewWatermark(record, timestamp).     // atomicity of record emission and offset state update     synchronized (checkpointLock) {         sourceContext.collectWithTimestamp(record, timestamp).         partitionState.setOffset(offset).     }     // new cross-partition watermark     if (newWatermark != null) {         updateMinPunctuatedWatermark(newWatermark).     } }
true;private;1;23;/**  * Checks whether a new per-partition watermark is also a new cross-partition watermark.  */ ;/**  * Checks whether a new per-partition watermark is also a new cross-partition watermark.  */ private void updateMinPunctuatedWatermark(Watermark nextWatermark) {     if (nextWatermark.getTimestamp() > maxWatermarkSoFar) {         long newMin = Long.MAX_VALUE.         for (KafkaTopicPartitionState<?> state : subscribedPartitionStates) {             @SuppressWarnings("unchecked")             final KafkaTopicPartitionStateWithPunctuatedWatermarks<T, KPH> withWatermarksState = (KafkaTopicPartitionStateWithPunctuatedWatermarks<T, KPH>) state.             newMin = Math.min(newMin, withWatermarksState.getCurrentPartitionWatermark()).         }         // double-check locking pattern         if (newMin > maxWatermarkSoFar) {             synchronized (checkpointLock) {                 if (newMin > maxWatermarkSoFar) {                     maxWatermarkSoFar = newMin.                     sourceContext.emitWatermark(new Watermark(newMin)).                 }             }         }     } }
true;private;5;73;/**  * Utility method that takes the topic partitions and creates the topic partition state  * holders, depending on the timestamp / watermark mode.  */ ;// ------------------------------------------------------------------------ // Utilities // ------------------------------------------------------------------------ /**  * Utility method that takes the topic partitions and creates the topic partition state  * holders, depending on the timestamp / watermark mode.  */ private List<KafkaTopicPartitionState<KPH>> createPartitionStateHolders(Map<KafkaTopicPartition, Long> partitionsToInitialOffsets, int timestampWatermarkMode, SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, ClassLoader userCodeClassLoader) throws IOException, ClassNotFoundException {     // CopyOnWrite as adding discovered partitions could happen in parallel     // while different threads iterate the partitions list     List<KafkaTopicPartitionState<KPH>> partitionStates = new CopyOnWriteArrayList<>().     switch(timestampWatermarkMode) {         case NO_TIMESTAMPS_WATERMARKS:             {                 for (Map.Entry<KafkaTopicPartition, Long> partitionEntry : partitionsToInitialOffsets.entrySet()) {                     // create the kafka version specific partition handle                     KPH kafkaHandle = createKafkaPartitionHandle(partitionEntry.getKey()).                     KafkaTopicPartitionState<KPH> partitionState = new KafkaTopicPartitionState<>(partitionEntry.getKey(), kafkaHandle).                     partitionState.setOffset(partitionEntry.getValue()).                     partitionStates.add(partitionState).                 }                 return partitionStates.             }         case PERIODIC_WATERMARKS:             {                 for (Map.Entry<KafkaTopicPartition, Long> partitionEntry : partitionsToInitialOffsets.entrySet()) {                     KPH kafkaHandle = createKafkaPartitionHandle(partitionEntry.getKey()).                     AssignerWithPeriodicWatermarks<T> assignerInstance = watermarksPeriodic.deserializeValue(userCodeClassLoader).                     KafkaTopicPartitionStateWithPeriodicWatermarks<T, KPH> partitionState = new KafkaTopicPartitionStateWithPeriodicWatermarks<>(partitionEntry.getKey(), kafkaHandle, assignerInstance).                     partitionState.setOffset(partitionEntry.getValue()).                     partitionStates.add(partitionState).                 }                 return partitionStates.             }         case PUNCTUATED_WATERMARKS:             {                 for (Map.Entry<KafkaTopicPartition, Long> partitionEntry : partitionsToInitialOffsets.entrySet()) {                     KPH kafkaHandle = createKafkaPartitionHandle(partitionEntry.getKey()).                     AssignerWithPunctuatedWatermarks<T> assignerInstance = watermarksPunctuated.deserializeValue(userCodeClassLoader).                     KafkaTopicPartitionStateWithPunctuatedWatermarks<T, KPH> partitionState = new KafkaTopicPartitionStateWithPunctuatedWatermarks<>(partitionEntry.getKey(), kafkaHandle, assignerInstance).                     partitionState.setOffset(partitionEntry.getValue()).                     partitionStates.add(partitionState).                 }                 return partitionStates.             }         default:             // cannot happen, add this as a guard for the future             throw new RuntimeException().     } }
true;private;6;20;/**  * Shortcut variant of {@link #createPartitionStateHolders(Map, int, SerializedValue, SerializedValue, ClassLoader)}  * that uses the same offset for all partitions when creating their state holders.  */ ;/**  * Shortcut variant of {@link #createPartitionStateHolders(Map, int, SerializedValue, SerializedValue, ClassLoader)}  * that uses the same offset for all partitions when creating their state holders.  */ private List<KafkaTopicPartitionState<KPH>> createPartitionStateHolders(List<KafkaTopicPartition> partitions, long initialOffset, int timestampWatermarkMode, SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, ClassLoader userCodeClassLoader) throws IOException, ClassNotFoundException {     Map<KafkaTopicPartition, Long> partitionsToInitialOffset = new HashMap<>(partitions.size()).     for (KafkaTopicPartition partition : partitions) {         partitionsToInitialOffset.put(partition, initialOffset).     }     return createPartitionStateHolders(partitionsToInitialOffset, timestampWatermarkMode, watermarksPeriodic, watermarksPunctuated, userCodeClassLoader). }
true;private;2;16;/**  * For each partition, register a new metric group to expose current offsets and committed offsets.  * Per-partition metric groups can be scoped by user variables {@link KafkaConsumerMetricConstants#OFFSETS_BY_TOPIC_METRICS_GROUP}  * and {@link KafkaConsumerMetricConstants#OFFSETS_BY_PARTITION_METRICS_GROUP}.  *  * <p>Note: this method also registers gauges for deprecated offset metrics, to maintain backwards compatibility.  *  * @param consumerMetricGroup The consumer metric group  * @param partitionOffsetStates The partition offset state holders, whose values will be used to update metrics  */ ;// ------------------------- Metrics ---------------------------------- /**  * For each partition, register a new metric group to expose current offsets and committed offsets.  * Per-partition metric groups can be scoped by user variables {@link KafkaConsumerMetricConstants#OFFSETS_BY_TOPIC_METRICS_GROUP}  * and {@link KafkaConsumerMetricConstants#OFFSETS_BY_PARTITION_METRICS_GROUP}.  *  * <p>Note: this method also registers gauges for deprecated offset metrics, to maintain backwards compatibility.  *  * @param consumerMetricGroup The consumer metric group  * @param partitionOffsetStates The partition offset state holders, whose values will be used to update metrics  */ private void registerOffsetMetrics(MetricGroup consumerMetricGroup, List<KafkaTopicPartitionState<KPH>> partitionOffsetStates) {     for (KafkaTopicPartitionState<KPH> ktp : partitionOffsetStates) {         MetricGroup topicPartitionGroup = consumerMetricGroup.addGroup(OFFSETS_BY_TOPIC_METRICS_GROUP, ktp.getTopic()).addGroup(OFFSETS_BY_PARTITION_METRICS_GROUP, Integer.toString(ktp.getPartition())).         topicPartitionGroup.gauge(CURRENT_OFFSETS_METRICS_GAUGE, new OffsetGauge(ktp, OffsetGaugeType.CURRENT_OFFSET)).         topicPartitionGroup.gauge(COMMITTED_OFFSETS_METRICS_GAUGE, new OffsetGauge(ktp, OffsetGaugeType.COMMITTED_OFFSET)).         legacyCurrentOffsetsMetricGroup.gauge(getLegacyOffsetsMetricsGaugeName(ktp), new OffsetGauge(ktp, OffsetGaugeType.CURRENT_OFFSET)).         legacyCommittedOffsetsMetricGroup.gauge(getLegacyOffsetsMetricsGaugeName(ktp), new OffsetGauge(ktp, OffsetGaugeType.COMMITTED_OFFSET)).     } }
false;private,static;1;3;;private static String getLegacyOffsetsMetricsGaugeName(KafkaTopicPartitionState<?> ktp) {     return ktp.getTopic() + "-" + ktp.getPartition(). }
false;public;0;11;;@Override public Long getValue() {     switch(gaugeType) {         case COMMITTED_OFFSET:             return ktp.getCommittedOffset().         case CURRENT_OFFSET:             return ktp.getOffset().         default:             throw new RuntimeException("Unknown gauge type: " + gaugeType).     } }
false;public;0;3;;// ------------------------------------------------- public void start() {     timerService.registerTimer(timerService.getCurrentProcessingTime() + interval, this). }
false;public;1;28;;@Override public void onProcessingTime(long timestamp) throws Exception {     long minAcrossAll = Long.MAX_VALUE.     boolean isEffectiveMinAggregation = false.     for (KafkaTopicPartitionState<?> state : allPartitions) {         // we access the current watermark for the periodic assigners under the state         // lock, to prevent concurrent modification to any internal variables         final long curr.         // noinspection SynchronizationOnLocalVariableOrMethodParameter         synchronized (state) {             curr = ((KafkaTopicPartitionStateWithPeriodicWatermarks<?, ?>) state).getCurrentWatermarkTimestamp().         }         minAcrossAll = Math.min(minAcrossAll, curr).         isEffectiveMinAggregation = true.     }     // emit next watermark, if there is one     if (isEffectiveMinAggregation && minAcrossAll > lastWatermarkTimestamp) {         lastWatermarkTimestamp = minAcrossAll.         emitter.emitWatermark(new Watermark(minAcrossAll)).     }     // schedule the next watermark     timerService.registerTimer(timerService.getCurrentProcessingTime() + interval, this). }
