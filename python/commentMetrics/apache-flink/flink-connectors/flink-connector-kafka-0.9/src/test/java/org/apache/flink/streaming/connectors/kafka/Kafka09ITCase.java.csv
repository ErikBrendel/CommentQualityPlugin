commented;modifiers;parameterAmount;loc;comment;code
false;public;0;4;;// ------------------------------------------------------------------------ // Suite of Tests // ------------------------------------------------------------------------ @Test(timeout = 60000) public void testFailOnNoBroker() throws Exception {     runFailOnNoBrokerTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testConcurrentProducerConsumerTopology() throws Exception {     runSimpleConcurrentProducerConsumerTopology(). }
false;public;0;4;;@Test(timeout = 60000) public void testKeyValueSupport() throws Exception {     runKeyValueTest(). }
false;public;0;4;;// --- canceling / failures --- @Test(timeout = 60000) public void testCancelingEmptyTopic() throws Exception {     runCancelingOnEmptyInputTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testCancelingFullTopic() throws Exception {     runCancelingOnFullInputTest(). }
false;public;0;4;;// --- source to partition mappings and exactly once --- @Test(timeout = 60000) public void testOneToOneSources() throws Exception {     runOneToOneExactlyOnceTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testOneSourceMultiplePartitions() throws Exception {     runOneSourceMultiplePartitionsExactlyOnceTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testMultipleSourcesOnePartition() throws Exception {     runMultipleSourcesOnePartitionExactlyOnceTest(). }
false;public;0;4;;// --- broker failure --- @Test(timeout = 60000) public void testBrokerFailure() throws Exception {     runBrokerFailureTest(). }
false;public;0;4;;// --- special executions --- @Test(timeout = 60000) public void testBigRecordJob() throws Exception {     runBigRecordTestTopology(). }
false;public;0;4;;@Test(timeout = 60000) public void testMultipleTopics() throws Exception {     runProduceConsumeMultipleTopics(). }
false;public;0;4;;@Test(timeout = 60000) public void testAllDeletes() throws Exception {     runAllDeletesTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testEndOfStream() throws Exception {     runEndOfStreamTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testMetrics() throws Throwable {     runMetricsTest(). }
false;public;0;4;;// --- startup mode --- @Test(timeout = 60000) public void testStartFromEarliestOffsets() throws Exception {     runStartFromEarliestOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromLatestOffsets() throws Exception {     runStartFromLatestOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromGroupOffsets() throws Exception {     runStartFromGroupOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromSpecificOffsets() throws Exception {     runStartFromSpecificOffsets(). }
false;public;0;4;;// --- offset committing --- @Test(timeout = 60000) public void testCommitOffsetsToKafka() throws Exception {     runCommitOffsetsToKafka(). }
false;public;0;4;;@Test(timeout = 60000) public void testAutoOffsetRetrievalAndCommitToKafka() throws Exception {     runAutoOffsetRetrievalAndCommitToKafka(). }
false;public;1;11;;@Override public void run(SourceContext<String> ctx) throws Exception {     long i = 0.     while (running) {         byte[] data = new byte[] { 1 }.         // 1 byte         ctx.collect(new String(data)).         if (i++ == 100L) {             running = false.         }     } }
false;public;0;4;;@Override public void cancel() {     running = false. }
false;public;2;4;;@Override public void invoke(String value, Context context) throws Exception { }
true;public;0;67;/**  * Kafka09 specific RateLimiter test. This test produces 100 bytes of data to a test topic  * and then runs a job with {@link FlinkKafkaConsumer09} as the source and a {@link GuavaFlinkConnectorRateLimiter} with  * a desired rate of 3 bytes / second. Based on the execution time, the test asserts that this rate was not surpassed.  * If no rate limiter is set on the consumer, the test should fail.  */ ;/**  * Kafka09 specific RateLimiter test. This test produces 100 bytes of data to a test topic  * and then runs a job with {@link FlinkKafkaConsumer09} as the source and a {@link GuavaFlinkConnectorRateLimiter} with  * a desired rate of 3 bytes / second. Based on the execution time, the test asserts that this rate was not surpassed.  * If no rate limiter is set on the consumer, the test should fail.  */ @Test(timeout = 60000) public void testRateLimitedConsumer() throws Exception {     final String testTopic = "testRateLimitedConsumer".     createTestTopic(testTopic, 3, 1).     // ---------- Produce a stream into Kafka -------------------     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(1).     env.getConfig().setRestartStrategy(RestartStrategies.noRestart()).     env.getConfig().disableSysoutLogging().     DataStream<String> stream = env.addSource(new SourceFunction<String>() {          private static final long serialVersionUID = 1L.          boolean running = true.          @Override         public void run(SourceContext<String> ctx) throws Exception {             long i = 0.             while (running) {                 byte[] data = new byte[] { 1 }.                 // 1 byte                 ctx.collect(new String(data)).                 if (i++ == 100L) {                     running = false.                 }             }         }          @Override         public void cancel() {             running = false.         }     }).     stream.addSink(new FlinkKafkaProducer09<>(testTopic, new SimpleStringSchema(), standardProps)).setParallelism(1).     env.execute("Produce 100 bytes of data to test topic").     // ---------- Consumer from Kafka in a ratelimited way -----------     env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(1).     env.getConfig().setRestartStrategy(RestartStrategies.noRestart()).     env.getConfig().disableSysoutLogging().     // ---------- RateLimiter config -------------     // bytes/second     long globalRate = 3.     FlinkKafkaConsumer09<String> consumer09 = new FlinkKafkaConsumer09<>(testTopic, new StringDeserializer(globalRate), standardProps).     FlinkConnectorRateLimiter rateLimiter = new GuavaFlinkConnectorRateLimiter().     rateLimiter.setRate(globalRate).     consumer09.setRateLimiter(rateLimiter).     DataStream<String> stream1 = env.addSource(consumer09).     stream1.addSink(new SinkFunction<String>() {          @Override         public void invoke(String value, Context context) throws Exception {         }     }).     env.execute("Consume 100 bytes of data from test topic").     // ------- Assertions --------------     Assert.assertNotNull(consumer09.getRateLimiter()).     Assert.assertEquals(consumer09.getRateLimiter().getRate(), globalRate).     deleteTestTopic(testTopic). }
false;public;0;4;;@Override public TypeInformation<String> getProducedType() {     return ti. }
false;public;5;8;;@Override public String deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset) throws IOException {     cnt++.     DataInputView in = new DataInputViewStreamWrapper(new ByteArrayInputStream(message)).     String e = ser.deserialize(in).     return e. }
false;public;1;12;;@Override public boolean isEndOfStream(String nextElement) {     if (cnt > 100L) {         endTime = System.currentTimeMillis().         // Approximate bytes/second read based on job execution time.         long bytesPerSecond = 100 * 1000L / (endTime - startTime).         Assert.assertTrue(bytesPerSecond > 0).         Assert.assertTrue(bytesPerSecond <= globalRate).         return true.     }     return false. }
