commented;modifiers;parameterAmount;loc;comment;code
false;public;1;6;;@Override public ConsumerRecords<?, ?> answer(InvocationOnMock invocation) throws InterruptedException {     sync.trigger().     blockerLatch.await().     return ConsumerRecords.empty(). }
false;public;1;5;;@Override public Void answer(InvocationOnMock invocation) {     blockerLatch.trigger().     return null. }
false;public;0;8;;@Override public void run() {     try {         fetcher.runFetchLoop().     } catch (Throwable t) {         error.set(t).     } }
false;public;0;8;;@Override public void run() {     try {         fetcher.commitInternalOffsetsToKafka(testCommitData, mock(KafkaCommitCallback.class)).     } catch (Throwable t) {         commitError.set(t).     } }
false;public;0;112;;@Test public void testCommitDoesNotBlock() throws Exception {     // test data     final KafkaTopicPartition testPartition = new KafkaTopicPartition("test", 42).     final Map<KafkaTopicPartition, Long> testCommitData = new HashMap<>().     testCommitData.put(testPartition, 11L).     // to synchronize when the consumer is in its blocking method     final OneShotLatch sync = new OneShotLatch().     // ----- the mock consumer with blocking poll calls ----     final MultiShotLatch blockerLatch = new MultiShotLatch().     KafkaConsumer<?, ?> mockConsumer = mock(KafkaConsumer.class).     when(mockConsumer.poll(anyLong())).thenAnswer(new Answer<ConsumerRecords<?, ?>>() {          @Override         public ConsumerRecords<?, ?> answer(InvocationOnMock invocation) throws InterruptedException {             sync.trigger().             blockerLatch.await().             return ConsumerRecords.empty().         }     }).     doAnswer(new Answer<Void>() {          @Override         public Void answer(InvocationOnMock invocation) {             blockerLatch.trigger().             return null.         }     }).when(mockConsumer).wakeup().     // make sure the fetcher creates the mock consumer     whenNew(KafkaConsumer.class).withAnyArguments().thenReturn(mockConsumer).     // ----- create the test fetcher -----     @SuppressWarnings("unchecked")     SourceContext<String> sourceContext = mock(SourceContext.class).     Map<KafkaTopicPartition, Long> partitionsWithInitialOffsets = Collections.singletonMap(new KafkaTopicPartition("test", 42), KafkaTopicPartitionStateSentinel.GROUP_OFFSET).     KafkaDeserializationSchema<String> schema = new KafkaDeserializationSchemaWrapper<>(new SimpleStringSchema()).     final Kafka09Fetcher<String> fetcher = new Kafka09Fetcher<>(sourceContext, partitionsWithInitialOffsets, null, /* periodic watermark extractor */     null, /* punctuated watermark extractor */     new TestProcessingTimeService(), 10, /* watermark interval */     this.getClass().getClassLoader(), "task_name", schema, new Properties(), 0L, new UnregisteredMetricsGroup(), new UnregisteredMetricsGroup(), false, null).     // ----- run the fetcher -----     final AtomicReference<Throwable> error = new AtomicReference<>().     final Thread fetcherRunner = new Thread("fetcher runner") {          @Override         public void run() {             try {                 fetcher.runFetchLoop().             } catch (Throwable t) {                 error.set(t).             }         }     }.     fetcherRunner.start().     // wait until the fetcher has reached the method of interest     sync.await().     // ----- trigger the offset commit -----     final AtomicReference<Throwable> commitError = new AtomicReference<>().     final Thread committer = new Thread("committer runner") {          @Override         public void run() {             try {                 fetcher.commitInternalOffsetsToKafka(testCommitData, mock(KafkaCommitCallback.class)).             } catch (Throwable t) {                 commitError.set(t).             }         }     }.     committer.start().     // ----- ensure that the committer finishes in time  -----     committer.join(30000).     assertFalse("The committer did not finish in time", committer.isAlive()).     // ----- test done, wait till the fetcher is done for a clean shutdown -----     fetcher.cancel().     fetcherRunner.join().     // check that there were no errors in the fetcher     final Throwable fetcherError = error.get().     if (fetcherError != null && !(fetcherError instanceof Handover.ClosedException)) {         throw new Exception("Exception in the fetcher", fetcherError).     }     final Throwable committerError = commitError.get().     if (committerError != null) {         throw new Exception("Exception in the committer", committerError).     } }
false;public;1;5;;@Override public ConsumerRecords<?, ?> answer(InvocationOnMock invocation) throws InterruptedException {     blockerLatch.await().     return ConsumerRecords.empty(). }
false;public;1;5;;@Override public Void answer(InvocationOnMock invocation) {     blockerLatch.trigger().     return null. }
false;public;1;13;;@Override public Void answer(InvocationOnMock invocation) {     @SuppressWarnings("unchecked")     Map<TopicPartition, OffsetAndMetadata> offsets = (Map<TopicPartition, OffsetAndMetadata>) invocation.getArguments()[0].     OffsetCommitCallback callback = (OffsetCommitCallback) invocation.getArguments()[1].     commitStore.add(offsets).     callback.onComplete(offsets, null).     return null. }
false;public;0;8;;@Override public void run() {     try {         fetcher.runFetchLoop().     } catch (Throwable t) {         error.set(t).     } }
false;public;0;143;;@Test public void ensureOffsetsGetCommitted() throws Exception {     // test data     final KafkaTopicPartition testPartition1 = new KafkaTopicPartition("test", 42).     final KafkaTopicPartition testPartition2 = new KafkaTopicPartition("another", 99).     final Map<KafkaTopicPartition, Long> testCommitData1 = new HashMap<>().     testCommitData1.put(testPartition1, 11L).     testCommitData1.put(testPartition2, 18L).     final Map<KafkaTopicPartition, Long> testCommitData2 = new HashMap<>().     testCommitData2.put(testPartition1, 19L).     testCommitData2.put(testPartition2, 28L).     final BlockingQueue<Map<TopicPartition, OffsetAndMetadata>> commitStore = new LinkedBlockingQueue<>().     // ----- the mock consumer with poll(), wakeup(), and commit(A)sync calls ----     final MultiShotLatch blockerLatch = new MultiShotLatch().     KafkaConsumer<?, ?> mockConsumer = mock(KafkaConsumer.class).     when(mockConsumer.poll(anyLong())).thenAnswer(new Answer<ConsumerRecords<?, ?>>() {          @Override         public ConsumerRecords<?, ?> answer(InvocationOnMock invocation) throws InterruptedException {             blockerLatch.await().             return ConsumerRecords.empty().         }     }).     doAnswer(new Answer<Void>() {          @Override         public Void answer(InvocationOnMock invocation) {             blockerLatch.trigger().             return null.         }     }).when(mockConsumer).wakeup().     doAnswer(new Answer<Void>() {          @Override         public Void answer(InvocationOnMock invocation) {             @SuppressWarnings("unchecked")             Map<TopicPartition, OffsetAndMetadata> offsets = (Map<TopicPartition, OffsetAndMetadata>) invocation.getArguments()[0].             OffsetCommitCallback callback = (OffsetCommitCallback) invocation.getArguments()[1].             commitStore.add(offsets).             callback.onComplete(offsets, null).             return null.         }     }).when(mockConsumer).commitAsync(Mockito.<Map<TopicPartition, OffsetAndMetadata>>any(), any(OffsetCommitCallback.class)).     // make sure the fetcher creates the mock consumer     whenNew(KafkaConsumer.class).withAnyArguments().thenReturn(mockConsumer).     // ----- create the test fetcher -----     @SuppressWarnings("unchecked")     SourceContext<String> sourceContext = mock(SourceContext.class).     Map<KafkaTopicPartition, Long> partitionsWithInitialOffsets = Collections.singletonMap(new KafkaTopicPartition("test", 42), KafkaTopicPartitionStateSentinel.GROUP_OFFSET).     KafkaDeserializationSchema<String> schema = new KafkaDeserializationSchemaWrapper<>(new SimpleStringSchema()).     final Kafka09Fetcher<String> fetcher = new Kafka09Fetcher<>(sourceContext, partitionsWithInitialOffsets, null, /* periodic watermark extractor */     null, /* punctuated watermark extractor */     new TestProcessingTimeService(), 10, /* watermark interval */     this.getClass().getClassLoader(), "task_name", schema, new Properties(), 0L, new UnregisteredMetricsGroup(), new UnregisteredMetricsGroup(), false, null).     // ----- run the fetcher -----     final AtomicReference<Throwable> error = new AtomicReference<>().     final Thread fetcherRunner = new Thread("fetcher runner") {          @Override         public void run() {             try {                 fetcher.runFetchLoop().             } catch (Throwable t) {                 error.set(t).             }         }     }.     fetcherRunner.start().     // ----- trigger the first offset commit -----     fetcher.commitInternalOffsetsToKafka(testCommitData1, mock(KafkaCommitCallback.class)).     Map<TopicPartition, OffsetAndMetadata> result1 = commitStore.take().     for (Entry<TopicPartition, OffsetAndMetadata> entry : result1.entrySet()) {         TopicPartition partition = entry.getKey().         if (partition.topic().equals("test")) {             assertEquals(42, partition.partition()).             assertEquals(12L, entry.getValue().offset()).         } else if (partition.topic().equals("another")) {             assertEquals(99, partition.partition()).             assertEquals(17L, entry.getValue().offset()).         }     }     // ----- trigger the second offset commit -----     fetcher.commitInternalOffsetsToKafka(testCommitData2, mock(KafkaCommitCallback.class)).     Map<TopicPartition, OffsetAndMetadata> result2 = commitStore.take().     for (Entry<TopicPartition, OffsetAndMetadata> entry : result2.entrySet()) {         TopicPartition partition = entry.getKey().         if (partition.topic().equals("test")) {             assertEquals(42, partition.partition()).             assertEquals(20L, entry.getValue().offset()).         } else if (partition.topic().equals("another")) {             assertEquals(99, partition.partition()).             assertEquals(27L, entry.getValue().offset()).         }     }     // ----- test done, wait till the fetcher is done for a clean shutdown -----     fetcher.cancel().     fetcherRunner.join().     // check that there were no errors in the fetcher     final Throwable caughtError = error.get().     if (caughtError != null && !(caughtError instanceof Handover.ClosedException)) {         throw new Exception("Exception in the fetcher", caughtError).     } }
false;public;1;4;;@Override public ConsumerRecords<?, ?> answer(InvocationOnMock invocation) {     return consumerRecords. }
false;public;0;8;;@Override public void run() {     try {         fetcher.runFetchLoop().     } catch (Throwable t) {         error.set(t).     } }
false;public;0;81;;@Test public void testCancellationWhenEmitBlocks() throws Exception {     // ----- some test data -----     final String topic = "test-topic".     final int partition = 3.     final byte[] payload = new byte[] { 1, 2, 3, 4 }.     final List<ConsumerRecord<byte[], byte[]>> records = Arrays.asList(new ConsumerRecord<>(topic, partition, 15, payload, payload), new ConsumerRecord<>(topic, partition, 16, payload, payload), new ConsumerRecord<>(topic, partition, 17, payload, payload)).     final Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> data = new HashMap<>().     data.put(new TopicPartition(topic, partition), records).     final ConsumerRecords<byte[], byte[]> consumerRecords = new ConsumerRecords<>(data).     // ----- the test consumer -----     final KafkaConsumer<?, ?> mockConsumer = mock(KafkaConsumer.class).     when(mockConsumer.poll(anyLong())).thenAnswer(new Answer<ConsumerRecords<?, ?>>() {          @Override         public ConsumerRecords<?, ?> answer(InvocationOnMock invocation) {             return consumerRecords.         }     }).     whenNew(KafkaConsumer.class).withAnyArguments().thenReturn(mockConsumer).     // ----- build a fetcher -----     BlockingSourceContext<String> sourceContext = new BlockingSourceContext<>().     Map<KafkaTopicPartition, Long> partitionsWithInitialOffsets = Collections.singletonMap(new KafkaTopicPartition(topic, partition), KafkaTopicPartitionStateSentinel.GROUP_OFFSET).     KafkaDeserializationSchema<String> schema = new KafkaDeserializationSchemaWrapper<>(new SimpleStringSchema()).     final Kafka09Fetcher<String> fetcher = new Kafka09Fetcher<>(sourceContext, partitionsWithInitialOffsets, null, /* periodic watermark extractor */     null, /* punctuated watermark extractor */     new TestProcessingTimeService(), 10, /* watermark interval */     this.getClass().getClassLoader(), "task_name", schema, new Properties(), 0L, new UnregisteredMetricsGroup(), new UnregisteredMetricsGroup(), false, null).     // ----- run the fetcher -----     final AtomicReference<Throwable> error = new AtomicReference<>().     final Thread fetcherRunner = new Thread("fetcher runner") {          @Override         public void run() {             try {                 fetcher.runFetchLoop().             } catch (Throwable t) {                 error.set(t).             }         }     }.     fetcherRunner.start().     // wait until the thread started to emit records to the source context     sourceContext.waitTillHasBlocker().     // now we try to cancel the fetcher, including the interruption usually done on the task thread     // once it has finished, there must be no more thread blocked on the source context     fetcher.cancel().     fetcherRunner.interrupt().     fetcherRunner.join().     assertFalse("fetcher threads did not properly finish", sourceContext.isStillBlocking()). }
false;public;1;4;;@Override public void collect(T element) {     block(). }
false;public;2;4;;@Override public void collectWithTimestamp(T element, long timestamp) {     block(). }
false;public;1;4;;@Override public void emitWatermark(Watermark mark) {     block(). }
false;public;0;4;;@Override public void markAsTemporarilyIdle() {     throw new UnsupportedOperationException(). }
false;public;0;4;;@Override public Object getCheckpointLock() {     return new Object(). }
false;public;0;2;;@Override public void close() { }
false;;0;3;;void waitTillHasBlocker() throws InterruptedException {     inBlocking.await(). }
false;;0;3;;boolean isStillBlocking() {     return lock.isLocked(). }
false;private;0;22;;@SuppressWarnings({ "InfiniteLoopStatement", "SynchronizationOnLocalVariableOrMethodParameter" }) private void block() {     lock.lock().     try {         inBlocking.trigger().         // put this thread to sleep indefinitely         final Object o = new Object().         while (true) {             synchronized (o) {                 o.wait().             }         }     } catch (InterruptedException e) {         // exit cleanly, simply reset the interruption flag         Thread.currentThread().interrupt().     } finally {         lock.unlock().     } }
