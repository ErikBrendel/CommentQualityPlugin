commented;modifiers;parameterAmount;loc;comment;code
false;protected;8;37;;@Override protected AbstractFetcher<T, ?> createFetcher(SourceContext<T> sourceContext, Map<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets, SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, StreamingRuntimeContext runtimeContext, OffsetCommitMode offsetCommitMode, MetricGroup consumerMetricGroup, boolean useMetrics) throws Exception {     // make sure that auto commit is disabled when our offset commit mode is ON_CHECKPOINTS.     // this overwrites whatever setting the user configured in the properties     adjustAutoCommitConfig(properties, offsetCommitMode).     // If a rateLimiter is set, then call rateLimiter.open() with the runtime context.     if (rateLimiter != null) {         rateLimiter.open(runtimeContext).     }     return new Kafka09Fetcher<>(sourceContext, assignedPartitionsWithInitialOffsets, watermarksPeriodic, watermarksPunctuated, runtimeContext.getProcessingTimeService(), runtimeContext.getExecutionConfig().getAutoWatermarkInterval(), runtimeContext.getUserCodeClassLoader(), runtimeContext.getTaskNameWithSubtasks(), deserializer, properties, pollTimeout, runtimeContext.getMetricGroup(), consumerMetricGroup, useMetrics, rateLimiter). }
false;protected;3;8;;@Override protected AbstractPartitionDiscoverer createPartitionDiscoverer(KafkaTopicsDescriptor topicsDescriptor, int indexOfThisSubtask, int numParallelSubtasks) {     return new Kafka09PartitionDiscoverer(topicsDescriptor, indexOfThisSubtask, numParallelSubtasks, properties). }
false;protected;0;5;;@Override protected boolean getIsAutoCommitEnabled() {     return getBoolean(properties, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true) && PropertiesUtil.getLong(properties, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 5000) > 0. }
false;protected;2;6;;@Override protected Map<KafkaTopicPartition, Long> fetchOffsetsWithTimestamp(Collection<KafkaTopicPartition> partitions, long timestamp) {     // this should not be reached, since we do not expose the timestamp-based startup feature in version 0.9.     throw new UnsupportedOperationException("Fetching partition offsets using timestamps is only supported in Kafka versions 0.10 and above."). }
true;private,static;1;16;/**  * Makes sure that the ByteArrayDeserializer is registered in the Kafka properties.  *  * @param props The Kafka properties to register the serializer in.  */ ;// ------------------------------------------------------------------------ // Utilities // ------------------------------------------------------------------------ /**  * Makes sure that the ByteArrayDeserializer is registered in the Kafka properties.  *  * @param props The Kafka properties to register the serializer in.  */ private static void setDeserializer(Properties props) {     final String deSerName = ByteArrayDeserializer.class.getName().     Object keyDeSer = props.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG).     Object valDeSer = props.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG).     if (keyDeSer != null && !keyDeSer.equals(deSerName)) {         LOG.warn("Ignoring configured key DeSerializer ({})", ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG).     }     if (valDeSer != null && !valDeSer.equals(deSerName)) {         LOG.warn("Ignoring configured value DeSerializer ({})", ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG).     }     props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, deSerName).     props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deSerName). }
true;public;1;3;/**  * Set a rate limiter to ratelimit bytes read from Kafka.  * @param kafkaRateLimiter  */ ;/**  * Set a rate limiter to ratelimit bytes read from Kafka.  * @param kafkaRateLimiter  */ public void setRateLimiter(FlinkConnectorRateLimiter kafkaRateLimiter) {     this.rateLimiter = kafkaRateLimiter. }
false;public;0;3;;public FlinkConnectorRateLimiter getRateLimiter() {     return rateLimiter. }
