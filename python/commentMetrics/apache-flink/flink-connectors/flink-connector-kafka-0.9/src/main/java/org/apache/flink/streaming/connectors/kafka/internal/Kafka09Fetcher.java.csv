commented;modifiers;parameterAmount;loc;comment;code
false;public;0;51;;// ------------------------------------------------------------------------ // Fetcher work methods // ------------------------------------------------------------------------ @Override public void runFetchLoop() throws Exception {     try {         final Handover handover = this.handover.         // kick off the actual Kafka consumer         consumerThread.start().         while (running) {             // this blocks until we get the next records             // it automatically re-throws exceptions encountered in the consumer thread             final ConsumerRecords<byte[], byte[]> records = handover.pollNext().             // get the records for each topic partition             for (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {                 List<ConsumerRecord<byte[], byte[]>> partitionRecords = records.records(partition.getKafkaPartitionHandle()).                 for (ConsumerRecord<byte[], byte[]> record : partitionRecords) {                     final T value = deserializer.deserialize(record).                     if (deserializer.isEndOfStream(value)) {                         // end of stream signaled                         running = false.                         break.                     }                     // emit the actual record. this also updates offset state atomically                     // and deals with timestamps and watermark generation                     emitRecord(value, partition, record.offset(), record).                 }             }         }     } finally {         // this signals the consumer thread that no more work is to be done         consumerThread.shutdown().     }     // on a clean exit, wait for the runner thread     try {         consumerThread.join().     } catch (InterruptedException e) {         // may be the result of a wake-up interruption after an exception.         // we ignore this here and only restore the interruption state         Thread.currentThread().interrupt().     } }
false;public;0;7;;@Override public void cancel() {     // flag the main thread to exit. A thread interrupt will come anyways.     running = false.     handover.close().     consumerThread.shutdown(). }
false;protected;4;9;;// ------------------------------------------------------------------------ // The below methods are overridden in the 0.10 fetcher, which otherwise // reuses most of the 0.9 fetcher behavior // ------------------------------------------------------------------------ protected void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partition, long offset, @SuppressWarnings("UnusedParameters") ConsumerRecord<?, ?> consumerRecord) throws Exception {     // the 0.9 Fetcher does not try to extract a timestamp     emitRecord(record, partition, offset). }
true;protected;0;3;/**  * Gets the name of this fetcher, for thread naming and logging purposes.  */ ;/**  * Gets the name of this fetcher, for thread naming and logging purposes.  */ protected String getFetcherName() {     return "Kafka 0.9 Fetcher". }
false;protected;0;3;;protected KafkaConsumerCallBridge09 createCallBridge() {     return new KafkaConsumerCallBridge09(). }
false;public;1;4;;// ------------------------------------------------------------------------ // Implement Methods of the AbstractFetcher // ------------------------------------------------------------------------ @Override public TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {     return new TopicPartition(partition.getTopic(), partition.getPartition()). }
false;protected;2;27;;@Override protected void doCommitInternalOffsetsToKafka(Map<KafkaTopicPartition, Long> offsets, @Nonnull KafkaCommitCallback commitCallback) throws Exception {     @SuppressWarnings("unchecked")     List<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates().     Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size()).     for (KafkaTopicPartitionState<TopicPartition> partition : partitions) {         Long lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition()).         if (lastProcessedOffset != null) {             checkState(lastProcessedOffset >= 0, "Illegal offset value to commit").             // committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.             // This does not affect Flink's checkpoints/saved state.             long offsetToCommit = lastProcessedOffset + 1.             offsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit)).             partition.setCommittedOffset(offsetToCommit).         }     }     // record the work to be committed by the main consumer thread and make sure the consumer notices that     consumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback). }
