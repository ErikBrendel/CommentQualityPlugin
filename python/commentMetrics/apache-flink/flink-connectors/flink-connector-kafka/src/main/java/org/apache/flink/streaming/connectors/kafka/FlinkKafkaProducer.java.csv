# id;timestamp;commentText;codeText;commentWords;codeWords
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema);1539704473;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			serializationSchema,_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer,string,broker,list,string,topic,id,keyed,serialization,schema,in,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema);1541587130;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			serializationSchema,_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer,string,broker,list,string,topic,id,keyed,serialization,schema,in,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema);1542186519;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			serializationSchema,_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer,string,broker,list,string,topic,id,keyed,serialization,schema,in,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema);1548148563;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			serializationSchema,_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer,string,broker,list,string,topic,id,keyed,serialization,schema,in,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema);1550567875;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			serializationSchema,_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer,string,broker,list,string,topic,id,keyed,serialization,schema,in,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema);1550652777;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer(String brokerList, String topicId, KeyedSerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			serializationSchema,_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer,string,broker,list,string,topic,id,keyed,serialization,schema,in,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		SerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1539704473;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer(_		String topicId,_		SerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		SerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1541587130;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer(_		String topicId,_		SerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		SerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1542186519;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer(_		String topicId,_		SerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		SerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1548148563;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer(_		String topicId,_		SerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		SerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1550567875;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer(_		String topicId,_		SerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		SerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1550652777;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer(_		String topicId,_		SerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer -> public void setLogFailuresOnly(boolean logFailuresOnly);1539704473;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducer -> public void setLogFailuresOnly(boolean logFailuresOnly);1541587130;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducer -> public void setLogFailuresOnly(boolean logFailuresOnly);1542186519;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducer -> public void setLogFailuresOnly(boolean logFailuresOnly);1548148563;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducer -> public void setLogFailuresOnly(boolean logFailuresOnly);1550567875;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducer -> public void setLogFailuresOnly(boolean logFailuresOnly);1550652777;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		FlinkKafkaProducer.Semantic semantic);1539704473;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional, FlinkKafkaProducer.Semantic, int)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration._@param semantic_Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic}).;public FlinkKafkaProducer(_		String topicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		FlinkKafkaProducer.Semantic semantic) {_		this(topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()),_			semantic,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,flink,kafka,producer,semantic,int,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,producer,semantic,semantic,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		FlinkKafkaProducer.Semantic semantic);1541587130;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional, FlinkKafkaProducer.Semantic, int)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration._@param semantic_Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic}).;public FlinkKafkaProducer(_		String topicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		FlinkKafkaProducer.Semantic semantic) {_		this(topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()),_			semantic,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,flink,kafka,producer,semantic,int,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,producer,semantic,semantic,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		FlinkKafkaProducer.Semantic semantic);1542186519;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional, FlinkKafkaProducer.Semantic, int)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration._@param semantic_Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic}).;public FlinkKafkaProducer(_		String topicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		FlinkKafkaProducer.Semantic semantic) {_		this(topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()),_			semantic,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,flink,kafka,producer,semantic,int,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,producer,semantic,semantic,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		FlinkKafkaProducer.Semantic semantic);1548148563;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional, FlinkKafkaProducer.Semantic, int)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration._@param semantic_Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic}).;public FlinkKafkaProducer(_		String topicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		FlinkKafkaProducer.Semantic semantic) {_		this(topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()),_			semantic,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,flink,kafka,producer,semantic,int,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,producer,semantic,semantic,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		FlinkKafkaProducer.Semantic semantic);1550567875;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional, FlinkKafkaProducer.Semantic, int)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration._@param semantic_Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic}).;public FlinkKafkaProducer(_		String topicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		FlinkKafkaProducer.Semantic semantic) {_		this(topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()),_			semantic,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,flink,kafka,producer,semantic,int,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,producer,semantic,semantic,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String topicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		FlinkKafkaProducer.Semantic semantic);1550652777;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional, FlinkKafkaProducer.Semantic, int)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration._@param semantic_Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic}).;public FlinkKafkaProducer(_		String topicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		FlinkKafkaProducer.Semantic semantic) {_		this(topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()),_			semantic,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,flink,kafka,producer,semantic,int,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,producer,semantic,semantic,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1539704473;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {_		this(_			defaultTopicId,_			serializationSchema,_			producerConfig,_			customPartitioner,_			FlinkKafkaProducer.Semantic.AT_LEAST_ONCE,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,default,topic,id,serialization,schema,producer,config,custom,partitioner,flink,kafka,producer,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1541587130;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {_		this(_			defaultTopicId,_			serializationSchema,_			producerConfig,_			customPartitioner,_			FlinkKafkaProducer.Semantic.AT_LEAST_ONCE,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,default,topic,id,serialization,schema,producer,config,custom,partitioner,flink,kafka,producer,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1542186519;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {_		this(_			defaultTopicId,_			serializationSchema,_			producerConfig,_			customPartitioner,_			FlinkKafkaProducer.Semantic.AT_LEAST_ONCE,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,default,topic,id,serialization,schema,producer,config,custom,partitioner,flink,kafka,producer,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1548148563;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {_		this(_			defaultTopicId,_			serializationSchema,_			producerConfig,_			customPartitioner,_			FlinkKafkaProducer.Semantic.AT_LEAST_ONCE,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,default,topic,id,serialization,schema,producer,config,custom,partitioner,flink,kafka,producer,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1550567875;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {_		this(_			defaultTopicId,_			serializationSchema,_			producerConfig,_			customPartitioner,_			FlinkKafkaProducer.Semantic.AT_LEAST_ONCE,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,default,topic,id,serialization,schema,producer,config,custom,partitioner,flink,kafka,producer,semantic
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner);1550652777;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner) {_		this(_			defaultTopicId,_			serializationSchema,_			producerConfig,_			customPartitioner,_			FlinkKafkaProducer.Semantic.AT_LEAST_ONCE,_			DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,this,default,topic,id,serialization,schema,producer,config,custom,partitioner,flink,kafka,producer,semantic
FlinkKafkaProducer -> @Override 	public void open(Configuration configuration) throws Exception;1539704473;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) throws Exception {_		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}__		super.open(configuration)__	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,throws,exception,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message,super,open,configuration
FlinkKafkaProducer -> @Override 	public void open(Configuration configuration) throws Exception;1541587130;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) throws Exception {_		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}__		super.open(configuration)__	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,throws,exception,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message,super,open,configuration
FlinkKafkaProducer -> @Override 	public void open(Configuration configuration) throws Exception;1542186519;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) throws Exception {_		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}__		super.open(configuration)__	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,throws,exception,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message,super,open,configuration
FlinkKafkaProducer -> @Override 	public void open(Configuration configuration) throws Exception;1548148563;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) throws Exception {_		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}__		super.open(configuration)__	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,throws,exception,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message,super,open,configuration
FlinkKafkaProducer -> @Override 	public void open(Configuration configuration) throws Exception;1550567875;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) throws Exception {_		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}__		super.open(configuration)__	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,throws,exception,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message,super,open,configuration
FlinkKafkaProducer -> @Override 	public void open(Configuration configuration) throws Exception;1550652777;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) throws Exception {_		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}__		super.open(configuration)__	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,throws,exception,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message,super,open,configuration
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig);1539704473;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig);1541587130;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig);1542186519;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig);1548148563;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig);1550567875;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig);1550652777;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, KeyedSerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			serializationSchema,_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,keyed,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> private void cleanUpUserContext();1539704473;After initialization make sure that all previous transactions from the current user context have been completed.;private void cleanUpUserContext() {_		if (!getUserContext().isPresent()) {_			return__		}_		abortTransactions(getUserContext().get().transactionalIds)__	};after,initialization,make,sure,that,all,previous,transactions,from,the,current,user,context,have,been,completed;private,void,clean,up,user,context,if,get,user,context,is,present,return,abort,transactions,get,user,context,get,transactional,ids
FlinkKafkaProducer -> private void cleanUpUserContext();1541587130;After initialization make sure that all previous transactions from the current user context have been completed.;private void cleanUpUserContext() {_		if (!getUserContext().isPresent()) {_			return__		}_		abortTransactions(getUserContext().get().transactionalIds)__	};after,initialization,make,sure,that,all,previous,transactions,from,the,current,user,context,have,been,completed;private,void,clean,up,user,context,if,get,user,context,is,present,return,abort,transactions,get,user,context,get,transactional,ids
FlinkKafkaProducer -> private void cleanUpUserContext();1542186519;After initialization make sure that all previous transactions from the current user context have been completed.;private void cleanUpUserContext() {_		if (!getUserContext().isPresent()) {_			return__		}_		abortTransactions(getUserContext().get().transactionalIds)__	};after,initialization,make,sure,that,all,previous,transactions,from,the,current,user,context,have,been,completed;private,void,clean,up,user,context,if,get,user,context,is,present,return,abort,transactions,get,user,context,get,transactional,ids
FlinkKafkaProducer -> private void cleanUpUserContext();1548148563;After initialization make sure that all previous transactions from the current user context have been completed.;private void cleanUpUserContext() {_		if (!getUserContext().isPresent()) {_			return__		}_		abortTransactions(getUserContext().get().transactionalIds)__	};after,initialization,make,sure,that,all,previous,transactions,from,the,current,user,context,have,been,completed;private,void,clean,up,user,context,if,get,user,context,is,present,return,abort,transactions,get,user,context,get,transactional,ids
FlinkKafkaProducer -> private void cleanUpUserContext();1550567875;After initialization make sure that all previous transactions from the current user context have been completed.;private void cleanUpUserContext() {_		if (!getUserContext().isPresent()) {_			return__		}_		abortTransactions(getUserContext().get().transactionalIds)__	};after,initialization,make,sure,that,all,previous,transactions,from,the,current,user,context,have,been,completed;private,void,clean,up,user,context,if,get,user,context,is,present,return,abort,transactions,get,user,context,get,transactional,ids
FlinkKafkaProducer -> private void cleanUpUserContext();1550652777;After initialization make sure that all previous transactions from the current user context have been completed.;private void cleanUpUserContext() {_		if (!getUserContext().isPresent()) {_			return__		}_		abortTransactions(getUserContext().get().transactionalIds)__	};after,initialization,make,sure,that,all,previous,transactions,from,the,current,user,context,have,been,completed;private,void,clean,up,user,context,if,get,user,context,is,present,return,abort,transactions,get,user,context,get,transactional,ids
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig);1539704473;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, SerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig);1541587130;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, SerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig);1542186519;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, SerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig);1548148563;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, SerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig);1550567875;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, SerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig);1550652777;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer(String, SerializationSchema, Properties, Optional)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer(String topicId, SerializationSchema<IN> serializationSchema, Properties producerConfig) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			producerConfig,_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer,string,serialization,schema,properties,optional,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer,string,topic,id,serialization,schema,in,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException;1539704473;For each checkpoint we create new {@link FlinkKafkaInternalProducer} so that new transactions will not clash_with transactions created during previous checkpoints ({@code producer.initTransactions()} assures that we_obtain new producerId and epoch counters).;private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException {_		String transactionalId = availableTransactionalIds.poll()__		if (transactionalId == null) {_			throw new FlinkKafkaException(_				FlinkKafkaErrorCode.PRODUCERS_POOL_EMPTY,_				"Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.")__		}_		FlinkKafkaInternalProducer<byte[], byte[]> producer = initTransactionalProducer(transactionalId, true)__		producer.initTransactions()__		return producer__	};for,each,checkpoint,we,create,new,link,flink,kafka,internal,producer,so,that,new,transactions,will,not,clash,with,transactions,created,during,previous,checkpoints,code,producer,init,transactions,assures,that,we,obtain,new,producer,id,and,epoch,counters;private,flink,kafka,internal,producer,byte,byte,create,transactional,producer,throws,flink,kafka,exception,string,transactional,id,available,transactional,ids,poll,if,transactional,id,null,throw,new,flink,kafka,exception,flink,kafka,error,code,too,many,ongoing,snapshots,increase,kafka,producers,pool,size,or,decrease,number,of,concurrent,checkpoints,flink,kafka,internal,producer,byte,byte,producer,init,transactional,producer,transactional,id,true,producer,init,transactions,return,producer
FlinkKafkaProducer -> private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException;1541587130;For each checkpoint we create new {@link FlinkKafkaInternalProducer} so that new transactions will not clash_with transactions created during previous checkpoints ({@code producer.initTransactions()} assures that we_obtain new producerId and epoch counters).;private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException {_		String transactionalId = availableTransactionalIds.poll()__		if (transactionalId == null) {_			throw new FlinkKafkaException(_				FlinkKafkaErrorCode.PRODUCERS_POOL_EMPTY,_				"Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.")__		}_		FlinkKafkaInternalProducer<byte[], byte[]> producer = initTransactionalProducer(transactionalId, true)__		producer.initTransactions()__		return producer__	};for,each,checkpoint,we,create,new,link,flink,kafka,internal,producer,so,that,new,transactions,will,not,clash,with,transactions,created,during,previous,checkpoints,code,producer,init,transactions,assures,that,we,obtain,new,producer,id,and,epoch,counters;private,flink,kafka,internal,producer,byte,byte,create,transactional,producer,throws,flink,kafka,exception,string,transactional,id,available,transactional,ids,poll,if,transactional,id,null,throw,new,flink,kafka,exception,flink,kafka,error,code,too,many,ongoing,snapshots,increase,kafka,producers,pool,size,or,decrease,number,of,concurrent,checkpoints,flink,kafka,internal,producer,byte,byte,producer,init,transactional,producer,transactional,id,true,producer,init,transactions,return,producer
FlinkKafkaProducer -> private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException;1542186519;For each checkpoint we create new {@link FlinkKafkaInternalProducer} so that new transactions will not clash_with transactions created during previous checkpoints ({@code producer.initTransactions()} assures that we_obtain new producerId and epoch counters).;private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException {_		String transactionalId = availableTransactionalIds.poll()__		if (transactionalId == null) {_			throw new FlinkKafkaException(_				FlinkKafkaErrorCode.PRODUCERS_POOL_EMPTY,_				"Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.")__		}_		FlinkKafkaInternalProducer<byte[], byte[]> producer = initTransactionalProducer(transactionalId, true)__		producer.initTransactions()__		return producer__	};for,each,checkpoint,we,create,new,link,flink,kafka,internal,producer,so,that,new,transactions,will,not,clash,with,transactions,created,during,previous,checkpoints,code,producer,init,transactions,assures,that,we,obtain,new,producer,id,and,epoch,counters;private,flink,kafka,internal,producer,byte,byte,create,transactional,producer,throws,flink,kafka,exception,string,transactional,id,available,transactional,ids,poll,if,transactional,id,null,throw,new,flink,kafka,exception,flink,kafka,error,code,too,many,ongoing,snapshots,increase,kafka,producers,pool,size,or,decrease,number,of,concurrent,checkpoints,flink,kafka,internal,producer,byte,byte,producer,init,transactional,producer,transactional,id,true,producer,init,transactions,return,producer
FlinkKafkaProducer -> private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException;1548148563;For each checkpoint we create new {@link FlinkKafkaInternalProducer} so that new transactions will not clash_with transactions created during previous checkpoints ({@code producer.initTransactions()} assures that we_obtain new producerId and epoch counters).;private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException {_		String transactionalId = availableTransactionalIds.poll()__		if (transactionalId == null) {_			throw new FlinkKafkaException(_				FlinkKafkaErrorCode.PRODUCERS_POOL_EMPTY,_				"Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.")__		}_		FlinkKafkaInternalProducer<byte[], byte[]> producer = initTransactionalProducer(transactionalId, true)__		producer.initTransactions()__		return producer__	};for,each,checkpoint,we,create,new,link,flink,kafka,internal,producer,so,that,new,transactions,will,not,clash,with,transactions,created,during,previous,checkpoints,code,producer,init,transactions,assures,that,we,obtain,new,producer,id,and,epoch,counters;private,flink,kafka,internal,producer,byte,byte,create,transactional,producer,throws,flink,kafka,exception,string,transactional,id,available,transactional,ids,poll,if,transactional,id,null,throw,new,flink,kafka,exception,flink,kafka,error,code,too,many,ongoing,snapshots,increase,kafka,producers,pool,size,or,decrease,number,of,concurrent,checkpoints,flink,kafka,internal,producer,byte,byte,producer,init,transactional,producer,transactional,id,true,producer,init,transactions,return,producer
FlinkKafkaProducer -> private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException;1550567875;For each checkpoint we create new {@link FlinkKafkaInternalProducer} so that new transactions will not clash_with transactions created during previous checkpoints ({@code producer.initTransactions()} assures that we_obtain new producerId and epoch counters).;private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException {_		String transactionalId = availableTransactionalIds.poll()__		if (transactionalId == null) {_			throw new FlinkKafkaException(_				FlinkKafkaErrorCode.PRODUCERS_POOL_EMPTY,_				"Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.")__		}_		FlinkKafkaInternalProducer<byte[], byte[]> producer = initTransactionalProducer(transactionalId, true)__		producer.initTransactions()__		return producer__	};for,each,checkpoint,we,create,new,link,flink,kafka,internal,producer,so,that,new,transactions,will,not,clash,with,transactions,created,during,previous,checkpoints,code,producer,init,transactions,assures,that,we,obtain,new,producer,id,and,epoch,counters;private,flink,kafka,internal,producer,byte,byte,create,transactional,producer,throws,flink,kafka,exception,string,transactional,id,available,transactional,ids,poll,if,transactional,id,null,throw,new,flink,kafka,exception,flink,kafka,error,code,too,many,ongoing,snapshots,increase,kafka,producers,pool,size,or,decrease,number,of,concurrent,checkpoints,flink,kafka,internal,producer,byte,byte,producer,init,transactional,producer,transactional,id,true,producer,init,transactions,return,producer
FlinkKafkaProducer -> private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException;1550652777;For each checkpoint we create new {@link FlinkKafkaInternalProducer} so that new transactions will not clash_with transactions created during previous checkpoints ({@code producer.initTransactions()} assures that we_obtain new producerId and epoch counters).;private FlinkKafkaInternalProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafkaException {_		String transactionalId = availableTransactionalIds.poll()__		if (transactionalId == null) {_			throw new FlinkKafkaException(_				FlinkKafkaErrorCode.PRODUCERS_POOL_EMPTY,_				"Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.")__		}_		FlinkKafkaInternalProducer<byte[], byte[]> producer = initTransactionalProducer(transactionalId, true)__		producer.initTransactions()__		return producer__	};for,each,checkpoint,we,create,new,link,flink,kafka,internal,producer,so,that,new,transactions,will,not,clash,with,transactions,created,during,previous,checkpoints,code,producer,init,transactions,assures,that,we,obtain,new,producer,id,and,epoch,counters;private,flink,kafka,internal,producer,byte,byte,create,transactional,producer,throws,flink,kafka,exception,string,transactional,id,available,transactional,ids,poll,if,transactional,id,null,throw,new,flink,kafka,exception,flink,kafka,error,code,too,many,ongoing,snapshots,increase,kafka,producers,pool,size,or,decrease,number,of,concurrent,checkpoints,flink,kafka,internal,producer,byte,byte,producer,init,transactional,producer,transactional,id,true,producer,init,transactions,return,producer
FlinkKafkaProducer -> @Override 	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout();1539704473;Disables the propagation of exceptions thrown when committing presumably timed out Kafka_transactions during recovery of the job. If a Kafka transaction is timed out, a commit will_never be successful. Hence, use this feature to avoid recovery loops of the Job. Exceptions_will still be logged to inform the user that data loss might have occurred.__<p>Note that we use {@link System#currentTimeMillis()} to track the age of a transaction._Moreover, only exceptions thrown during the recovery are caught, i.e., the producer will_attempt at least one commit of the transaction before giving up.</p>;@Override_	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout() {_		super.ignoreFailuresAfterTransactionTimeout()__		return this__	};disables,the,propagation,of,exceptions,thrown,when,committing,presumably,timed,out,kafka,transactions,during,recovery,of,the,job,if,a,kafka,transaction,is,timed,out,a,commit,will,never,be,successful,hence,use,this,feature,to,avoid,recovery,loops,of,the,job,exceptions,will,still,be,logged,to,inform,the,user,that,data,loss,might,have,occurred,p,note,that,we,use,link,system,current,time,millis,to,track,the,age,of,a,transaction,moreover,only,exceptions,thrown,during,the,recovery,are,caught,i,e,the,producer,will,attempt,at,least,one,commit,of,the,transaction,before,giving,up,p;override,public,flink,kafka,producer,in,ignore,failures,after,transaction,timeout,super,ignore,failures,after,transaction,timeout,return,this
FlinkKafkaProducer -> @Override 	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout();1541587130;Disables the propagation of exceptions thrown when committing presumably timed out Kafka_transactions during recovery of the job. If a Kafka transaction is timed out, a commit will_never be successful. Hence, use this feature to avoid recovery loops of the Job. Exceptions_will still be logged to inform the user that data loss might have occurred.__<p>Note that we use {@link System#currentTimeMillis()} to track the age of a transaction._Moreover, only exceptions thrown during the recovery are caught, i.e., the producer will_attempt at least one commit of the transaction before giving up.</p>;@Override_	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout() {_		super.ignoreFailuresAfterTransactionTimeout()__		return this__	};disables,the,propagation,of,exceptions,thrown,when,committing,presumably,timed,out,kafka,transactions,during,recovery,of,the,job,if,a,kafka,transaction,is,timed,out,a,commit,will,never,be,successful,hence,use,this,feature,to,avoid,recovery,loops,of,the,job,exceptions,will,still,be,logged,to,inform,the,user,that,data,loss,might,have,occurred,p,note,that,we,use,link,system,current,time,millis,to,track,the,age,of,a,transaction,moreover,only,exceptions,thrown,during,the,recovery,are,caught,i,e,the,producer,will,attempt,at,least,one,commit,of,the,transaction,before,giving,up,p;override,public,flink,kafka,producer,in,ignore,failures,after,transaction,timeout,super,ignore,failures,after,transaction,timeout,return,this
FlinkKafkaProducer -> @Override 	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout();1542186519;Disables the propagation of exceptions thrown when committing presumably timed out Kafka_transactions during recovery of the job. If a Kafka transaction is timed out, a commit will_never be successful. Hence, use this feature to avoid recovery loops of the Job. Exceptions_will still be logged to inform the user that data loss might have occurred.__<p>Note that we use {@link System#currentTimeMillis()} to track the age of a transaction._Moreover, only exceptions thrown during the recovery are caught, i.e., the producer will_attempt at least one commit of the transaction before giving up.</p>;@Override_	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout() {_		super.ignoreFailuresAfterTransactionTimeout()__		return this__	};disables,the,propagation,of,exceptions,thrown,when,committing,presumably,timed,out,kafka,transactions,during,recovery,of,the,job,if,a,kafka,transaction,is,timed,out,a,commit,will,never,be,successful,hence,use,this,feature,to,avoid,recovery,loops,of,the,job,exceptions,will,still,be,logged,to,inform,the,user,that,data,loss,might,have,occurred,p,note,that,we,use,link,system,current,time,millis,to,track,the,age,of,a,transaction,moreover,only,exceptions,thrown,during,the,recovery,are,caught,i,e,the,producer,will,attempt,at,least,one,commit,of,the,transaction,before,giving,up,p;override,public,flink,kafka,producer,in,ignore,failures,after,transaction,timeout,super,ignore,failures,after,transaction,timeout,return,this
FlinkKafkaProducer -> @Override 	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout();1548148563;Disables the propagation of exceptions thrown when committing presumably timed out Kafka_transactions during recovery of the job. If a Kafka transaction is timed out, a commit will_never be successful. Hence, use this feature to avoid recovery loops of the Job. Exceptions_will still be logged to inform the user that data loss might have occurred.__<p>Note that we use {@link System#currentTimeMillis()} to track the age of a transaction._Moreover, only exceptions thrown during the recovery are caught, i.e., the producer will_attempt at least one commit of the transaction before giving up.</p>;@Override_	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout() {_		super.ignoreFailuresAfterTransactionTimeout()__		return this__	};disables,the,propagation,of,exceptions,thrown,when,committing,presumably,timed,out,kafka,transactions,during,recovery,of,the,job,if,a,kafka,transaction,is,timed,out,a,commit,will,never,be,successful,hence,use,this,feature,to,avoid,recovery,loops,of,the,job,exceptions,will,still,be,logged,to,inform,the,user,that,data,loss,might,have,occurred,p,note,that,we,use,link,system,current,time,millis,to,track,the,age,of,a,transaction,moreover,only,exceptions,thrown,during,the,recovery,are,caught,i,e,the,producer,will,attempt,at,least,one,commit,of,the,transaction,before,giving,up,p;override,public,flink,kafka,producer,in,ignore,failures,after,transaction,timeout,super,ignore,failures,after,transaction,timeout,return,this
FlinkKafkaProducer -> @Override 	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout();1550567875;Disables the propagation of exceptions thrown when committing presumably timed out Kafka_transactions during recovery of the job. If a Kafka transaction is timed out, a commit will_never be successful. Hence, use this feature to avoid recovery loops of the Job. Exceptions_will still be logged to inform the user that data loss might have occurred.__<p>Note that we use {@link System#currentTimeMillis()} to track the age of a transaction._Moreover, only exceptions thrown during the recovery are caught, i.e., the producer will_attempt at least one commit of the transaction before giving up.</p>;@Override_	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout() {_		super.ignoreFailuresAfterTransactionTimeout()__		return this__	};disables,the,propagation,of,exceptions,thrown,when,committing,presumably,timed,out,kafka,transactions,during,recovery,of,the,job,if,a,kafka,transaction,is,timed,out,a,commit,will,never,be,successful,hence,use,this,feature,to,avoid,recovery,loops,of,the,job,exceptions,will,still,be,logged,to,inform,the,user,that,data,loss,might,have,occurred,p,note,that,we,use,link,system,current,time,millis,to,track,the,age,of,a,transaction,moreover,only,exceptions,thrown,during,the,recovery,are,caught,i,e,the,producer,will,attempt,at,least,one,commit,of,the,transaction,before,giving,up,p;override,public,flink,kafka,producer,in,ignore,failures,after,transaction,timeout,super,ignore,failures,after,transaction,timeout,return,this
FlinkKafkaProducer -> @Override 	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout();1550652777;Disables the propagation of exceptions thrown when committing presumably timed out Kafka_transactions during recovery of the job. If a Kafka transaction is timed out, a commit will_never be successful. Hence, use this feature to avoid recovery loops of the Job. Exceptions_will still be logged to inform the user that data loss might have occurred.__<p>Note that we use {@link System#currentTimeMillis()} to track the age of a transaction._Moreover, only exceptions thrown during the recovery are caught, i.e., the producer will_attempt at least one commit of the transaction before giving up.</p>;@Override_	public FlinkKafkaProducer<IN> ignoreFailuresAfterTransactionTimeout() {_		super.ignoreFailuresAfterTransactionTimeout()__		return this__	};disables,the,propagation,of,exceptions,thrown,when,committing,presumably,timed,out,kafka,transactions,during,recovery,of,the,job,if,a,kafka,transaction,is,timed,out,a,commit,will,never,be,successful,hence,use,this,feature,to,avoid,recovery,loops,of,the,job,exceptions,will,still,be,logged,to,inform,the,user,that,data,loss,might,have,occurred,p,note,that,we,use,link,system,current,time,millis,to,track,the,age,of,a,transaction,moreover,only,exceptions,thrown,during,the,recovery,are,caught,i,e,the,producer,will,attempt,at,least,one,commit,of,the,transaction,before,giving,up,p;override,public,flink,kafka,producer,in,ignore,failures,after,transaction,timeout,super,ignore,failures,after,transaction,timeout,return,this
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner, 		FlinkKafkaProducer.Semantic semantic, 		int kafkaProducersPoolSize);1539704473;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion._@param semantic Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic})._@param kafkaProducersPoolSize Overwrite default KafkaProducers pool size (see {@link FlinkKafkaProducer.Semantic#EXACTLY_ONCE}).;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner,_		FlinkKafkaProducer.Semantic semantic,_		int kafkaProducersPoolSize) {_		super(new FlinkKafkaProducer.TransactionStateSerializer(), new FlinkKafkaProducer.ContextStateSerializer())___		this.defaultTopicId = checkNotNull(defaultTopicId, "defaultTopicId is null")__		this.schema = checkNotNull(serializationSchema, "serializationSchema is null")__		this.producerConfig = checkNotNull(producerConfig, "producerConfig is null")__		this.flinkKafkaPartitioner = checkNotNull(customPartitioner, "customPartitioner is null").orElse(null)__		this.semantic = checkNotNull(semantic, "semantic is null")__		this.kafkaProducersPoolSize = kafkaProducersPoolSize__		checkState(kafkaProducersPoolSize > 0, "kafkaProducersPoolSize must be non empty")___		ClosureCleaner.clean(this.flinkKafkaPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		if (!producerConfig.containsKey(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)) {_			long timeout = DEFAULT_KAFKA_TRANSACTION_TIMEOUT.toMilliseconds()__			checkState(timeout < Integer.MAX_VALUE && timeout > 0, "timeout does not fit into 32 bit integer")__			this.producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, (int) timeout)__			LOG.warn("Property [{}] not specified. Setting it to {}", ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, DEFAULT_KAFKA_TRANSACTION_TIMEOUT)__		}__		_		_		_		if (semantic == FlinkKafkaProducer.Semantic.EXACTLY_ONCE) {_			final Object object = this.producerConfig.get(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)__			final long transactionTimeout__			if (object instanceof String && StringUtils.isNumeric((String) object)) {_				transactionTimeout = Long.parseLong((String) object)__			} else if (object instanceof Number) {_				transactionTimeout = ((Number) object).longValue()__			} else {_				throw new IllegalArgumentException(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG_					+ " must be numeric, was " + object)__			}_			super.setTransactionTimeout(transactionTimeout)__			super.enableTransactionTimeoutWarnings(0.8)__		}__		this.topicPartitionsMap = new HashMap<>()__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic,param,kafka,producers,pool,size,overwrite,default,kafka,producers,pool,size,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,flink,kafka,producer,semantic,semantic,int,kafka,producers,pool,size,super,new,flink,kafka,producer,transaction,state,serializer,new,flink,kafka,producer,context,state,serializer,this,default,topic,id,check,not,null,default,topic,id,default,topic,id,is,null,this,schema,check,not,null,serialization,schema,serialization,schema,is,null,this,producer,config,check,not,null,producer,config,producer,config,is,null,this,flink,kafka,partitioner,check,not,null,custom,partitioner,custom,partitioner,is,null,or,else,null,this,semantic,check,not,null,semantic,semantic,is,null,this,kafka,producers,pool,size,kafka,producers,pool,size,check,state,kafka,producers,pool,size,0,kafka,producers,pool,size,must,be,non,empty,closure,cleaner,clean,this,flink,kafka,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,if,producer,config,contains,key,producer,config,long,timeout,to,milliseconds,check,state,timeout,integer,timeout,0,timeout,does,not,fit,into,32,bit,integer,this,producer,config,put,producer,config,int,timeout,log,warn,property,not,specified,setting,it,to,producer,config,if,semantic,flink,kafka,producer,semantic,final,object,object,this,producer,config,get,producer,config,final,long,transaction,timeout,if,object,instanceof,string,string,utils,is,numeric,string,object,transaction,timeout,long,parse,long,string,object,else,if,object,instanceof,number,transaction,timeout,number,object,long,value,else,throw,new,illegal,argument,exception,producer,config,must,be,numeric,was,object,super,set,transaction,timeout,transaction,timeout,super,enable,transaction,timeout,warnings,0,8,this,topic,partitions,map,new,hash,map
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner, 		FlinkKafkaProducer.Semantic semantic, 		int kafkaProducersPoolSize);1541587130;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion._@param semantic Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic})._@param kafkaProducersPoolSize Overwrite default KafkaProducers pool size (see {@link FlinkKafkaProducer.Semantic#EXACTLY_ONCE}).;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner,_		FlinkKafkaProducer.Semantic semantic,_		int kafkaProducersPoolSize) {_		super(new FlinkKafkaProducer.TransactionStateSerializer(), new FlinkKafkaProducer.ContextStateSerializer())___		this.defaultTopicId = checkNotNull(defaultTopicId, "defaultTopicId is null")__		this.schema = checkNotNull(serializationSchema, "serializationSchema is null")__		this.producerConfig = checkNotNull(producerConfig, "producerConfig is null")__		this.flinkKafkaPartitioner = checkNotNull(customPartitioner, "customPartitioner is null").orElse(null)__		this.semantic = checkNotNull(semantic, "semantic is null")__		this.kafkaProducersPoolSize = kafkaProducersPoolSize__		checkState(kafkaProducersPoolSize > 0, "kafkaProducersPoolSize must be non empty")___		ClosureCleaner.clean(this.flinkKafkaPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		if (!producerConfig.containsKey(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)) {_			long timeout = DEFAULT_KAFKA_TRANSACTION_TIMEOUT.toMilliseconds()__			checkState(timeout < Integer.MAX_VALUE && timeout > 0, "timeout does not fit into 32 bit integer")__			this.producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, (int) timeout)__			LOG.warn("Property [{}] not specified. Setting it to {}", ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, DEFAULT_KAFKA_TRANSACTION_TIMEOUT)__		}__		_		_		_		if (semantic == FlinkKafkaProducer.Semantic.EXACTLY_ONCE) {_			final Object object = this.producerConfig.get(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)__			final long transactionTimeout__			if (object instanceof String && StringUtils.isNumeric((String) object)) {_				transactionTimeout = Long.parseLong((String) object)__			} else if (object instanceof Number) {_				transactionTimeout = ((Number) object).longValue()__			} else {_				throw new IllegalArgumentException(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG_					+ " must be numeric, was " + object)__			}_			super.setTransactionTimeout(transactionTimeout)__			super.enableTransactionTimeoutWarnings(0.8)__		}__		this.topicPartitionsMap = new HashMap<>()__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic,param,kafka,producers,pool,size,overwrite,default,kafka,producers,pool,size,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,flink,kafka,producer,semantic,semantic,int,kafka,producers,pool,size,super,new,flink,kafka,producer,transaction,state,serializer,new,flink,kafka,producer,context,state,serializer,this,default,topic,id,check,not,null,default,topic,id,default,topic,id,is,null,this,schema,check,not,null,serialization,schema,serialization,schema,is,null,this,producer,config,check,not,null,producer,config,producer,config,is,null,this,flink,kafka,partitioner,check,not,null,custom,partitioner,custom,partitioner,is,null,or,else,null,this,semantic,check,not,null,semantic,semantic,is,null,this,kafka,producers,pool,size,kafka,producers,pool,size,check,state,kafka,producers,pool,size,0,kafka,producers,pool,size,must,be,non,empty,closure,cleaner,clean,this,flink,kafka,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,if,producer,config,contains,key,producer,config,long,timeout,to,milliseconds,check,state,timeout,integer,timeout,0,timeout,does,not,fit,into,32,bit,integer,this,producer,config,put,producer,config,int,timeout,log,warn,property,not,specified,setting,it,to,producer,config,if,semantic,flink,kafka,producer,semantic,final,object,object,this,producer,config,get,producer,config,final,long,transaction,timeout,if,object,instanceof,string,string,utils,is,numeric,string,object,transaction,timeout,long,parse,long,string,object,else,if,object,instanceof,number,transaction,timeout,number,object,long,value,else,throw,new,illegal,argument,exception,producer,config,must,be,numeric,was,object,super,set,transaction,timeout,transaction,timeout,super,enable,transaction,timeout,warnings,0,8,this,topic,partitions,map,new,hash,map
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner, 		FlinkKafkaProducer.Semantic semantic, 		int kafkaProducersPoolSize);1542186519;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion._@param semantic Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic})._@param kafkaProducersPoolSize Overwrite default KafkaProducers pool size (see {@link FlinkKafkaProducer.Semantic#EXACTLY_ONCE}).;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner,_		FlinkKafkaProducer.Semantic semantic,_		int kafkaProducersPoolSize) {_		super(new FlinkKafkaProducer.TransactionStateSerializer(), new FlinkKafkaProducer.ContextStateSerializer())___		this.defaultTopicId = checkNotNull(defaultTopicId, "defaultTopicId is null")__		this.schema = checkNotNull(serializationSchema, "serializationSchema is null")__		this.producerConfig = checkNotNull(producerConfig, "producerConfig is null")__		this.flinkKafkaPartitioner = checkNotNull(customPartitioner, "customPartitioner is null").orElse(null)__		this.semantic = checkNotNull(semantic, "semantic is null")__		this.kafkaProducersPoolSize = kafkaProducersPoolSize__		checkState(kafkaProducersPoolSize > 0, "kafkaProducersPoolSize must be non empty")___		ClosureCleaner.clean(this.flinkKafkaPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		if (!producerConfig.containsKey(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)) {_			long timeout = DEFAULT_KAFKA_TRANSACTION_TIMEOUT.toMilliseconds()__			checkState(timeout < Integer.MAX_VALUE && timeout > 0, "timeout does not fit into 32 bit integer")__			this.producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, (int) timeout)__			LOG.warn("Property [{}] not specified. Setting it to {}", ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, DEFAULT_KAFKA_TRANSACTION_TIMEOUT)__		}__		_		_		_		if (semantic == FlinkKafkaProducer.Semantic.EXACTLY_ONCE) {_			final Object object = this.producerConfig.get(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)__			final long transactionTimeout__			if (object instanceof String && StringUtils.isNumeric((String) object)) {_				transactionTimeout = Long.parseLong((String) object)__			} else if (object instanceof Number) {_				transactionTimeout = ((Number) object).longValue()__			} else {_				throw new IllegalArgumentException(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG_					+ " must be numeric, was " + object)__			}_			super.setTransactionTimeout(transactionTimeout)__			super.enableTransactionTimeoutWarnings(0.8)__		}__		this.topicPartitionsMap = new HashMap<>()__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic,param,kafka,producers,pool,size,overwrite,default,kafka,producers,pool,size,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,flink,kafka,producer,semantic,semantic,int,kafka,producers,pool,size,super,new,flink,kafka,producer,transaction,state,serializer,new,flink,kafka,producer,context,state,serializer,this,default,topic,id,check,not,null,default,topic,id,default,topic,id,is,null,this,schema,check,not,null,serialization,schema,serialization,schema,is,null,this,producer,config,check,not,null,producer,config,producer,config,is,null,this,flink,kafka,partitioner,check,not,null,custom,partitioner,custom,partitioner,is,null,or,else,null,this,semantic,check,not,null,semantic,semantic,is,null,this,kafka,producers,pool,size,kafka,producers,pool,size,check,state,kafka,producers,pool,size,0,kafka,producers,pool,size,must,be,non,empty,closure,cleaner,clean,this,flink,kafka,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,if,producer,config,contains,key,producer,config,long,timeout,to,milliseconds,check,state,timeout,integer,timeout,0,timeout,does,not,fit,into,32,bit,integer,this,producer,config,put,producer,config,int,timeout,log,warn,property,not,specified,setting,it,to,producer,config,if,semantic,flink,kafka,producer,semantic,final,object,object,this,producer,config,get,producer,config,final,long,transaction,timeout,if,object,instanceof,string,string,utils,is,numeric,string,object,transaction,timeout,long,parse,long,string,object,else,if,object,instanceof,number,transaction,timeout,number,object,long,value,else,throw,new,illegal,argument,exception,producer,config,must,be,numeric,was,object,super,set,transaction,timeout,transaction,timeout,super,enable,transaction,timeout,warnings,0,8,this,topic,partitions,map,new,hash,map
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner, 		FlinkKafkaProducer.Semantic semantic, 		int kafkaProducersPoolSize);1548148563;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion._@param semantic Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic})._@param kafkaProducersPoolSize Overwrite default KafkaProducers pool size (see {@link FlinkKafkaProducer.Semantic#EXACTLY_ONCE}).;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner,_		FlinkKafkaProducer.Semantic semantic,_		int kafkaProducersPoolSize) {_		super(new FlinkKafkaProducer.TransactionStateSerializer(), new FlinkKafkaProducer.ContextStateSerializer())___		this.defaultTopicId = checkNotNull(defaultTopicId, "defaultTopicId is null")__		this.schema = checkNotNull(serializationSchema, "serializationSchema is null")__		this.producerConfig = checkNotNull(producerConfig, "producerConfig is null")__		this.flinkKafkaPartitioner = checkNotNull(customPartitioner, "customPartitioner is null").orElse(null)__		this.semantic = checkNotNull(semantic, "semantic is null")__		this.kafkaProducersPoolSize = kafkaProducersPoolSize__		checkState(kafkaProducersPoolSize > 0, "kafkaProducersPoolSize must be non empty")___		ClosureCleaner.clean(this.flinkKafkaPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		if (!producerConfig.containsKey(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)) {_			long timeout = DEFAULT_KAFKA_TRANSACTION_TIMEOUT.toMilliseconds()__			checkState(timeout < Integer.MAX_VALUE && timeout > 0, "timeout does not fit into 32 bit integer")__			this.producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, (int) timeout)__			LOG.warn("Property [{}] not specified. Setting it to {}", ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, DEFAULT_KAFKA_TRANSACTION_TIMEOUT)__		}__		_		_		_		if (semantic == FlinkKafkaProducer.Semantic.EXACTLY_ONCE) {_			final Object object = this.producerConfig.get(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)__			final long transactionTimeout__			if (object instanceof String && StringUtils.isNumeric((String) object)) {_				transactionTimeout = Long.parseLong((String) object)__			} else if (object instanceof Number) {_				transactionTimeout = ((Number) object).longValue()__			} else {_				throw new IllegalArgumentException(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG_					+ " must be numeric, was " + object)__			}_			super.setTransactionTimeout(transactionTimeout)__			super.enableTransactionTimeoutWarnings(0.8)__		}__		this.topicPartitionsMap = new HashMap<>()__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic,param,kafka,producers,pool,size,overwrite,default,kafka,producers,pool,size,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,flink,kafka,producer,semantic,semantic,int,kafka,producers,pool,size,super,new,flink,kafka,producer,transaction,state,serializer,new,flink,kafka,producer,context,state,serializer,this,default,topic,id,check,not,null,default,topic,id,default,topic,id,is,null,this,schema,check,not,null,serialization,schema,serialization,schema,is,null,this,producer,config,check,not,null,producer,config,producer,config,is,null,this,flink,kafka,partitioner,check,not,null,custom,partitioner,custom,partitioner,is,null,or,else,null,this,semantic,check,not,null,semantic,semantic,is,null,this,kafka,producers,pool,size,kafka,producers,pool,size,check,state,kafka,producers,pool,size,0,kafka,producers,pool,size,must,be,non,empty,closure,cleaner,clean,this,flink,kafka,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,if,producer,config,contains,key,producer,config,long,timeout,to,milliseconds,check,state,timeout,integer,timeout,0,timeout,does,not,fit,into,32,bit,integer,this,producer,config,put,producer,config,int,timeout,log,warn,property,not,specified,setting,it,to,producer,config,if,semantic,flink,kafka,producer,semantic,final,object,object,this,producer,config,get,producer,config,final,long,transaction,timeout,if,object,instanceof,string,string,utils,is,numeric,string,object,transaction,timeout,long,parse,long,string,object,else,if,object,instanceof,number,transaction,timeout,number,object,long,value,else,throw,new,illegal,argument,exception,producer,config,must,be,numeric,was,object,super,set,transaction,timeout,transaction,timeout,super,enable,transaction,timeout,warnings,0,8,this,topic,partitions,map,new,hash,map
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner, 		FlinkKafkaProducer.Semantic semantic, 		int kafkaProducersPoolSize);1550567875;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion._@param semantic Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic})._@param kafkaProducersPoolSize Overwrite default KafkaProducers pool size (see {@link FlinkKafkaProducer.Semantic#EXACTLY_ONCE}).;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner,_		FlinkKafkaProducer.Semantic semantic,_		int kafkaProducersPoolSize) {_		super(new FlinkKafkaProducer.TransactionStateSerializer(), new FlinkKafkaProducer.ContextStateSerializer())___		this.defaultTopicId = checkNotNull(defaultTopicId, "defaultTopicId is null")__		this.schema = checkNotNull(serializationSchema, "serializationSchema is null")__		this.producerConfig = checkNotNull(producerConfig, "producerConfig is null")__		this.flinkKafkaPartitioner = checkNotNull(customPartitioner, "customPartitioner is null").orElse(null)__		this.semantic = checkNotNull(semantic, "semantic is null")__		this.kafkaProducersPoolSize = kafkaProducersPoolSize__		checkState(kafkaProducersPoolSize > 0, "kafkaProducersPoolSize must be non empty")___		ClosureCleaner.clean(this.flinkKafkaPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		if (!producerConfig.containsKey(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)) {_			long timeout = DEFAULT_KAFKA_TRANSACTION_TIMEOUT.toMilliseconds()__			checkState(timeout < Integer.MAX_VALUE && timeout > 0, "timeout does not fit into 32 bit integer")__			this.producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, (int) timeout)__			LOG.warn("Property [{}] not specified. Setting it to {}", ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, DEFAULT_KAFKA_TRANSACTION_TIMEOUT)__		}__		_		_		_		if (semantic == FlinkKafkaProducer.Semantic.EXACTLY_ONCE) {_			final Object object = this.producerConfig.get(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)__			final long transactionTimeout__			if (object instanceof String && StringUtils.isNumeric((String) object)) {_				transactionTimeout = Long.parseLong((String) object)__			} else if (object instanceof Number) {_				transactionTimeout = ((Number) object).longValue()__			} else {_				throw new IllegalArgumentException(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG_					+ " must be numeric, was " + object)__			}_			super.setTransactionTimeout(transactionTimeout)__			super.enableTransactionTimeoutWarnings(0.8)__		}__		this.topicPartitionsMap = new HashMap<>()__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic,param,kafka,producers,pool,size,overwrite,default,kafka,producers,pool,size,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,flink,kafka,producer,semantic,semantic,int,kafka,producers,pool,size,super,new,flink,kafka,producer,transaction,state,serializer,new,flink,kafka,producer,context,state,serializer,this,default,topic,id,check,not,null,default,topic,id,default,topic,id,is,null,this,schema,check,not,null,serialization,schema,serialization,schema,is,null,this,producer,config,check,not,null,producer,config,producer,config,is,null,this,flink,kafka,partitioner,check,not,null,custom,partitioner,custom,partitioner,is,null,or,else,null,this,semantic,check,not,null,semantic,semantic,is,null,this,kafka,producers,pool,size,kafka,producers,pool,size,check,state,kafka,producers,pool,size,0,kafka,producers,pool,size,must,be,non,empty,closure,cleaner,clean,this,flink,kafka,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,if,producer,config,contains,key,producer,config,long,timeout,to,milliseconds,check,state,timeout,integer,timeout,0,timeout,does,not,fit,into,32,bit,integer,this,producer,config,put,producer,config,int,timeout,log,warn,property,not,specified,setting,it,to,producer,config,if,semantic,flink,kafka,producer,semantic,final,object,object,this,producer,config,get,producer,config,final,long,transaction,timeout,if,object,instanceof,string,string,utils,is,numeric,string,object,transaction,timeout,long,parse,long,string,object,else,if,object,instanceof,number,transaction,timeout,number,object,long,value,else,throw,new,illegal,argument,exception,producer,config,must,be,numeric,was,object,super,set,transaction,timeout,transaction,timeout,super,enable,transaction,timeout,warnings,0,8,this,topic,partitions,map,new,hash,map
FlinkKafkaProducer -> public FlinkKafkaProducer( 		String defaultTopicId, 		KeyedSerializationSchema<IN> serializationSchema, 		Properties producerConfig, 		Optional<FlinkKafkaPartitioner<IN>> customPartitioner, 		FlinkKafkaProducer.Semantic semantic, 		int kafkaProducersPoolSize);1550652777;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If a partitioner is not provided, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion._@param semantic Defines semantic that will be used by this producer (see {@link FlinkKafkaProducer.Semantic})._@param kafkaProducersPoolSize Overwrite default KafkaProducers pool size (see {@link FlinkKafkaProducer.Semantic#EXACTLY_ONCE}).;public FlinkKafkaProducer(_		String defaultTopicId,_		KeyedSerializationSchema<IN> serializationSchema,_		Properties producerConfig,_		Optional<FlinkKafkaPartitioner<IN>> customPartitioner,_		FlinkKafkaProducer.Semantic semantic,_		int kafkaProducersPoolSize) {_		super(new FlinkKafkaProducer.TransactionStateSerializer(), new FlinkKafkaProducer.ContextStateSerializer())___		this.defaultTopicId = checkNotNull(defaultTopicId, "defaultTopicId is null")__		this.schema = checkNotNull(serializationSchema, "serializationSchema is null")__		this.producerConfig = checkNotNull(producerConfig, "producerConfig is null")__		this.flinkKafkaPartitioner = checkNotNull(customPartitioner, "customPartitioner is null").orElse(null)__		this.semantic = checkNotNull(semantic, "semantic is null")__		this.kafkaProducersPoolSize = kafkaProducersPoolSize__		checkState(kafkaProducersPoolSize > 0, "kafkaProducersPoolSize must be non empty")___		ClosureCleaner.clean(this.flinkKafkaPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		if (!producerConfig.containsKey(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)) {_			long timeout = DEFAULT_KAFKA_TRANSACTION_TIMEOUT.toMilliseconds()__			checkState(timeout < Integer.MAX_VALUE && timeout > 0, "timeout does not fit into 32 bit integer")__			this.producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, (int) timeout)__			LOG.warn("Property [{}] not specified. Setting it to {}", ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, DEFAULT_KAFKA_TRANSACTION_TIMEOUT)__		}__		_		_		_		if (semantic == FlinkKafkaProducer.Semantic.EXACTLY_ONCE) {_			final Object object = this.producerConfig.get(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG)__			final long transactionTimeout__			if (object instanceof String && StringUtils.isNumeric((String) object)) {_				transactionTimeout = Long.parseLong((String) object)__			} else if (object instanceof Number) {_				transactionTimeout = ((Number) object).longValue()__			} else {_				throw new IllegalArgumentException(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG_					+ " must be numeric, was " + object)__			}_			super.setTransactionTimeout(transactionTimeout)__			super.enableTransactionTimeoutWarnings(0.8)__		}__		this.topicPartitionsMap = new HashMap<>()__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,a,partitioner,is,not,provided,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,semantic,defines,semantic,that,will,be,used,by,this,producer,see,link,flink,kafka,producer,semantic,param,kafka,producers,pool,size,overwrite,default,kafka,producers,pool,size,see,link,flink,kafka,producer,semantic;public,flink,kafka,producer,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,optional,flink,kafka,partitioner,in,custom,partitioner,flink,kafka,producer,semantic,semantic,int,kafka,producers,pool,size,super,new,flink,kafka,producer,transaction,state,serializer,new,flink,kafka,producer,context,state,serializer,this,default,topic,id,check,not,null,default,topic,id,default,topic,id,is,null,this,schema,check,not,null,serialization,schema,serialization,schema,is,null,this,producer,config,check,not,null,producer,config,producer,config,is,null,this,flink,kafka,partitioner,check,not,null,custom,partitioner,custom,partitioner,is,null,or,else,null,this,semantic,check,not,null,semantic,semantic,is,null,this,kafka,producers,pool,size,kafka,producers,pool,size,check,state,kafka,producers,pool,size,0,kafka,producers,pool,size,must,be,non,empty,closure,cleaner,clean,this,flink,kafka,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,if,producer,config,contains,key,producer,config,long,timeout,to,milliseconds,check,state,timeout,integer,timeout,0,timeout,does,not,fit,into,32,bit,integer,this,producer,config,put,producer,config,int,timeout,log,warn,property,not,specified,setting,it,to,producer,config,if,semantic,flink,kafka,producer,semantic,final,object,object,this,producer,config,get,producer,config,final,long,transaction,timeout,if,object,instanceof,string,string,utils,is,numeric,string,object,transaction,timeout,long,parse,long,string,object,else,if,object,instanceof,number,transaction,timeout,number,object,long,value,else,throw,new,illegal,argument,exception,producer,config,must,be,numeric,was,object,super,set,transaction,timeout,transaction,timeout,super,enable,transaction,timeout,warnings,0,8,this,topic,partitions,map,new,hash,map
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema);1539704473;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer,string,broker,list,string,topic,id,serialization,schema,in,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema);1541587130;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer,string,broker,list,string,topic,id,serialization,schema,in,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema);1542186519;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer,string,broker,list,string,topic,id,serialization,schema,in,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema);1548148563;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer,string,broker,list,string,topic,id,serialization,schema,in,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema);1550567875;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer,string,broker,list,string,topic,id,serialization,schema,in,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema);1550652777;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer(String brokerList, String topicId, SerializationSchema<IN> serializationSchema) {_		this(_			topicId,_			new KeyedSerializationSchemaWrapper<>(serializationSchema),_			getPropertiesFromBrokerList(brokerList),_			Optional.of(new FlinkFixedPartitioner<IN>()))__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer,string,broker,list,string,topic,id,serialization,schema,in,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,optional,of,new,flink,fixed,partitioner,in
FlinkKafkaProducer -> private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException;1539704473;Flush pending records._@param transaction;private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException {_		if (transaction.producer != null) {_			transaction.producer.flush()__		}_		long pendingRecordsCount = pendingRecords.get()__		if (pendingRecordsCount != 0) {_			throw new IllegalStateException("Pending record count must be zero at this point: " + pendingRecordsCount)__		}__		_		checkErroneous()__	};flush,pending,records,param,transaction;private,void,flush,flink,kafka,producer,kafka,transaction,state,transaction,throws,flink,kafka,exception,if,transaction,producer,null,transaction,producer,flush,long,pending,records,count,pending,records,get,if,pending,records,count,0,throw,new,illegal,state,exception,pending,record,count,must,be,zero,at,this,point,pending,records,count,check,erroneous
FlinkKafkaProducer -> private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException;1541587130;Flush pending records._@param transaction;private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException {_		if (transaction.producer != null) {_			transaction.producer.flush()__		}_		long pendingRecordsCount = pendingRecords.get()__		if (pendingRecordsCount != 0) {_			throw new IllegalStateException("Pending record count must be zero at this point: " + pendingRecordsCount)__		}__		_		checkErroneous()__	};flush,pending,records,param,transaction;private,void,flush,flink,kafka,producer,kafka,transaction,state,transaction,throws,flink,kafka,exception,if,transaction,producer,null,transaction,producer,flush,long,pending,records,count,pending,records,get,if,pending,records,count,0,throw,new,illegal,state,exception,pending,record,count,must,be,zero,at,this,point,pending,records,count,check,erroneous
FlinkKafkaProducer -> private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException;1542186519;Flush pending records._@param transaction;private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException {_		if (transaction.producer != null) {_			transaction.producer.flush()__		}_		long pendingRecordsCount = pendingRecords.get()__		if (pendingRecordsCount != 0) {_			throw new IllegalStateException("Pending record count must be zero at this point: " + pendingRecordsCount)__		}__		_		checkErroneous()__	};flush,pending,records,param,transaction;private,void,flush,flink,kafka,producer,kafka,transaction,state,transaction,throws,flink,kafka,exception,if,transaction,producer,null,transaction,producer,flush,long,pending,records,count,pending,records,get,if,pending,records,count,0,throw,new,illegal,state,exception,pending,record,count,must,be,zero,at,this,point,pending,records,count,check,erroneous
FlinkKafkaProducer -> private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException;1548148563;Flush pending records._@param transaction;private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException {_		if (transaction.producer != null) {_			transaction.producer.flush()__		}_		long pendingRecordsCount = pendingRecords.get()__		if (pendingRecordsCount != 0) {_			throw new IllegalStateException("Pending record count must be zero at this point: " + pendingRecordsCount)__		}__		_		checkErroneous()__	};flush,pending,records,param,transaction;private,void,flush,flink,kafka,producer,kafka,transaction,state,transaction,throws,flink,kafka,exception,if,transaction,producer,null,transaction,producer,flush,long,pending,records,count,pending,records,get,if,pending,records,count,0,throw,new,illegal,state,exception,pending,record,count,must,be,zero,at,this,point,pending,records,count,check,erroneous
FlinkKafkaProducer -> private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException;1550567875;Flush pending records._@param transaction;private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException {_		if (transaction.producer != null) {_			transaction.producer.flush()__		}_		long pendingRecordsCount = pendingRecords.get()__		if (pendingRecordsCount != 0) {_			throw new IllegalStateException("Pending record count must be zero at this point: " + pendingRecordsCount)__		}__		_		checkErroneous()__	};flush,pending,records,param,transaction;private,void,flush,flink,kafka,producer,kafka,transaction,state,transaction,throws,flink,kafka,exception,if,transaction,producer,null,transaction,producer,flush,long,pending,records,count,pending,records,get,if,pending,records,count,0,throw,new,illegal,state,exception,pending,record,count,must,be,zero,at,this,point,pending,records,count,check,erroneous
FlinkKafkaProducer -> private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException;1550652777;Flush pending records._@param transaction;private void flush(FlinkKafkaProducer.KafkaTransactionState transaction) throws FlinkKafkaException {_		if (transaction.producer != null) {_			transaction.producer.flush()__		}_		long pendingRecordsCount = pendingRecords.get()__		if (pendingRecordsCount != 0) {_			throw new IllegalStateException("Pending record count must be zero at this point: " + pendingRecordsCount)__		}__		_		checkErroneous()__	};flush,pending,records,param,transaction;private,void,flush,flink,kafka,producer,kafka,transaction,state,transaction,throws,flink,kafka,exception,if,transaction,producer,null,transaction,producer,flush,long,pending,records,count,pending,records,get,if,pending,records,count,0,throw,new,illegal,state,exception,pending,record,count,must,be,zero,at,this,point,pending,records,count,check,erroneous
FlinkKafkaProducer -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1539704473;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1541587130;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1542186519;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1548148563;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1550567875;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1550652777;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
