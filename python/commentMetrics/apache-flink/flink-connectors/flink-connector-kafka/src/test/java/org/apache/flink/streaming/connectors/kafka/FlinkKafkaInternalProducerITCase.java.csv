commented;modifiers;parameterAmount;loc;comment;code
false;public;0;12;;@Before public void before() {     transactionalId = UUID.randomUUID().toString().     extraProperties = new Properties().     extraProperties.putAll(standardProps).     extraProperties.put("transactional.id", transactionalId).     extraProperties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer").     extraProperties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer").     extraProperties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer").     extraProperties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer").     extraProperties.put("isolation.level", "read_committed"). }
false;public;0;12;;@Test(timeout = 30000L) public void testHappyPath() throws IOException {     String topicName = "flink-kafka-producer-happy-path".     try (Producer<String, String> kafkaProducer = new FlinkKafkaInternalProducer<>(extraProperties)) {         kafkaProducer.initTransactions().         kafkaProducer.beginTransaction().         kafkaProducer.send(new ProducerRecord<>(topicName, "42", "42")).         kafkaProducer.commitTransaction().     }     assertRecord(topicName, "42", "42").     deleteTestTopic(topicName). }
false;public;0;29;;@Test(timeout = 30000L) public void testResumeTransaction() throws IOException {     String topicName = "flink-kafka-producer-resume-transaction".     try (FlinkKafkaInternalProducer<String, String> kafkaProducer = new FlinkKafkaInternalProducer<>(extraProperties)) {         kafkaProducer.initTransactions().         kafkaProducer.beginTransaction().         kafkaProducer.send(new ProducerRecord<>(topicName, "42", "42")).         kafkaProducer.flush().         long producerId = kafkaProducer.getProducerId().         short epoch = kafkaProducer.getEpoch().         try (FlinkKafkaInternalProducer<String, String> resumeProducer = new FlinkKafkaInternalProducer<>(extraProperties)) {             resumeProducer.resumeTransaction(producerId, epoch).             resumeProducer.commitTransaction().         }         assertRecord(topicName, "42", "42").         // this shouldn't throw - in case of network split, old producer might attempt to commit it's transaction         kafkaProducer.commitTransaction().         // this shouldn't fail also, for same reason as above         try (FlinkKafkaInternalProducer<String, String> resumeProducer = new FlinkKafkaInternalProducer<>(extraProperties)) {             resumeProducer.resumeTransaction(producerId, epoch).             resumeProducer.commitTransaction().         }     }     deleteTestTopic(topicName). }
false;private;3;10;;private void assertRecord(String topicName, String expectedKey, String expectedValue) {     try (KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<>(extraProperties)) {         kafkaConsumer.subscribe(Collections.singletonList(topicName)).         ConsumerRecords<String, String> records = kafkaConsumer.poll(10000).         ConsumerRecord<String, String> record = Iterables.getOnlyElement(records).         assertEquals(expectedKey, record.key()).         assertEquals(expectedValue, record.value()).     } }
