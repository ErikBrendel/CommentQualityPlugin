commented;modifiers;parameterAmount;loc;comment;code
false;public;1;3;;public void setProducerSemantic(FlinkKafkaProducer.Semantic producerSemantic) {     this.producerSemantic = producerSemantic. }
false;public;1;60;;@Override public void prepare(Config config) {     // increase the timeout since in Travis ZK connection takes long time for secure connection.     if (config.isSecureMode()) {         // run only one kafka server to avoid multiple ZK connections from many instances - Travis timeout         config.setKafkaServersNumber(1).         zkTimeout = zkTimeout * 15.     }     this.config = config.     File tempDir = new File(System.getProperty("java.io.tmpdir")).     tmpZkDir = new File(tempDir, "kafkaITcase-zk-dir-" + (UUID.randomUUID().toString())).     assertTrue("cannot create zookeeper temp dir", tmpZkDir.mkdirs()).     tmpKafkaParent = new File(tempDir, "kafkaITcase-kafka-dir-" + (UUID.randomUUID().toString())).     assertTrue("cannot create kafka temp dir", tmpKafkaParent.mkdirs()).     tmpKafkaDirs = new ArrayList<>(config.getKafkaServersNumber()).     for (int i = 0. i < config.getKafkaServersNumber(). i++) {         File tmpDir = new File(tmpKafkaParent, "server-" + i).         assertTrue("cannot create kafka temp dir", tmpDir.mkdir()).         tmpKafkaDirs.add(tmpDir).     }     zookeeper = null.     brokers = null.     try {         zookeeper = new TestingServer(-1, tmpZkDir).         zookeeperConnectionString = zookeeper.getConnectString().         LOG.info("Starting Zookeeper with zookeeperConnectionString: {}", zookeeperConnectionString).         LOG.info("Starting KafkaServer").         brokers = new ArrayList<>(config.getKafkaServersNumber()).         ListenerName listenerName = ListenerName.forSecurityProtocol(config.isSecureMode() ? SecurityProtocol.SASL_PLAINTEXT : SecurityProtocol.PLAINTEXT).         for (int i = 0. i < config.getKafkaServersNumber(). i++) {             KafkaServer kafkaServer = getKafkaServer(i, tmpKafkaDirs.get(i)).             brokers.add(kafkaServer).             brokerConnectionString += hostAndPortToUrlString(KAFKA_HOST, kafkaServer.socketServer().boundPort(listenerName)).             brokerConnectionString += ",".         }         LOG.info("ZK and KafkaServer started.").     } catch (Throwable t) {         t.printStackTrace().         fail("Test setup failed: " + t.getMessage()).     }     standardProps = new Properties().     standardProps.setProperty("zookeeper.connect", zookeeperConnectionString).     standardProps.setProperty("bootstrap.servers", brokerConnectionString).     standardProps.setProperty("group.id", "flink-tests").     standardProps.setProperty("enable.auto.commit", "false").     standardProps.setProperty("zookeeper.session.timeout.ms", String.valueOf(zkTimeout)).     standardProps.setProperty("zookeeper.connection.timeout.ms", String.valueOf(zkTimeout)).     // read from the beginning.     standardProps.setProperty("auto.offset.reset", "earliest").     // make a lot of fetches (MESSAGES MUST BE SMALLER!)     standardProps.setProperty("max.partition.fetch.bytes", "256"). }
false;public;1;10;;@Override public void deleteTestTopic(String topic) {     LOG.info("Deleting topic {}", topic).     try (AdminClient adminClient = AdminClient.create(getStandardProperties())) {         tryDelete(adminClient, topic).     } catch (Exception e) {         e.printStackTrace().         fail(String.format("Delete test topic : %s failed, %s", topic, e.getMessage())).     } }
false;private;2;12;;private void tryDelete(AdminClient adminClient, String topic) throws Exception {     try {         adminClient.deleteTopics(Collections.singleton(topic)).all().get(DELETE_TIMEOUT_SECONDS, TimeUnit.SECONDS).     } catch (TimeoutException e) {         LOG.info("Did not receive delete topic response within %d seconds. Checking if it succeeded", DELETE_TIMEOUT_SECONDS).         if (adminClient.listTopics().names().get(DELETE_TIMEOUT_SECONDS, TimeUnit.SECONDS).contains(topic)) {             throw new Exception("Topic still exists after timeout").         }     } }
false;public;4;11;;@Override public void createTestTopic(String topic, int numberOfPartitions, int replicationFactor, Properties properties) {     LOG.info("Creating topic {}", topic).     try (AdminClient adminClient = AdminClient.create(getStandardProperties())) {         NewTopic topicObj = new NewTopic(topic, numberOfPartitions, (short) replicationFactor).         adminClient.createTopics(Collections.singleton(topicObj)).all().get().     } catch (Exception e) {         e.printStackTrace().         fail("Create test topic : " + topic + " failed, " + e.getMessage()).     } }
false;public;0;4;;@Override public Properties getStandardProperties() {     return standardProps. }
false;public;0;15;;@Override public Properties getSecureProperties() {     Properties prop = new Properties().     if (config.isSecureMode()) {         prop.put("security.inter.broker.protocol", "SASL_PLAINTEXT").         prop.put("security.protocol", "SASL_PLAINTEXT").         prop.put("sasl.kerberos.service.name", "kafka").         // add special timeout for Travis         prop.setProperty("zookeeper.session.timeout.ms", String.valueOf(zkTimeout)).         prop.setProperty("zookeeper.connection.timeout.ms", String.valueOf(zkTimeout)).         prop.setProperty("metadata.fetch.timeout.ms", "120000").     }     return prop. }
false;public;0;4;;@Override public String getBrokerConnectionString() {     return brokerConnectionString. }
false;public;0;4;;@Override public String getVersion() {     return "2.0". }
false;public;0;4;;@Override public List<KafkaServer> getBrokers() {     return brokers. }
false;public;3;4;;@Override public <T> FlinkKafkaConsumerBase<T> getConsumer(List<String> topics, KafkaDeserializationSchema<T> readSchema, Properties props) {     return new FlinkKafkaConsumer<T>(topics, readSchema, props). }
false;public;4;27;;@Override public <K, V> Collection<ConsumerRecord<K, V>> getAllRecordsFromTopic(Properties properties, String topic, int partition, long timeout) {     List<ConsumerRecord<K, V>> result = new ArrayList<>().     try (KafkaConsumer<K, V> consumer = new KafkaConsumer<>(properties)) {         consumer.assign(Arrays.asList(new TopicPartition(topic, partition))).         while (true) {             boolean processedAtLeastOneRecord = false.             // wait for new records with timeout and break the loop if we didn't get any             Iterator<ConsumerRecord<K, V>> iterator = consumer.poll(timeout).iterator().             while (iterator.hasNext()) {                 ConsumerRecord<K, V> record = iterator.next().                 result.add(record).                 processedAtLeastOneRecord = true.             }             if (!processedAtLeastOneRecord) {                 break.             }         }         consumer.commitSync().     }     return UnmodifiableList.decorate(result). }
false;public;4;10;;@Override public <T> StreamSink<T> getProducerSink(String topic, KeyedSerializationSchema<T> serSchema, Properties props, FlinkKafkaPartitioner<T> partitioner) {     return new StreamSink<>(new FlinkKafkaProducer<T>(topic, serSchema, props, Optional.ofNullable(partitioner), producerSemantic, FlinkKafkaProducer.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)). }
false;public;5;10;;@Override public <T> DataStreamSink<T> produceIntoKafka(DataStream<T> stream, String topic, KeyedSerializationSchema<T> serSchema, Properties props, FlinkKafkaPartitioner<T> partitioner) {     return stream.addSink(new FlinkKafkaProducer<T>(topic, serSchema, props, Optional.ofNullable(partitioner), producerSemantic, FlinkKafkaProducer.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE)). }
false;public;4;14;;@Override public <T> DataStreamSink<T> writeToKafkaWithTimestamps(DataStream<T> stream, String topic, KeyedSerializationSchema<T> serSchema, Properties props) {     FlinkKafkaProducer<T> prod = new FlinkKafkaProducer<T>(topic, serSchema, props, Optional.of(new FlinkFixedPartitioner<>()), producerSemantic, FlinkKafkaProducer.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE).     prod.setWriteTimestampToKafka(true).     return stream.addSink(prod). }
false;public;0;4;;@Override public KafkaOffsetHandler createOffsetHandler() {     return new KafkaOffsetHandlerImpl(). }
false;public;1;4;;@Override public void restartBroker(int leaderId) throws Exception {     brokers.set(leaderId, getKafkaServer(leaderId, tmpKafkaDirs.get(leaderId))). }
false;public;1;6;;@Override public int getLeaderToShutDown(String topic) throws Exception {     AdminClient client = AdminClient.create(getStandardProperties()).     TopicDescription result = client.describeTopics(Collections.singleton(topic)).all().get().get(topic).     return result.partitions().get(0).leader().id(). }
false;public;1;4;;@Override public int getBrokerId(KafkaServer server) {     return server.config().brokerId(). }
false;public;0;4;;@Override public boolean isSecureRunSupported() {     return true. }
false;public;0;38;;@Override public void shutdown() throws Exception {     for (KafkaServer broker : brokers) {         if (broker != null) {             broker.shutdown().         }     }     brokers.clear().     if (zookeeper != null) {         try {             zookeeper.stop().         } catch (Exception e) {             LOG.warn("ZK.stop() failed", e).         }         zookeeper = null.     }     if (tmpKafkaParent != null && tmpKafkaParent.exists()) {         try {             FileUtils.deleteDirectory(tmpKafkaParent).         } catch (Exception e) {         // ignore         }     }     if (tmpZkDir != null && tmpZkDir.exists()) {         try {             FileUtils.deleteDirectory(tmpZkDir).         } catch (Exception e) {         // ignore         }     } }
false;protected;2;59;;protected KafkaServer getKafkaServer(int brokerId, File tmpFolder) throws Exception {     Properties kafkaProperties = new Properties().     // properties have to be Strings     kafkaProperties.put("advertised.host.name", KAFKA_HOST).     kafkaProperties.put("broker.id", Integer.toString(brokerId)).     kafkaProperties.put("log.dir", tmpFolder.toString()).     kafkaProperties.put("zookeeper.connect", zookeeperConnectionString).     kafkaProperties.put("message.max.bytes", String.valueOf(50 * 1024 * 1024)).     kafkaProperties.put("replica.fetch.max.bytes", String.valueOf(50 * 1024 * 1024)).     // 2hours     kafkaProperties.put("transaction.max.timeout.ms", Integer.toString(1000 * 60 * 60 * 2)).     // for CI stability, increase zookeeper session timeout     kafkaProperties.put("zookeeper.session.timeout.ms", zkTimeout).     kafkaProperties.put("zookeeper.connection.timeout.ms", zkTimeout).     if (config.getKafkaServerProperties() != null) {         kafkaProperties.putAll(config.getKafkaServerProperties()).     }     final int numTries = 5.     for (int i = 1. i <= numTries. i++) {         int kafkaPort = NetUtils.getAvailablePort().         kafkaProperties.put("port", Integer.toString(kafkaPort)).         if (config.isHideKafkaBehindProxy()) {             NetworkFailuresProxy proxy = createProxy(KAFKA_HOST, kafkaPort).             kafkaProperties.put("advertised.port", proxy.getLocalPort()).         }         // to support secure kafka cluster         if (config.isSecureMode()) {             LOG.info("Adding Kafka secure configurations").             kafkaProperties.put("listeners", "SASL_PLAINTEXT://" + KAFKA_HOST + ":" + kafkaPort).             kafkaProperties.put("advertised.listeners", "SASL_PLAINTEXT://" + KAFKA_HOST + ":" + kafkaPort).             kafkaProperties.putAll(getSecureProperties()).         }         KafkaConfig kafkaConfig = new KafkaConfig(kafkaProperties).         try {             scala.Option<String> stringNone = scala.Option.apply(null).             KafkaServer server = new KafkaServer(kafkaConfig, Time.SYSTEM, stringNone, new ArraySeq<KafkaMetricsReporter>(0)).             server.startup().             return server.         } catch (KafkaException e) {             if (e.getCause() instanceof BindException) {                 // port conflict, retry...                 LOG.info("Port conflict when starting Kafka Broker. Retrying...").             } else {                 throw e.             }         }     }     throw new Exception("Could not start Kafka after " + numTries + " retries due to port conflicts."). }
false;public;2;5;;@Override public Long getCommittedOffset(String topicName, int partition) {     OffsetAndMetadata committed = offsetClient.committed(new TopicPartition(topicName, partition)).     return (committed != null) ? committed.offset() : null. }
false;public;3;6;;@Override public void setCommittedOffset(String topicName, int partition, long offset) {     Map<TopicPartition, OffsetAndMetadata> partitionAndOffset = new HashMap<>().     partitionAndOffset.put(new TopicPartition(topicName, partition), new OffsetAndMetadata(offset)).     offsetClient.commitSync(partitionAndOffset). }
false;public;0;4;;@Override public void close() {     offsetClient.close(). }
