commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;4;;@BeforeClass public static void verifyOS() {     Assume.assumeTrue("HDFS cluster cannot be started on Windows without extensions.", !OperatingSystem.isWindows()). }
true;public,static;0;10;/**  * The bucket file prefix is the absolute path to the part files, which is stored within the savepoint.  */ ;/**  * The bucket file prefix is the absolute path to the part files, which is stored within the savepoint.  */ @Parameterized.Parameters(name = "Migration Savepoint / Bucket Files Prefix: {0}") public static Collection<Tuple2<MigrationVersion, String>> parameters() {     return Arrays.asList(Tuple2.of(MigrationVersion.v1_2, "/var/folders/v_/ry2wp5fx0y7c1rvr41xy9_700000gn/T/junit9160378385359106772/junit479663758539998903/1970-01-01--01/part-0-"), Tuple2.of(MigrationVersion.v1_3, "/var/folders/tv/b_1d8fvx23dgk1_xs8db_95h0000gn/T/junit4273542175898623023/junit3801102997056424640/1970-01-01--01/part-0-"), Tuple2.of(MigrationVersion.v1_4, "/var/folders/tv/b_1d8fvx23dgk1_xs8db_95h0000gn/T/junit3198043255809479705/junit8947526563966405708/1970-01-01--01/part-0-"), Tuple2.of(MigrationVersion.v1_5, "/tmp/junit4927100426019463155/junit2465610012100182280/1970-01-01--00/part-0-"), Tuple2.of(MigrationVersion.v1_6, "/tmp/junit3459711376354834545/junit5114611885650086135/1970-01-01--00/part-0-"), Tuple2.of(MigrationVersion.v1_7, "/var/folders/r2/tdhx810x7yxb7q9_brnp49x40000gp/T/junit4288325607215628863/junit8132783417241536320/1970-01-01--08/part-0-")). }
true;public;0;39;/**  * Manually run this to write binary snapshot data. Remove @Ignore to run.  */ ;/**  * Manually run this to write binary snapshot data. Remove @Ignore to run.  */ @Ignore @Test public void writeSnapshot() throws Exception {     final File outDir = tempFolder.newFolder().     BucketingSink<String> sink = new BucketingSink<String>(outDir.getAbsolutePath()).setWriter(new StringWriter<String>()).setBatchSize(5).setPartPrefix(PART_PREFIX).setInProgressPrefix("").setPendingPrefix("").setValidLengthPrefix("").setInProgressSuffix(IN_PROGRESS_SUFFIX).setPendingSuffix(PENDING_SUFFIX).setValidLengthSuffix(VALID_LENGTH_SUFFIX).     OneInputStreamOperatorTestHarness<String, Object> testHarness = new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink)).     testHarness.setup().     testHarness.open().     testHarness.processElement(new StreamRecord<>("test1", 0L)).     testHarness.processElement(new StreamRecord<>("test2", 0L)).     checkLocalFs(outDir, 1, 1, 0, 0).     testHarness.processElement(new StreamRecord<>("test3", 0L)).     testHarness.processElement(new StreamRecord<>("test4", 0L)).     testHarness.processElement(new StreamRecord<>("test5", 0L)).     checkLocalFs(outDir, 1, 4, 0, 0).     OperatorSubtaskState snapshot = testHarness.snapshot(0L, 0L).     OperatorSnapshotUtil.writeStateHandle(snapshot, "src/test/resources/bucketing-sink-migration-test-flink" + flinkGenerateSavepointVersion + "-snapshot").     testHarness.close(). }
false;public;0;36;;@Test public void testRestore() throws Exception {     final File outDir = tempFolder.newFolder().     ValidatingBucketingSink<String> sink = (ValidatingBucketingSink<String>) new ValidatingBucketingSink<String>(outDir.getAbsolutePath(), expectedBucketFilesPrefix).setWriter(new StringWriter<String>()).setBatchSize(5).setPartPrefix(PART_PREFIX).setInProgressPrefix("").setPendingPrefix("").setValidLengthPrefix("").setInProgressSuffix(IN_PROGRESS_SUFFIX).setPendingSuffix(PENDING_SUFFIX).setValidLengthSuffix(VALID_LENGTH_SUFFIX).setUseTruncate(// don't use truncate because files do not exist     false).     OneInputStreamOperatorTestHarness<String, Object> testHarness = new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), 10, 1, 0).     testHarness.setup().     testHarness.initializeState(OperatorSnapshotUtil.getResourceFilename("bucketing-sink-migration-test-flink" + testMigrateVersion + "-snapshot")).     testHarness.open().     assertTrue(sink.initializeCalled).     testHarness.processElement(new StreamRecord<>("test1", 0L)).     testHarness.processElement(new StreamRecord<>("test2", 0L)).     checkLocalFs(outDir, 1, 1, 0, 0).     testHarness.close(). }
true;public;1;43;/**  * The actual paths in this depend on the binary checkpoint so it you update this the paths  * here have to be updated as well.  */ ;/**  * The actual paths in this depend on the binary checkpoint so it you update this the paths  * here have to be updated as well.  */ @Override public void initializeState(FunctionInitializationContext context) throws Exception {     OperatorStateStore stateStore = context.getOperatorStateStore().     ListState<State<T>> restoredBucketStates = stateStore.getSerializableListState("bucket-states").     if (context.isRestored()) {         for (State<T> states : restoredBucketStates.get()) {             for (String bucketPath : states.bucketStates.keySet()) {                 BucketState state = states.getBucketState(new Path(bucketPath)).                 String current = state.currentFile.                 long validLength = state.currentFileValidLength.                 Assert.assertEquals(expectedBucketFilesPrefix + "4", current).                 Assert.assertEquals(6, validLength).                 List<String> pendingFiles = state.pendingFiles.                 assertTrue(pendingFiles.isEmpty()).                 final Map<Long, List<String>> pendingFilesPerCheckpoint = state.pendingFilesPerCheckpoint.                 Assert.assertEquals(1, pendingFilesPerCheckpoint.size()).                 for (Map.Entry<Long, List<String>> entry : pendingFilesPerCheckpoint.entrySet()) {                     long checkpoint = entry.getKey().                     List<String> files = entry.getValue().                     Assert.assertEquals(0L, checkpoint).                     Assert.assertEquals(4, files.size()).                     for (int i = 0. i < 4. i++) {                         Assert.assertEquals(expectedBucketFilesPrefix + i, files.get(i)).                     }                 }             }         }     }     initializeCalled = true.     super.initializeState(context). }
