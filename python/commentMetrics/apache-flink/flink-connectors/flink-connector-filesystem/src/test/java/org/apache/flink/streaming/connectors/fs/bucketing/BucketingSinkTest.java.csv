# id;timestamp;commentText;codeText;commentWords;codeWords
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1480685315;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2*60*1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6*60*1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1492530125;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2*60*1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6*60*1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1495923089;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1498894422;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1507304600;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1511347989;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1516970987;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1519567828;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1524138809;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1526068396;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1530796781;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1531134089;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testCustomBucketingInactiveBucketCleanup() throws Exception;1550863152;This uses a custom bucketing function which determines the bucket from the input._We use a simulated clock to reduce the number of buckets being written to over time._This causes buckets to become 'inactive' and their file parts 'closed' by the sink.;@Test_	public void testCustomBucketingInactiveBucketCleanup() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int step1NumIds = 4__		final int step2NumIds = 2__		final int numElementsPerStep = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)))__		}__		testHarness.setProcessingTime(2 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		testHarness.setProcessingTime(6 * 60 * 1000L)___		for (int i = 0_ i < numElementsPerStep_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)))__		}__		_		_		int numFiles = 0__		int numInProgress = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getAbsolutePath().endsWith("crc")) {_				continue__			}_			if (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {_				numInProgress++__			}_			numFiles++__		}__		testHarness.close()___		Assert.assertEquals(4, numFiles)__		Assert.assertEquals(2, numInProgress)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input,we,use,a,simulated,clock,to,reduce,the,number,of,buckets,being,written,to,over,time,this,causes,buckets,to,become,inactive,and,their,file,parts,closed,by,the,sink;test,public,void,test,custom,bucketing,inactive,bucket,cleanup,throws,exception,file,data,dir,temp,folder,new,folder,final,int,step1num,ids,4,final,int,step2num,ids,2,final,int,num,elements,per,step,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step1num,ids,test,harness,set,processing,time,2,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,test,harness,set,processing,time,6,60,1000l,for,int,i,0,i,num,elements,per,step,i,test,harness,process,element,new,stream,record,integer,to,string,i,step2num,ids,int,num,files,0,int,num,in,progress,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,absolute,path,ends,with,crc,continue,if,file,get,path,ends,with,num,in,progress,num,files,test,harness,close,assert,assert,equals,4,num,files,assert,assert,equals,2,num,in,progress
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1480685315;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1492530125;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1495923089;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1498894422;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1507304600;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1511347989;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1516970987;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1519567828;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1524138809;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1526068396;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1530796781;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1531134089;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1550863152;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"___		final int numElements = 20___		BucketingSink<Tuple2<IntWritable, Text>> sink = new BucketingSink<Tuple2<IntWritable, Text>>(outPath)_			.setWriter(new SequenceFileWriter<IntWritable, Text>())_			.setBucketer(new BasePathBucketer<Tuple2<IntWritable, Text>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		sink.setInputType(TypeInformation.of(new TypeHint<Tuple2<IntWritable, Text>>(){}), new ExecutionConfig())___		OneInputStreamOperatorTestHarness<Tuple2<IntWritable, Text>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				new IntWritable(i),_				new Text("message #" + Integer.toString(i))_			)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i++) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,final,int,num,elements,20,bucketing,sink,tuple2,int,writable,text,sink,new,bucketing,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,base,path,bucketer,tuple2,int,writable,text,set,part,prefix,set,pending,prefix,set,pending,suffix,sink,set,input,type,type,information,of,new,type,hint,tuple2,int,writable,text,new,execution,config,one,input,stream,operator,test,harness,tuple2,int,writable,text,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,new,int,writable,i,new,text,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1480685315;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1492530125;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1495923089;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1498894422;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1507304600;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1511347989;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1516970987;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1519567828;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1524138809;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1526068396;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1530796781;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1531134089;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1550863152;This uses {@link DateTimeBucketer} to_produce rolling files. We use {@link OneInputStreamOperatorTestHarness} to manually_advance processing time.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20___		final String outPath = hdfsURI + "/rolling-out"___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new DateTimeBucketer<String>("ss"))_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			_			if (i % 5 == 0) {_				testHarness.setProcessingTime(i * 1000L)__			}_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,link,date,time,bucketer,to,produce,rolling,files,we,use,link,one,input,stream,operator,test,harness,to,manually,advance,processing,time;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,date,time,bucketer,string,ss,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,if,i,5,0,test,harness,set,processing,time,i,1000l,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1480685315;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1492530125;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1495923089;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1498894422;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1507304600;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1511347989;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1516970987;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1519567828;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1524138809;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1526068396;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1530796781;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1531134089;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingStringWriter() throws Exception;1550863152;This tests {@link StringWriter} with_non-bucketing output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-out"___		final int numElements = 20___		BucketingSink<String> sink = new BucketingSink<String>(outPath)_			.setBucketer(new BasePathBucketer<String>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>("message #" + Integer.toString(i)))__		}__		testHarness.close()___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i++) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,bucketing,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,out,final,int,num,elements,20,bucketing,sink,string,sink,new,bucketing,sink,string,out,path,set,bucketer,new,base,path,bucketer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,message,integer,to,string,i,test,harness,close,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1480685315;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1492530125;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1495923089;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1498894422;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1507304600;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1511347989;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1516970987;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1519567828;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1524138809;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1526068396;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1530796781;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1531134089;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testCustomBucketing() throws Exception;1550863152;This uses a custom bucketing function which determines the bucket from the input.;@Test_	public void testCustomBucketing() throws Exception {_		File dataDir = tempFolder.newFolder()___		final int numIds = 4__		final int numElements = 20___		OneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Integer.toString(i % numIds)))__		}__		testHarness.close()___		_		int numFiles = 0__		for (File file: FileUtils.listFiles(dataDir, null, true)) {_			if (file.getName().startsWith(PART_PREFIX)) {_				numFiles++__			}_		}__		Assert.assertEquals(4, numFiles)__	};this,uses,a,custom,bucketing,function,which,determines,the,bucket,from,the,input;test,public,void,test,custom,bucketing,throws,exception,file,data,dir,temp,folder,new,folder,final,int,num,ids,4,final,int,num,elements,20,one,input,stream,operator,test,harness,string,object,test,harness,create,test,sink,data,dir,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,integer,to,string,i,num,ids,test,harness,close,int,num,files,0,for,file,file,file,utils,list,files,data,dir,null,true,if,file,get,name,starts,with,num,files,assert,assert,equals,4,num,files
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1480685315;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1492530125;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1495923089;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1498894422;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1507304600;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1511347989;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1516970987;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1519567828;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1524138809;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1526068396;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1530796781;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1531134089;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1550863152;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"___		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1480685315;This tests user defined hdfs configuration_@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer,String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer,String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1492530125;This tests user defined hdfs configuration_@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer,String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer,String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1495923089;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1498894422;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1507304600;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1511347989;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1516970987;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1519567828;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1524138809;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1526068396;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1530796781;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1531134089;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
BucketingSinkTest -> @Test 	public void testUserDefinedConfiguration() throws Exception;1550863152;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		final int numElements = 20___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Schema.Type.INT)__		Schema valueSchema = Schema.create(Schema.Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")___		BucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, "io.file.buffer.size", "40960"))_			.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())_			.setPartPrefix(PART_PREFIX)_			.setPendingPrefix("")_			.setPendingSuffix("")___		OneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =_			createTestSink(sink, 1, 0)___		testHarness.setProcessingTime(0L)___		testHarness.setup()__		testHarness.open()___		for (int i = 0_ i < numElements_ i++) {_			testHarness.processElement(new StreamRecord<>(Tuple2.of(_				i, "message #" + Integer.toString(i)_			)))__		}__		testHarness.close()___		GenericData.setStringType(valueSchema, GenericData.StringType.String)__		Schema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/" + PART_PREFIX + "-0-0"))___		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i++) {_			AvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =_				new AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next())__			int key = wrappedEntry.getKey()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,string,out,path,hdfs,uri,string,non,rolling,with,config,final,int,num,elements,20,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,schema,type,int,schema,value,schema,schema,create,schema,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,bucketing,sink,tuple2,integer,string,sink,new,bucketing,sink,tuple2,integer,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,integer,string,properties,io,file,buffer,size,40960,set,bucketer,new,base,path,bucketer,tuple2,integer,string,set,part,prefix,set,pending,prefix,set,pending,suffix,one,input,stream,operator,test,harness,tuple2,integer,string,object,test,harness,create,test,sink,sink,1,0,test,harness,set,processing,time,0l,test,harness,setup,test,harness,open,for,int,i,0,i,num,elements,i,test,harness,process,element,new,stream,record,tuple2,of,i,message,integer,to,string,i,test,harness,close,generic,data,set,string,type,value,schema,generic,data,string,type,string,schema,element,schema,avro,key,value,sink,writer,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,in,stream,element,reader,for,int,i,0,i,num,elements,i,avro,key,value,sink,writer,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,sink,writer,avro,key,value,data,file,stream,next,int,key,wrapped,entry,get,key,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
