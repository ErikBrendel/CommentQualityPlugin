commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;25;;@BeforeClass public static void setup() throws Exception {     LOG.info("In RollingSinkITCase: Starting MiniDFSCluster ").     dataDir = tempFolder.newFolder().     conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, dataDir.getAbsolutePath()).     MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf).     hdfsCluster = builder.build().     dfs = hdfsCluster.getFileSystem().     hdfsURI = "hdfs://" + NetUtils.hostAndPortToUrlString(hdfsCluster.getURI().getHost(), hdfsCluster.getNameNodePort()) + "/".     miniClusterResource = new MiniClusterResource(new MiniClusterResourceConfiguration.Builder().setNumberTaskManagers(1).setNumberSlotsPerTaskManager(4).build()).     miniClusterResource.before(). }
false;public,static;0;9;;@AfterClass public static void teardown() throws Exception {     LOG.info("In RollingSinkITCase: tearing down MiniDFSCluster ").     hdfsCluster.shutdown().     if (miniClusterResource != null) {         miniClusterResource.after().     } }
false;public;1;4;;@Override public String map(Tuple2<Integer, String> value) throws Exception {     return value.f1. }
true;public;0;51;/**  * This tests {@link StringWriter} with  * non-rolling output.  */ ;/**  * This tests {@link StringWriter} with  * non-rolling output.  */ @Test public void testNonRollingStringWriter() throws Exception {     final int numElements = 20.     final String outPath = hdfsURI + "/string-non-rolling-out".     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(2).     DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements)).broadcast().filter(new OddEvenFilter()).     RollingSink<String> sink = new RollingSink<String>(outPath).setBucketer(new NonRollingBucketer()).setPartPrefix("part").setPendingPrefix("").setPendingSuffix("").     source.map(new MapFunction<Tuple2<Integer, String>, String>() {          private static final long serialVersionUID = 1L.          @Override         public String map(Tuple2<Integer, String> value) throws Exception {             return value.f1.         }     }).addSink(sink).     env.execute("RollingSink String Write Test").     FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0")).     BufferedReader br = new BufferedReader(new InputStreamReader(inStream)).     for (int i = 0. i < numElements. i += 2) {         String line = br.readLine().         Assert.assertEquals("message #" + i, line).     }     inStream.close().     inStream = dfs.open(new Path(outPath + "/part-1-0")).     br = new BufferedReader(new InputStreamReader(inStream)).     for (int i = 1. i < numElements. i += 2) {         String line = br.readLine().         Assert.assertEquals("message #" + i, line).     }     inStream.close(). }
false;public;1;4;;@Override public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {     return Tuple2.of(new IntWritable(value.f0), new Text(value.f1)). }
true;public;0;68;/**  * This tests {@link SequenceFileWriter}  * with non-rolling output and without compression.  */ ;/**  * This tests {@link SequenceFileWriter}  * with non-rolling output and without compression.  */ @Test public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {     final int numElements = 20.     final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out".     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(2).     DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements)).broadcast().filter(new OddEvenFilter()).     DataStream<Tuple2<IntWritable, Text>> mapped = source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {          private static final long serialVersionUID = 1L.          @Override         public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {             return Tuple2.of(new IntWritable(value.f0), new Text(value.f1)).         }     }).     RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath).setWriter(new SequenceFileWriter<IntWritable, Text>()).setBucketer(new NonRollingBucketer()).setPartPrefix("part").setPendingPrefix("").setPendingSuffix("").     mapped.addSink(sink).     env.execute("RollingSink String Write Test").     FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0")).     SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration()).     IntWritable intWritable = new IntWritable().     Text txt = new Text().     for (int i = 0. i < numElements. i += 2) {         reader.next(intWritable, txt).         Assert.assertEquals(i, intWritable.get()).         Assert.assertEquals("message #" + i, txt.toString()).     }     reader.close().     inStream.close().     inStream = dfs.open(new Path(outPath + "/part-1-0")).     reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration()).     for (int i = 1. i < numElements. i += 2) {         reader.next(intWritable, txt).         Assert.assertEquals(i, intWritable.get()).         Assert.assertEquals("message #" + i, txt.toString()).     }     reader.close().     inStream.close(). }
false;public;1;4;;@Override public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {     return Tuple2.of(new IntWritable(value.f0), new Text(value.f1)). }
true;public;0;68;/**  * This tests {@link SequenceFileWriter}  * with non-rolling output but with compression.  */ ;/**  * This tests {@link SequenceFileWriter}  * with non-rolling output but with compression.  */ @Test public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {     final int numElements = 20.     final String outPath = hdfsURI + "/seq-non-rolling-out".     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(2).     DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements)).broadcast().filter(new OddEvenFilter()).     DataStream<Tuple2<IntWritable, Text>> mapped = source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {          private static final long serialVersionUID = 1L.          @Override         public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {             return Tuple2.of(new IntWritable(value.f0), new Text(value.f1)).         }     }).     RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath).setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK)).setBucketer(new NonRollingBucketer()).setPartPrefix("part").setPendingPrefix("").setPendingSuffix("").     mapped.addSink(sink).     env.execute("RollingSink String Write Test").     FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0")).     SequenceFile.Reader reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration()).     IntWritable intWritable = new IntWritable().     Text txt = new Text().     for (int i = 0. i < numElements. i += 2) {         reader.next(intWritable, txt).         Assert.assertEquals(i, intWritable.get()).         Assert.assertEquals("message #" + i, txt.toString()).     }     reader.close().     inStream.close().     inStream = dfs.open(new Path(outPath + "/part-1-0")).     reader = new SequenceFile.Reader(inStream, 1000, 0, 100000, new Configuration()).     for (int i = 1. i < numElements. i += 2) {         reader.next(intWritable, txt).         Assert.assertEquals(i, intWritable.get()).         Assert.assertEquals("message #" + i, txt.toString()).     }     reader.close().     inStream.close(). }
true;public;0;58;/**  * This tests {@link AvroKeyValueSinkWriter}  * with non-rolling output and without compression.  */ ;/**  * This tests {@link AvroKeyValueSinkWriter}  * with non-rolling output and without compression.  */ @Test public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {     final int numElements = 20.     final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out".     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(2).     DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements)).broadcast().filter(new OddEvenFilter()).     Map<String, String> properties = new HashMap<>().     Schema keySchema = Schema.create(Type.INT).     Schema valueSchema = Schema.create(Type.STRING).     properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString()).     properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString()).     RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath).setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties)).setBucketer(new NonRollingBucketer()).setPartPrefix("part").setPendingPrefix("").setPendingSuffix("").     source.addSink(sink).     env.execute("RollingSink Avro KeyValue Writer Test").     GenericData.setStringType(valueSchema, StringType.String).     Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema).     FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0")).     SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema).     DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader).     for (int i = 0. i < numElements. i += 2) {         AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next()).         int key = wrappedEntry.getKey().intValue().         Assert.assertEquals(i, key).         String value = wrappedEntry.getValue().         Assert.assertEquals("message #" + i, value).     }     dataFileStream.close().     inStream.close().     inStream = dfs.open(new Path(outPath + "/part-1-0")).     dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader).     for (int i = 1. i < numElements. i += 2) {         AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next()).         int key = wrappedEntry.getKey().intValue().         Assert.assertEquals(i, key).         String value = wrappedEntry.getValue().         Assert.assertEquals("message #" + i, value).     }     dataFileStream.close().     inStream.close(). }
true;public;0;60;/**  * This tests {@link AvroKeyValueSinkWriter}  * with non-rolling output and with compression.  */ ;/**  * This tests {@link AvroKeyValueSinkWriter}  * with non-rolling output and with compression.  */ @Test public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {     final int numElements = 20.     final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out".     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(2).     DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements)).broadcast().filter(new OddEvenFilter()).     Map<String, String> properties = new HashMap<>().     Schema keySchema = Schema.create(Type.INT).     Schema valueSchema = Schema.create(Type.STRING).     properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString()).     properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString()).     properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true)).     properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC).     RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath).setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties)).setBucketer(new NonRollingBucketer()).setPartPrefix("part").setPendingPrefix("").setPendingSuffix("").     source.addSink(sink).     env.execute("RollingSink Avro KeyValue Writer Test").     GenericData.setStringType(valueSchema, StringType.String).     Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema).     FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0")).     SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema).     DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader).     for (int i = 0. i < numElements. i += 2) {         AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next()).         int key = wrappedEntry.getKey().intValue().         Assert.assertEquals(i, key).         String value = wrappedEntry.getValue().         Assert.assertEquals("message #" + i, value).     }     dataFileStream.close().     inStream.close().     inStream = dfs.open(new Path(outPath + "/part-1-0")).     dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader).     for (int i = 1. i < numElements. i += 2) {         AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next()).         int key = wrappedEntry.getKey().intValue().         Assert.assertEquals(i, key).         String value = wrappedEntry.getValue().         Assert.assertEquals("message #" + i, value).     }     dataFileStream.close().     inStream.close(). }
false;public;1;4;;@Override public String map(Tuple2<Integer, String> value) throws Exception {     return value.f1. }
true;public;0;55;/**  * This tests user defined hdfs configuration.  * @throws Exception  */ ;/**  * This tests user defined hdfs configuration.  * @throws Exception  */ @Test public void testUserDefinedConfiguration() throws Exception {     final int numElements = 20.     final String outPath = hdfsURI + "/string-non-rolling-with-config".     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(2).     DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements)).broadcast().filter(new OddEvenFilter()).     Configuration conf = new Configuration().     conf.set("io.file.buffer.size", "40960").     RollingSink<String> sink = new RollingSink<String>(outPath).setFSConfig(conf).setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960")).setBucketer(new NonRollingBucketer()).setPartPrefix("part").setPendingPrefix("").setPendingSuffix("").     source.map(new MapFunction<Tuple2<Integer, String>, String>() {          private static final long serialVersionUID = 1L.          @Override         public String map(Tuple2<Integer, String> value) throws Exception {             return value.f1.         }     }).addSink(sink).     env.execute("RollingSink with configuration Test").     FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0")).     BufferedReader br = new BufferedReader(new InputStreamReader(inStream)).     for (int i = 0. i < numElements. i += 2) {         String line = br.readLine().         Assert.assertEquals("message #" + i, line).     }     inStream.close().     inStream = dfs.open(new Path(outPath + "/part-1-0")).     br = new BufferedReader(new InputStreamReader(inStream)).     for (int i = 1. i < numElements. i += 2) {         String line = br.readLine().         Assert.assertEquals("message #" + i, line).     }     inStream.close(). }
false;public;2;14;;@Override public void flatMap(Tuple2<Integer, String> value, Collector<String> out) throws Exception {     out.collect(value.f1).     count++.     if (count >= 5) {         if (getRuntimeContext().getIndexOfThisSubtask() == 0) {             latch1.trigger().         } else {             latch2.trigger().         }         count = 0.     } }
true;public;0;106;/**  * This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to  * produce rolling files. The clock of DateTimeBucketer is set to  * {@link ModifyableClock} to keep the time in lockstep with the processing of elements using  * latches.  */ ;/**  * This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to  * produce rolling files. The clock of DateTimeBucketer is set to  * {@link ModifyableClock} to keep the time in lockstep with the processing of elements using  * latches.  */ @Test public void testDateTimeRollingStringWriter() throws Exception {     final int numElements = 20.     final String outPath = hdfsURI + "/rolling-out".     DateTimeBucketer.setClock(new ModifyableClock()).     ModifyableClock.setCurrentTime(0).     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(2).     DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(numElements)).broadcast().     // the parallel flatMap is chained to the sink, so when it has seen 5 elements it can     // fire the latch     DataStream<String> mapped = source.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {          private static final long serialVersionUID = 1L.          int count = 0.          @Override         public void flatMap(Tuple2<Integer, String> value, Collector<String> out) throws Exception {             out.collect(value.f1).             count++.             if (count >= 5) {                 if (getRuntimeContext().getIndexOfThisSubtask() == 0) {                     latch1.trigger().                 } else {                     latch2.trigger().                 }                 count = 0.             }         }     }).     RollingSink<String> sink = new RollingSink<String>(outPath).setBucketer(new DateTimeBucketer("ss")).setPartPrefix("part").setPendingPrefix("").setPendingSuffix("").     mapped.addSink(sink).     env.execute("RollingSink String Write Test").     RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true).     // we should have 8 rolling files, 4 time intervals and parallelism of 2     int numFiles = 0.     while (files.hasNext()) {         LocatedFileStatus file = files.next().         numFiles++.         if (file.getPath().toString().contains("rolling-out/00")) {             FSDataInputStream inStream = dfs.open(file.getPath()).             BufferedReader br = new BufferedReader(new InputStreamReader(inStream)).             for (int i = 0. i < 5. i++) {                 String line = br.readLine().                 Assert.assertEquals("message #" + i, line).             }             inStream.close().         } else if (file.getPath().toString().contains("rolling-out/05")) {             FSDataInputStream inStream = dfs.open(file.getPath()).             BufferedReader br = new BufferedReader(new InputStreamReader(inStream)).             for (int i = 5. i < 10. i++) {                 String line = br.readLine().                 Assert.assertEquals("message #" + i, line).             }             inStream.close().         } else if (file.getPath().toString().contains("rolling-out/10")) {             FSDataInputStream inStream = dfs.open(file.getPath()).             BufferedReader br = new BufferedReader(new InputStreamReader(inStream)).             for (int i = 10. i < 15. i++) {                 String line = br.readLine().                 Assert.assertEquals("message #" + i, line).             }             inStream.close().         } else if (file.getPath().toString().contains("rolling-out/15")) {             FSDataInputStream inStream = dfs.open(file.getPath()).             BufferedReader br = new BufferedReader(new InputStreamReader(inStream)).             for (int i = 15. i < 20. i++) {                 String line = br.readLine().                 Assert.assertEquals("message #" + i, line).             }             inStream.close().         } else {             Assert.fail("File " + file + " does not match any expected roll pattern.").         }     }     Assert.assertEquals(8, numFiles). }
false;public;0;56;;@Test public void testBucketStateTransitions() throws Exception {     final File outDir = tempFolder.newFolder().     OneInputStreamOperatorTestHarness<String, Object> testHarness = createRescalingTestSink(outDir, 1, 0).     testHarness.setup().     testHarness.open().     testHarness.setProcessingTime(0L).     // we have a bucket size of 5 bytes, so each record will get its own bucket,     // i.e. the bucket should roll after every record.     testHarness.processElement(new StreamRecord<>("test1", 1L)).     testHarness.processElement(new StreamRecord<>("test2", 1L)).     checkLocalFs(outDir, 1, 1, 0, 0).     testHarness.processElement(new StreamRecord<>("test3", 1L)).     checkLocalFs(outDir, 1, 2, 0, 0).     testHarness.snapshot(0, 0).     checkLocalFs(outDir, 1, 2, 0, 0).     testHarness.notifyOfCompletedCheckpoint(0).     checkLocalFs(outDir, 1, 0, 2, 0).     OperatorSubtaskState snapshot = testHarness.snapshot(1, 0).     testHarness.close().     checkLocalFs(outDir, 0, 1, 2, 0).     testHarness = createRescalingTestSink(outDir, 1, 0).     testHarness.setup().     testHarness.initializeState(snapshot).     testHarness.open().     checkLocalFs(outDir, 0, 0, 3, 1).     snapshot = testHarness.snapshot(2, 0).     testHarness.processElement(new StreamRecord<>("test4", 10)).     checkLocalFs(outDir, 1, 0, 3, 1).     testHarness = createRescalingTestSink(outDir, 1, 0).     testHarness.setup().     testHarness.initializeState(snapshot).     testHarness.open().     // the in-progress file remains as we do not clean up now     checkLocalFs(outDir, 1, 0, 3, 1).     testHarness.close().     // at close it is not moved to final because it is not part     // of the current task's state, it was just a not cleaned up leftover.     checkLocalFs(outDir, 1, 0, 3, 1). }
false;public;0;63;;@Test public void testScalingDown() throws Exception {     final File outDir = tempFolder.newFolder().     OneInputStreamOperatorTestHarness<String, Object> testHarness1 = createRescalingTestSink(outDir, 3, 0).     testHarness1.setup().     testHarness1.open().     OneInputStreamOperatorTestHarness<String, Object> testHarness2 = createRescalingTestSink(outDir, 3, 1).     testHarness2.setup().     testHarness2.open().     OneInputStreamOperatorTestHarness<String, Object> testHarness3 = createRescalingTestSink(outDir, 3, 2).     testHarness3.setup().     testHarness3.open().     testHarness1.processElement(new StreamRecord<>("test1", 0L)).     checkLocalFs(outDir, 1, 0, 0, 0).     testHarness2.processElement(new StreamRecord<>("test2", 0L)).     testHarness2.processElement(new StreamRecord<>("test3", 0L)).     testHarness2.processElement(new StreamRecord<>("test4", 0L)).     testHarness2.processElement(new StreamRecord<>("test5", 0L)).     testHarness2.processElement(new StreamRecord<>("test6", 0L)).     checkLocalFs(outDir, 2, 4, 0, 0).     testHarness3.processElement(new StreamRecord<>("test7", 0L)).     testHarness3.processElement(new StreamRecord<>("test8", 0L)).     checkLocalFs(outDir, 3, 5, 0, 0).     // intentionally we snapshot them in a not ascending order so that the states are shuffled     OperatorSubtaskState mergedSnapshot = AbstractStreamOperatorTestHarness.repackageState(testHarness3.snapshot(0, 0), testHarness1.snapshot(0, 0), testHarness2.snapshot(0, 0)).     // with the above state reshuffling, we expect testHarness4 to take the     // state of the previous testHarness3 and testHarness1 while testHarness5     // will take that of the previous testHarness1     OperatorSubtaskState initState1 = AbstractStreamOperatorTestHarness.repartitionOperatorState(mergedSnapshot, maxParallelism, 3, 2, 0).     OperatorSubtaskState initState2 = AbstractStreamOperatorTestHarness.repartitionOperatorState(mergedSnapshot, maxParallelism, 3, 2, 1).     OneInputStreamOperatorTestHarness<String, Object> testHarness4 = createRescalingTestSink(outDir, 2, 0).     testHarness4.setup().     testHarness4.initializeState(initState1).     testHarness4.open().     // we do not have a length file for part-2-0 because bucket part-2-0     // was not "in-progress", but "pending" (its full content is valid).     checkLocalFs(outDir, 1, 4, 3, 2).     OneInputStreamOperatorTestHarness<String, Object> testHarness5 = createRescalingTestSink(outDir, 2, 1).     testHarness5.setup().     testHarness5.initializeState(initState2).     testHarness5.open().     checkLocalFs(outDir, 0, 0, 8, 3). }
false;public;0;76;;@Test public void testScalingUp() throws Exception {     final File outDir = tempFolder.newFolder().     OneInputStreamOperatorTestHarness<String, Object> testHarness1 = createRescalingTestSink(outDir, 2, 0).     testHarness1.setup().     testHarness1.open().     OneInputStreamOperatorTestHarness<String, Object> testHarness2 = createRescalingTestSink(outDir, 2, 0).     testHarness2.setup().     testHarness2.open().     testHarness1.processElement(new StreamRecord<>("test1", 0L)).     testHarness1.processElement(new StreamRecord<>("test2", 0L)).     checkLocalFs(outDir, 1, 1, 0, 0).     testHarness2.processElement(new StreamRecord<>("test3", 0L)).     testHarness2.processElement(new StreamRecord<>("test4", 0L)).     testHarness2.processElement(new StreamRecord<>("test5", 0L)).     checkLocalFs(outDir, 2, 3, 0, 0).     // intentionally we snapshot them in the reverse order so that the states are shuffled     OperatorSubtaskState mergedSnapshot = AbstractStreamOperatorTestHarness.repackageState(testHarness2.snapshot(0, 0), testHarness1.snapshot(0, 0)).     OperatorSubtaskState initState1 = AbstractStreamOperatorTestHarness.repartitionOperatorState(mergedSnapshot, maxParallelism, 2, 3, 0).     OperatorSubtaskState initState2 = AbstractStreamOperatorTestHarness.repartitionOperatorState(mergedSnapshot, maxParallelism, 2, 3, 1).     OperatorSubtaskState initState3 = AbstractStreamOperatorTestHarness.repartitionOperatorState(mergedSnapshot, maxParallelism, 2, 3, 2).     testHarness1 = createRescalingTestSink(outDir, 3, 0).     testHarness1.setup().     testHarness1.initializeState(initState1).     testHarness1.open().     checkLocalFs(outDir, 1, 1, 3, 1).     testHarness2 = createRescalingTestSink(outDir, 3, 1).     testHarness2.setup().     testHarness2.initializeState(initState2).     testHarness2.open().     checkLocalFs(outDir, 0, 0, 5, 2).     OneInputStreamOperatorTestHarness<String, Object> testHarness3 = createRescalingTestSink(outDir, 3, 2).     testHarness3.setup().     testHarness3.initializeState(initState3).     testHarness3.open().     checkLocalFs(outDir, 0, 0, 5, 2).     testHarness1.processElement(new StreamRecord<>("test6", 0)).     testHarness2.processElement(new StreamRecord<>("test6", 0)).     testHarness3.processElement(new StreamRecord<>("test6", 0)).     // 3 for the different tasks     checkLocalFs(outDir, 3, 0, 5, 2).     testHarness1.snapshot(1, 0).     testHarness2.snapshot(1, 0).     testHarness3.snapshot(1, 0).     testHarness1.close().     testHarness2.close().     testHarness3.close().     checkLocalFs(outDir, 0, 3, 5, 2). }
false;private;3;16;;private OneInputStreamOperatorTestHarness<String, Object> createRescalingTestSink(File outDir, int totalParallelism, int taskIdx) throws Exception {     RollingSink<String> sink = new RollingSink<String>(outDir.getAbsolutePath()).setWriter(new StringWriter<String>()).setBatchSize(5).setPartPrefix(PART_PREFIX).setInProgressPrefix("").setPendingPrefix("").setValidLengthPrefix("").setInProgressSuffix(IN_PROGRESS_SUFFIX).setPendingSuffix(PENDING_SUFFIX).setValidLengthSuffix(VALID_LENGTH_SUFFIX).     return createTestSink(sink, totalParallelism, taskIdx). }
false;private;3;4;;private <T> OneInputStreamOperatorTestHarness<T, Object> createTestSink(RollingSink<T> sink, int totalParallelism, int taskIdx) throws Exception {     return new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), maxParallelism, totalParallelism, taskIdx). }
false;public;1;6;;@Override public void run(SourceContext<Tuple2<Integer, String>> ctx) throws Exception {     for (int i = 0. i < numElements && running. i++) {         ctx.collect(Tuple2.of(i, "message #" + i)).     } }
false;public;0;4;;@Override public void cancel() {     running = false. }
false;public;1;13;;@Override public void run(SourceContext<Tuple2<Integer, String>> ctx) throws Exception {     for (int i = 0. i < numElements && running. i++) {         if (i % 5 == 0 && i > 0) {             // update the clock after "five seconds", so we get 20 seconds in total             // with 5 elements in each time window             latch1.await().             latch2.await().             ModifyableClock.setCurrentTime(i * 1000).         }         ctx.collect(Tuple2.of(i, "message #" + i)).     } }
false;public;0;4;;@Override public void cancel() {     running = false. }
false;public;2;5;;@Override public void open(FileSystem fs, Path path) throws IOException {     super.open(fs, path).     Assert.assertEquals(expect, fs.getConf().get(key)). }
false;public;0;4;;@Override public StreamWriterWithConfigCheck<T> duplicate() {     return new StreamWriterWithConfigCheck<>(key, expect). }
false;public;1;8;;@Override public boolean filter(Tuple2<Integer, String> value) throws Exception {     if (getRuntimeContext().getIndexOfThisSubtask() == 0) {         return value.f0 % 2 == 0.     } else {         return value.f0 % 2 == 1.     } }
false;public,static;1;3;;public static void setCurrentTime(long currentTime) {     ModifyableClock.currentTime = currentTime. }
false;public;0;4;;@Override public long currentTimeMillis() {     return currentTime. }
