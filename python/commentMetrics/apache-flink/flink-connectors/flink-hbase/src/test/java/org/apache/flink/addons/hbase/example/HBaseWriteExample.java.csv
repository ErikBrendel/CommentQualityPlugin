commented;modifiers;parameterAmount;loc;comment;code
false;public;1;5;;@Override public void open(Configuration parameters) throws Exception {     super.open(parameters).     reuse = new Tuple2<Text, Mutation>(). }
false;public;1;8;;@Override public Tuple2<Text, Mutation> map(Tuple2<String, Integer> t) throws Exception {     reuse.f0 = new Text(t.f0).     Put put = new Put(t.f0.getBytes(ConfigConstants.DEFAULT_CHARSET)).     put.add(HBaseFlinkTestConstants.CF_SOME, HBaseFlinkTestConstants.Q_SOME, Bytes.toBytes(t.f1)).     reuse.f1 = put.     return reuse. }
false;public,static;1;46;;// ************************************************************************* // PROGRAM // ************************************************************************* public static void main(String[] args) throws Exception {     if (!parseParameters(args)) {         return.     }     // set up the execution environment     final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment().     // get input data     DataSet<String> text = getTextDataSet(env).     DataSet<Tuple2<String, Integer>> counts = // split up the lines in pairs (2-tuples) containing: (word, 1)     text.flatMap(new Tokenizer()).groupBy(0).sum(1).     // emit result     Job job = Job.getInstance().     job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, outputTableName).     // TODO is "mapred.output.dir" really useful?     job.getConfiguration().set("mapred.output.dir", HBaseFlinkTestConstants.TMP_DIR).     counts.map(new RichMapFunction<Tuple2<String, Integer>, Tuple2<Text, Mutation>>() {          private transient Tuple2<Text, Mutation> reuse.          @Override         public void open(Configuration parameters) throws Exception {             super.open(parameters).             reuse = new Tuple2<Text, Mutation>().         }          @Override         public Tuple2<Text, Mutation> map(Tuple2<String, Integer> t) throws Exception {             reuse.f0 = new Text(t.f0).             Put put = new Put(t.f0.getBytes(ConfigConstants.DEFAULT_CHARSET)).             put.add(HBaseFlinkTestConstants.CF_SOME, HBaseFlinkTestConstants.Q_SOME, Bytes.toBytes(t.f1)).             reuse.f1 = put.             return reuse.         }     }).output(new HadoopOutputFormat<Text, Mutation>(new TableOutputFormat<Text>(), job)).     // execute program     env.execute("WordCount (HBase sink) Example"). }
false;public;2;12;;@Override public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {     // normalize and split the line     String[] tokens = value.toLowerCase().split("\\W+").     // emit the pairs     for (String token : tokens) {         if (token.length() > 0) {             out.collect(new Tuple2<String, Integer>(token, 1)).         }     } }
false;private,static;1;19;;private static boolean parseParameters(String[] args) {     if (args.length > 0) {         // parse input arguments         fileOutput = true.         if (args.length == 2) {             textPath = args[0].             outputTableName = args[1].         } else {             System.err.println("Usage: HBaseWriteExample <text path> <output table>").             return false.         }     } else {         System.out.println("Executing HBaseWriteExample example with built-in default data.").         System.out.println("  Provide parameters to read input data from a file.").         System.out.println("  Usage: HBaseWriteExample <text path> <output table>").     }     return true. }
false;private,static;1;9;;private static DataSet<String> getTextDataSet(ExecutionEnvironment env) {     if (fileOutput) {         // read the text file from given input path         return env.readTextFile(textPath).     } else {         // get default test text data         return getDefaultTextLineDataSet(env).     } }
false;private,static;1;3;;private static DataSet<String> getDefaultTextLineDataSet(ExecutionEnvironment env) {     return env.fromElements(WORDS). }
