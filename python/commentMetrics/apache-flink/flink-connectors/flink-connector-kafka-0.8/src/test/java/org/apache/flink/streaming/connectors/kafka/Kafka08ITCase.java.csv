commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;5;;@BeforeClass public static void prepare() throws ClassNotFoundException {     // Somehow KafkaConsumer 0.8 doesn't handle broker failures if they are behind a proxy     prepare(false). }
false;public;0;4;;// ------------------------------------------------------------------------ // Suite of Tests // ------------------------------------------------------------------------ @Test(timeout = 60000) public void testFailOnNoBroker() throws Exception {     runFailOnNoBrokerTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testConcurrentProducerConsumerTopology() throws Exception {     runSimpleConcurrentProducerConsumerTopology(). }
false;public;0;4;;@Test(timeout = 60000) public void testKeyValueSupport() throws Exception {     runKeyValueTest(). }
false;public;0;4;;// --- canceling / failures --- @Test(timeout = 60000) public void testCancelingEmptyTopic() throws Exception {     runCancelingOnEmptyInputTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testCancelingFullTopic() throws Exception {     runCancelingOnFullInputTest(). }
false;public;0;24;;@Test(timeout = 60000) public void testInvalidOffset() throws Exception {     final int parallelism = 1.     // write 20 messages into topic:     final String topic = writeSequence("invalidOffsetTopic", 20, parallelism, 1).     // set invalid offset:     try (CuratorFramework curatorClient = ((KafkaTestEnvironmentImpl) kafkaServer).createCuratorClient()) {         ZookeeperOffsetHandler.setOffsetInZooKeeper(curatorClient, standardProps.getProperty("group.id"), topic, 0, 1234).     }     // read from topic     final int valuesCount = 20.     final int startFrom = 0.     final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.getConfig().disableSysoutLogging().     readSequence(env, StartupMode.GROUP_OFFSETS, null, null, standardProps, parallelism, topic, valuesCount, startFrom).     deleteTestTopic(topic). }
false;public;0;4;;// --- source to partition mappings and exactly once --- @Test(timeout = 60000) public void testOneToOneSources() throws Exception {     runOneToOneExactlyOnceTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testOneSourceMultiplePartitions() throws Exception {     runOneSourceMultiplePartitionsExactlyOnceTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testMultipleSourcesOnePartition() throws Exception {     runMultipleSourcesOnePartitionExactlyOnceTest(). }
false;public;0;4;;// --- broker failure --- @Test(timeout = 60000) public void testBrokerFailure() throws Exception {     runBrokerFailureTest(). }
false;public;0;4;;// --- startup mode --- @Test(timeout = 60000) public void testStartFromEarliestOffsets() throws Exception {     runStartFromEarliestOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromLatestOffsets() throws Exception {     runStartFromLatestOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromGroupOffsets() throws Exception {     runStartFromGroupOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromSpecificOffsets() throws Exception {     runStartFromSpecificOffsets(). }
false;public;0;4;;// --- offset committing --- @Test(timeout = 60000) public void testCommitOffsetsToZookeeper() throws Exception {     runCommitOffsetsToKafka(). }
false;public;0;4;;@Test(timeout = 60000) public void testAutoOffsetRetrievalAndCommitToZookeeper() throws Exception {     runAutoOffsetRetrievalAndCommitToKafka(). }
false;public;0;24;;@Test public void runOffsetManipulationInZooKeeperTest() {     try {         final String topicName = "ZookeeperOffsetHandlerTest-Topic".         final String groupId = "ZookeeperOffsetHandlerTest-Group".         final Long offset = (long) (Math.random() * Long.MAX_VALUE).         final Long fetchedOffset.         try (CuratorFramework curatorFramework = ((KafkaTestEnvironmentImpl) kafkaServer).createCuratorClient()) {             kafkaServer.createTestTopic(topicName, 3, 2).             ZookeeperOffsetHandler.setOffsetInZooKeeper(curatorFramework, groupId, topicName, 0, offset).             fetchedOffset = ZookeeperOffsetHandler.getOffsetFromZooKeeper(curatorFramework, groupId, topicName, 0).         }         assertEquals(offset, fetchedOffset).     } catch (Exception e) {         e.printStackTrace().         fail(e.getMessage()).     } }
false;public;0;45;;@Test(timeout = 60000) public void testOffsetAutocommitTest() throws Exception {     final int parallelism = 3.     // write a sequence from 0 to 99 to each of the 3 partitions.     final String topicName = writeSequence("testOffsetAutocommit", 100, parallelism, 1).     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     // NOTE: We are not enabling the checkpointing!     env.getConfig().disableSysoutLogging().     env.getConfig().setRestartStrategy(RestartStrategies.noRestart()).     env.setParallelism(parallelism).     // the readSequence operation sleeps for 20 ms between each record.     // setting a delay of 25*20 = 500 for the commit interval makes     // sure that we commit roughly 3-4 times while reading, however     // at least once.     Properties readProps = new Properties().     readProps.putAll(standardProps).     // make sure that auto commit is enabled in the properties     readProps.setProperty("auto.commit.enable", "true").     readProps.setProperty("auto.commit.interval.ms", "500").     // read so that the offset can be committed to ZK     readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, parallelism, topicName, 100, 0).     final Long o1, o2, o3.     // get the offset     try (CuratorFramework curatorFramework = ((KafkaTestEnvironmentImpl) kafkaServer).createCuratorClient()) {         o1 = ZookeeperOffsetHandler.getOffsetFromZooKeeper(curatorFramework, standardProps.getProperty("group.id"), topicName, 0).         o2 = ZookeeperOffsetHandler.getOffsetFromZooKeeper(curatorFramework, standardProps.getProperty("group.id"), topicName, 1).         o3 = ZookeeperOffsetHandler.getOffsetFromZooKeeper(curatorFramework, standardProps.getProperty("group.id"), topicName, 2).     }     LOG.info("Got final offsets from zookeeper o1={}, o2={}, o3={}", o1, o2, o3).     // ensure that the offset has been committed     boolean atLeastOneOffsetSet = (o1 != null && o1 > 0 && o1 <= 100) || (o2 != null && o2 > 0 && o2 <= 100) || (o3 != null && o3 > 0 && o3 <= 100).     assertTrue("Expecting at least one offset to be set o1=" + o1 + " o2=" + o2 + " o3=" + o3, atLeastOneOffsetSet).     deleteTestTopic(topicName). }
false;public;0;4;;// --- special executions --- @Test(timeout = 60000) public void testBigRecordJob() throws Exception {     runBigRecordTestTopology(). }
false;public;0;4;;@Test(timeout = 60000) public void testMultipleTopics() throws Exception {     runProduceConsumeMultipleTopics(). }
false;public;0;4;;@Test(timeout = 60000) public void testAllDeletes() throws Exception {     runAllDeletesTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testEndOfStream() throws Exception {     runEndOfStreamTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testMetrics() throws Throwable {     runMetricsTest(). }
