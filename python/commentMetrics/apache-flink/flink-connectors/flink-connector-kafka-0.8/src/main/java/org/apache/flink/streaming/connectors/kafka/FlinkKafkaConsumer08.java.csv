# id;timestamp;commentText;codeText;commentWords;codeWords
FlinkKafkaConsumer08 -> @PublicEvolving 	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props);1512405117;Creates a new Kafka streaming source consumer for Kafka 0.8.x. Use this constructor to_subscribe to multiple topics based on a regular expression pattern.__<p>If partition discovery is enabled (by setting a non-negative value for_{@link FlinkKafkaConsumer08#KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS} in the properties), topics_with names matching the pattern will also be subscribed to as they are created on the fly.__@param subscriptionPattern_The regular expression for a pattern of topic names to subscribe to._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;@PublicEvolving_	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(subscriptionPattern, new KeyedDeserializationSchemaWrapper<>(valueDeserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,use,this,constructor,to,subscribe,to,multiple,topics,based,on,a,regular,expression,pattern,p,if,partition,discovery,is,enabled,by,setting,a,non,negative,value,for,link,flink,kafka,consumer08,in,the,properties,topics,with,names,matching,the,pattern,will,also,be,subscribed,to,as,they,are,created,on,the,fly,param,subscription,pattern,the,regular,expression,for,a,pattern,of,topic,names,to,subscribe,to,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,evolving,public,flink,kafka,consumer08,pattern,subscription,pattern,deserialization,schema,t,value,deserializer,properties,props,this,subscription,pattern,new,keyed,deserialization,schema,wrapper,value,deserializer,props
FlinkKafkaConsumer08 -> @PublicEvolving 	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props);1515757409;Creates a new Kafka streaming source consumer for Kafka 0.8.x. Use this constructor to_subscribe to multiple topics based on a regular expression pattern.__<p>If partition discovery is enabled (by setting a non-negative value for_{@link FlinkKafkaConsumer08#KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS} in the properties), topics_with names matching the pattern will also be subscribed to as they are created on the fly.__@param subscriptionPattern_The regular expression for a pattern of topic names to subscribe to._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;@PublicEvolving_	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(subscriptionPattern, new KeyedDeserializationSchemaWrapper<>(valueDeserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,use,this,constructor,to,subscribe,to,multiple,topics,based,on,a,regular,expression,pattern,p,if,partition,discovery,is,enabled,by,setting,a,non,negative,value,for,link,flink,kafka,consumer08,in,the,properties,topics,with,names,matching,the,pattern,will,also,be,subscribed,to,as,they,are,created,on,the,fly,param,subscription,pattern,the,regular,expression,for,a,pattern,of,topic,names,to,subscribe,to,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,evolving,public,flink,kafka,consumer08,pattern,subscription,pattern,deserialization,schema,t,value,deserializer,properties,props,this,subscription,pattern,new,keyed,deserialization,schema,wrapper,value,deserializer,props
FlinkKafkaConsumer08 -> @PublicEvolving 	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props);1517943538;Creates a new Kafka streaming source consumer for Kafka 0.8.x. Use this constructor to_subscribe to multiple topics based on a regular expression pattern.__<p>If partition discovery is enabled (by setting a non-negative value for_{@link FlinkKafkaConsumer08#KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS} in the properties), topics_with names matching the pattern will also be subscribed to as they are created on the fly.__@param subscriptionPattern_The regular expression for a pattern of topic names to subscribe to._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;@PublicEvolving_	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(subscriptionPattern, new KeyedDeserializationSchemaWrapper<>(valueDeserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,use,this,constructor,to,subscribe,to,multiple,topics,based,on,a,regular,expression,pattern,p,if,partition,discovery,is,enabled,by,setting,a,non,negative,value,for,link,flink,kafka,consumer08,in,the,properties,topics,with,names,matching,the,pattern,will,also,be,subscribed,to,as,they,are,created,on,the,fly,param,subscription,pattern,the,regular,expression,for,a,pattern,of,topic,names,to,subscribe,to,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,evolving,public,flink,kafka,consumer08,pattern,subscription,pattern,deserialization,schema,t,value,deserializer,properties,props,this,subscription,pattern,new,keyed,deserialization,schema,wrapper,value,deserializer,props
FlinkKafkaConsumer08 -> @PublicEvolving 	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props);1519973085;Creates a new Kafka streaming source consumer for Kafka 0.8.x. Use this constructor to_subscribe to multiple topics based on a regular expression pattern.__<p>If partition discovery is enabled (by setting a non-negative value for_{@link FlinkKafkaConsumer08#KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS} in the properties), topics_with names matching the pattern will also be subscribed to as they are created on the fly.__@param subscriptionPattern_The regular expression for a pattern of topic names to subscribe to._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;@PublicEvolving_	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(subscriptionPattern, new KeyedDeserializationSchemaWrapper<>(valueDeserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,use,this,constructor,to,subscribe,to,multiple,topics,based,on,a,regular,expression,pattern,p,if,partition,discovery,is,enabled,by,setting,a,non,negative,value,for,link,flink,kafka,consumer08,in,the,properties,topics,with,names,matching,the,pattern,will,also,be,subscribed,to,as,they,are,created,on,the,fly,param,subscription,pattern,the,regular,expression,for,a,pattern,of,topic,names,to,subscribe,to,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,evolving,public,flink,kafka,consumer08,pattern,subscription,pattern,deserialization,schema,t,value,deserializer,properties,props,this,subscription,pattern,new,keyed,deserialization,schema,wrapper,value,deserializer,props
FlinkKafkaConsumer08 -> @PublicEvolving 	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props);1550834396;Creates a new Kafka streaming source consumer for Kafka 0.8.x. Use this constructor to_subscribe to multiple topics based on a regular expression pattern.__<p>If partition discovery is enabled (by setting a non-negative value for_{@link FlinkKafkaConsumer08#KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS} in the properties), topics_with names matching the pattern will also be subscribed to as they are created on the fly.__@param subscriptionPattern_The regular expression for a pattern of topic names to subscribe to._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;@PublicEvolving_	public FlinkKafkaConsumer08(Pattern subscriptionPattern, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(subscriptionPattern, new KafkaDeserializationSchemaWrapper<>(valueDeserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,use,this,constructor,to,subscribe,to,multiple,topics,based,on,a,regular,expression,pattern,p,if,partition,discovery,is,enabled,by,setting,a,non,negative,value,for,link,flink,kafka,consumer08,in,the,properties,topics,with,names,matching,the,pattern,will,also,be,subscribed,to,as,they,are,created,on,the,fly,param,subscription,pattern,the,regular,expression,for,a,pattern,of,topic,names,to,subscribe,to,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,evolving,public,flink,kafka,consumer08,pattern,subscription,pattern,deserialization,schema,t,value,deserializer,properties,props,this,subscription,pattern,new,kafka,deserialization,schema,wrapper,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1480685315;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		super(topics, deserializer)___		checkNotNull(topics, "topics")__		this.kafkaProperties = checkNotNull(props, "props")___		_		validateZooKeeperConfig(props)___		this.invalidOffsetBehavior = getInvalidOffsetBehavior(props)__		this.autoCommitInterval = PropertiesUtil.getLong(props, "auto.commit.interval.ms", 60000)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,super,topics,deserializer,check,not,null,topics,topics,this,kafka,properties,check,not,null,props,props,validate,zoo,keeper,config,props,this,invalid,offset,behavior,get,invalid,offset,behavior,props,this,auto,commit,interval,properties,util,get,long,props,auto,commit,interval,ms,60000
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1485274811;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		super(topics, deserializer)___		checkNotNull(topics, "topics")__		this.kafkaProperties = checkNotNull(props, "props")___		_		validateZooKeeperConfig(props)___		this.invalidOffsetBehavior = getInvalidOffsetBehavior(props)__		this.autoCommitInterval = PropertiesUtil.getLong(props, "auto.commit.interval.ms", 60000)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,super,topics,deserializer,check,not,null,topics,topics,this,kafka,properties,check,not,null,props,props,validate,zoo,keeper,config,props,this,invalid,offset,behavior,get,invalid,offset,behavior,props,this,auto,commit,interval,properties,util,get,long,props,auto,commit,interval,ms,60000
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1487173364;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		super(topics, deserializer)___		checkNotNull(topics, "topics")__		this.kafkaProperties = checkNotNull(props, "props")___		_		validateZooKeeperConfig(props)___		_		validateAutoOffsetResetValue(props)___		this.autoCommitInterval = PropertiesUtil.getLong(props, "auto.commit.interval.ms", 60000)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,super,topics,deserializer,check,not,null,topics,topics,this,kafka,properties,check,not,null,props,props,validate,zoo,keeper,config,props,validate,auto,offset,reset,value,props,this,auto,commit,interval,properties,util,get,long,props,auto,commit,interval,ms,60000
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1488214488;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		super(topics, deserializer)___		checkNotNull(topics, "topics")__		this.kafkaProperties = checkNotNull(props, "props")___		_		validateZooKeeperConfig(props)___		_		validateAutoOffsetResetValue(props)___		this.autoCommitInterval = PropertiesUtil.getLong(props, "auto.commit.interval.ms", 60000)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,super,topics,deserializer,check,not,null,topics,topics,this,kafka,properties,check,not,null,props,props,validate,zoo,keeper,config,props,validate,auto,offset,reset,value,props,this,auto,commit,interval,properties,util,get,long,props,auto,commit,interval,ms,60000
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1489510697;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		super(topics, deserializer)___		checkNotNull(topics, "topics")__		this.kafkaProperties = checkNotNull(props, "props")___		_		validateZooKeeperConfig(props)___		_		validateAutoOffsetResetValue(props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,super,topics,deserializer,check,not,null,topics,topics,this,kafka,properties,check,not,null,props,props,validate,zoo,keeper,config,props,validate,auto,offset,reset,value,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1495923077;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		super(topics, deserializer)___		checkNotNull(topics, "topics")__		this.kafkaProperties = checkNotNull(props, "props")___		_		validateZooKeeperConfig(props)___		_		validateAutoOffsetResetValue(props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,super,topics,deserializer,check,not,null,topics,topics,this,kafka,properties,check,not,null,props,props,validate,zoo,keeper,config,props,validate,auto,offset,reset,value,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1498894422;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		super(_				topics,_				null,_				deserializer,_				getLong(props, KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED))___		this.kafkaProperties = checkNotNull(props, "props")___		_		validateZooKeeperConfig(props)___		_		validateAutoOffsetResetValue(props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,super,topics,null,deserializer,get,long,props,this,kafka,properties,check,not,null,props,props,validate,zoo,keeper,config,props,validate,auto,offset,reset,value,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1500863105;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		super(_				topics,_				null,_				deserializer,_				getLong(props, KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED))___		this.kafkaProperties = checkNotNull(props, "props")___		_		validateZooKeeperConfig(props)___		_		validateAutoOffsetResetValue(props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,super,topics,null,deserializer,get,long,props,this,kafka,properties,check,not,null,props,props,validate,zoo,keeper,config,props,validate,auto,offset,reset,value,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1509723634;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		super(_				topics,_				null,_				deserializer,_				getLong(props, KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED))___		this.kafkaProperties = checkNotNull(props, "props")___		_		validateZooKeeperConfig(props)___		_		validateAutoOffsetResetValue(props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,super,topics,null,deserializer,get,long,props,this,kafka,properties,check,not,null,props,props,validate,zoo,keeper,config,props,validate,auto,offset,reset,value,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1512405117;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(topics, null, deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,this,topics,null,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1515757409;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(topics, null, deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,this,topics,null,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1517943538;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(topics, null, deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,this,topics,null,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props);1519973085;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics and a key/value deserialization schema.__@param topics_The Kafka topics to read from._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(topics, null, deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,and,a,key,value,deserialization,schema,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,keyed,deserialization,schema,t,deserializer,properties,props,this,topics,null,deserializer,props
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1480685315;Validate the ZK configuration, checking for required parameters_@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1485274811;Validate the ZK configuration, checking for required parameters_@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1487173364;Validate the ZK configuration, checking for required parameters_@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1488214488;Validate the ZK configuration, checking for required parameters_@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1489510697;Validate the ZK configuration, checking for required parameters_@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}_		_		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1495923077;Validate the ZK configuration, checking for required parameters._@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1498894422;Validate the ZK configuration, checking for required parameters.__@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1500863105;Validate the ZK configuration, checking for required parameters.__@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1509723634;Validate the ZK configuration, checking for required parameters.__@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1512405117;Validate the ZK configuration, checking for required parameters.__@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1515757409;Validate the ZK configuration, checking for required parameters.__@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1517943538;Validate the ZK configuration, checking for required parameters.__@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1519973085;Validate the ZK configuration, checking for required parameters.__@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> protected static void validateZooKeeperConfig(Properties props);1550834396;Validate the ZK configuration, checking for required parameters.__@param props Properties to check;protected static void validateZooKeeperConfig(Properties props) {_		if (props.getProperty("zookeeper.connect") == null) {_			throw new IllegalArgumentException("Required property 'zookeeper.connect' has not been set in the properties")__		}_		if (props.getProperty(ConsumerConfig.GROUP_ID_CONFIG) == null) {_			throw new IllegalArgumentException("Required property '" + ConsumerConfig.GROUP_ID_CONFIG_					+ "' has not been set in the properties")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.session.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.session.timeout.ms' is not a valid integer")__		}__		try {_			_			Integer.parseInt(props.getProperty("zookeeper.connection.timeout.ms", "0"))__		}_		catch (NumberFormatException e) {_			throw new IllegalArgumentException("Property 'zookeeper.connection.timeout.ms' is not a valid integer")__		}_	};validate,the,zk,configuration,checking,for,required,parameters,param,props,properties,to,check;protected,static,void,validate,zoo,keeper,config,properties,props,if,props,get,property,zookeeper,connect,null,throw,new,illegal,argument,exception,required,property,zookeeper,connect,has,not,been,set,in,the,properties,if,props,get,property,consumer,config,null,throw,new,illegal,argument,exception,required,property,consumer,config,has,not,been,set,in,the,properties,try,integer,parse,int,props,get,property,zookeeper,session,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,session,timeout,ms,is,not,a,valid,integer,try,integer,parse,int,props,get,property,zookeeper,connection,timeout,ms,0,catch,number,format,exception,e,throw,new,illegal,argument,exception,property,zookeeper,connection,timeout,ms,is,not,a,valid,integer
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1480685315;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1485274811;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1487173364;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1488214488;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1489510697;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1495923077;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1498894422;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1500863105;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1509723634;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1512405117;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1515757409;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1517943538;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props);1519973085;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param topic_The name of the topic that should be consumed._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(Collections.singletonList(topic), deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,topic,the,name,of,the,topic,that,should,be,consumed,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,keyed,deserialization,schema,t,deserializer,properties,props,this,collections,singleton,list,topic,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1480685315;Creates a new Kafka streaming source consumer for Kafka 0.8.x__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1485274811;Creates a new Kafka streaming source consumer for Kafka 0.8.x__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1487173364;Creates a new Kafka streaming source consumer for Kafka 0.8.x__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1488214488;Creates a new Kafka streaming source consumer for Kafka 0.8.x__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1489510697;Creates a new Kafka streaming source consumer for Kafka 0.8.x__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1495923077;Creates a new Kafka streaming source consumer for Kafka 0.8.x.__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1498894422;Creates a new Kafka streaming source consumer for Kafka 0.8.x.__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1500863105;Creates a new Kafka streaming source consumer for Kafka 0.8.x.__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1509723634;Creates a new Kafka streaming source consumer for Kafka 0.8.x.__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1512405117;Creates a new Kafka streaming source consumer for Kafka 0.8.x.__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1515757409;Creates a new Kafka streaming source consumer for Kafka 0.8.x.__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1517943538;Creates a new Kafka streaming source consumer for Kafka 0.8.x.__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1519973085;Creates a new Kafka streaming source consumer for Kafka 0.8.x.__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props);1550834396;Creates a new Kafka streaming source consumer for Kafka 0.8.x.__@param topic_The name of the topic that should be consumed._@param valueDeserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;public FlinkKafkaConsumer08(String topic, DeserializationSchema<T> valueDeserializer, Properties props) {_		this(Collections.singletonList(topic), valueDeserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,param,topic,the,name,of,the,topic,that,should,be,consumed,param,value,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,flink,kafka,consumer08,string,topic,deserialization,schema,t,value,deserializer,properties,props,this,collections,singleton,list,topic,value,deserializer,props
FlinkKafkaConsumer08 -> private static Node brokerToNode(Broker broker);1480685315;Turn a broker instance into a node instance_@param broker broker instance_@return Node representing the given broker;private static Node brokerToNode(Broker broker) {_		return new Node(broker.id(), broker.host(), broker.port())__	};turn,a,broker,instance,into,a,node,instance,param,broker,broker,instance,return,node,representing,the,given,broker;private,static,node,broker,to,node,broker,broker,return,new,node,broker,id,broker,host,broker,port
FlinkKafkaConsumer08 -> private static Node brokerToNode(Broker broker);1485274811;Turn a broker instance into a node instance_@param broker broker instance_@return Node representing the given broker;private static Node brokerToNode(Broker broker) {_		return new Node(broker.id(), broker.host(), broker.port())__	};turn,a,broker,instance,into,a,node,instance,param,broker,broker,instance,return,node,representing,the,given,broker;private,static,node,broker,to,node,broker,broker,return,new,node,broker,id,broker,host,broker,port
FlinkKafkaConsumer08 -> private static Node brokerToNode(Broker broker);1487173364;Turn a broker instance into a node instance_@param broker broker instance_@return Node representing the given broker;private static Node brokerToNode(Broker broker) {_		return new Node(broker.id(), broker.host(), broker.port())__	};turn,a,broker,instance,into,a,node,instance,param,broker,broker,instance,return,node,representing,the,given,broker;private,static,node,broker,to,node,broker,broker,return,new,node,broker,id,broker,host,broker,port
FlinkKafkaConsumer08 -> private static Node brokerToNode(Broker broker);1488214488;Turn a broker instance into a node instance_@param broker broker instance_@return Node representing the given broker;private static Node brokerToNode(Broker broker) {_		return new Node(broker.id(), broker.host(), broker.port())__	};turn,a,broker,instance,into,a,node,instance,param,broker,broker,instance,return,node,representing,the,given,broker;private,static,node,broker,to,node,broker,broker,return,new,node,broker,id,broker,host,broker,port
FlinkKafkaConsumer08 -> private static Node brokerToNode(Broker broker);1489510697;Turn a broker instance into a node instance_@param broker broker instance_@return Node representing the given broker;private static Node brokerToNode(Broker broker) {_		return new Node(broker.id(), broker.host(), broker.port())__	};turn,a,broker,instance,into,a,node,instance,param,broker,broker,instance,return,node,representing,the,given,broker;private,static,node,broker,to,node,broker,broker,return,new,node,broker,id,broker,host,broker,port
FlinkKafkaConsumer08 -> private static Node brokerToNode(Broker broker);1495923077;Turn a broker instance into a node instance._@param broker broker instance_@return Node representing the given broker;private static Node brokerToNode(Broker broker) {_		return new Node(broker.id(), broker.host(), broker.port())__	};turn,a,broker,instance,into,a,node,instance,param,broker,broker,instance,return,node,representing,the,given,broker;private,static,node,broker,to,node,broker,broker,return,new,node,broker,id,broker,host,broker,port
FlinkKafkaConsumer08 -> @PublicEvolving 	public FlinkKafkaConsumer08(Pattern subscriptionPattern, KeyedDeserializationSchema<T> deserializer, Properties props);1512405117;Creates a new Kafka streaming source consumer for Kafka 0.8.x. Use this constructor to_subscribe to multiple topics based on a regular expression pattern.__<p>If partition discovery is enabled (by setting a non-negative value for_{@link FlinkKafkaConsumer08#KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS} in the properties), topics_with names matching the pattern will also be subscribed to as they are created on the fly.__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param subscriptionPattern_The regular expression for a pattern of topic names to subscribe to._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;@PublicEvolving_	public FlinkKafkaConsumer08(Pattern subscriptionPattern, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(null, subscriptionPattern, deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,use,this,constructor,to,subscribe,to,multiple,topics,based,on,a,regular,expression,pattern,p,if,partition,discovery,is,enabled,by,setting,a,non,negative,value,for,link,flink,kafka,consumer08,in,the,properties,topics,with,names,matching,the,pattern,will,also,be,subscribed,to,as,they,are,created,on,the,fly,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,subscription,pattern,the,regular,expression,for,a,pattern,of,topic,names,to,subscribe,to,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,evolving,public,flink,kafka,consumer08,pattern,subscription,pattern,keyed,deserialization,schema,t,deserializer,properties,props,this,null,subscription,pattern,deserializer,props
FlinkKafkaConsumer08 -> @PublicEvolving 	public FlinkKafkaConsumer08(Pattern subscriptionPattern, KeyedDeserializationSchema<T> deserializer, Properties props);1515757409;Creates a new Kafka streaming source consumer for Kafka 0.8.x. Use this constructor to_subscribe to multiple topics based on a regular expression pattern.__<p>If partition discovery is enabled (by setting a non-negative value for_{@link FlinkKafkaConsumer08#KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS} in the properties), topics_with names matching the pattern will also be subscribed to as they are created on the fly.__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param subscriptionPattern_The regular expression for a pattern of topic names to subscribe to._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;@PublicEvolving_	public FlinkKafkaConsumer08(Pattern subscriptionPattern, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(null, subscriptionPattern, deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,use,this,constructor,to,subscribe,to,multiple,topics,based,on,a,regular,expression,pattern,p,if,partition,discovery,is,enabled,by,setting,a,non,negative,value,for,link,flink,kafka,consumer08,in,the,properties,topics,with,names,matching,the,pattern,will,also,be,subscribed,to,as,they,are,created,on,the,fly,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,subscription,pattern,the,regular,expression,for,a,pattern,of,topic,names,to,subscribe,to,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,evolving,public,flink,kafka,consumer08,pattern,subscription,pattern,keyed,deserialization,schema,t,deserializer,properties,props,this,null,subscription,pattern,deserializer,props
FlinkKafkaConsumer08 -> @PublicEvolving 	public FlinkKafkaConsumer08(Pattern subscriptionPattern, KeyedDeserializationSchema<T> deserializer, Properties props);1517943538;Creates a new Kafka streaming source consumer for Kafka 0.8.x. Use this constructor to_subscribe to multiple topics based on a regular expression pattern.__<p>If partition discovery is enabled (by setting a non-negative value for_{@link FlinkKafkaConsumer08#KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS} in the properties), topics_with names matching the pattern will also be subscribed to as they are created on the fly.__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param subscriptionPattern_The regular expression for a pattern of topic names to subscribe to._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;@PublicEvolving_	public FlinkKafkaConsumer08(Pattern subscriptionPattern, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(null, subscriptionPattern, deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,use,this,constructor,to,subscribe,to,multiple,topics,based,on,a,regular,expression,pattern,p,if,partition,discovery,is,enabled,by,setting,a,non,negative,value,for,link,flink,kafka,consumer08,in,the,properties,topics,with,names,matching,the,pattern,will,also,be,subscribed,to,as,they,are,created,on,the,fly,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,subscription,pattern,the,regular,expression,for,a,pattern,of,topic,names,to,subscribe,to,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,evolving,public,flink,kafka,consumer08,pattern,subscription,pattern,keyed,deserialization,schema,t,deserializer,properties,props,this,null,subscription,pattern,deserializer,props
FlinkKafkaConsumer08 -> @PublicEvolving 	public FlinkKafkaConsumer08(Pattern subscriptionPattern, KeyedDeserializationSchema<T> deserializer, Properties props);1519973085;Creates a new Kafka streaming source consumer for Kafka 0.8.x. Use this constructor to_subscribe to multiple topics based on a regular expression pattern.__<p>If partition discovery is enabled (by setting a non-negative value for_{@link FlinkKafkaConsumer08#KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS} in the properties), topics_with names matching the pattern will also be subscribed to as they are created on the fly.__<p>This constructor allows passing a {@see KeyedDeserializationSchema} for reading key/value_pairs, offsets, and topic names from Kafka.__@param subscriptionPattern_The regular expression for a pattern of topic names to subscribe to._@param deserializer_The keyed de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties used to configure the Kafka consumer client, and the ZooKeeper client.;@PublicEvolving_	public FlinkKafkaConsumer08(Pattern subscriptionPattern, KeyedDeserializationSchema<T> deserializer, Properties props) {_		this(null, subscriptionPattern, deserializer, props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,use,this,constructor,to,subscribe,to,multiple,topics,based,on,a,regular,expression,pattern,p,if,partition,discovery,is,enabled,by,setting,a,non,negative,value,for,link,flink,kafka,consumer08,in,the,properties,topics,with,names,matching,the,pattern,will,also,be,subscribed,to,as,they,are,created,on,the,fly,p,this,constructor,allows,passing,a,see,keyed,deserialization,schema,for,reading,key,value,pairs,offsets,and,topic,names,from,kafka,param,subscription,pattern,the,regular,expression,for,a,pattern,of,topic,names,to,subscribe,to,param,deserializer,the,keyed,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,used,to,configure,the,kafka,consumer,client,and,the,zoo,keeper,client;public,evolving,public,flink,kafka,consumer08,pattern,subscription,pattern,keyed,deserialization,schema,t,deserializer,properties,props,this,null,subscription,pattern,deserializer,props
FlinkKafkaConsumer08 -> private static void validateSeedBrokers(String[] seedBrokers, Exception exception);1480685315;Validate that at least one seed broker is valid in case of a_ClosedChannelException.__@param seedBrokers_array containing the seed brokers e.g. ["host1:port1",_"host2:port2"]_@param exception_instance;private static void validateSeedBrokers(String[] seedBrokers, Exception exception) {_		if (!(exception instanceof ClosedChannelException)) {_			return__		}_		int unknownHosts = 0__		for (String broker : seedBrokers) {_			URL brokerUrl = NetUtils.getCorrectHostnamePort(broker.trim())__			try {_				InetAddress.getByName(brokerUrl.getHost())__			} catch (UnknownHostException e) {_				unknownHosts++__			}_		}_		_		if (unknownHosts == seedBrokers.length) {_			throw new IllegalArgumentException("All the servers provided in: '"_					+ ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG + "' config are invalid. (unknown hosts)")__		}_	};validate,that,at,least,one,seed,broker,is,valid,in,case,of,a,closed,channel,exception,param,seed,brokers,array,containing,the,seed,brokers,e,g,host1,port1,host2,port2,param,exception,instance;private,static,void,validate,seed,brokers,string,seed,brokers,exception,exception,if,exception,instanceof,closed,channel,exception,return,int,unknown,hosts,0,for,string,broker,seed,brokers,url,broker,url,net,utils,get,correct,hostname,port,broker,trim,try,inet,address,get,by,name,broker,url,get,host,catch,unknown,host,exception,e,unknown,hosts,if,unknown,hosts,seed,brokers,length,throw,new,illegal,argument,exception,all,the,servers,provided,in,consumer,config,config,are,invalid,unknown,hosts
FlinkKafkaConsumer08 -> private static void validateSeedBrokers(String[] seedBrokers, Exception exception);1485274811;Validate that at least one seed broker is valid in case of a_ClosedChannelException.__@param seedBrokers_array containing the seed brokers e.g. ["host1:port1",_"host2:port2"]_@param exception_instance;private static void validateSeedBrokers(String[] seedBrokers, Exception exception) {_		if (!(exception instanceof ClosedChannelException)) {_			return__		}_		int unknownHosts = 0__		for (String broker : seedBrokers) {_			URL brokerUrl = NetUtils.getCorrectHostnamePort(broker.trim())__			try {_				InetAddress.getByName(brokerUrl.getHost())__			} catch (UnknownHostException e) {_				unknownHosts++__			}_		}_		_		if (unknownHosts == seedBrokers.length) {_			throw new IllegalArgumentException("All the servers provided in: '"_					+ ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG + "' config are invalid. (unknown hosts)")__		}_	};validate,that,at,least,one,seed,broker,is,valid,in,case,of,a,closed,channel,exception,param,seed,brokers,array,containing,the,seed,brokers,e,g,host1,port1,host2,port2,param,exception,instance;private,static,void,validate,seed,brokers,string,seed,brokers,exception,exception,if,exception,instanceof,closed,channel,exception,return,int,unknown,hosts,0,for,string,broker,seed,brokers,url,broker,url,net,utils,get,correct,hostname,port,broker,trim,try,inet,address,get,by,name,broker,url,get,host,catch,unknown,host,exception,e,unknown,hosts,if,unknown,hosts,seed,brokers,length,throw,new,illegal,argument,exception,all,the,servers,provided,in,consumer,config,config,are,invalid,unknown,hosts
FlinkKafkaConsumer08 -> private static void validateSeedBrokers(String[] seedBrokers, Exception exception);1487173364;Validate that at least one seed broker is valid in case of a_ClosedChannelException.__@param seedBrokers_array containing the seed brokers e.g. ["host1:port1",_"host2:port2"]_@param exception_instance;private static void validateSeedBrokers(String[] seedBrokers, Exception exception) {_		if (!(exception instanceof ClosedChannelException)) {_			return__		}_		int unknownHosts = 0__		for (String broker : seedBrokers) {_			URL brokerUrl = NetUtils.getCorrectHostnamePort(broker.trim())__			try {_				InetAddress.getByName(brokerUrl.getHost())__			} catch (UnknownHostException e) {_				unknownHosts++__			}_		}_		_		if (unknownHosts == seedBrokers.length) {_			throw new IllegalArgumentException("All the servers provided in: '"_					+ ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG + "' config are invalid. (unknown hosts)")__		}_	};validate,that,at,least,one,seed,broker,is,valid,in,case,of,a,closed,channel,exception,param,seed,brokers,array,containing,the,seed,brokers,e,g,host1,port1,host2,port2,param,exception,instance;private,static,void,validate,seed,brokers,string,seed,brokers,exception,exception,if,exception,instanceof,closed,channel,exception,return,int,unknown,hosts,0,for,string,broker,seed,brokers,url,broker,url,net,utils,get,correct,hostname,port,broker,trim,try,inet,address,get,by,name,broker,url,get,host,catch,unknown,host,exception,e,unknown,hosts,if,unknown,hosts,seed,brokers,length,throw,new,illegal,argument,exception,all,the,servers,provided,in,consumer,config,config,are,invalid,unknown,hosts
FlinkKafkaConsumer08 -> private static void validateSeedBrokers(String[] seedBrokers, Exception exception);1488214488;Validate that at least one seed broker is valid in case of a_ClosedChannelException.__@param seedBrokers_array containing the seed brokers e.g. ["host1:port1",_"host2:port2"]_@param exception_instance;private static void validateSeedBrokers(String[] seedBrokers, Exception exception) {_		if (!(exception instanceof ClosedChannelException)) {_			return__		}_		int unknownHosts = 0__		for (String broker : seedBrokers) {_			URL brokerUrl = NetUtils.getCorrectHostnamePort(broker.trim())__			try {_				InetAddress.getByName(brokerUrl.getHost())__			} catch (UnknownHostException e) {_				unknownHosts++__			}_		}_		_		if (unknownHosts == seedBrokers.length) {_			throw new IllegalArgumentException("All the servers provided in: '"_					+ ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG + "' config are invalid. (unknown hosts)")__		}_	};validate,that,at,least,one,seed,broker,is,valid,in,case,of,a,closed,channel,exception,param,seed,brokers,array,containing,the,seed,brokers,e,g,host1,port1,host2,port2,param,exception,instance;private,static,void,validate,seed,brokers,string,seed,brokers,exception,exception,if,exception,instanceof,closed,channel,exception,return,int,unknown,hosts,0,for,string,broker,seed,brokers,url,broker,url,net,utils,get,correct,hostname,port,broker,trim,try,inet,address,get,by,name,broker,url,get,host,catch,unknown,host,exception,e,unknown,hosts,if,unknown,hosts,seed,brokers,length,throw,new,illegal,argument,exception,all,the,servers,provided,in,consumer,config,config,are,invalid,unknown,hosts
FlinkKafkaConsumer08 -> private static void validateSeedBrokers(String[] seedBrokers, Exception exception);1489510697;Validate that at least one seed broker is valid in case of a_ClosedChannelException.__@param seedBrokers_array containing the seed brokers e.g. ["host1:port1",_"host2:port2"]_@param exception_instance;private static void validateSeedBrokers(String[] seedBrokers, Exception exception) {_		if (!(exception instanceof ClosedChannelException)) {_			return__		}_		int unknownHosts = 0__		for (String broker : seedBrokers) {_			URL brokerUrl = NetUtils.getCorrectHostnamePort(broker.trim())__			try {_				InetAddress.getByName(brokerUrl.getHost())__			} catch (UnknownHostException e) {_				unknownHosts++__			}_		}_		_		if (unknownHosts == seedBrokers.length) {_			throw new IllegalArgumentException("All the servers provided in: '"_					+ ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG + "' config are invalid. (unknown hosts)")__		}_	};validate,that,at,least,one,seed,broker,is,valid,in,case,of,a,closed,channel,exception,param,seed,brokers,array,containing,the,seed,brokers,e,g,host1,port1,host2,port2,param,exception,instance;private,static,void,validate,seed,brokers,string,seed,brokers,exception,exception,if,exception,instanceof,closed,channel,exception,return,int,unknown,hosts,0,for,string,broker,seed,brokers,url,broker,url,net,utils,get,correct,hostname,port,broker,trim,try,inet,address,get,by,name,broker,url,get,host,catch,unknown,host,exception,e,unknown,hosts,if,unknown,hosts,seed,brokers,length,throw,new,illegal,argument,exception,all,the,servers,provided,in,consumer,config,config,are,invalid,unknown,hosts
FlinkKafkaConsumer08 -> private static void validateSeedBrokers(String[] seedBrokers, Exception exception);1495923077;Validate that at least one seed broker is valid in case of a_ClosedChannelException.__@param seedBrokers_array containing the seed brokers e.g. ["host1:port1",_"host2:port2"]_@param exception_instance;private static void validateSeedBrokers(String[] seedBrokers, Exception exception) {_		if (!(exception instanceof ClosedChannelException)) {_			return__		}_		int unknownHosts = 0__		for (String broker : seedBrokers) {_			URL brokerUrl = NetUtils.getCorrectHostnamePort(broker.trim())__			try {_				InetAddress.getByName(brokerUrl.getHost())__			} catch (UnknownHostException e) {_				unknownHosts++__			}_		}_		_		if (unknownHosts == seedBrokers.length) {_			throw new IllegalArgumentException("All the servers provided in: '"_					+ ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG + "' config are invalid. (unknown hosts)")__		}_	};validate,that,at,least,one,seed,broker,is,valid,in,case,of,a,closed,channel,exception,param,seed,brokers,array,containing,the,seed,brokers,e,g,host1,port1,host2,port2,param,exception,instance;private,static,void,validate,seed,brokers,string,seed,brokers,exception,exception,if,exception,instanceof,closed,channel,exception,return,int,unknown,hosts,0,for,string,broker,seed,brokers,url,broker,url,net,utils,get,correct,hostname,port,broker,trim,try,inet,address,get,by,name,broker,url,get,host,catch,unknown,host,exception,e,unknown,hosts,if,unknown,hosts,seed,brokers,length,throw,new,illegal,argument,exception,all,the,servers,provided,in,consumer,config,config,are,invalid,unknown,hosts
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1480685315;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1485274811;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1487173364;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1488214488;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1489510697;Creates a new Kafka streaming source consumer for Kafka 0.8.x__This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1495923077;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1498894422;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1500863105;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1509723634;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1512405117;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1515757409;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1517943538;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1519973085;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KeyedDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,keyed,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props);1550834396;Creates a new Kafka streaming source consumer for Kafka 0.8.x__<p>This constructor allows passing multiple topics to the consumer.__@param topics_The Kafka topics to read from._@param deserializer_The de-/serializer used to convert between Kafka's byte messages and Flink's objects._@param props_The properties that are used to configure both the fetcher and the offset handler.;public FlinkKafkaConsumer08(List<String> topics, DeserializationSchema<T> deserializer, Properties props) {_		this(topics, new KafkaDeserializationSchemaWrapper<>(deserializer), props)__	};creates,a,new,kafka,streaming,source,consumer,for,kafka,0,8,x,p,this,constructor,allows,passing,multiple,topics,to,the,consumer,param,topics,the,kafka,topics,to,read,from,param,deserializer,the,de,serializer,used,to,convert,between,kafka,s,byte,messages,and,flink,s,objects,param,props,the,properties,that,are,used,to,configure,both,the,fetcher,and,the,offset,handler;public,flink,kafka,consumer08,list,string,topics,deserialization,schema,t,deserializer,properties,props,this,topics,new,kafka,deserialization,schema,wrapper,deserializer,props
FlinkKafkaConsumer08 -> public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties);1480685315;Send request to Kafka to get partitions for topic.__@param topics The name of the topics._@param properties The properties for the Kafka Consumer that is used to query the partitions for the topic.;public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties) {_		String seedBrokersConfString = properties.getProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		final int numRetries = getInt(properties, GET_PARTITIONS_RETRIES_KEY, DEFAULT_GET_PARTITIONS_RETRIES)__		_		checkNotNull(seedBrokersConfString, "Configuration property %s not set", ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		String[] seedBrokers = seedBrokersConfString.split(",")__		List<KafkaTopicPartitionLeader> partitions = new ArrayList<>()___		final String clientId = "flink-kafka-consumer-partition-lookup"__		final int soTimeout = getInt(properties, "socket.timeout.ms", 30000)__		final int bufferSize = getInt(properties, "socket.receive.buffer.bytes", 65536)___		Random rnd = new Random()__		retryLoop: for (int retry = 0_ retry < numRetries_ retry++) {_			_			_			int index = rnd.nextInt(seedBrokers.length)__			brokersLoop: for (int arrIdx = 0_ arrIdx < seedBrokers.length_ arrIdx++) {_				String seedBroker = seedBrokers[index]__				LOG.info("Trying to get topic metadata from broker {} in try {}/{}", seedBroker, retry, numRetries)__				if (++index == seedBrokers.length) {_					index = 0__				}__				URL brokerUrl = NetUtils.getCorrectHostnamePort(seedBroker)__				SimpleConsumer consumer = null__				try {_					consumer = new SimpleConsumer(brokerUrl.getHost(), brokerUrl.getPort(), soTimeout, bufferSize, clientId)___					TopicMetadataRequest req = new TopicMetadataRequest(topics)__					kafka.javaapi.TopicMetadataResponse resp = consumer.send(req)___					List<TopicMetadata> metaData = resp.topicsMetadata()___					_					partitions.clear()__					for (TopicMetadata item : metaData) {_						if (item.errorCode() != ErrorMapping.NoError()) {_							_							LOG.warn("Error while getting metadata from broker " + seedBroker + " to find partitions " +_									"for " + topics.toString() + ". Error: " + ErrorMapping.exceptionFor(item.errorCode()).getMessage())__							continue brokersLoop__						}_						if (!topics.contains(item.topic())) {_							LOG.warn("Received metadata from topic " + item.topic() + " even though it was not requested. Skipping ...")__							continue brokersLoop__						}_						for (PartitionMetadata part : item.partitionsMetadata()) {_							Node leader = brokerToNode(part.leader())__							KafkaTopicPartition ktp = new KafkaTopicPartition(item.topic(), part.partitionId())__							KafkaTopicPartitionLeader pInfo = new KafkaTopicPartitionLeader(ktp, leader)__							partitions.add(pInfo)__						}_					}_					break retryLoop_ _				} catch (Exception e) {_					_					validateSeedBrokers(seedBrokers, e)__					LOG.warn("Error communicating with broker " + seedBroker + " to find partitions for " + topics.toString() + "." +_							"" + e.getClass() + ". Message: " + e.getMessage())__					LOG.debug("Detailed trace", e)__					_					try {_						Thread.sleep(500)__					} catch (InterruptedException e1) {_						_					}_				} finally {_					if (consumer != null) {_						consumer.close()__					}_				}_			} _		} _		return partitions__	};send,request,to,kafka,to,get,partitions,for,topic,param,topics,the,name,of,the,topics,param,properties,the,properties,for,the,kafka,consumer,that,is,used,to,query,the,partitions,for,the,topic;public,static,list,kafka,topic,partition,leader,get,partitions,for,topic,list,string,topics,properties,properties,string,seed,brokers,conf,string,properties,get,property,consumer,config,final,int,num,retries,get,int,properties,check,not,null,seed,brokers,conf,string,configuration,property,s,not,set,consumer,config,string,seed,brokers,seed,brokers,conf,string,split,list,kafka,topic,partition,leader,partitions,new,array,list,final,string,client,id,flink,kafka,consumer,partition,lookup,final,int,so,timeout,get,int,properties,socket,timeout,ms,30000,final,int,buffer,size,get,int,properties,socket,receive,buffer,bytes,65536,random,rnd,new,random,retry,loop,for,int,retry,0,retry,num,retries,retry,int,index,rnd,next,int,seed,brokers,length,brokers,loop,for,int,arr,idx,0,arr,idx,seed,brokers,length,arr,idx,string,seed,broker,seed,brokers,index,log,info,trying,to,get,topic,metadata,from,broker,in,try,seed,broker,retry,num,retries,if,index,seed,brokers,length,index,0,url,broker,url,net,utils,get,correct,hostname,port,seed,broker,simple,consumer,consumer,null,try,consumer,new,simple,consumer,broker,url,get,host,broker,url,get,port,so,timeout,buffer,size,client,id,topic,metadata,request,req,new,topic,metadata,request,topics,kafka,javaapi,topic,metadata,response,resp,consumer,send,req,list,topic,metadata,meta,data,resp,topics,metadata,partitions,clear,for,topic,metadata,item,meta,data,if,item,error,code,error,mapping,no,error,log,warn,error,while,getting,metadata,from,broker,seed,broker,to,find,partitions,for,topics,to,string,error,error,mapping,exception,for,item,error,code,get,message,continue,brokers,loop,if,topics,contains,item,topic,log,warn,received,metadata,from,topic,item,topic,even,though,it,was,not,requested,skipping,continue,brokers,loop,for,partition,metadata,part,item,partitions,metadata,node,leader,broker,to,node,part,leader,kafka,topic,partition,ktp,new,kafka,topic,partition,item,topic,part,partition,id,kafka,topic,partition,leader,p,info,new,kafka,topic,partition,leader,ktp,leader,partitions,add,p,info,break,retry,loop,catch,exception,e,validate,seed,brokers,seed,brokers,e,log,warn,error,communicating,with,broker,seed,broker,to,find,partitions,for,topics,to,string,e,get,class,message,e,get,message,log,debug,detailed,trace,e,try,thread,sleep,500,catch,interrupted,exception,e1,finally,if,consumer,null,consumer,close,return,partitions
FlinkKafkaConsumer08 -> public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties);1485274811;Send request to Kafka to get partitions for topic.__@param topics The name of the topics._@param properties The properties for the Kafka Consumer that is used to query the partitions for the topic.;public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties) {_		String seedBrokersConfString = properties.getProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		final int numRetries = getInt(properties, GET_PARTITIONS_RETRIES_KEY, DEFAULT_GET_PARTITIONS_RETRIES)__		_		checkNotNull(seedBrokersConfString, "Configuration property %s not set", ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		String[] seedBrokers = seedBrokersConfString.split(",")__		List<KafkaTopicPartitionLeader> partitions = new ArrayList<>()___		final String clientId = "flink-kafka-consumer-partition-lookup"__		final int soTimeout = getInt(properties, "socket.timeout.ms", 30000)__		final int bufferSize = getInt(properties, "socket.receive.buffer.bytes", 65536)___		Random rnd = new Random()__		retryLoop: for (int retry = 0_ retry < numRetries_ retry++) {_			_			_			int index = rnd.nextInt(seedBrokers.length)__			brokersLoop: for (int arrIdx = 0_ arrIdx < seedBrokers.length_ arrIdx++) {_				String seedBroker = seedBrokers[index]__				LOG.info("Trying to get topic metadata from broker {} in try {}/{}", seedBroker, retry, numRetries)__				if (++index == seedBrokers.length) {_					index = 0__				}__				URL brokerUrl = NetUtils.getCorrectHostnamePort(seedBroker)__				SimpleConsumer consumer = null__				try {_					consumer = new SimpleConsumer(brokerUrl.getHost(), brokerUrl.getPort(), soTimeout, bufferSize, clientId)___					TopicMetadataRequest req = new TopicMetadataRequest(topics)__					kafka.javaapi.TopicMetadataResponse resp = consumer.send(req)___					List<TopicMetadata> metaData = resp.topicsMetadata()___					_					partitions.clear()__					for (TopicMetadata item : metaData) {_						if (item.errorCode() != ErrorMapping.NoError()) {_							_							LOG.warn("Error while getting metadata from broker " + seedBroker + " to find partitions " +_									"for " + topics.toString() + ". Error: " + ErrorMapping.exceptionFor(item.errorCode()).getMessage())__							continue brokersLoop__						}_						if (!topics.contains(item.topic())) {_							LOG.warn("Received metadata from topic " + item.topic() + " even though it was not requested. Skipping ...")__							continue brokersLoop__						}_						for (PartitionMetadata part : item.partitionsMetadata()) {_							Node leader = brokerToNode(part.leader())__							KafkaTopicPartition ktp = new KafkaTopicPartition(item.topic(), part.partitionId())__							KafkaTopicPartitionLeader pInfo = new KafkaTopicPartitionLeader(ktp, leader)__							partitions.add(pInfo)__						}_					}_					break retryLoop_ _				}_				catch (Exception e) {_					_					validateSeedBrokers(seedBrokers, e)__					LOG.warn("Error communicating with broker {} to find partitions for {}. {} Message: {}",_							seedBroker, topics, e.getClass().getName(), e.getMessage())__					LOG.debug("Detailed trace", e)__					_					try {_						Thread.sleep(500)__					} catch (InterruptedException e1) {_						_					}_				} finally {_					if (consumer != null) {_						consumer.close()__					}_				}_			} _		} _		return partitions__	};send,request,to,kafka,to,get,partitions,for,topic,param,topics,the,name,of,the,topics,param,properties,the,properties,for,the,kafka,consumer,that,is,used,to,query,the,partitions,for,the,topic;public,static,list,kafka,topic,partition,leader,get,partitions,for,topic,list,string,topics,properties,properties,string,seed,brokers,conf,string,properties,get,property,consumer,config,final,int,num,retries,get,int,properties,check,not,null,seed,brokers,conf,string,configuration,property,s,not,set,consumer,config,string,seed,brokers,seed,brokers,conf,string,split,list,kafka,topic,partition,leader,partitions,new,array,list,final,string,client,id,flink,kafka,consumer,partition,lookup,final,int,so,timeout,get,int,properties,socket,timeout,ms,30000,final,int,buffer,size,get,int,properties,socket,receive,buffer,bytes,65536,random,rnd,new,random,retry,loop,for,int,retry,0,retry,num,retries,retry,int,index,rnd,next,int,seed,brokers,length,brokers,loop,for,int,arr,idx,0,arr,idx,seed,brokers,length,arr,idx,string,seed,broker,seed,brokers,index,log,info,trying,to,get,topic,metadata,from,broker,in,try,seed,broker,retry,num,retries,if,index,seed,brokers,length,index,0,url,broker,url,net,utils,get,correct,hostname,port,seed,broker,simple,consumer,consumer,null,try,consumer,new,simple,consumer,broker,url,get,host,broker,url,get,port,so,timeout,buffer,size,client,id,topic,metadata,request,req,new,topic,metadata,request,topics,kafka,javaapi,topic,metadata,response,resp,consumer,send,req,list,topic,metadata,meta,data,resp,topics,metadata,partitions,clear,for,topic,metadata,item,meta,data,if,item,error,code,error,mapping,no,error,log,warn,error,while,getting,metadata,from,broker,seed,broker,to,find,partitions,for,topics,to,string,error,error,mapping,exception,for,item,error,code,get,message,continue,brokers,loop,if,topics,contains,item,topic,log,warn,received,metadata,from,topic,item,topic,even,though,it,was,not,requested,skipping,continue,brokers,loop,for,partition,metadata,part,item,partitions,metadata,node,leader,broker,to,node,part,leader,kafka,topic,partition,ktp,new,kafka,topic,partition,item,topic,part,partition,id,kafka,topic,partition,leader,p,info,new,kafka,topic,partition,leader,ktp,leader,partitions,add,p,info,break,retry,loop,catch,exception,e,validate,seed,brokers,seed,brokers,e,log,warn,error,communicating,with,broker,to,find,partitions,for,message,seed,broker,topics,e,get,class,get,name,e,get,message,log,debug,detailed,trace,e,try,thread,sleep,500,catch,interrupted,exception,e1,finally,if,consumer,null,consumer,close,return,partitions
FlinkKafkaConsumer08 -> public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties);1487173364;Send request to Kafka to get partitions for topic.__@param topics The name of the topics._@param properties The properties for the Kafka Consumer that is used to query the partitions for the topic.;public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties) {_		String seedBrokersConfString = properties.getProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		final int numRetries = getInt(properties, GET_PARTITIONS_RETRIES_KEY, DEFAULT_GET_PARTITIONS_RETRIES)__		_		checkNotNull(seedBrokersConfString, "Configuration property %s not set", ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		String[] seedBrokers = seedBrokersConfString.split(",")__		List<KafkaTopicPartitionLeader> partitions = new ArrayList<>()___		final String clientId = "flink-kafka-consumer-partition-lookup"__		final int soTimeout = getInt(properties, "socket.timeout.ms", 30000)__		final int bufferSize = getInt(properties, "socket.receive.buffer.bytes", 65536)___		Random rnd = new Random()__		retryLoop: for (int retry = 0_ retry < numRetries_ retry++) {_			_			_			int index = rnd.nextInt(seedBrokers.length)__			brokersLoop: for (int arrIdx = 0_ arrIdx < seedBrokers.length_ arrIdx++) {_				String seedBroker = seedBrokers[index]__				LOG.info("Trying to get topic metadata from broker {} in try {}/{}", seedBroker, retry, numRetries)__				if (++index == seedBrokers.length) {_					index = 0__				}__				URL brokerUrl = NetUtils.getCorrectHostnamePort(seedBroker)__				SimpleConsumer consumer = null__				try {_					consumer = new SimpleConsumer(brokerUrl.getHost(), brokerUrl.getPort(), soTimeout, bufferSize, clientId)___					TopicMetadataRequest req = new TopicMetadataRequest(topics)__					kafka.javaapi.TopicMetadataResponse resp = consumer.send(req)___					List<TopicMetadata> metaData = resp.topicsMetadata()___					_					partitions.clear()__					for (TopicMetadata item : metaData) {_						if (item.errorCode() != ErrorMapping.NoError()) {_							_							LOG.warn("Error while getting metadata from broker " + seedBroker + " to find partitions " +_									"for " + topics.toString() + ". Error: " + ErrorMapping.exceptionFor(item.errorCode()).getMessage())__							continue brokersLoop__						}_						if (!topics.contains(item.topic())) {_							LOG.warn("Received metadata from topic " + item.topic() + " even though it was not requested. Skipping ...")__							continue brokersLoop__						}_						for (PartitionMetadata part : item.partitionsMetadata()) {_							Node leader = brokerToNode(part.leader())__							KafkaTopicPartition ktp = new KafkaTopicPartition(item.topic(), part.partitionId())__							KafkaTopicPartitionLeader pInfo = new KafkaTopicPartitionLeader(ktp, leader)__							partitions.add(pInfo)__						}_					}_					break retryLoop_ _				}_				catch (Exception e) {_					_					validateSeedBrokers(seedBrokers, e)__					LOG.warn("Error communicating with broker {} to find partitions for {}. {} Message: {}",_							seedBroker, topics, e.getClass().getName(), e.getMessage())__					LOG.debug("Detailed trace", e)__					_					try {_						Thread.sleep(500)__					} catch (InterruptedException e1) {_						_					}_				} finally {_					if (consumer != null) {_						consumer.close()__					}_				}_			} _		} _		return partitions__	};send,request,to,kafka,to,get,partitions,for,topic,param,topics,the,name,of,the,topics,param,properties,the,properties,for,the,kafka,consumer,that,is,used,to,query,the,partitions,for,the,topic;public,static,list,kafka,topic,partition,leader,get,partitions,for,topic,list,string,topics,properties,properties,string,seed,brokers,conf,string,properties,get,property,consumer,config,final,int,num,retries,get,int,properties,check,not,null,seed,brokers,conf,string,configuration,property,s,not,set,consumer,config,string,seed,brokers,seed,brokers,conf,string,split,list,kafka,topic,partition,leader,partitions,new,array,list,final,string,client,id,flink,kafka,consumer,partition,lookup,final,int,so,timeout,get,int,properties,socket,timeout,ms,30000,final,int,buffer,size,get,int,properties,socket,receive,buffer,bytes,65536,random,rnd,new,random,retry,loop,for,int,retry,0,retry,num,retries,retry,int,index,rnd,next,int,seed,brokers,length,brokers,loop,for,int,arr,idx,0,arr,idx,seed,brokers,length,arr,idx,string,seed,broker,seed,brokers,index,log,info,trying,to,get,topic,metadata,from,broker,in,try,seed,broker,retry,num,retries,if,index,seed,brokers,length,index,0,url,broker,url,net,utils,get,correct,hostname,port,seed,broker,simple,consumer,consumer,null,try,consumer,new,simple,consumer,broker,url,get,host,broker,url,get,port,so,timeout,buffer,size,client,id,topic,metadata,request,req,new,topic,metadata,request,topics,kafka,javaapi,topic,metadata,response,resp,consumer,send,req,list,topic,metadata,meta,data,resp,topics,metadata,partitions,clear,for,topic,metadata,item,meta,data,if,item,error,code,error,mapping,no,error,log,warn,error,while,getting,metadata,from,broker,seed,broker,to,find,partitions,for,topics,to,string,error,error,mapping,exception,for,item,error,code,get,message,continue,brokers,loop,if,topics,contains,item,topic,log,warn,received,metadata,from,topic,item,topic,even,though,it,was,not,requested,skipping,continue,brokers,loop,for,partition,metadata,part,item,partitions,metadata,node,leader,broker,to,node,part,leader,kafka,topic,partition,ktp,new,kafka,topic,partition,item,topic,part,partition,id,kafka,topic,partition,leader,p,info,new,kafka,topic,partition,leader,ktp,leader,partitions,add,p,info,break,retry,loop,catch,exception,e,validate,seed,brokers,seed,brokers,e,log,warn,error,communicating,with,broker,to,find,partitions,for,message,seed,broker,topics,e,get,class,get,name,e,get,message,log,debug,detailed,trace,e,try,thread,sleep,500,catch,interrupted,exception,e1,finally,if,consumer,null,consumer,close,return,partitions
FlinkKafkaConsumer08 -> public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties);1488214488;Send request to Kafka to get partitions for topic.__@param topics The name of the topics._@param properties The properties for the Kafka Consumer that is used to query the partitions for the topic.;public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties) {_		String seedBrokersConfString = properties.getProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		final int numRetries = getInt(properties, GET_PARTITIONS_RETRIES_KEY, DEFAULT_GET_PARTITIONS_RETRIES)__		_		checkNotNull(seedBrokersConfString, "Configuration property %s not set", ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		String[] seedBrokers = seedBrokersConfString.split(",")__		List<KafkaTopicPartitionLeader> partitions = new ArrayList<>()___		final String clientId = "flink-kafka-consumer-partition-lookup"__		final int soTimeout = getInt(properties, "socket.timeout.ms", 30000)__		final int bufferSize = getInt(properties, "socket.receive.buffer.bytes", 65536)___		Random rnd = new Random()__		retryLoop: for (int retry = 0_ retry < numRetries_ retry++) {_			_			_			int index = rnd.nextInt(seedBrokers.length)__			brokersLoop: for (int arrIdx = 0_ arrIdx < seedBrokers.length_ arrIdx++) {_				String seedBroker = seedBrokers[index]__				LOG.info("Trying to get topic metadata from broker {} in try {}/{}", seedBroker, retry, numRetries)__				if (++index == seedBrokers.length) {_					index = 0__				}__				URL brokerUrl = NetUtils.getCorrectHostnamePort(seedBroker)__				SimpleConsumer consumer = null__				try {_					consumer = new SimpleConsumer(brokerUrl.getHost(), brokerUrl.getPort(), soTimeout, bufferSize, clientId)___					TopicMetadataRequest req = new TopicMetadataRequest(topics)__					kafka.javaapi.TopicMetadataResponse resp = consumer.send(req)___					List<TopicMetadata> metaData = resp.topicsMetadata()___					_					partitions.clear()__					for (TopicMetadata item : metaData) {_						if (item.errorCode() != ErrorMapping.NoError()) {_							_							LOG.warn("Error while getting metadata from broker " + seedBroker + " to find partitions " +_									"for " + topics.toString() + ". Error: " + ErrorMapping.exceptionFor(item.errorCode()).getMessage())__							continue brokersLoop__						}_						if (!topics.contains(item.topic())) {_							LOG.warn("Received metadata from topic " + item.topic() + " even though it was not requested. Skipping ...")__							continue brokersLoop__						}_						for (PartitionMetadata part : item.partitionsMetadata()) {_							Node leader = brokerToNode(part.leader())__							KafkaTopicPartition ktp = new KafkaTopicPartition(item.topic(), part.partitionId())__							KafkaTopicPartitionLeader pInfo = new KafkaTopicPartitionLeader(ktp, leader)__							partitions.add(pInfo)__						}_					}_					break retryLoop_ _				}_				catch (Exception e) {_					_					validateSeedBrokers(seedBrokers, e)__					LOG.warn("Error communicating with broker {} to find partitions for {}. {} Message: {}",_							seedBroker, topics, e.getClass().getName(), e.getMessage())__					LOG.debug("Detailed trace", e)__					_					try {_						Thread.sleep(500)__					} catch (InterruptedException e1) {_						_					}_				} finally {_					if (consumer != null) {_						consumer.close()__					}_				}_			} _		} _		return partitions__	};send,request,to,kafka,to,get,partitions,for,topic,param,topics,the,name,of,the,topics,param,properties,the,properties,for,the,kafka,consumer,that,is,used,to,query,the,partitions,for,the,topic;public,static,list,kafka,topic,partition,leader,get,partitions,for,topic,list,string,topics,properties,properties,string,seed,brokers,conf,string,properties,get,property,consumer,config,final,int,num,retries,get,int,properties,check,not,null,seed,brokers,conf,string,configuration,property,s,not,set,consumer,config,string,seed,brokers,seed,brokers,conf,string,split,list,kafka,topic,partition,leader,partitions,new,array,list,final,string,client,id,flink,kafka,consumer,partition,lookup,final,int,so,timeout,get,int,properties,socket,timeout,ms,30000,final,int,buffer,size,get,int,properties,socket,receive,buffer,bytes,65536,random,rnd,new,random,retry,loop,for,int,retry,0,retry,num,retries,retry,int,index,rnd,next,int,seed,brokers,length,brokers,loop,for,int,arr,idx,0,arr,idx,seed,brokers,length,arr,idx,string,seed,broker,seed,brokers,index,log,info,trying,to,get,topic,metadata,from,broker,in,try,seed,broker,retry,num,retries,if,index,seed,brokers,length,index,0,url,broker,url,net,utils,get,correct,hostname,port,seed,broker,simple,consumer,consumer,null,try,consumer,new,simple,consumer,broker,url,get,host,broker,url,get,port,so,timeout,buffer,size,client,id,topic,metadata,request,req,new,topic,metadata,request,topics,kafka,javaapi,topic,metadata,response,resp,consumer,send,req,list,topic,metadata,meta,data,resp,topics,metadata,partitions,clear,for,topic,metadata,item,meta,data,if,item,error,code,error,mapping,no,error,log,warn,error,while,getting,metadata,from,broker,seed,broker,to,find,partitions,for,topics,to,string,error,error,mapping,exception,for,item,error,code,get,message,continue,brokers,loop,if,topics,contains,item,topic,log,warn,received,metadata,from,topic,item,topic,even,though,it,was,not,requested,skipping,continue,brokers,loop,for,partition,metadata,part,item,partitions,metadata,node,leader,broker,to,node,part,leader,kafka,topic,partition,ktp,new,kafka,topic,partition,item,topic,part,partition,id,kafka,topic,partition,leader,p,info,new,kafka,topic,partition,leader,ktp,leader,partitions,add,p,info,break,retry,loop,catch,exception,e,validate,seed,brokers,seed,brokers,e,log,warn,error,communicating,with,broker,to,find,partitions,for,message,seed,broker,topics,e,get,class,get,name,e,get,message,log,debug,detailed,trace,e,try,thread,sleep,500,catch,interrupted,exception,e1,finally,if,consumer,null,consumer,close,return,partitions
FlinkKafkaConsumer08 -> public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties);1489510697;Send request to Kafka to get partitions for topic.__@param topics The name of the topics._@param properties The properties for the Kafka Consumer that is used to query the partitions for the topic.;public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties) {_		String seedBrokersConfString = properties.getProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		final int numRetries = getInt(properties, GET_PARTITIONS_RETRIES_KEY, DEFAULT_GET_PARTITIONS_RETRIES)__		_		checkNotNull(seedBrokersConfString, "Configuration property %s not set", ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		String[] seedBrokers = seedBrokersConfString.split(",")__		List<KafkaTopicPartitionLeader> partitions = new ArrayList<>()___		final String clientId = "flink-kafka-consumer-partition-lookup"__		final int soTimeout = getInt(properties, "socket.timeout.ms", 30000)__		final int bufferSize = getInt(properties, "socket.receive.buffer.bytes", 65536)___		Random rnd = new Random()__		retryLoop: for (int retry = 0_ retry < numRetries_ retry++) {_			_			_			int index = rnd.nextInt(seedBrokers.length)__			brokersLoop: for (int arrIdx = 0_ arrIdx < seedBrokers.length_ arrIdx++) {_				String seedBroker = seedBrokers[index]__				LOG.info("Trying to get topic metadata from broker {} in try {}/{}", seedBroker, retry, numRetries)__				if (++index == seedBrokers.length) {_					index = 0__				}__				URL brokerUrl = NetUtils.getCorrectHostnamePort(seedBroker)__				SimpleConsumer consumer = null__				try {_					consumer = new SimpleConsumer(brokerUrl.getHost(), brokerUrl.getPort(), soTimeout, bufferSize, clientId)___					TopicMetadataRequest req = new TopicMetadataRequest(topics)__					kafka.javaapi.TopicMetadataResponse resp = consumer.send(req)___					List<TopicMetadata> metaData = resp.topicsMetadata()___					_					partitions.clear()__					for (TopicMetadata item : metaData) {_						if (item.errorCode() != ErrorMapping.NoError()) {_							_							LOG.warn("Error while getting metadata from broker " + seedBroker + " to find partitions " +_									"for " + topics.toString() + ". Error: " + ErrorMapping.exceptionFor(item.errorCode()).getMessage())__							continue brokersLoop__						}_						if (!topics.contains(item.topic())) {_							LOG.warn("Received metadata from topic " + item.topic() + " even though it was not requested. Skipping ...")__							continue brokersLoop__						}_						for (PartitionMetadata part : item.partitionsMetadata()) {_							Node leader = brokerToNode(part.leader())__							KafkaTopicPartition ktp = new KafkaTopicPartition(item.topic(), part.partitionId())__							KafkaTopicPartitionLeader pInfo = new KafkaTopicPartitionLeader(ktp, leader)__							partitions.add(pInfo)__						}_					}_					break retryLoop_ _				}_				catch (Exception e) {_					_					validateSeedBrokers(seedBrokers, e)__					LOG.warn("Error communicating with broker {} to find partitions for {}. {} Message: {}",_							seedBroker, topics, e.getClass().getName(), e.getMessage())__					LOG.debug("Detailed trace", e)__					_					try {_						Thread.sleep(500)__					} catch (InterruptedException e1) {_						_					}_				} finally {_					if (consumer != null) {_						consumer.close()__					}_				}_			} _		} _		return partitions__	};send,request,to,kafka,to,get,partitions,for,topic,param,topics,the,name,of,the,topics,param,properties,the,properties,for,the,kafka,consumer,that,is,used,to,query,the,partitions,for,the,topic;public,static,list,kafka,topic,partition,leader,get,partitions,for,topic,list,string,topics,properties,properties,string,seed,brokers,conf,string,properties,get,property,consumer,config,final,int,num,retries,get,int,properties,check,not,null,seed,brokers,conf,string,configuration,property,s,not,set,consumer,config,string,seed,brokers,seed,brokers,conf,string,split,list,kafka,topic,partition,leader,partitions,new,array,list,final,string,client,id,flink,kafka,consumer,partition,lookup,final,int,so,timeout,get,int,properties,socket,timeout,ms,30000,final,int,buffer,size,get,int,properties,socket,receive,buffer,bytes,65536,random,rnd,new,random,retry,loop,for,int,retry,0,retry,num,retries,retry,int,index,rnd,next,int,seed,brokers,length,brokers,loop,for,int,arr,idx,0,arr,idx,seed,brokers,length,arr,idx,string,seed,broker,seed,brokers,index,log,info,trying,to,get,topic,metadata,from,broker,in,try,seed,broker,retry,num,retries,if,index,seed,brokers,length,index,0,url,broker,url,net,utils,get,correct,hostname,port,seed,broker,simple,consumer,consumer,null,try,consumer,new,simple,consumer,broker,url,get,host,broker,url,get,port,so,timeout,buffer,size,client,id,topic,metadata,request,req,new,topic,metadata,request,topics,kafka,javaapi,topic,metadata,response,resp,consumer,send,req,list,topic,metadata,meta,data,resp,topics,metadata,partitions,clear,for,topic,metadata,item,meta,data,if,item,error,code,error,mapping,no,error,log,warn,error,while,getting,metadata,from,broker,seed,broker,to,find,partitions,for,topics,to,string,error,error,mapping,exception,for,item,error,code,get,message,continue,brokers,loop,if,topics,contains,item,topic,log,warn,received,metadata,from,topic,item,topic,even,though,it,was,not,requested,skipping,continue,brokers,loop,for,partition,metadata,part,item,partitions,metadata,node,leader,broker,to,node,part,leader,kafka,topic,partition,ktp,new,kafka,topic,partition,item,topic,part,partition,id,kafka,topic,partition,leader,p,info,new,kafka,topic,partition,leader,ktp,leader,partitions,add,p,info,break,retry,loop,catch,exception,e,validate,seed,brokers,seed,brokers,e,log,warn,error,communicating,with,broker,to,find,partitions,for,message,seed,broker,topics,e,get,class,get,name,e,get,message,log,debug,detailed,trace,e,try,thread,sleep,500,catch,interrupted,exception,e1,finally,if,consumer,null,consumer,close,return,partitions
FlinkKafkaConsumer08 -> public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties);1495923077;Send request to Kafka to get partitions for topic.__@param topics The name of the topics._@param properties The properties for the Kafka Consumer that is used to query the partitions for the topic.;public static List<KafkaTopicPartitionLeader> getPartitionsForTopic(List<String> topics, Properties properties) {_		String seedBrokersConfString = properties.getProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		final int numRetries = getInt(properties, GET_PARTITIONS_RETRIES_KEY, DEFAULT_GET_PARTITIONS_RETRIES)___		checkNotNull(seedBrokersConfString, "Configuration property %s not set", ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)__		String[] seedBrokers = seedBrokersConfString.split(",")__		List<KafkaTopicPartitionLeader> partitions = new ArrayList<>()___		final String clientId = "flink-kafka-consumer-partition-lookup"__		final int soTimeout = getInt(properties, "socket.timeout.ms", 30000)__		final int bufferSize = getInt(properties, "socket.receive.buffer.bytes", 65536)___		Random rnd = new Random()__		retryLoop: for (int retry = 0_ retry < numRetries_ retry++) {_			_			_			int index = rnd.nextInt(seedBrokers.length)__			brokersLoop: for (int arrIdx = 0_ arrIdx < seedBrokers.length_ arrIdx++) {_				String seedBroker = seedBrokers[index]__				LOG.info("Trying to get topic metadata from broker {} in try {}/{}", seedBroker, retry, numRetries)__				if (++index == seedBrokers.length) {_					index = 0__				}__				URL brokerUrl = NetUtils.getCorrectHostnamePort(seedBroker)__				SimpleConsumer consumer = null__				try {_					consumer = new SimpleConsumer(brokerUrl.getHost(), brokerUrl.getPort(), soTimeout, bufferSize, clientId)___					TopicMetadataRequest req = new TopicMetadataRequest(topics)__					kafka.javaapi.TopicMetadataResponse resp = consumer.send(req)___					List<TopicMetadata> metaData = resp.topicsMetadata()___					_					partitions.clear()__					for (TopicMetadata item : metaData) {_						if (item.errorCode() != ErrorMapping.NoError()) {_							_							LOG.warn("Error while getting metadata from broker " + seedBroker + " to find partitions " +_									"for " + topics.toString() + ". Error: " + ErrorMapping.exceptionFor(item.errorCode()).getMessage())__							continue brokersLoop__						}_						if (!topics.contains(item.topic())) {_							LOG.warn("Received metadata from topic " + item.topic() + " even though it was not requested. Skipping ...")__							continue brokersLoop__						}_						for (PartitionMetadata part : item.partitionsMetadata()) {_							Node leader = brokerToNode(part.leader())__							KafkaTopicPartition ktp = new KafkaTopicPartition(item.topic(), part.partitionId())__							KafkaTopicPartitionLeader pInfo = new KafkaTopicPartitionLeader(ktp, leader)__							partitions.add(pInfo)__						}_					}_					break retryLoop_ _				}_				catch (Exception e) {_					_					validateSeedBrokers(seedBrokers, e)__					LOG.warn("Error communicating with broker {} to find partitions for {}. {} Message: {}",_							seedBroker, topics, e.getClass().getName(), e.getMessage())__					LOG.debug("Detailed trace", e)__					_					try {_						Thread.sleep(500)__					} catch (InterruptedException e1) {_						_					}_				} finally {_					if (consumer != null) {_						consumer.close()__					}_				}_			} _		} _		return partitions__	};send,request,to,kafka,to,get,partitions,for,topic,param,topics,the,name,of,the,topics,param,properties,the,properties,for,the,kafka,consumer,that,is,used,to,query,the,partitions,for,the,topic;public,static,list,kafka,topic,partition,leader,get,partitions,for,topic,list,string,topics,properties,properties,string,seed,brokers,conf,string,properties,get,property,consumer,config,final,int,num,retries,get,int,properties,check,not,null,seed,brokers,conf,string,configuration,property,s,not,set,consumer,config,string,seed,brokers,seed,brokers,conf,string,split,list,kafka,topic,partition,leader,partitions,new,array,list,final,string,client,id,flink,kafka,consumer,partition,lookup,final,int,so,timeout,get,int,properties,socket,timeout,ms,30000,final,int,buffer,size,get,int,properties,socket,receive,buffer,bytes,65536,random,rnd,new,random,retry,loop,for,int,retry,0,retry,num,retries,retry,int,index,rnd,next,int,seed,brokers,length,brokers,loop,for,int,arr,idx,0,arr,idx,seed,brokers,length,arr,idx,string,seed,broker,seed,brokers,index,log,info,trying,to,get,topic,metadata,from,broker,in,try,seed,broker,retry,num,retries,if,index,seed,brokers,length,index,0,url,broker,url,net,utils,get,correct,hostname,port,seed,broker,simple,consumer,consumer,null,try,consumer,new,simple,consumer,broker,url,get,host,broker,url,get,port,so,timeout,buffer,size,client,id,topic,metadata,request,req,new,topic,metadata,request,topics,kafka,javaapi,topic,metadata,response,resp,consumer,send,req,list,topic,metadata,meta,data,resp,topics,metadata,partitions,clear,for,topic,metadata,item,meta,data,if,item,error,code,error,mapping,no,error,log,warn,error,while,getting,metadata,from,broker,seed,broker,to,find,partitions,for,topics,to,string,error,error,mapping,exception,for,item,error,code,get,message,continue,brokers,loop,if,topics,contains,item,topic,log,warn,received,metadata,from,topic,item,topic,even,though,it,was,not,requested,skipping,continue,brokers,loop,for,partition,metadata,part,item,partitions,metadata,node,leader,broker,to,node,part,leader,kafka,topic,partition,ktp,new,kafka,topic,partition,item,topic,part,partition,id,kafka,topic,partition,leader,p,info,new,kafka,topic,partition,leader,ktp,leader,partitions,add,p,info,break,retry,loop,catch,exception,e,validate,seed,brokers,seed,brokers,e,log,warn,error,communicating,with,broker,to,find,partitions,for,message,seed,broker,topics,e,get,class,get,name,e,get,message,log,debug,detailed,trace,e,try,thread,sleep,500,catch,interrupted,exception,e1,finally,if,consumer,null,consumer,close,return,partitions
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1487173364;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1488214488;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1489510697;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1495923077;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1498894422;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1500863105;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1509723634;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1512405117;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1515757409;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1517943538;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1519973085;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
FlinkKafkaConsumer08 -> private static void validateAutoOffsetResetValue(Properties config);1550834396;Check for invalid "auto.offset.reset" values. Should be called in constructor for eager checking before submitting_the job. Note that 'none' is also considered invalid, as we don't want to deliberately throw an exception_right after a task is started.__@param config kafka consumer properties to check;private static void validateAutoOffsetResetValue(Properties config) {_		final String val = config.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "largest")__		if (!(val.equals("largest") || val.equals("latest") || val.equals("earliest") || val.equals("smallest"))) {_			_			throw new IllegalArgumentException("Cannot use '" + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG_				+ "' value '" + val + "'. Possible values: 'latest', 'largest', 'earliest', or 'smallest'.")__		}_	};check,for,invalid,auto,offset,reset,values,should,be,called,in,constructor,for,eager,checking,before,submitting,the,job,note,that,none,is,also,considered,invalid,as,we,don,t,want,to,deliberately,throw,an,exception,right,after,a,task,is,started,param,config,kafka,consumer,properties,to,check;private,static,void,validate,auto,offset,reset,value,properties,config,final,string,val,config,get,property,consumer,config,largest,if,val,equals,largest,val,equals,latest,val,equals,earliest,val,equals,smallest,throw,new,illegal,argument,exception,cannot,use,consumer,config,value,val,possible,values,latest,largest,earliest,or,smallest
