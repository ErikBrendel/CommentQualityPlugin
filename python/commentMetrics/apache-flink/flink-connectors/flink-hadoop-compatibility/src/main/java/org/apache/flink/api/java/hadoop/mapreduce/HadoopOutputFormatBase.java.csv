commented;modifiers;parameterAmount;loc;comment;code
false;public;0;3;;public org.apache.hadoop.conf.Configuration getConfiguration() {     return this.configuration. }
false;public;1;10;;// -------------------------------------------------------------------------------------------- // OutputFormat // -------------------------------------------------------------------------------------------- @Override public void configure(Configuration parameters) {     // enforce sequential configure() calls     synchronized (CONFIGURE_MUTEX) {         if (this.mapreduceOutputFormat instanceof Configurable) {             ((Configurable) this.mapreduceOutputFormat).setConf(this.configuration).         }     } }
true;public;2;51;/**  * create the temporary output file for hadoop RecordWriter.  * @param taskNumber The number of the parallel instance.  * @param numTasks The number of parallel tasks.  * @throws java.io.IOException  */ ;/**  * create the temporary output file for hadoop RecordWriter.  * @param taskNumber The number of the parallel instance.  * @param numTasks The number of parallel tasks.  * @throws java.io.IOException  */ @Override public void open(int taskNumber, int numTasks) throws IOException {     // enforce sequential open() calls     synchronized (OPEN_MUTEX) {         if (Integer.toString(taskNumber + 1).length() > 6) {             throw new IOException("Task id too large.").         }         this.taskNumber = taskNumber + 1.         // for hadoop 2.2         this.configuration.set("mapreduce.output.basename", "tmp").         TaskAttemptID taskAttemptID = TaskAttemptID.forName("attempt__0000_r_" + String.format("%" + (6 - Integer.toString(taskNumber + 1).length()) + "s", " ").replace(" ", "0") + Integer.toString(taskNumber + 1) + "_0").         this.configuration.set("mapred.task.id", taskAttemptID.toString()).         this.configuration.setInt("mapred.task.partition", taskNumber + 1).         // for hadoop 2.2         this.configuration.set("mapreduce.task.attempt.id", taskAttemptID.toString()).         this.configuration.setInt("mapreduce.task.partition", taskNumber + 1).         try {             this.context = new TaskAttemptContextImpl(this.configuration, taskAttemptID).             this.outputCommitter = this.mapreduceOutputFormat.getOutputCommitter(this.context).             this.outputCommitter.setupJob(new JobContextImpl(this.configuration, new JobID())).         } catch (Exception e) {             throw new RuntimeException(e).         }         this.context.getCredentials().addAll(this.credentials).         Credentials currentUserCreds = getCredentialsFromUGI(UserGroupInformation.getCurrentUser()).         if (currentUserCreds != null) {             this.context.getCredentials().addAll(currentUserCreds).         }         // compatible for hadoop 2.2.0, the temporary output directory is different from hadoop 1.2.1         if (outputCommitter instanceof FileOutputCommitter) {             this.configuration.set("mapreduce.task.output.dir", ((FileOutputCommitter) this.outputCommitter).getWorkPath().toString()).         }         try {             this.recordWriter = this.mapreduceOutputFormat.getRecordWriter(this.context).         } catch (InterruptedException e) {             throw new IOException("Could not create RecordWriter.", e).         }     } }
true;public;0;29;/**  * commit the task by moving the output file out from the temporary directory.  * @throws java.io.IOException  */ ;/**  * commit the task by moving the output file out from the temporary directory.  * @throws java.io.IOException  */ @Override public void close() throws IOException {     // enforce sequential close() calls     synchronized (CLOSE_MUTEX) {         try {             this.recordWriter.close(this.context).         } catch (InterruptedException e) {             throw new IOException("Could not close RecordReader.", e).         }         if (this.outputCommitter.needsTaskCommit(this.context)) {             this.outputCommitter.commitTask(this.context).         }         Path outputPath = new Path(this.configuration.get("mapred.output.dir")).         // rename tmp-file to final name         FileSystem fs = FileSystem.get(outputPath.toUri(), this.configuration).         String taskNumberStr = Integer.toString(this.taskNumber).         String tmpFileTemplate = "tmp-r-00000".         String tmpFile = tmpFileTemplate.substring(0, 11 - taskNumberStr.length()) + taskNumberStr.         if (fs.exists(new Path(outputPath.toString() + "/" + tmpFile))) {             fs.rename(new Path(outputPath.toString() + "/" + tmpFile), new Path(outputPath.toString() + "/" + taskNumberStr)).         }     } }
false;public;1;29;;@Override public void finalizeGlobal(int parallelism) throws IOException {     JobContext jobContext.     TaskAttemptContext taskContext.     try {         TaskAttemptID taskAttemptID = TaskAttemptID.forName("attempt__0000_r_" + String.format("%" + (6 - Integer.toString(1).length()) + "s", " ").replace(" ", "0") + Integer.toString(1) + "_0").         jobContext = new JobContextImpl(this.configuration, new JobID()).         taskContext = new TaskAttemptContextImpl(this.configuration, taskAttemptID).         this.outputCommitter = this.mapreduceOutputFormat.getOutputCommitter(taskContext).     } catch (Exception e) {         throw new RuntimeException(e).     }     jobContext.getCredentials().addAll(this.credentials).     Credentials currentUserCreds = getCredentialsFromUGI(UserGroupInformation.getCurrentUser()).     if (currentUserCreds != null) {         jobContext.getCredentials().addAll(currentUserCreds).     }     // finalize HDFS output format     if (this.outputCommitter != null) {         this.outputCommitter.commitJob(jobContext).     } }
false;private;1;5;;// -------------------------------------------------------------------------------------------- // Custom serialization methods // -------------------------------------------------------------------------------------------- private void writeObject(ObjectOutputStream out) throws IOException {     super.write(out).     out.writeUTF(this.mapreduceOutputFormat.getClass().getName()).     this.configuration.write(out). }
false;private;1;18;;@SuppressWarnings("unchecked") private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {     super.read(in).     String hadoopOutputFormatClassName = in.readUTF().     org.apache.hadoop.conf.Configuration configuration = new org.apache.hadoop.conf.Configuration().     configuration.readFields(in).     if (this.configuration == null) {         this.configuration = configuration.     }     try {         this.mapreduceOutputFormat = (org.apache.hadoop.mapreduce.OutputFormat<K, V>) Class.forName(hadoopOutputFormatClassName, true, Thread.currentThread().getContextClassLoader()).newInstance().     } catch (Exception e) {         throw new RuntimeException("Unable to instantiate the hadoop output format", e).     } }
