commented;modifiers;parameterAmount;loc;comment;code
false;public;0;3;;public JobConf getJobConf() {     return jobConf. }
false;public;1;13;;// -------------------------------------------------------------------------------------------- // InputFormat // -------------------------------------------------------------------------------------------- @Override public void configure(Configuration parameters) {     // enforce sequential configuration() calls     synchronized (CONFIGURE_MUTEX) {         // configure MR InputFormat if necessary         if (this.mapredInputFormat instanceof Configurable) {             ((Configurable) this.mapredInputFormat).setConf(this.jobConf).         } else if (this.mapredInputFormat instanceof JobConfigurable) {             ((JobConfigurable) this.mapredInputFormat).configure(this.jobConf).         }     } }
false;public;1;29;;@Override public BaseStatistics getStatistics(BaseStatistics cachedStats) throws IOException {     // only gather base statistics for FileInputFormats     if (!(mapredInputFormat instanceof FileInputFormat)) {         return null.     }     final FileBaseStatistics cachedFileStats = (cachedStats instanceof FileBaseStatistics) ? (FileBaseStatistics) cachedStats : null.     try {         final org.apache.hadoop.fs.Path[] paths = FileInputFormat.getInputPaths(this.jobConf).         return getFileStats(cachedFileStats, paths, new ArrayList<FileStatus>(1)).     } catch (IOException ioex) {         if (LOG.isWarnEnabled()) {             LOG.warn("Could not determine statistics due to an io error: " + ioex.getMessage()).         }     } catch (Throwable t) {         if (LOG.isErrorEnabled()) {             LOG.error("Unexpected problem while getting the file statistics: " + t.getMessage(), t).         }     }     // no statistics available     return null. }
false;public;1;10;;@Override public HadoopInputSplit[] createInputSplits(int minNumSplits) throws IOException {     org.apache.hadoop.mapred.InputSplit[] splitArray = mapredInputFormat.getSplits(jobConf, minNumSplits).     HadoopInputSplit[] hiSplit = new HadoopInputSplit[splitArray.length].     for (int i = 0. i < splitArray.length. i++) {         hiSplit[i] = new HadoopInputSplit(i, splitArray[i], jobConf).     }     return hiSplit. }
false;public;1;4;;@Override public InputSplitAssigner getInputSplitAssigner(HadoopInputSplit[] inputSplits) {     return new LocatableInputSplitAssigner(inputSplits). }
false;public;1;15;;@Override public void open(HadoopInputSplit split) throws IOException {     // enforce sequential open() calls     synchronized (OPEN_MUTEX) {         this.recordReader = this.mapredInputFormat.getRecordReader(split.getHadoopInputSplit(), jobConf, new HadoopDummyReporter()).         if (this.recordReader instanceof Configurable) {             ((Configurable) this.recordReader).setConf(jobConf).         }         key = this.recordReader.createKey().         value = this.recordReader.createValue().         this.fetched = false.     } }
false;public;0;7;;@Override public boolean reachedEnd() throws IOException {     if (!fetched) {         fetchNext().     }     return !hasNext. }
false;protected;0;4;;protected void fetchNext() throws IOException {     hasNext = this.recordReader.next(key, value).     fetched = true. }
false;public;0;10;;@Override public void close() throws IOException {     if (this.recordReader != null) {         // enforce sequential close() calls         synchronized (CLOSE_MUTEX) {             this.recordReader.close().         }     } }
false;private;3;48;;// -------------------------------------------------------------------------------------------- // Helper methods // -------------------------------------------------------------------------------------------- private FileBaseStatistics getFileStats(FileBaseStatistics cachedStats, org.apache.hadoop.fs.Path[] hadoopFilePaths, ArrayList<FileStatus> files) throws IOException {     long latestModTime = 0L.     // get the file info and check whether the cached statistics are still valid.     for (org.apache.hadoop.fs.Path hadoopPath : hadoopFilePaths) {         final Path filePath = new Path(hadoopPath.toUri()).         final FileSystem fs = FileSystem.get(filePath.toUri()).         final FileStatus file = fs.getFileStatus(filePath).         latestModTime = Math.max(latestModTime, file.getModificationTime()).         // enumerate all files and check their modification time stamp.         if (file.isDir()) {             FileStatus[] fss = fs.listStatus(filePath).             files.ensureCapacity(files.size() + fss.length).             for (FileStatus s : fss) {                 if (!s.isDir()) {                     files.add(s).                     latestModTime = Math.max(s.getModificationTime(), latestModTime).                 }             }         } else {             files.add(file).         }     }     // check whether the cached statistics are still valid, if we have any     if (cachedStats != null && latestModTime <= cachedStats.getLastModificationTime()) {         return cachedStats.     }     // calculate the whole length     long len = 0.     for (FileStatus s : files) {         len += s.getLen().     }     // sanity check     if (len <= 0) {         len = BaseStatistics.SIZE_UNKNOWN.     }     return new FileBaseStatistics(latestModTime, len, BaseStatistics.AVG_RECORD_BYTES_UNKNOWN). }
false;private;1;7;;// -------------------------------------------------------------------------------------------- // Custom serialization methods // -------------------------------------------------------------------------------------------- private void writeObject(ObjectOutputStream out) throws IOException {     super.write(out).     out.writeUTF(mapredInputFormat.getClass().getName()).     out.writeUTF(keyClass.getName()).     out.writeUTF(valueClass.getName()).     jobConf.write(out). }
false;private;1;34;;@SuppressWarnings("unchecked") private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {     super.read(in).     String hadoopInputFormatClassName = in.readUTF().     String keyClassName = in.readUTF().     String valueClassName = in.readUTF().     if (jobConf == null) {         jobConf = new JobConf().     }     jobConf.readFields(in).     try {         this.mapredInputFormat = (org.apache.hadoop.mapred.InputFormat<K, V>) Class.forName(hadoopInputFormatClassName, true, Thread.currentThread().getContextClassLoader()).newInstance().     } catch (Exception e) {         throw new RuntimeException("Unable to instantiate the hadoop input format", e).     }     try {         this.keyClass = (Class<K>) Class.forName(keyClassName, true, Thread.currentThread().getContextClassLoader()).     } catch (Exception e) {         throw new RuntimeException("Unable to find key class.", e).     }     try {         this.valueClass = (Class<V>) Class.forName(valueClassName, true, Thread.currentThread().getContextClassLoader()).     } catch (Exception e) {         throw new RuntimeException("Unable to find value class.", e).     }     ReflectionUtils.setConf(mapredInputFormat, jobConf).     jobConf.getCredentials().addAll(this.credentials).     Credentials currentUserCreds = getCredentialsFromUGI(UserGroupInformation.getCurrentUser()).     if (currentUserCreds != null) {         jobConf.getCredentials().addAll(currentUserCreds).     } }
