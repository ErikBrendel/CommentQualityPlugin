commented;modifiers;parameterAmount;loc;comment;code
false;public;0;9;;// ------------------------------------------------------------------------ // Properties // ------------------------------------------------------------------------ @Override public String[] getHostnames() {     try {         return this.hadoopInputSplit.getLocations().     } catch (IOException e) {         return new String[0].     } }
false;public;0;3;;public org.apache.hadoop.mapred.InputSplit getHadoopInputSplit() {     return hadoopInputSplit. }
false;public;0;3;;public JobConf getJobConf() {     return this.jobConf. }
false;private;1;10;;// ------------------------------------------------------------------------ // Serialization // ------------------------------------------------------------------------ private void writeObject(ObjectOutputStream out) throws IOException {     // serialize the parent fields and the final fields     out.defaultWriteObject().     // the job conf knows how to serialize itself     jobConf.write(out).     // write the input split     hadoopInputSplit.write(out). }
false;private;1;23;;private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {     // read the parent fields and the final fields     in.defaultReadObject().     // the job conf knows how to deserialize itself     jobConf = new JobConf().     jobConf.readFields(in).     try {         hadoopInputSplit = (org.apache.hadoop.mapred.InputSplit) WritableFactories.newInstance(splitType).     } catch (Exception e) {         throw new RuntimeException("Unable to instantiate Hadoop InputSplit", e).     }     if (hadoopInputSplit instanceof Configurable) {         ((Configurable) hadoopInputSplit).setConf(this.jobConf).     } else if (hadoopInputSplit instanceof JobConfigurable) {         ((JobConfigurable) hadoopInputSplit).configure(this.jobConf).     }     hadoopInputSplit.readFields(in). }
