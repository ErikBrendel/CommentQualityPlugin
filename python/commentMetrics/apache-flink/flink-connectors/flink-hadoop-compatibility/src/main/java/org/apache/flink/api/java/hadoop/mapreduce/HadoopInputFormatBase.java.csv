commented;modifiers;parameterAmount;loc;comment;code
false;public;0;3;;public org.apache.hadoop.conf.Configuration getConfiguration() {     return this.configuration. }
false;public;1;10;;// -------------------------------------------------------------------------------------------- // InputFormat // -------------------------------------------------------------------------------------------- @Override public void configure(Configuration parameters) {     // enforce sequential configuration() calls     synchronized (CONFIGURE_MUTEX) {         if (mapreduceInputFormat instanceof Configurable) {             ((Configurable) mapreduceInputFormat).setConf(configuration).         }     } }
false;public;1;30;;@Override public BaseStatistics getStatistics(BaseStatistics cachedStats) throws IOException {     // only gather base statistics for FileInputFormats     if (!(mapreduceInputFormat instanceof FileInputFormat)) {         return null.     }     JobContext jobContext = new JobContextImpl(configuration, null).     final FileBaseStatistics cachedFileStats = (cachedStats instanceof FileBaseStatistics) ? (FileBaseStatistics) cachedStats : null.     try {         final org.apache.hadoop.fs.Path[] paths = FileInputFormat.getInputPaths(jobContext).         return getFileStats(cachedFileStats, paths, new ArrayList<FileStatus>(1)).     } catch (IOException ioex) {         if (LOG.isWarnEnabled()) {             LOG.warn("Could not determine statistics due to an io error: " + ioex.getMessage()).         }     } catch (Throwable t) {         if (LOG.isErrorEnabled()) {             LOG.error("Unexpected problem while getting the file statistics: " + t.getMessage(), t).         }     }     // no statistics available     return null. }
false;public;1;26;;@Override public HadoopInputSplit[] createInputSplits(int minNumSplits) throws IOException {     configuration.setInt("mapreduce.input.fileinputformat.split.minsize", minNumSplits).     JobContext jobContext = new JobContextImpl(configuration, new JobID()).     jobContext.getCredentials().addAll(this.credentials).     Credentials currentUserCreds = getCredentialsFromUGI(UserGroupInformation.getCurrentUser()).     if (currentUserCreds != null) {         jobContext.getCredentials().addAll(currentUserCreds).     }     List<org.apache.hadoop.mapreduce.InputSplit> splits.     try {         splits = this.mapreduceInputFormat.getSplits(jobContext).     } catch (InterruptedException e) {         throw new IOException("Could not get Splits.", e).     }     HadoopInputSplit[] hadoopInputSplits = new HadoopInputSplit[splits.size()].     for (int i = 0. i < hadoopInputSplits.length. i++) {         hadoopInputSplits[i] = new HadoopInputSplit(i, splits.get(i), jobContext).     }     return hadoopInputSplits. }
false;public;1;4;;@Override public InputSplitAssigner getInputSplitAssigner(HadoopInputSplit[] inputSplits) {     return new LocatableInputSplitAssigner(inputSplits). }
false;public;1;19;;@Override public void open(HadoopInputSplit split) throws IOException {     // enforce sequential open() calls     synchronized (OPEN_MUTEX) {         TaskAttemptContext context = new TaskAttemptContextImpl(configuration, new TaskAttemptID()).         try {             this.recordReader = this.mapreduceInputFormat.createRecordReader(split.getHadoopInputSplit(), context).             this.recordReader.initialize(split.getHadoopInputSplit(), context).         } catch (InterruptedException e) {             throw new IOException("Could not create RecordReader.", e).         } finally {             this.fetched = false.         }     } }
false;public;0;7;;@Override public boolean reachedEnd() throws IOException {     if (!this.fetched) {         fetchNext().     }     return !this.hasNext. }
false;protected;0;9;;protected void fetchNext() throws IOException {     try {         this.hasNext = this.recordReader.nextKeyValue().     } catch (InterruptedException e) {         throw new IOException("Could not fetch next KeyValue pair.", e).     } finally {         this.fetched = true.     } }
false;public;0;10;;@Override public void close() throws IOException {     if (this.recordReader != null) {         // enforce sequential close() calls         synchronized (CLOSE_MUTEX) {             this.recordReader.close().         }     } }
false;private;3;48;;// -------------------------------------------------------------------------------------------- // Helper methods // -------------------------------------------------------------------------------------------- private FileBaseStatistics getFileStats(FileBaseStatistics cachedStats, org.apache.hadoop.fs.Path[] hadoopFilePaths, ArrayList<FileStatus> files) throws IOException {     long latestModTime = 0L.     // get the file info and check whether the cached statistics are still valid.     for (org.apache.hadoop.fs.Path hadoopPath : hadoopFilePaths) {         final Path filePath = new Path(hadoopPath.toUri()).         final FileSystem fs = FileSystem.get(filePath.toUri()).         final FileStatus file = fs.getFileStatus(filePath).         latestModTime = Math.max(latestModTime, file.getModificationTime()).         // enumerate all files and check their modification time stamp.         if (file.isDir()) {             FileStatus[] fss = fs.listStatus(filePath).             files.ensureCapacity(files.size() + fss.length).             for (FileStatus s : fss) {                 if (!s.isDir()) {                     files.add(s).                     latestModTime = Math.max(s.getModificationTime(), latestModTime).                 }             }         } else {             files.add(file).         }     }     // check whether the cached statistics are still valid, if we have any     if (cachedStats != null && latestModTime <= cachedStats.getLastModificationTime()) {         return cachedStats.     }     // calculate the whole length     long len = 0.     for (FileStatus s : files) {         len += s.getLen().     }     // sanity check     if (len <= 0) {         len = BaseStatistics.SIZE_UNKNOWN.     }     return new FileBaseStatistics(latestModTime, len, BaseStatistics.AVG_RECORD_BYTES_UNKNOWN). }
false;private;1;7;;// -------------------------------------------------------------------------------------------- // Custom serialization methods // -------------------------------------------------------------------------------------------- private void writeObject(ObjectOutputStream out) throws IOException {     super.write(out).     out.writeUTF(this.mapreduceInputFormat.getClass().getName()).     out.writeUTF(this.keyClass.getName()).     out.writeUTF(this.valueClass.getName()).     this.configuration.write(out). }
false;private;1;30;;@SuppressWarnings("unchecked") private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {     super.read(in).     String hadoopInputFormatClassName = in.readUTF().     String keyClassName = in.readUTF().     String valueClassName = in.readUTF().     org.apache.hadoop.conf.Configuration configuration = new org.apache.hadoop.conf.Configuration().     configuration.readFields(in).     if (this.configuration == null) {         this.configuration = configuration.     }     try {         this.mapreduceInputFormat = (org.apache.hadoop.mapreduce.InputFormat<K, V>) Class.forName(hadoopInputFormatClassName, true, Thread.currentThread().getContextClassLoader()).newInstance().     } catch (Exception e) {         throw new RuntimeException("Unable to instantiate the hadoop input format", e).     }     try {         this.keyClass = (Class<K>) Class.forName(keyClassName, true, Thread.currentThread().getContextClassLoader()).     } catch (Exception e) {         throw new RuntimeException("Unable to find key class.", e).     }     try {         this.valueClass = (Class<V>) Class.forName(valueClassName, true, Thread.currentThread().getContextClassLoader()).     } catch (Exception e) {         throw new RuntimeException("Unable to find value class.", e).     } }
