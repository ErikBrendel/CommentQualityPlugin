commented;modifiers;parameterAmount;loc;comment;code
true;public,static;1;12;/**  * Merge HadoopConfiguration into JobConf. This is necessary for the HDFS configuration.  */ ;/**  * Merge HadoopConfiguration into JobConf. This is necessary for the HDFS configuration.  */ public static void mergeHadoopConf(JobConf jobConf) {     // we have to load the global configuration here, because the HadoopInputFormatBase does not     // have access to a Flink configuration object     org.apache.flink.configuration.Configuration flinkConfiguration = GlobalConfiguration.loadConfiguration().     Configuration hadoopConf = getHadoopConfiguration(flinkConfiguration).     for (Map.Entry<String, String> e : hadoopConf) {         if (jobConf.get(e.getKey()) == null) {             jobConf.set(e.getKey(), e.getValue()).         }     } }
true;public,static;1;55;/**  * Returns a new Hadoop Configuration object using the path to the hadoop conf configured  * in the main configuration (flink-conf.yaml).  * This method is public because its being used in the HadoopDataSource.  *  * @param flinkConfiguration Flink configuration object  * @return A Hadoop configuration instance  */ ;/**  * Returns a new Hadoop Configuration object using the path to the hadoop conf configured  * in the main configuration (flink-conf.yaml).  * This method is public because its being used in the HadoopDataSource.  *  * @param flinkConfiguration Flink configuration object  * @return A Hadoop configuration instance  */ public static Configuration getHadoopConfiguration(org.apache.flink.configuration.Configuration flinkConfiguration) {     Configuration retConf = new Configuration().     // We need to load both core-site.xml and hdfs-site.xml to determine the default fs path and     // the hdfs configuration     // Try to load HDFS configuration from Hadoop's own configuration files     // 1. approach: Flink configuration     final String hdfsDefaultPath = flinkConfiguration.getString(ConfigConstants.HDFS_DEFAULT_CONFIG, null).     if (hdfsDefaultPath != null) {         retConf.addResource(new org.apache.hadoop.fs.Path(hdfsDefaultPath)).     } else {         LOG.debug("Cannot find hdfs-default configuration file").     }     final String hdfsSitePath = flinkConfiguration.getString(ConfigConstants.HDFS_SITE_CONFIG, null).     if (hdfsSitePath != null) {         retConf.addResource(new org.apache.hadoop.fs.Path(hdfsSitePath)).     } else {         LOG.debug("Cannot find hdfs-site configuration file").     }     // 2. Approach environment variables     String[] possibleHadoopConfPaths = new String[4].     possibleHadoopConfPaths[0] = flinkConfiguration.getString(ConfigConstants.PATH_HADOOP_CONFIG, null).     possibleHadoopConfPaths[1] = System.getenv("HADOOP_CONF_DIR").     if (System.getenv("HADOOP_HOME") != null) {         possibleHadoopConfPaths[2] = System.getenv("HADOOP_HOME") + "/conf".         // hadoop 2.2         possibleHadoopConfPaths[3] = System.getenv("HADOOP_HOME") + "/etc/hadoop".     }     for (String possibleHadoopConfPath : possibleHadoopConfPaths) {         if (possibleHadoopConfPath != null) {             if (new File(possibleHadoopConfPath).exists()) {                 if (new File(possibleHadoopConfPath + "/core-site.xml").exists()) {                     retConf.addResource(new org.apache.hadoop.fs.Path(possibleHadoopConfPath + "/core-site.xml")).                     if (LOG.isDebugEnabled()) {                         LOG.debug("Adding " + possibleHadoopConfPath + "/core-site.xml to hadoop configuration").                     }                 }                 if (new File(possibleHadoopConfPath + "/hdfs-site.xml").exists()) {                     retConf.addResource(new org.apache.hadoop.fs.Path(possibleHadoopConfPath + "/hdfs-site.xml")).                     if (LOG.isDebugEnabled()) {                         LOG.debug("Adding " + possibleHadoopConfPath + "/hdfs-site.xml to hadoop configuration").                     }                 }             }         }     }     return retConf. }
