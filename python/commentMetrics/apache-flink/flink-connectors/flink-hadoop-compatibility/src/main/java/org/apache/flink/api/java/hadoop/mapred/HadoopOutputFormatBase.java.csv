commented;modifiers;parameterAmount;loc;comment;code
false;public;0;3;;public JobConf getJobConf() {     return jobConf. }
false;public;1;13;;// -------------------------------------------------------------------------------------------- // OutputFormat // -------------------------------------------------------------------------------------------- @Override public void configure(Configuration parameters) {     // enforce sequential configure() calls     synchronized (CONFIGURE_MUTEX) {         // configure MR OutputFormat if necessary         if (this.mapredOutputFormat instanceof Configurable) {             ((Configurable) this.mapredOutputFormat).setConf(this.jobConf).         } else if (this.mapredOutputFormat instanceof JobConfigurable) {             ((JobConfigurable) this.mapredOutputFormat).configure(this.jobConf).         }     } }
true;public;2;31;/**  * create the temporary output file for hadoop RecordWriter.  * @param taskNumber The number of the parallel instance.  * @param numTasks The number of parallel tasks.  * @throws java.io.IOException  */ ;/**  * create the temporary output file for hadoop RecordWriter.  * @param taskNumber The number of the parallel instance.  * @param numTasks The number of parallel tasks.  * @throws java.io.IOException  */ @Override public void open(int taskNumber, int numTasks) throws IOException {     // enforce sequential open() calls     synchronized (OPEN_MUTEX) {         if (Integer.toString(taskNumber + 1).length() > 6) {             throw new IOException("Task id too large.").         }         TaskAttemptID taskAttemptID = TaskAttemptID.forName("attempt__0000_r_" + String.format("%" + (6 - Integer.toString(taskNumber + 1).length()) + "s", " ").replace(" ", "0") + Integer.toString(taskNumber + 1) + "_0").         this.jobConf.set("mapred.task.id", taskAttemptID.toString()).         this.jobConf.setInt("mapred.task.partition", taskNumber + 1).         // for hadoop 2.2         this.jobConf.set("mapreduce.task.attempt.id", taskAttemptID.toString()).         this.jobConf.setInt("mapreduce.task.partition", taskNumber + 1).         this.context = new TaskAttemptContextImpl(this.jobConf, taskAttemptID).         this.outputCommitter = this.jobConf.getOutputCommitter().         JobContext jobContext = new JobContextImpl(this.jobConf, new JobID()).         this.outputCommitter.setupJob(jobContext).         this.recordWriter = this.mapredOutputFormat.getRecordWriter(null, this.jobConf, Integer.toString(taskNumber + 1), new HadoopDummyProgressable()).     } }
true;public;0;12;/**  * commit the task by moving the output file out from the temporary directory.  * @throws java.io.IOException  */ ;/**  * commit the task by moving the output file out from the temporary directory.  * @throws java.io.IOException  */ @Override public void close() throws IOException {     // enforce sequential close() calls     synchronized (CLOSE_MUTEX) {         this.recordWriter.close(new HadoopDummyReporter()).         if (this.outputCommitter.needsTaskCommit(this.context)) {             this.outputCommitter.commitTask(this.context).         }     } }
false;public;1;13;;@Override public void finalizeGlobal(int parallelism) throws IOException {     try {         JobContext jobContext = new JobContextImpl(this.jobConf, new JobID()).         OutputCommitter outputCommitter = this.jobConf.getOutputCommitter().         // finalize HDFS output format         outputCommitter.commitJob(jobContext).     } catch (Exception e) {         throw new RuntimeException(e).     } }
false;private;1;5;;// -------------------------------------------------------------------------------------------- // Custom serialization methods // -------------------------------------------------------------------------------------------- private void writeObject(ObjectOutputStream out) throws IOException {     super.write(out).     out.writeUTF(mapredOutputFormat.getClass().getName()).     jobConf.write(out). }
false;private;1;21;;@SuppressWarnings("unchecked") private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {     super.read(in).     String hadoopOutputFormatName = in.readUTF().     if (jobConf == null) {         jobConf = new JobConf().     }     jobConf.readFields(in).     try {         this.mapredOutputFormat = (org.apache.hadoop.mapred.OutputFormat<K, V>) Class.forName(hadoopOutputFormatName, true, Thread.currentThread().getContextClassLoader()).newInstance().     } catch (Exception e) {         throw new RuntimeException("Unable to instantiate the hadoop output format", e).     }     ReflectionUtils.setConf(mapredOutputFormat, jobConf).     jobConf.getCredentials().addAll(this.credentials).     Credentials currentUserCreds = getCredentialsFromUGI(UserGroupInformation.getCurrentUser()).     if (currentUserCreds != null) {         jobConf.getCredentials().addAll(currentUserCreds).     } }
