commented;modifiers;parameterAmount;loc;comment;code
false;public;0;10;;@Test public void testConfigureWithConfigurableInstance() {     ConfigurableDummyInputFormat inputFormat = mock(ConfigurableDummyInputFormat.class).     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(inputFormat, String.class, Long.class, new JobConf()).     verify(inputFormat, times(1)).setConf(any(JobConf.class)).     hadoopInputFormat.configure(new org.apache.flink.configuration.Configuration()).     verify(inputFormat, times(2)).setConf(any(JobConf.class)). }
false;public;0;10;;@Test public void testConfigureWithJobConfigurableInstance() {     JobConfigurableDummyInputFormat inputFormat = mock(JobConfigurableDummyInputFormat.class).     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(inputFormat, String.class, Long.class, new JobConf()).     verify(inputFormat, times(1)).configure(any(JobConf.class)).     hadoopInputFormat.configure(new org.apache.flink.configuration.Configuration()).     verify(inputFormat, times(2)).configure(any(JobConf.class)). }
false;public;0;18;;@Test public void testOpenClose() throws Exception {     DummyRecordReader recordReader = mock(DummyRecordReader.class).     DummyInputFormat inputFormat = mock(DummyInputFormat.class).     when(inputFormat.getRecordReader(any(InputSplit.class), any(JobConf.class), any(Reporter.class))).thenReturn(recordReader).     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(inputFormat, String.class, Long.class, new JobConf()).     hadoopInputFormat.open(getHadoopInputSplit()).     verify(inputFormat, times(1)).getRecordReader(any(InputSplit.class), any(JobConf.class), any(Reporter.class)).     verify(recordReader, times(1)).createKey().     verify(recordReader, times(1)).createValue().     assertThat(hadoopInputFormat.fetched, is(false)).     hadoopInputFormat.close().     verify(recordReader, times(1)).close(). }
false;public;0;17;;@Test public void testOpenWithConfigurableReader() throws Exception {     ConfigurableDummyRecordReader recordReader = mock(ConfigurableDummyRecordReader.class).     DummyInputFormat inputFormat = mock(DummyInputFormat.class).     when(inputFormat.getRecordReader(any(InputSplit.class), any(JobConf.class), any(Reporter.class))).thenReturn(recordReader).     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(inputFormat, String.class, Long.class, new JobConf()).     hadoopInputFormat.open(getHadoopInputSplit()).     verify(inputFormat, times(1)).getRecordReader(any(InputSplit.class), any(JobConf.class), any(Reporter.class)).     verify(recordReader, times(1)).setConf(any(JobConf.class)).     verify(recordReader, times(1)).createKey().     verify(recordReader, times(1)).createValue().     assertThat(hadoopInputFormat.fetched, is(false)). }
false;public;0;13;;@Test public void testCreateInputSplits() throws Exception {     FileSplit[] result = new FileSplit[1].     result[0] = getFileSplit().     DummyInputFormat inputFormat = mock(DummyInputFormat.class).     when(inputFormat.getSplits(any(JobConf.class), anyInt())).thenReturn(result).     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(inputFormat, String.class, Long.class, new JobConf()).     hadoopInputFormat.createInputSplits(2).     verify(inputFormat, times(1)).getSplits(any(JobConf.class), anyInt()). }
false;public;0;9;;@Test public void testReachedEndWithElementsRemaining() throws IOException {     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(new DummyInputFormat(), String.class, Long.class, new JobConf()).     hadoopInputFormat.fetched = true.     hadoopInputFormat.hasNext = true.     assertThat(hadoopInputFormat.reachedEnd(), is(false)). }
false;public;0;8;;@Test public void testReachedEndWithNoElementsRemaining() throws IOException {     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(new DummyInputFormat(), String.class, Long.class, new JobConf()).     hadoopInputFormat.fetched = true.     hadoopInputFormat.hasNext = false.     assertThat(hadoopInputFormat.reachedEnd(), is(true)). }
false;public;0;16;;@Test public void testFetchNext() throws IOException {     DummyRecordReader recordReader = mock(DummyRecordReader.class).     when(recordReader.next(nullable(String.class), nullable(Long.class))).thenReturn(true).     DummyInputFormat inputFormat = mock(DummyInputFormat.class).     when(inputFormat.getRecordReader(any(InputSplit.class), any(JobConf.class), any(Reporter.class))).thenReturn(recordReader).     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(inputFormat, String.class, Long.class, new JobConf()).     hadoopInputFormat.open(getHadoopInputSplit()).     hadoopInputFormat.fetchNext().     verify(recordReader, times(1)).next(nullable(String.class), anyLong()).     assertThat(hadoopInputFormat.hasNext, is(true)).     assertThat(hadoopInputFormat.fetched, is(true)). }
false;public;0;11;;@Test public void checkTypeInformation() throws Exception {     HadoopInputFormat<Void, Long> hadoopInputFormat = new HadoopInputFormat<>(new DummyVoidKeyInputFormat<Long>(), Void.class, Long.class, new JobConf()).     TypeInformation<Tuple2<Void, Long>> tupleType = hadoopInputFormat.getProducedType().     TypeInformation<Tuple2<Void, Long>> expectedType = new TupleTypeInfo<>(BasicTypeInfo.VOID_TYPE_INFO, BasicTypeInfo.LONG_TYPE_INFO).     assertThat(tupleType.isTupleType(), is(true)).     assertThat(tupleType, is(equalTo(expectedType))). }
false;public;0;6;;@Test public void testCloseWithoutOpen() throws Exception {     HadoopInputFormat<Void, Long> hadoopInputFormat = new HadoopInputFormat<>(new DummyVoidKeyInputFormat<Long>(), Void.class, Long.class, new JobConf()).     hadoopInputFormat.close(). }
false;private;0;3;;private HadoopInputSplit getHadoopInputSplit() {     return new HadoopInputSplit(1, getFileSplit(), new JobConf()). }
false;private;0;3;;private FileSplit getFileSplit() {     return new FileSplit(new Path("path"), 1, 2, new String[] {}). }
false;public;3;4;;@Override public org.apache.hadoop.mapred.RecordReader<Void, T> getRecordReader(org.apache.hadoop.mapred.InputSplit inputSplit, JobConf jobConf, Reporter reporter) throws IOException {     return null. }
false;public;0;4;;@Override public float getProgress() throws IOException {     return 0. }
false;public;2;4;;@Override public boolean next(String s, Long aLong) throws IOException {     return false. }
false;public;0;4;;@Override public String createKey() {     return null. }
false;public;0;4;;@Override public Long createValue() {     return null. }
false;public;0;4;;@Override public long getPos() throws IOException {     return 0. }
false;public;0;4;;@Override public void close() throws IOException { }
false;public;1;2;;@Override public void setConf(Configuration configuration) { }
false;public;0;4;;@Override public Configuration getConf() {     return null. }
false;public;2;4;;@Override public boolean next(String s, Long aLong) throws IOException {     return false. }
false;public;0;4;;@Override public String createKey() {     return null. }
false;public;0;4;;@Override public Long createValue() {     return null. }
false;public;0;4;;@Override public long getPos() throws IOException {     return 0. }
false;public;0;4;;@Override public void close() throws IOException { }
false;public;0;4;;@Override public float getProgress() throws IOException {     return 0. }
false;public;2;4;;@Override public InputSplit[] getSplits(JobConf jobConf, int i) throws IOException {     return new InputSplit[0]. }
false;public;3;4;;@Override public RecordReader<String, Long> getRecordReader(InputSplit inputSplit, JobConf jobConf, Reporter reporter) throws IOException {     return null. }
false;public;1;2;;@Override public void setConf(Configuration configuration) { }
false;public;0;4;;@Override public Configuration getConf() {     return null. }
false;public;1;2;;@Override public void configure(JobConf jobConf) { }
