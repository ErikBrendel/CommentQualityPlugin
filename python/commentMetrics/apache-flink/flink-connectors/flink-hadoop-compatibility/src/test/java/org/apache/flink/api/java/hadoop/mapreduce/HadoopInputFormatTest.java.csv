commented;modifiers;parameterAmount;loc;comment;code
false;public;0;10;;@Test public void testConfigure() throws Exception {     ConfigurableDummyInputFormat inputFormat = mock(ConfigurableDummyInputFormat.class).     HadoopInputFormat<String, Long> hadoopInputFormat = setupHadoopInputFormat(inputFormat, Job.getInstance(), null).     hadoopInputFormat.configure(new org.apache.flink.configuration.Configuration()).     verify(inputFormat, times(1)).setConf(any(Configuration.class)). }
false;public;0;9;;@Test public void testCreateInputSplits() throws Exception {     DummyInputFormat inputFormat = mock(DummyInputFormat.class).     HadoopInputFormat<String, Long> hadoopInputFormat = setupHadoopInputFormat(inputFormat, Job.getInstance(), null).     hadoopInputFormat.createInputSplits(2).     verify(inputFormat, times(1)).getSplits(any(JobContext.class)). }
false;public;0;12;;@Test public void testOpen() throws Exception {     DummyInputFormat inputFormat = mock(DummyInputFormat.class).     when(inputFormat.createRecordReader(nullable(InputSplit.class), any(TaskAttemptContext.class))).thenReturn(new DummyRecordReader()).     HadoopInputSplit inputSplit = mock(HadoopInputSplit.class).     HadoopInputFormat<String, Long> hadoopInputFormat = setupHadoopInputFormat(inputFormat, Job.getInstance(), null).     hadoopInputFormat.open(inputSplit).     verify(inputFormat, times(1)).createRecordReader(nullable(InputSplit.class), any(TaskAttemptContext.class)).     assertThat(hadoopInputFormat.fetched, is(false)). }
false;public;0;10;;@Test public void testClose() throws Exception {     DummyRecordReader recordReader = mock(DummyRecordReader.class).     HadoopInputFormat<String, Long> hadoopInputFormat = setupHadoopInputFormat(new DummyInputFormat(), Job.getInstance(), recordReader).     hadoopInputFormat.close().     verify(recordReader, times(1)).close(). }
false;public;0;5;;@Test public void testCloseWithoutOpen() throws Exception {     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(new DummyInputFormat(), String.class, Long.class, Job.getInstance()).     hadoopInputFormat.close(). }
false;public;0;10;;@Test public void testFetchNextInitialState() throws Exception {     DummyRecordReader recordReader = new DummyRecordReader().     HadoopInputFormat<String, Long> hadoopInputFormat = setupHadoopInputFormat(new DummyInputFormat(), Job.getInstance(), recordReader).     hadoopInputFormat.fetchNext().     assertThat(hadoopInputFormat.fetched, is(true)).     assertThat(hadoopInputFormat.hasNext, is(false)). }
false;public;0;12;;@Test public void testFetchNextRecordReaderHasNewValue() throws Exception {     DummyRecordReader recordReader = mock(DummyRecordReader.class).     when(recordReader.nextKeyValue()).thenReturn(true).     HadoopInputFormat<String, Long> hadoopInputFormat = setupHadoopInputFormat(new DummyInputFormat(), Job.getInstance(), recordReader).     hadoopInputFormat.fetchNext().     assertThat(hadoopInputFormat.fetched, is(true)).     assertThat(hadoopInputFormat.hasNext, is(true)). }
false;public;0;13;;@Test public void testFetchNextRecordReaderThrowsException() throws Exception {     DummyRecordReader recordReader = mock(DummyRecordReader.class).     when(recordReader.nextKeyValue()).thenThrow(new InterruptedException()).     HadoopInputFormat<String, Long> hadoopInputFormat = setupHadoopInputFormat(new DummyInputFormat(), Job.getInstance(), recordReader).     exception.expect(IOException.class).     hadoopInputFormat.fetchNext().     assertThat(hadoopInputFormat.hasNext, is(true)). }
false;public;0;12;;@Test public void checkTypeInformation() throws Exception {     HadoopInputFormat<Void, Long> hadoopInputFormat = new HadoopInputFormat<>(new DummyVoidKeyInputFormat<Long>(), Void.class, Long.class, Job.getInstance()).     TypeInformation<Tuple2<Void, Long>> tupleType = hadoopInputFormat.getProducedType().     TypeInformation<Tuple2<Void, Long>> expectedType = new TupleTypeInfo<>(BasicTypeInfo.VOID_TYPE_INFO, BasicTypeInfo.LONG_TYPE_INFO).     assertThat(tupleType.isTupleType(), is(true)).     assertThat(tupleType, is(equalTo(expectedType))). }
false;private;3;9;;private HadoopInputFormat<String, Long> setupHadoopInputFormat(InputFormat<String, Long> inputFormat, Job job, RecordReader<String, Long> recordReader) {     HadoopInputFormat<String, Long> hadoopInputFormat = new HadoopInputFormat<>(inputFormat, String.class, Long.class, job).     hadoopInputFormat.recordReader = recordReader.     return hadoopInputFormat. }
false;public;2;4;;@Override public RecordReader<Void, T> createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {     return null. }
false;public;2;4;;@Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException { }
false;public;0;4;;@Override public boolean nextKeyValue() throws IOException, InterruptedException {     return false. }
false;public;0;4;;@Override public String getCurrentKey() throws IOException, InterruptedException {     return null. }
false;public;0;4;;@Override public Long getCurrentValue() throws IOException, InterruptedException {     return null. }
false;public;0;4;;@Override public float getProgress() throws IOException, InterruptedException {     return 0. }
false;public;0;4;;@Override public void close() throws IOException { }
false;public;1;4;;@Override public List<InputSplit> getSplits(JobContext jobContext) throws IOException, InterruptedException {     return null. }
false;public;2;4;;@Override public RecordReader<String, Long> createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {     return new DummyRecordReader(). }
false;public;1;2;;@Override public void setConf(Configuration configuration) { }
false;public;0;4;;@Override public Configuration getConf() {     return null. }
