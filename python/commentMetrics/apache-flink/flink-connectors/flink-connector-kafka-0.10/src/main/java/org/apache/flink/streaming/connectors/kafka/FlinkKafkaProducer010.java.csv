commented;modifiers;parameterAmount;loc;comment;code
true;public;1;3;/**  * If set to true, Flink will write the (event time) timestamp attached to each record into Kafka.  * Timestamps must be positive for Kafka to accept them.  *  * @param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.  */ ;// ------------------- User configuration ---------------------- /**  * If set to true, Flink will write the (event time) timestamp attached to each record into Kafka.  * Timestamps must be positive for Kafka to accept them.  *  * @param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.  */ public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {     this.writeTimestampToKafka = writeTimestampToKafka. }
true;public,static;4;7;/**  * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to  * the topic.  *  * <p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)  *  * @param inStream The stream to write to Kafka  * @param topicId ID of the Kafka topic.  * @param serializationSchema User defined serialization schema supporting key/value messages  * @param producerConfig Properties with the producer configuration.  *  * @deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}  * and call {@link #setWriteTimestampToKafka(boolean)}.  */ ;// ----------------------------- Deprecated constructors / factory methods  --------------------------- /**  * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to  * the topic.  *  * <p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)  *  * @param inStream The stream to write to Kafka  * @param topicId ID of the Kafka topic.  * @param serializationSchema User defined serialization schema supporting key/value messages  * @param producerConfig Properties with the producer configuration.  *  * @deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}  * and call {@link #setWriteTimestampToKafka(boolean)}.  */ @Deprecated public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {     return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>()). }
true;public,static;4;7;/**  * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to  * the topic.  *  * <p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)  *  * @param inStream The stream to write to Kafka  * @param topicId ID of the Kafka topic.  * @param serializationSchema User defined (keyless) serialization schema.  * @param producerConfig Properties with the producer configuration.  *  * @deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}  * and call {@link #setWriteTimestampToKafka(boolean)}.  */ ;/**  * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to  * the topic.  *  * <p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)  *  * @param inStream The stream to write to Kafka  * @param topicId ID of the Kafka topic.  * @param serializationSchema User defined (keyless) serialization schema.  * @param producerConfig Properties with the producer configuration.  *  * @deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}  * and call {@link #setWriteTimestampToKafka(boolean)}.  */ @Deprecated public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {     return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>()). }
true;public,static;5;10;/**  * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to  * the topic.  *  * <p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)  *  *  @param inStream The stream to write to Kafka  *  @param topicId The name of the target topic  *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages  *  @param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument.  *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.  *  * @deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}  * and call {@link #setWriteTimestampToKafka(boolean)}.  */ ;/**  * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to  * the topic.  *  * <p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)  *  *  @param inStream The stream to write to Kafka  *  @param topicId The name of the target topic  *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages  *  @param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument.  *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.  *  * @deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}  * and call {@link #setWriteTimestampToKafka(boolean)}.  */ @Deprecated public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {     FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner).     DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer).     return new FlinkKafkaProducer010Configuration<>(streamSink, inStream, kafkaProducer). }
true;public,static;5;12;/**  * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to  * the topic.  *  * <p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)  *  *  @param inStream The stream to write to Kafka  *  @param topicId The name of the target topic  *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages  *  @param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument.  *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.  *  *  @deprecated This is a deprecated since it does not correctly handle partitioning when  *              producing to multiple topics. Use  *              {@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.  */ ;/**  * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to  * the topic.  *  * <p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)  *  *  @param inStream The stream to write to Kafka  *  @param topicId The name of the target topic  *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages  *  @param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument.  *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.  *  *  @deprecated This is a deprecated since it does not correctly handle partitioning when  *              producing to multiple topics. Use  *              {@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.  */ @Deprecated public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {     FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner)).     DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer).     return new FlinkKafkaProducer010Configuration<T>(streamSink, inStream, kafkaProducer). }
false;public;2;35;;// ----------------------------- Generic element processing  --------------------------- @Override public void invoke(T value, Context context) throws Exception {     checkErroneous().     byte[] serializedKey = schema.serializeKey(value).     byte[] serializedValue = schema.serializeValue(value).     String targetTopic = schema.getTargetTopic(value).     if (targetTopic == null) {         targetTopic = defaultTopicId.     }     Long timestamp = null.     if (this.writeTimestampToKafka) {         timestamp = context.timestamp().     }     ProducerRecord<byte[], byte[]> record.     int[] partitions = topicPartitionsMap.get(targetTopic).     if (null == partitions) {         partitions = getPartitionsByTopic(targetTopic, producer).         topicPartitionsMap.put(targetTopic, partitions).     }     if (flinkKafkaPartitioner == null) {         record = new ProducerRecord<>(targetTopic, null, timestamp, serializedKey, serializedValue).     } else {         record = new ProducerRecord<>(targetTopic, flinkKafkaPartitioner.partition(value, serializedKey, serializedValue, targetTopic, partitions), timestamp, serializedKey, serializedValue).     }     if (flushOnCheckpoint) {         synchronized (pendingRecordsLock) {             pendingRecords++.         }     }     producer.send(record, callback). }
true;public;1;3;/**  * Defines whether the producer should fail on errors, or only log them.  * If this is set to true, then exceptions will be only logged, if set to false,  * exceptions will be eventually thrown and cause the streaming program to  * fail (and enter recovery).  *  * @param logFailuresOnly The flag to indicate logging-only on exceptions.  */ ;/**  * Defines whether the producer should fail on errors, or only log them.  * If this is set to true, then exceptions will be only logged, if set to false,  * exceptions will be eventually thrown and cause the streaming program to  * fail (and enter recovery).  *  * @param logFailuresOnly The flag to indicate logging-only on exceptions.  */ public void setLogFailuresOnly(boolean logFailuresOnly) {     producer.setLogFailuresOnly(logFailuresOnly). }
true;public;1;3;/**  * If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers  * to be acknowledged by the Kafka producer on a checkpoint.  * This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.  *  * @param flush Flag indicating the flushing mode (true = flush on checkpoint)  */ ;/**  * If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers  * to be acknowledged by the Kafka producer on a checkpoint.  * This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.  *  * @param flush Flag indicating the flushing mode (true = flush on checkpoint)  */ public void setFlushOnCheckpoint(boolean flush) {     producer.setFlushOnCheckpoint(flush). }
true;public;1;3;/**  * If set to true, Flink will write the (event time) timestamp attached to each record into Kafka.  * Timestamps must be positive for Kafka to accept them.  *  * @param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.  */ ;/**  * If set to true, Flink will write the (event time) timestamp attached to each record into Kafka.  * Timestamps must be positive for Kafka to accept them.  *  * @param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.  */ public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {     producer.writeTimestampToKafka = writeTimestampToKafka. }
false;public;0;4;;// ************************************************************************* // Override methods to use the transformation in this class. // ************************************************************************* @Override public SinkTransformation<T> getTransformation() {     return transformation. }
false;public;1;5;;@Override public DataStreamSink<T> name(String name) {     transformation.setName(name).     return this. }
false;public;1;5;;@Override public DataStreamSink<T> uid(String uid) {     transformation.setUid(uid).     return this. }
false;public;1;5;;@Override public DataStreamSink<T> setUidHash(String uidHash) {     transformation.setUidHash(uidHash).     return this. }
false;public;1;5;;@Override public DataStreamSink<T> setParallelism(int parallelism) {     transformation.setParallelism(parallelism).     return this. }
false;public;0;5;;@Override public DataStreamSink<T> disableChaining() {     this.transformation.setChainingStrategy(ChainingStrategy.NEVER).     return this. }
false;public;1;5;;@Override public DataStreamSink<T> slotSharingGroup(String slotSharingGroup) {     transformation.setSlotSharingGroup(slotSharingGroup).     return this. }
