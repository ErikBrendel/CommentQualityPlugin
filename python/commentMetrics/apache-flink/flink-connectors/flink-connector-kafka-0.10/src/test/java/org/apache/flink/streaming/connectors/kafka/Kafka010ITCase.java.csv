# id;timestamp;commentText;codeText;commentWords;codeWords
Kafka010ITCase -> @Ignore("This test is disabled because of: https://issues.apache.org/jira/browse/FLINK-9217") 	@Test(timeout = 60000) 	public void testTimestamps() throws Exception;1530258217;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Ignore("This test is disabled because of: https://issues.apache.org/jira/browse/FLINK-9217")_	@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(Types.LONG, env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;ignore,this,test,is,disabled,because,of,https,issues,apache,org,jira,browse,flink,9217,test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,types,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Ignore("This test is disabled because of: https://issues.apache.org/jira/browse/FLINK-9217") 	@Test(timeout = 60000) 	public void testTimestamps() throws Exception;1550834396;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Ignore("This test is disabled because of: https://issues.apache.org/jira/browse/FLINK-9217")_	@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(Types.LONG, env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;ignore,this,test,is,disabled,because,of,https,issues,apache,org,jira,browse,flink,9217,test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,types,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1480685315;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while(running) {_					ctx.collectWithTimestamp(i, i*2)__					if(i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new KafkaPartitioner<Long>() {_			@Override_			public int partition(Long next, byte[] serializedKey, byte[] serializedValue, int numPartitions) {_				return (int)(next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if(lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,kafka,partitioner,long,override,public,int,partition,long,next,byte,serialized,key,byte,serialized,value,int,num,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1487173364;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while(running) {_					ctx.collectWithTimestamp(i, i*2)__					if(i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new KafkaPartitioner<Long>() {_			@Override_			public int partition(Long next, byte[] serializedKey, byte[] serializedValue, int numPartitions) {_				return (int)(next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if(lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,kafka,partitioner,long,override,public,int,partition,long,next,byte,serialized,key,byte,serialized,value,int,num,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1489419493;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while(running) {_					ctx.collectWithTimestamp(i, i*2)__					if(i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new KafkaPartitioner<Long>() {_			@Override_			public int partition(Long next, byte[] serializedKey, byte[] serializedValue, int numPartitions) {_				return (int)(next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if(lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,kafka,partitioner,long,override,public,int,partition,long,next,byte,serialized,key,byte,serialized,value,int,num,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1493975167;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while(running) {_					ctx.collectWithTimestamp(i, i*2)__					if(i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new KafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] serializedKey, byte[] serializedValue, int numPartitions) {_				return (int)(next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if(lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,serialized,key,byte,serialized,value,int,num,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1495175928;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while(running) {_					ctx.collectWithTimestamp(i, i*2)__					if(i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int)(next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if(lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1495923077;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1498894422;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1509723634;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1519973085;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1519973085;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1523020981;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka010ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1525452496;Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255105836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1000L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(Types.LONG, env.getConfig())__		FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		})__		prod.setParallelism(3)__		prod.setWriteTimestampToKafka(true)__		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111073247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 10 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,10,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255105836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1000l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,types,long,env,get,config,flink,kafka,producer010,flink,kafka,producer010configuration,prod,flink,kafka,producer010,write,to,kafka,with,timestamps,stream,with,timestamps,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,parallelism,3,prod,set,write,timestamp,to,kafka,true,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer010,long,kafka,source,new,flink,kafka,consumer010,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111073247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,10,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
