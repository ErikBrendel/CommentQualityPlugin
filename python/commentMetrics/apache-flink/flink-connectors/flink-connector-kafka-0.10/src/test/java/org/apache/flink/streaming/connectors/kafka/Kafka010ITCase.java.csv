commented;modifiers;parameterAmount;loc;comment;code
false;public;0;4;;// ------------------------------------------------------------------------ // Suite of Tests // ------------------------------------------------------------------------ @Test(timeout = 60000) public void testFailOnNoBroker() throws Exception {     runFailOnNoBrokerTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testConcurrentProducerConsumerTopology() throws Exception {     runSimpleConcurrentProducerConsumerTopology(). }
false;public;0;4;;@Test(timeout = 60000) public void testKeyValueSupport() throws Exception {     runKeyValueTest(). }
false;public;0;4;;// --- canceling / failures --- @Test(timeout = 60000) public void testCancelingEmptyTopic() throws Exception {     runCancelingOnEmptyInputTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testCancelingFullTopic() throws Exception {     runCancelingOnFullInputTest(). }
false;public;0;4;;// --- source to partition mappings and exactly once --- @Test(timeout = 60000) public void testOneToOneSources() throws Exception {     runOneToOneExactlyOnceTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testOneSourceMultiplePartitions() throws Exception {     runOneSourceMultiplePartitionsExactlyOnceTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testMultipleSourcesOnePartition() throws Exception {     runMultipleSourcesOnePartitionExactlyOnceTest(). }
false;public;0;4;;// --- broker failure --- @Test(timeout = 60000) public void testBrokerFailure() throws Exception {     runBrokerFailureTest(). }
false;public;0;4;;// --- special executions --- @Test(timeout = 60000) public void testBigRecordJob() throws Exception {     runBigRecordTestTopology(). }
false;public;0;4;;@Test(timeout = 60000) public void testMultipleTopics() throws Exception {     runProduceConsumeMultipleTopics(). }
false;public;0;4;;@Test(timeout = 60000) public void testAllDeletes() throws Exception {     runAllDeletesTest(). }
false;public;0;4;;@Test(timeout = 60000) public void testMetricsAndEndOfStream() throws Exception {     runEndOfStreamTest(). }
false;public;0;4;;// --- startup mode --- @Test(timeout = 60000) public void testStartFromEarliestOffsets() throws Exception {     runStartFromEarliestOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromLatestOffsets() throws Exception {     runStartFromLatestOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromGroupOffsets() throws Exception {     runStartFromGroupOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromSpecificOffsets() throws Exception {     runStartFromSpecificOffsets(). }
false;public;0;4;;@Test(timeout = 60000) public void testStartFromTimestamp() throws Exception {     runStartFromTimestamp(). }
false;public;0;4;;// --- offset committing --- @Test(timeout = 60000) public void testCommitOffsetsToKafka() throws Exception {     runCommitOffsetsToKafka(). }
false;public;0;4;;@Test(timeout = 60000) public void testAutoOffsetRetrievalAndCommitToKafka() throws Exception {     runAutoOffsetRetrievalAndCommitToKafka(). }
false;public;1;10;;@Override public void run(SourceContext<Long> ctx) throws Exception {     long i = 0.     while (running) {         ctx.collectWithTimestamp(i, i * 2).         if (i++ == 1000L) {             running = false.         }     } }
false;public;0;4;;@Override public void cancel() {     running = false. }
false;public;5;4;;@Override public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {     return (int) (next % 3). }
false;public;2;8;;@Nullable @Override public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {     if (lastElement % 10 == 0) {         return new Watermark(lastElement).     }     return null. }
false;public;2;4;;@Override public long extractTimestamp(Long element, long previousElementTimestamp) {     return previousElementTimestamp. }
true;public;0;84;/**  * Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.  */ ;/**  * Kafka 0.10 specific test, ensuring Timestamps are properly written to and read from Kafka.  */ @Ignore("This test is disabled because of: https://issues.apache.org/jira/browse/FLINK-9217") @Test(timeout = 60000) public void testTimestamps() throws Exception {     final String topic = "tstopic".     createTestTopic(topic, 3, 1).     // ---------- Produce an event time stream into Kafka -------------------     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(1).     env.getConfig().setRestartStrategy(RestartStrategies.noRestart()).     env.getConfig().disableSysoutLogging().     env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime).     DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {          private static final long serialVersionUID = -2255105836471289626L.          boolean running = true.          @Override         public void run(SourceContext<Long> ctx) throws Exception {             long i = 0.             while (running) {                 ctx.collectWithTimestamp(i, i * 2).                 if (i++ == 1000L) {                     running = false.                 }             }         }          @Override         public void cancel() {             running = false.         }     }).     final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(Types.LONG, env.getConfig()).     FlinkKafkaProducer010.FlinkKafkaProducer010Configuration prod = FlinkKafkaProducer010.writeToKafkaWithTimestamps(streamWithTimestamps, topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, new FlinkKafkaPartitioner<Long>() {          private static final long serialVersionUID = -6730989584364230617L.          @Override         public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {             return (int) (next % 3).         }     }).     prod.setParallelism(3).     prod.setWriteTimestampToKafka(true).     env.execute("Produce some").     // ---------- Consume stream from Kafka -------------------     env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setParallelism(1).     env.getConfig().setRestartStrategy(RestartStrategies.noRestart()).     env.getConfig().disableSysoutLogging().     env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime).     FlinkKafkaConsumer010<Long> kafkaSource = new FlinkKafkaConsumer010<>(topic, new LimitedLongDeserializer(), standardProps).     kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {          private static final long serialVersionUID = -4834111073247835189L.          @Nullable         @Override         public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {             if (lastElement % 10 == 0) {                 return new Watermark(lastElement).             }             return null.         }          @Override         public long extractTimestamp(Long element, long previousElementTimestamp) {             return previousElementTimestamp.         }     }).     DataStream<Long> stream = env.addSource(kafkaSource).     GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class).     stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1).     env.execute("Consume again").     deleteTestTopic(topic). }
false;public;1;4;;@Override public void invoke(Long value) throws Exception {     throw new RuntimeException("Unexpected"). }
false;public;1;7;;@Override public void processElement(StreamRecord<Long> element) throws Exception {     elCount++.     if (element.getValue() * 2 != element.getTimestamp()) {         throw new RuntimeException("Invalid timestamp: " + element).     } }
false;public;1;14;;@Override public void processWatermark(Watermark mark) throws Exception {     wmCount++.     if (lastWM <= mark.getTimestamp()) {         lastWM = mark.getTimestamp().     } else {         throw new RuntimeException("Received watermark higher than the last one").     }     if (mark.getTimestamp() % 10 != 0 && mark.getTimestamp() != Long.MAX_VALUE) {         throw new RuntimeException("Invalid watermark: " + mark.getTimestamp()).     } }
false;public;0;11;;@Override public void close() throws Exception {     super.close().     if (elCount != 1000L) {         throw new RuntimeException("Wrong final element count " + elCount).     }     if (wmCount <= 2) {         throw new RuntimeException("Almost no watermarks have been sent " + wmCount).     } }
false;public;0;4;;@Override public TypeInformation<Long> getProducedType() {     return ti. }
false;public;1;7;;@Override public Long deserialize(ConsumerRecord<byte[], byte[]> record) throws IOException {     cnt++.     DataInputView in = new DataInputViewStreamWrapper(new ByteArrayInputStream(record.value())).     Long e = ser.deserialize(in).     return e. }
false;public;1;4;;@Override public boolean isEndOfStream(Long nextElement) {     return cnt > 1000L. }
