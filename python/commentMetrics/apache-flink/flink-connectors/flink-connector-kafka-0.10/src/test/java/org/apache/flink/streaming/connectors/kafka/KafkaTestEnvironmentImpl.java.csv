commented;modifiers;parameterAmount;loc;comment;code
false;public;0;3;;public String getBrokerConnectionString() {     return brokerConnectionString. }
false;public;0;4;;@Override public Properties getStandardProperties() {     return standardProps. }
false;public;0;15;;@Override public Properties getSecureProperties() {     Properties prop = new Properties().     if (config.isSecureMode()) {         prop.put("security.inter.broker.protocol", "SASL_PLAINTEXT").         prop.put("security.protocol", "SASL_PLAINTEXT").         prop.put("sasl.kerberos.service.name", "kafka").         // add special timeout for Travis         prop.setProperty("zookeeper.session.timeout.ms", String.valueOf(zkTimeout)).         prop.setProperty("zookeeper.connection.timeout.ms", String.valueOf(zkTimeout)).         prop.setProperty("metadata.fetch.timeout.ms", "120000").     }     return prop. }
false;public;0;4;;@Override public String getVersion() {     return "0.10". }
false;public;0;4;;@Override public List<KafkaServer> getBrokers() {     return brokers. }
false;public;3;4;;@Override public <T> FlinkKafkaConsumerBase<T> getConsumer(List<String> topics, KafkaDeserializationSchema<T> readSchema, Properties props) {     return new FlinkKafkaConsumer010<>(topics, readSchema, props). }
false;public;4;27;;@Override public <K, V> Collection<ConsumerRecord<K, V>> getAllRecordsFromTopic(Properties properties, String topic, int partition, long timeout) {     List<ConsumerRecord<K, V>> result = new ArrayList<>().     try (KafkaConsumer<K, V> consumer = new KafkaConsumer<>(properties)) {         consumer.assign(Arrays.asList(new TopicPartition(topic, partition))).         while (true) {             boolean processedAtLeastOneRecord = false.             // wait for new records with timeout and break the loop if we didn't get any             Iterator<ConsumerRecord<K, V>> iterator = consumer.poll(timeout).iterator().             while (iterator.hasNext()) {                 ConsumerRecord<K, V> record = iterator.next().                 result.add(record).                 processedAtLeastOneRecord = true.             }             if (!processedAtLeastOneRecord) {                 break.             }         }         consumer.commitSync().     }     return UnmodifiableList.decorate(result). }
false;public;4;6;;@Override public <T> StreamSink<T> getProducerSink(String topic, KeyedSerializationSchema<T> serSchema, Properties props, FlinkKafkaPartitioner<T> partitioner) {     FlinkKafkaProducer010<T> prod = new FlinkKafkaProducer010<>(topic, serSchema, props, partitioner).     prod.setFlushOnCheckpoint(true).     return new StreamSink<>(prod). }
false;public;5;6;;@Override public <T> DataStreamSink<T> produceIntoKafka(DataStream<T> stream, String topic, KeyedSerializationSchema<T> serSchema, Properties props, FlinkKafkaPartitioner<T> partitioner) {     FlinkKafkaProducer010<T> prod = new FlinkKafkaProducer010<>(topic, serSchema, props, partitioner).     prod.setFlushOnCheckpoint(true).     return stream.addSink(prod). }
false;public;4;7;;@Override public <T> DataStreamSink<T> writeToKafkaWithTimestamps(DataStream<T> stream, String topic, KeyedSerializationSchema<T> serSchema, Properties props) {     FlinkKafkaProducer010<T> prod = new FlinkKafkaProducer010<>(topic, serSchema, props).     prod.setFlushOnCheckpoint(true).     prod.setWriteTimestampToKafka(true).     return stream.addSink(prod). }
false;public;0;4;;@Override public KafkaOffsetHandler createOffsetHandler() {     return new KafkaOffsetHandlerImpl(). }
false;public;1;4;;@Override public void restartBroker(int leaderId) throws Exception {     brokers.set(leaderId, getKafkaServer(leaderId, tmpKafkaDirs.get(leaderId))). }
false;public;1;22;;@Override public int getLeaderToShutDown(String topic) throws Exception {     ZkUtils zkUtils = getZkUtils().     try {         MetadataResponse.PartitionMetadata firstPart = null.         do {             if (firstPart != null) {                 LOG.info("Unable to find leader. error code {}", firstPart.error().code()).                 // not the first try. Sleep a bit                 Thread.sleep(150).             }             List<MetadataResponse.PartitionMetadata> partitionMetadata = AdminUtils.fetchTopicMetadataFromZk(topic, zkUtils).partitionMetadata().             firstPart = partitionMetadata.get(0).         } while (firstPart.error().code() != 0).         return firstPart.leader().id().     } finally {         zkUtils.close().     } }
false;public;1;4;;@Override public int getBrokerId(KafkaServer server) {     return server.config().brokerId(). }
false;public;0;4;;@Override public boolean isSecureRunSupported() {     return true. }
false;public;1;60;;@Override public void prepare(Config config) {     // increase the timeout since in Travis ZK connection takes long time for secure connection.     if (config.isSecureMode()) {         // run only one kafka server to avoid multiple ZK connections from many instances - Travis timeout         config.setKafkaServersNumber(1).         zkTimeout = zkTimeout * 15.     }     this.config = config.     File tempDir = new File(System.getProperty("java.io.tmpdir")).     tmpZkDir = new File(tempDir, "kafkaITcase-zk-dir-" + (UUID.randomUUID().toString())).     assertTrue("cannot create zookeeper temp dir", tmpZkDir.mkdirs()).     tmpKafkaParent = new File(tempDir, "kafkaITcase-kafka-dir-" + (UUID.randomUUID().toString())).     assertTrue("cannot create kafka temp dir", tmpKafkaParent.mkdirs()).     tmpKafkaDirs = new ArrayList<>(config.getKafkaServersNumber()).     for (int i = 0. i < config.getKafkaServersNumber(). i++) {         File tmpDir = new File(tmpKafkaParent, "server-" + i).         assertTrue("cannot create kafka temp dir", tmpDir.mkdir()).         tmpKafkaDirs.add(tmpDir).     }     zookeeper = null.     brokers = null.     try {         zookeeper = new TestingServer(-1, tmpZkDir).         zookeeperConnectionString = zookeeper.getConnectString().         LOG.info("Starting Zookeeper with zookeeperConnectionString: {}", zookeeperConnectionString).         LOG.info("Starting KafkaServer").         brokers = new ArrayList<>(config.getKafkaServersNumber()).         ListenerName listenerName = ListenerName.forSecurityProtocol(config.isSecureMode() ? SecurityProtocol.SASL_PLAINTEXT : SecurityProtocol.PLAINTEXT).         for (int i = 0. i < config.getKafkaServersNumber(). i++) {             KafkaServer kafkaServer = getKafkaServer(i, tmpKafkaDirs.get(i)).             brokers.add(kafkaServer).             brokerConnectionString += hostAndPortToUrlString(KAFKA_HOST, kafkaServer.socketServer().boundPort(listenerName)).             brokerConnectionString += ",".         }         LOG.info("ZK and KafkaServer started.").     } catch (Throwable t) {         t.printStackTrace().         fail("Test setup failed: " + t.getMessage()).     }     standardProps = new Properties().     standardProps.setProperty("zookeeper.connect", zookeeperConnectionString).     standardProps.setProperty("bootstrap.servers", brokerConnectionString).     standardProps.setProperty("group.id", "flink-tests").     standardProps.setProperty("enable.auto.commit", "false").     standardProps.setProperty("zookeeper.session.timeout.ms", String.valueOf(zkTimeout)).     standardProps.setProperty("zookeeper.connection.timeout.ms", String.valueOf(zkTimeout)).     // read from the beginning. (earliest is kafka 0.10 value)     standardProps.setProperty("auto.offset.reset", "earliest").     // make a lot of fetches (MESSAGES MUST BE SMALLER!)     standardProps.setProperty("max.partition.fetch.bytes", "256"). }
false;public;0;39;;@Override public void shutdown() throws Exception {     for (KafkaServer broker : brokers) {         if (broker != null) {             broker.shutdown().         }     }     brokers.clear().     if (zookeeper != null) {         try {             zookeeper.stop().         } catch (Exception e) {             LOG.warn("ZK.stop() failed", e).         }         zookeeper = null.     }     if (tmpKafkaParent != null && tmpKafkaParent.exists()) {         try {             FileUtils.deleteDirectory(tmpKafkaParent).         } catch (Exception e) {         // ignore         }     }     if (tmpZkDir != null && tmpZkDir.exists()) {         try {             FileUtils.deleteDirectory(tmpZkDir).         } catch (Exception e) {         // ignore         }     }     super.shutdown(). }
false;public;0;5;;public ZkUtils getZkUtils() {     ZkClient creator = new ZkClient(zookeeperConnectionString, Integer.valueOf(standardProps.getProperty("zookeeper.session.timeout.ms")), Integer.valueOf(standardProps.getProperty("zookeeper.connection.timeout.ms")), new ZooKeeperStringSerializer()).     return ZkUtils.apply(creator, false). }
false;public;4;41;;@Override public void createTestTopic(String topic, int numberOfPartitions, int replicationFactor, Properties topicConfig) {     // create topic with one client     LOG.info("Creating topic {}", topic).     ZkUtils zkUtils = getZkUtils().     try {         AdminUtils.createTopic(zkUtils, topic, numberOfPartitions, replicationFactor, topicConfig, kafka.admin.RackAwareMode.Enforced$.MODULE$).     } finally {         zkUtils.close().     }     // validate that the topic has been created     final long deadline = System.nanoTime() + 30_000_000_000L.     do {         try {             if (config.isSecureMode()) {                 // increase wait time since in Travis ZK timeout occurs frequently                 int wait = zkTimeout / 100.                 LOG.info("waiting for {} msecs before the topic {} can be checked", wait, topic).                 Thread.sleep(wait).             } else {                 Thread.sleep(100).             }         } catch (InterruptedException e) {         // restore interrupted state         }         // we could use AdminUtils.topicExists(zkUtils, topic) here, but it's results are         // not always correct.         // create a new ZK utils connection         ZkUtils checkZKConn = getZkUtils().         if (AdminUtils.topicExists(checkZKConn, topic)) {             checkZKConn.close().             return.         }         checkZKConn.close().     } while (System.nanoTime() < deadline).     fail("Test topic could not be created"). }
false;public;1;16;;@Override public void deleteTestTopic(String topic) {     ZkUtils zkUtils = getZkUtils().     try {         LOG.info("Deleting topic {}", topic).         ZkClient zk = new ZkClient(zookeeperConnectionString, Integer.valueOf(standardProps.getProperty("zookeeper.session.timeout.ms")), Integer.valueOf(standardProps.getProperty("zookeeper.connection.timeout.ms")), new ZooKeeperStringSerializer()).         AdminUtils.deleteTopic(zkUtils, topic).         zk.close().     } finally {         zkUtils.close().     } }
true;protected;2;58;/**  * Copied from com.github.sakserv.minicluster.KafkaLocalBrokerIntegrationTest (ASL licensed).  */ ;/**  * Copied from com.github.sakserv.minicluster.KafkaLocalBrokerIntegrationTest (ASL licensed).  */ protected KafkaServer getKafkaServer(int brokerId, File tmpFolder) throws Exception {     Properties kafkaProperties = new Properties().     // properties have to be Strings     kafkaProperties.put("advertised.host.name", KAFKA_HOST).     kafkaProperties.put("broker.id", Integer.toString(brokerId)).     kafkaProperties.put("log.dir", tmpFolder.toString()).     kafkaProperties.put("zookeeper.connect", zookeeperConnectionString).     kafkaProperties.put("message.max.bytes", String.valueOf(50 * 1024 * 1024)).     kafkaProperties.put("replica.fetch.max.bytes", String.valueOf(50 * 1024 * 1024)).     // for CI stability, increase zookeeper session timeout     kafkaProperties.put("zookeeper.session.timeout.ms", zkTimeout).     kafkaProperties.put("zookeeper.connection.timeout.ms", zkTimeout).     if (config.getKafkaServerProperties() != null) {         kafkaProperties.putAll(config.getKafkaServerProperties()).     }     final int numTries = 5.     for (int i = 1. i <= numTries. i++) {         int kafkaPort = NetUtils.getAvailablePort().         kafkaProperties.put("port", Integer.toString(kafkaPort)).         if (config.isHideKafkaBehindProxy()) {             NetworkFailuresProxy proxy = createProxy(KAFKA_HOST, kafkaPort).             kafkaProperties.put("advertised.port", proxy.getLocalPort()).         }         // to support secure kafka cluster         if (config.isSecureMode()) {             LOG.info("Adding Kafka secure configurations").             kafkaProperties.put("listeners", "SASL_PLAINTEXT://" + KAFKA_HOST + ":" + kafkaPort).             kafkaProperties.put("advertised.listeners", "SASL_PLAINTEXT://" + KAFKA_HOST + ":" + kafkaPort).             kafkaProperties.putAll(getSecureProperties()).         }         KafkaConfig kafkaConfig = new KafkaConfig(kafkaProperties).         try {             scala.Option<String> stringNone = scala.Option.apply(null).             KafkaServer server = new KafkaServer(kafkaConfig, Time.SYSTEM, stringNone, new ArraySeq<KafkaMetricsReporter>(0)).             server.startup().             return server.         } catch (KafkaException e) {             if (e.getCause() instanceof BindException) {                 // port conflict, retry...                 LOG.info("Port conflict when starting Kafka Broker. Retrying...").             } else {                 throw e.             }         }     }     throw new Exception("Could not start Kafka after " + numTries + " retries due to port conflicts."). }
false;public;2;5;;@Override public Long getCommittedOffset(String topicName, int partition) {     OffsetAndMetadata committed = offsetClient.committed(new TopicPartition(topicName, partition)).     return (committed != null) ? committed.offset() : null. }
false;public;3;6;;@Override public void setCommittedOffset(String topicName, int partition, long offset) {     Map<TopicPartition, OffsetAndMetadata> partitionAndOffset = new HashMap<>().     partitionAndOffset.put(new TopicPartition(topicName, partition), new OffsetAndMetadata(offset)).     offsetClient.commitSync(partitionAndOffset). }
false;public;0;4;;@Override public void close() {     offsetClient.close(). }
