# id;timestamp;commentText;codeText;commentWords;codeWords
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1509616757;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorStateHandles snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,state,handles,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1509723634;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorStateHandles snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,state,handles,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1511180690;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorStateHandles snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,state,handles,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1511444658;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorStateHandles snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,state,handles,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1511444658;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorStateHandles snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,state,handles,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1515177485;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorStateHandles snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,state,handles,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1518433170;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorStateHandles snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,state,handles,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1519567828;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1519568061;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1521662511;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1525661787;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1526978550;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1534491183;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1541587130;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1541587192;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42))__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1549282380;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42))__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1549282380;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42))__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1550863152;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42))__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1521662511;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1525661787;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1526978550;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1534491183;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1541587130;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46), 30_000L)___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1541587192;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46))___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1549282380;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46))___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1549282380;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46))___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception;1550863152;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testFailBeforeNotifyAndResumeWorkAfterwards() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		OneInputStreamOperatorTestHarness<Integer, Object> testHarness = createTestHarness(topic)___		testHarness.setup()__		testHarness.open()__		testHarness.processElement(42, 0)__		testHarness.snapshot(0, 1)__		testHarness.processElement(43, 2)__		OperatorSubtaskState snapshot1 = testHarness.snapshot(1, 3)___		testHarness.processElement(44, 4)__		testHarness.snapshot(2, 5)__		testHarness.processElement(45, 6)___		_		_		testHarness = createTestHarness(topic)__		testHarness.setup()__		_		testHarness.initializeState(snapshot1)__		testHarness.open()___		_		testHarness.processElement(46, 7)__		testHarness.snapshot(4, 8)__		testHarness.processElement(47, 9)__		testHarness.notifyOfCompletedCheckpoint(4)___		_		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42, 43, 46))___		testHarness.close()__		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,fail,before,notify,and,resume,work,afterwards,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,one,input,stream,operator,test,harness,integer,object,test,harness,create,test,harness,topic,test,harness,setup,test,harness,open,test,harness,process,element,42,0,test,harness,snapshot,0,1,test,harness,process,element,43,2,operator,subtask,state,snapshot1,test,harness,snapshot,1,3,test,harness,process,element,44,4,test,harness,snapshot,2,5,test,harness,process,element,45,6,test,harness,create,test,harness,topic,test,harness,setup,test,harness,initialize,state,snapshot1,test,harness,open,test,harness,process,element,46,7,test,harness,snapshot,4,8,test,harness,process,element,47,9,test,harness,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,43,46,test,harness,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleUpAfterScalingDown() throws Exception;1521662511;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorSubtaskState = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,subtask,state,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleUpAfterScalingDown() throws Exception;1525661787;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorSubtaskState = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,subtask,state,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleUpAfterScalingDown() throws Exception;1526978550;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorSubtaskState = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,subtask,state,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleUpAfterScalingDown() throws Exception;1534491183;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorSubtaskState = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,subtask,state,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleUpAfterScalingDown() throws Exception;1541587130;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorSubtaskState = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,subtask,state,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleUpAfterScalingDown() throws Exception;1541587192;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorSubtaskState = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()))__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,subtask,state,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleUpAfterScalingDown() throws Exception;1549282380;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorSubtaskState = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()))__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,subtask,state,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleUpAfterScalingDown() throws Exception;1549282380;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorSubtaskState = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()))__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,subtask,state,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleUpAfterScalingDown() throws Exception;1550863152;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		OperatorSubtaskState operatorSubtaskState = repartitionAndExecute(_			topic,_			new OperatorSubtaskState(),_			parallelism1,_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism1,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()))__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,operator,subtask,state,operator,subtask,state,repartition,and,execute,topic,new,operator,subtask,state,parallelism1,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism1,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,repartition,and,execute,topic,operator,subtask,state,parallelism3,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1509616757;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1509723634;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1511180690;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1511444658;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1511444658;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1515177485;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1518433170;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1519567828;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1519568061;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test(timeout = 120_000L)_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,timeout,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1511444658;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test(timeout = 120_000L)_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorStateHandles snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				assertIsCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)__			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			assertIsCausedBy(ProducerFencedException.class, ex)__		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,timeout,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,state,handles,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,assert,is,caused,by,flink,kafka011error,code,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,assert,is,caused,by,producer,fenced,exception,class,ex
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1511444658;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test(timeout = 120_000L)_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorStateHandles snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				assertIsCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)__			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			assertIsCausedBy(ProducerFencedException.class, ex)__		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,timeout,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,state,handles,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,assert,is,caused,by,flink,kafka011error,code,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,assert,is,caused,by,producer,fenced,exception,class,ex
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1515177485;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test(timeout = 120_000L)_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorStateHandles snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				assertIsCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)__			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			assertIsCausedBy(ProducerFencedException.class, ex)__		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,timeout,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,state,handles,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,assert,is,caused,by,flink,kafka011error,code,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,assert,is,caused,by,producer,fenced,exception,class,ex
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1518433170;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test(timeout = 120_000L)_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorStateHandles snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,timeout,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,state,handles,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1519567828;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test(timeout = 120_000L)_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,timeout,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception;1519568061;This test ensures that transactions reusing transactional.ids (after returning to the pool) will not clash_with previous transactions using same transactional.ids.;@Test(timeout = 120_000L)_	public void testRestoreToCheckpointAfterExceedingProducersPool() throws Exception {_		String topic = "flink-kafka-producer-fail-before-notify"___		try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 = createTestHarness(topic)) {_			testHarness1.setup()__			testHarness1.open()__			testHarness1.processElement(42, 0)__			OperatorSubtaskState snapshot = testHarness1.snapshot(0, 0)__			testHarness1.processElement(43, 0)__			testHarness1.notifyOfCompletedCheckpoint(0)__			try {_				for (int i = 0_ i < FlinkKafkaProducer011.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE_ i++) {_					testHarness1.snapshot(i + 1, 0)__					testHarness1.processElement(i, 0)__				}_				throw new IllegalStateException("This should not be reached.")__			}_			catch (Exception ex) {_				if (!isCausedBy(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, ex)) {_					throw ex__				}_			}__			_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness2 = createTestHarness(topic)) {_				testHarness2.setup()__				_				testHarness2.initializeState(snapshot)__				testHarness2.open()__			}__			assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(42), 30_000L)__			deleteTestTopic(topic)__		}_		catch (Exception ex) {_			_			if (!findThrowable(ex, ProducerFencedException.class).isPresent()) {_				throw ex__			}_		}_	};this,test,ensures,that,transactions,reusing,transactional,ids,after,returning,to,the,pool,will,not,clash,with,previous,transactions,using,same,transactional,ids;test,timeout,public,void,test,restore,to,checkpoint,after,exceeding,producers,pool,throws,exception,string,topic,flink,kafka,producer,fail,before,notify,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,test,harness1,setup,test,harness1,open,test,harness1,process,element,42,0,operator,subtask,state,snapshot,test,harness1,snapshot,0,0,test,harness1,process,element,43,0,test,harness1,notify,of,completed,checkpoint,0,try,for,int,i,0,i,flink,kafka,producer011,i,test,harness1,snapshot,i,1,0,test,harness1,process,element,i,0,throw,new,illegal,state,exception,this,should,not,be,reached,catch,exception,ex,if,is,caused,by,flink,kafka011error,code,ex,throw,ex,try,one,input,stream,operator,test,harness,integer,object,test,harness2,create,test,harness,topic,test,harness2,setup,test,harness2,initialize,state,snapshot,test,harness2,open,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,42,delete,test,topic,topic,catch,exception,ex,if,find,throwable,ex,producer,fenced,exception,class,is,present,throw,ex
FlinkKafkaProducer011ITCase -> public void resourceCleanUp(Semantic semantic) throws Exception;1525661787;This tests checks whether there is some resource leak in form of growing threads number.;public void resourceCleanUp(Semantic semantic) throws Exception {_		String topic = "flink-kafka-producer-resource-cleanup-" + semantic___		final int allowedEpsilonThreadCountGrow = 50___		Optional<Integer> initialActiveThreads = Optional.empty()__		for (int i = 0_ i < allowedEpsilonThreadCountGrow * 2_ i++) {_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 =_					createTestHarness(topic, 1, 1, 0, semantic)) {_				testHarness1.setup()__				testHarness1.open()__			}__			if (initialActiveThreads.isPresent()) {_				assertThat("active threads count",_					Thread.activeCount(),_					lessThan(initialActiveThreads.get() + allowedEpsilonThreadCountGrow))__			}_			else {_				initialActiveThreads = Optional.of(Thread.activeCount())__			}_		}_	};this,tests,checks,whether,there,is,some,resource,leak,in,form,of,growing,threads,number;public,void,resource,clean,up,semantic,semantic,throws,exception,string,topic,flink,kafka,producer,resource,cleanup,semantic,final,int,allowed,epsilon,thread,count,grow,50,optional,integer,initial,active,threads,optional,empty,for,int,i,0,i,allowed,epsilon,thread,count,grow,2,i,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,1,1,0,semantic,test,harness1,setup,test,harness1,open,if,initial,active,threads,is,present,assert,that,active,threads,count,thread,active,count,less,than,initial,active,threads,get,allowed,epsilon,thread,count,grow,else,initial,active,threads,optional,of,thread,active,count
FlinkKafkaProducer011ITCase -> public void resourceCleanUp(Semantic semantic) throws Exception;1526978550;This tests checks whether there is some resource leak in form of growing threads number.;public void resourceCleanUp(Semantic semantic) throws Exception {_		String topic = "flink-kafka-producer-resource-cleanup-" + semantic___		final int allowedEpsilonThreadCountGrow = 50___		Optional<Integer> initialActiveThreads = Optional.empty()__		for (int i = 0_ i < allowedEpsilonThreadCountGrow * 2_ i++) {_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 =_					createTestHarness(topic, 1, 1, 0, semantic)) {_				testHarness1.setup()__				testHarness1.open()__			}__			if (initialActiveThreads.isPresent()) {_				assertThat("active threads count",_					Thread.activeCount(),_					lessThan(initialActiveThreads.get() + allowedEpsilonThreadCountGrow))__			}_			else {_				initialActiveThreads = Optional.of(Thread.activeCount())__			}_		}_	};this,tests,checks,whether,there,is,some,resource,leak,in,form,of,growing,threads,number;public,void,resource,clean,up,semantic,semantic,throws,exception,string,topic,flink,kafka,producer,resource,cleanup,semantic,final,int,allowed,epsilon,thread,count,grow,50,optional,integer,initial,active,threads,optional,empty,for,int,i,0,i,allowed,epsilon,thread,count,grow,2,i,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,1,1,0,semantic,test,harness1,setup,test,harness1,open,if,initial,active,threads,is,present,assert,that,active,threads,count,thread,active,count,less,than,initial,active,threads,get,allowed,epsilon,thread,count,grow,else,initial,active,threads,optional,of,thread,active,count
FlinkKafkaProducer011ITCase -> public void resourceCleanUp(Semantic semantic) throws Exception;1534491183;This tests checks whether there is some resource leak in form of growing threads number.;public void resourceCleanUp(Semantic semantic) throws Exception {_		String topic = "flink-kafka-producer-resource-cleanup-" + semantic___		final int allowedEpsilonThreadCountGrow = 50___		Optional<Integer> initialActiveThreads = Optional.empty()__		for (int i = 0_ i < allowedEpsilonThreadCountGrow * 2_ i++) {_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 =_					createTestHarness(topic, 1, 1, 0, semantic)) {_				testHarness1.setup()__				testHarness1.open()__			}__			if (initialActiveThreads.isPresent()) {_				assertThat("active threads count",_					Thread.activeCount(),_					lessThan(initialActiveThreads.get() + allowedEpsilonThreadCountGrow))__			}_			else {_				initialActiveThreads = Optional.of(Thread.activeCount())__			}_		}_	};this,tests,checks,whether,there,is,some,resource,leak,in,form,of,growing,threads,number;public,void,resource,clean,up,semantic,semantic,throws,exception,string,topic,flink,kafka,producer,resource,cleanup,semantic,final,int,allowed,epsilon,thread,count,grow,50,optional,integer,initial,active,threads,optional,empty,for,int,i,0,i,allowed,epsilon,thread,count,grow,2,i,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,1,1,0,semantic,test,harness1,setup,test,harness1,open,if,initial,active,threads,is,present,assert,that,active,threads,count,thread,active,count,less,than,initial,active,threads,get,allowed,epsilon,thread,count,grow,else,initial,active,threads,optional,of,thread,active,count
FlinkKafkaProducer011ITCase -> public void resourceCleanUp(Semantic semantic) throws Exception;1541587130;This tests checks whether there is some resource leak in form of growing threads number.;public void resourceCleanUp(Semantic semantic) throws Exception {_		String topic = "flink-kafka-producer-resource-cleanup-" + semantic___		final int allowedEpsilonThreadCountGrow = 50___		Optional<Integer> initialActiveThreads = Optional.empty()__		for (int i = 0_ i < allowedEpsilonThreadCountGrow * 2_ i++) {_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 =_					createTestHarness(topic, 1, 1, 0, semantic)) {_				testHarness1.setup()__				testHarness1.open()__			}__			if (initialActiveThreads.isPresent()) {_				assertThat("active threads count",_					Thread.activeCount(),_					lessThan(initialActiveThreads.get() + allowedEpsilonThreadCountGrow))__			}_			else {_				initialActiveThreads = Optional.of(Thread.activeCount())__			}_		}_	};this,tests,checks,whether,there,is,some,resource,leak,in,form,of,growing,threads,number;public,void,resource,clean,up,semantic,semantic,throws,exception,string,topic,flink,kafka,producer,resource,cleanup,semantic,final,int,allowed,epsilon,thread,count,grow,50,optional,integer,initial,active,threads,optional,empty,for,int,i,0,i,allowed,epsilon,thread,count,grow,2,i,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,1,1,0,semantic,test,harness1,setup,test,harness1,open,if,initial,active,threads,is,present,assert,that,active,threads,count,thread,active,count,less,than,initial,active,threads,get,allowed,epsilon,thread,count,grow,else,initial,active,threads,optional,of,thread,active,count
FlinkKafkaProducer011ITCase -> public void resourceCleanUp(Semantic semantic) throws Exception;1541587192;This tests checks whether there is some resource leak in form of growing threads number.;public void resourceCleanUp(Semantic semantic) throws Exception {_		String topic = "flink-kafka-producer-resource-cleanup-" + semantic___		final int allowedEpsilonThreadCountGrow = 50___		Optional<Integer> initialActiveThreads = Optional.empty()__		for (int i = 0_ i < allowedEpsilonThreadCountGrow * 2_ i++) {_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 =_					createTestHarness(topic, 1, 1, 0, semantic)) {_				testHarness1.setup()__				testHarness1.open()__			}__			if (initialActiveThreads.isPresent()) {_				assertThat("active threads count",_					Thread.activeCount(),_					lessThan(initialActiveThreads.get() + allowedEpsilonThreadCountGrow))__			}_			else {_				initialActiveThreads = Optional.of(Thread.activeCount())__			}_		}_	};this,tests,checks,whether,there,is,some,resource,leak,in,form,of,growing,threads,number;public,void,resource,clean,up,semantic,semantic,throws,exception,string,topic,flink,kafka,producer,resource,cleanup,semantic,final,int,allowed,epsilon,thread,count,grow,50,optional,integer,initial,active,threads,optional,empty,for,int,i,0,i,allowed,epsilon,thread,count,grow,2,i,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,1,1,0,semantic,test,harness1,setup,test,harness1,open,if,initial,active,threads,is,present,assert,that,active,threads,count,thread,active,count,less,than,initial,active,threads,get,allowed,epsilon,thread,count,grow,else,initial,active,threads,optional,of,thread,active,count
FlinkKafkaProducer011ITCase -> public void resourceCleanUp(Semantic semantic) throws Exception;1549282380;This tests checks whether there is some resource leak in form of growing threads number.;public void resourceCleanUp(Semantic semantic) throws Exception {_		String topic = "flink-kafka-producer-resource-cleanup-" + semantic___		final int allowedEpsilonThreadCountGrow = 50___		Optional<Integer> initialActiveThreads = Optional.empty()__		for (int i = 0_ i < allowedEpsilonThreadCountGrow * 2_ i++) {_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 =_					createTestHarness(topic, 1, 1, 0, semantic)) {_				testHarness1.setup()__				testHarness1.open()__			}__			if (initialActiveThreads.isPresent()) {_				assertThat("active threads count",_					Thread.activeCount(),_					lessThan(initialActiveThreads.get() + allowedEpsilonThreadCountGrow))__			}_			else {_				initialActiveThreads = Optional.of(Thread.activeCount())__			}_		}_	};this,tests,checks,whether,there,is,some,resource,leak,in,form,of,growing,threads,number;public,void,resource,clean,up,semantic,semantic,throws,exception,string,topic,flink,kafka,producer,resource,cleanup,semantic,final,int,allowed,epsilon,thread,count,grow,50,optional,integer,initial,active,threads,optional,empty,for,int,i,0,i,allowed,epsilon,thread,count,grow,2,i,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,1,1,0,semantic,test,harness1,setup,test,harness1,open,if,initial,active,threads,is,present,assert,that,active,threads,count,thread,active,count,less,than,initial,active,threads,get,allowed,epsilon,thread,count,grow,else,initial,active,threads,optional,of,thread,active,count
FlinkKafkaProducer011ITCase -> public void resourceCleanUp(Semantic semantic) throws Exception;1549282380;This tests checks whether there is some resource leak in form of growing threads number.;public void resourceCleanUp(Semantic semantic) throws Exception {_		String topic = "flink-kafka-producer-resource-cleanup-" + semantic___		final int allowedEpsilonThreadCountGrow = 50___		Optional<Integer> initialActiveThreads = Optional.empty()__		for (int i = 0_ i < allowedEpsilonThreadCountGrow * 2_ i++) {_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 =_					createTestHarness(topic, 1, 1, 0, semantic)) {_				testHarness1.setup()__				testHarness1.open()__			}__			if (initialActiveThreads.isPresent()) {_				assertThat("active threads count",_					Thread.activeCount(),_					lessThan(initialActiveThreads.get() + allowedEpsilonThreadCountGrow))__			}_			else {_				initialActiveThreads = Optional.of(Thread.activeCount())__			}_		}_	};this,tests,checks,whether,there,is,some,resource,leak,in,form,of,growing,threads,number;public,void,resource,clean,up,semantic,semantic,throws,exception,string,topic,flink,kafka,producer,resource,cleanup,semantic,final,int,allowed,epsilon,thread,count,grow,50,optional,integer,initial,active,threads,optional,empty,for,int,i,0,i,allowed,epsilon,thread,count,grow,2,i,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,1,1,0,semantic,test,harness1,setup,test,harness1,open,if,initial,active,threads,is,present,assert,that,active,threads,count,thread,active,count,less,than,initial,active,threads,get,allowed,epsilon,thread,count,grow,else,initial,active,threads,optional,of,thread,active,count
FlinkKafkaProducer011ITCase -> public void resourceCleanUp(Semantic semantic) throws Exception;1550863152;This tests checks whether there is some resource leak in form of growing threads number.;public void resourceCleanUp(Semantic semantic) throws Exception {_		String topic = "flink-kafka-producer-resource-cleanup-" + semantic___		final int allowedEpsilonThreadCountGrow = 50___		Optional<Integer> initialActiveThreads = Optional.empty()__		for (int i = 0_ i < allowedEpsilonThreadCountGrow * 2_ i++) {_			try (OneInputStreamOperatorTestHarness<Integer, Object> testHarness1 =_					createTestHarness(topic, 1, 1, 0, semantic)) {_				testHarness1.setup()__				testHarness1.open()__			}__			if (initialActiveThreads.isPresent()) {_				assertThat("active threads count",_					Thread.activeCount(),_					lessThan(initialActiveThreads.get() + allowedEpsilonThreadCountGrow))__			}_			else {_				initialActiveThreads = Optional.of(Thread.activeCount())__			}_		}_	};this,tests,checks,whether,there,is,some,resource,leak,in,form,of,growing,threads,number;public,void,resource,clean,up,semantic,semantic,throws,exception,string,topic,flink,kafka,producer,resource,cleanup,semantic,final,int,allowed,epsilon,thread,count,grow,50,optional,integer,initial,active,threads,optional,empty,for,int,i,0,i,allowed,epsilon,thread,count,grow,2,i,try,one,input,stream,operator,test,harness,integer,object,test,harness1,create,test,harness,topic,1,1,0,semantic,test,harness1,setup,test,harness1,open,if,initial,active,threads,is,present,assert,that,active,threads,count,thread,active,count,less,than,initial,active,threads,get,allowed,epsilon,thread,count,grow,else,initial,active,threads,optional,of,thread,active,count
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleUpAfterScalingDown() throws Exception;1509616757;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test(timeout = 120_000L)_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorStateHandles = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,timeout,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,state,handles,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleUpAfterScalingDown() throws Exception;1509723634;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test(timeout = 120_000L)_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorStateHandles = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,timeout,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,state,handles,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleUpAfterScalingDown() throws Exception;1511180690;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test(timeout = 120_000L)_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorStateHandles = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,timeout,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,state,handles,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleUpAfterScalingDown() throws Exception;1511444658;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test(timeout = 120_000L)_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorStateHandles = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,timeout,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,state,handles,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleUpAfterScalingDown() throws Exception;1511444658;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test(timeout = 120_000L)_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorStateHandles = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,timeout,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,state,handles,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleUpAfterScalingDown() throws Exception;1515177485;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test(timeout = 120_000L)_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorStateHandles = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,timeout,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,state,handles,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleUpAfterScalingDown() throws Exception;1518433170;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test(timeout = 120_000L)_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorStateHandles = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,timeout,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,state,handles,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleUpAfterScalingDown() throws Exception;1519567828;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test(timeout = 120_000L)_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorStateHandles = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorStateHandles = repartitionAndExecute(_			topic,_			operatorStateHandles,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,timeout,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,state,handles,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,state,handles,repartition,and,execute,topic,operator,state,handles,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test(timeout = 120_000L) 	public void testScaleUpAfterScalingDown() throws Exception;1519568061;Each instance of FlinkKafkaProducer011 uses it's own pool of transactional ids. After the restore from checkpoint_transactional ids are redistributed across the subtasks. In case of scale down, the surplus transactional ids_are dropped. In case of scale up, new one are generated (for the new subtasks). This test make sure that sequence_of scaling down and up again works fine. Especially it checks whether the newly generated ids in scaling up_do not overlap with ids that were used before scaling down. For example we start with 4 ids and parallelism 4:_[1], [2], [3], [4] - one assigned per each subtask_we scale down to parallelism 2:_[1, 2], [3, 4] - first subtask got id 1 and 2, second got ids 3 and 4_surplus ids are dropped from the pools and we scale up to parallelism 3:_[1 or 2], [3 or 4], [???]_new subtask have to generate new id(s), but he can not use ids that are potentially in use, so it has to generate_new ones that are greater then 4.;@Test(timeout = 120_000L)_	public void testScaleUpAfterScalingDown() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		final int parallelism1 = 4__		final int parallelism2 = 2__		final int parallelism3 = 3__		final int maxParallelism = Math.max(parallelism1, Math.max(parallelism2, parallelism3))___		List<OperatorStateHandle> operatorSubtaskState = repartitionAndExecute(_			topic,_			Collections.emptyList(),_			parallelism1,_			maxParallelism,_			IntStream.range(0, parallelism1).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism2,_			maxParallelism,_			IntStream.range(parallelism1,  parallelism1 + parallelism2).boxed().iterator())___		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			parallelism3,_			maxParallelism,_			IntStream.range(parallelism1 + parallelism2,  parallelism1 + parallelism2 + parallelism3).boxed().iterator())___		_		_		__		operatorSubtaskState = repartitionAndExecute(_			topic,_			operatorSubtaskState,_			1,_			maxParallelism,_			Collections.emptyIterator())___		assertExactlyOnceForTopic(_			createProperties(),_			topic,_			0,_			IntStream.range(0, parallelism1 + parallelism2 + parallelism3).boxed().collect(Collectors.toList()),_			30_000L)__		deleteTestTopic(topic)__	};each,instance,of,flink,kafka,producer011,uses,it,s,own,pool,of,transactional,ids,after,the,restore,from,checkpoint,transactional,ids,are,redistributed,across,the,subtasks,in,case,of,scale,down,the,surplus,transactional,ids,are,dropped,in,case,of,scale,up,new,one,are,generated,for,the,new,subtasks,this,test,make,sure,that,sequence,of,scaling,down,and,up,again,works,fine,especially,it,checks,whether,the,newly,generated,ids,in,scaling,up,do,not,overlap,with,ids,that,were,used,before,scaling,down,for,example,we,start,with,4,ids,and,parallelism,4,1,2,3,4,one,assigned,per,each,subtask,we,scale,down,to,parallelism,2,1,2,3,4,first,subtask,got,id,1,and,2,second,got,ids,3,and,4,surplus,ids,are,dropped,from,the,pools,and,we,scale,up,to,parallelism,3,1,or,2,3,or,4,new,subtask,have,to,generate,new,id,s,but,he,can,not,use,ids,that,are,potentially,in,use,so,it,has,to,generate,new,ones,that,are,greater,then,4;test,timeout,public,void,test,scale,up,after,scaling,down,throws,exception,string,topic,scale,down,before,first,checkpoint,final,int,parallelism1,4,final,int,parallelism2,2,final,int,parallelism3,3,final,int,max,parallelism,math,max,parallelism1,math,max,parallelism2,parallelism3,list,operator,state,handle,operator,subtask,state,repartition,and,execute,topic,collections,empty,list,parallelism1,max,parallelism,int,stream,range,0,parallelism1,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism2,max,parallelism,int,stream,range,parallelism1,parallelism1,parallelism2,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,parallelism3,max,parallelism,int,stream,range,parallelism1,parallelism2,parallelism1,parallelism2,parallelism3,boxed,iterator,operator,subtask,state,repartition,and,execute,topic,operator,subtask,state,1,max,parallelism,collections,empty,iterator,assert,exactly,once,for,topic,create,properties,topic,0,int,stream,range,0,parallelism1,parallelism2,parallelism3,boxed,collect,collectors,to,list,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1521662511;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1525661787;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex,_				Semantic.EXACTLY_ONCE)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0, Semantic.EXACTLY_ONCE)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,semantic,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,semantic,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1526978550;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex,_				Semantic.EXACTLY_ONCE)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0, Semantic.EXACTLY_ONCE)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,semantic,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,semantic,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1534491183;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex,_				Semantic.EXACTLY_ONCE)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0, Semantic.EXACTLY_ONCE)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,semantic,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,semantic,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1541587130;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex,_				EXACTLY_ONCE)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0, EXACTLY_ONCE)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46), 30_000L)___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1541587192;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex,_				EXACTLY_ONCE)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0, EXACTLY_ONCE)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46))___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1549282380;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex,_				EXACTLY_ONCE)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0, EXACTLY_ONCE)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46))___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1549282380;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex,_				EXACTLY_ONCE)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0, EXACTLY_ONCE)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46))___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
FlinkKafkaProducer011ITCase -> @Test 	public void testScaleDownBeforeFirstCheckpoint() throws Exception;1550863152;This tests checks whether FlinkKafkaProducer011 correctly aborts lingering transactions after a failure,_which happened before first checkpoint and was followed up by reducing the parallelism._If such transactions were left alone lingering it consumers would be unable to read committed records_that were created after this lingering transaction.;@Test_	public void testScaleDownBeforeFirstCheckpoint() throws Exception {_		String topic = "scale-down-before-first-checkpoint"___		List<AutoCloseable> operatorsToClose = new ArrayList<>()__		int preScaleDownParallelism = Math.max(2, FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR)__		for (int subtaskIndex = 0_ subtaskIndex < preScaleDownParallelism_ subtaskIndex++) {_			OneInputStreamOperatorTestHarness<Integer, Object> preScaleDownOperator = createTestHarness(_				topic,_				preScaleDownParallelism,_				preScaleDownParallelism,_				subtaskIndex,_				EXACTLY_ONCE)___			preScaleDownOperator.setup()__			preScaleDownOperator.open()__			preScaleDownOperator.processElement(subtaskIndex * 2, 0)__			preScaleDownOperator.snapshot(0, 1)__			preScaleDownOperator.processElement(subtaskIndex * 2 + 1, 2)___			operatorsToClose.add(preScaleDownOperator)__		}__		_		__		_		OneInputStreamOperatorTestHarness<Integer, Object> postScaleDownOperator1 = createTestHarness(topic, 1, 1, 0, EXACTLY_ONCE)___		postScaleDownOperator1.setup()__		postScaleDownOperator1.open()___		_		postScaleDownOperator1.processElement(46, 7)__		postScaleDownOperator1.snapshot(4, 8)__		postScaleDownOperator1.processElement(47, 9)__		postScaleDownOperator1.notifyOfCompletedCheckpoint(4)___		_		_		_		_		assertExactlyOnceForTopic(createProperties(), topic, 0, Arrays.asList(46))___		postScaleDownOperator1.close()__		_		for (AutoCloseable operatorToClose : operatorsToClose) {_			closeIgnoringProducerFenced(operatorToClose)__		}_		deleteTestTopic(topic)__	};this,tests,checks,whether,flink,kafka,producer011,correctly,aborts,lingering,transactions,after,a,failure,which,happened,before,first,checkpoint,and,was,followed,up,by,reducing,the,parallelism,if,such,transactions,were,left,alone,lingering,it,consumers,would,be,unable,to,read,committed,records,that,were,created,after,this,lingering,transaction;test,public,void,test,scale,down,before,first,checkpoint,throws,exception,string,topic,scale,down,before,first,checkpoint,list,auto,closeable,operators,to,close,new,array,list,int,pre,scale,down,parallelism,math,max,2,flink,kafka,producer011,for,int,subtask,index,0,subtask,index,pre,scale,down,parallelism,subtask,index,one,input,stream,operator,test,harness,integer,object,pre,scale,down,operator,create,test,harness,topic,pre,scale,down,parallelism,pre,scale,down,parallelism,subtask,index,pre,scale,down,operator,setup,pre,scale,down,operator,open,pre,scale,down,operator,process,element,subtask,index,2,0,pre,scale,down,operator,snapshot,0,1,pre,scale,down,operator,process,element,subtask,index,2,1,2,operators,to,close,add,pre,scale,down,operator,one,input,stream,operator,test,harness,integer,object,post,scale,down,operator1,create,test,harness,topic,1,1,0,post,scale,down,operator1,setup,post,scale,down,operator1,open,post,scale,down,operator1,process,element,46,7,post,scale,down,operator1,snapshot,4,8,post,scale,down,operator1,process,element,47,9,post,scale,down,operator1,notify,of,completed,checkpoint,4,assert,exactly,once,for,topic,create,properties,topic,0,arrays,as,list,46,post,scale,down,operator1,close,for,auto,closeable,operator,to,close,operators,to,close,close,ignoring,producer,fenced,operator,to,close,delete,test,topic,topic
