# id;timestamp;commentText;codeText;commentWords;codeWords
Kafka011ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1507568316;Kafka 0.11 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255115836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1110L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer011<Long> prod = new FlinkKafkaProducer011<>(topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, Optional.of(new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		}))__		prod.setWriteTimestampToKafka(true)___		streamWithTimestamps.addSink(prod).setParallelism(3)___		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer011<Long> kafkaSource = new FlinkKafkaConsumer011<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111173247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 11 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,11,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255115836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1110l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer011,long,prod,new,flink,kafka,producer011,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,optional,of,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,write,timestamp,to,kafka,true,stream,with,timestamps,add,sink,prod,set,parallelism,3,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer011,long,kafka,source,new,flink,kafka,consumer011,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111173247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,11,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka011ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1509723634;Kafka 0.11 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255115836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1110L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer011<Long> prod = new FlinkKafkaProducer011<>(topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, Optional.of(new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		}))__		prod.setWriteTimestampToKafka(true)___		streamWithTimestamps.addSink(prod).setParallelism(3)___		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer011<Long> kafkaSource = new FlinkKafkaConsumer011<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111173247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 11 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,11,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255115836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1110l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer011,long,prod,new,flink,kafka,producer011,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,optional,of,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,write,timestamp,to,kafka,true,stream,with,timestamps,add,sink,prod,set,parallelism,3,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer011,long,kafka,source,new,flink,kafka,consumer011,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111173247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,11,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka011ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1519973085;Kafka 0.11 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255115836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1110L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer011<Long> prod = new FlinkKafkaProducer011<>(topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, Optional.of(new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		}))__		prod.setWriteTimestampToKafka(true)___		streamWithTimestamps.addSink(prod).setParallelism(3)___		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer011<Long> kafkaSource = new FlinkKafkaConsumer011<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111173247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 11 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,11,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255115836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1110l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer011,long,prod,new,flink,kafka,producer011,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,optional,of,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,write,timestamp,to,kafka,true,stream,with,timestamps,add,sink,prod,set,parallelism,3,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer011,long,kafka,source,new,flink,kafka,consumer011,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111173247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,11,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka011ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1519973085;Kafka 0.11 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255115836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1110L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer011<Long> prod = new FlinkKafkaProducer011<>(topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, Optional.of(new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		}))__		prod.setWriteTimestampToKafka(true)___		streamWithTimestamps.addSink(prod).setParallelism(3)___		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer011<Long> kafkaSource = new FlinkKafkaConsumer011<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111173247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 11 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,11,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255115836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1110l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer011,long,prod,new,flink,kafka,producer011,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,optional,of,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,write,timestamp,to,kafka,true,stream,with,timestamps,add,sink,prod,set,parallelism,3,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer011,long,kafka,source,new,flink,kafka,consumer011,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111173247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,11,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka011ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1523020981;Kafka 0.11 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255115836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1110L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(TypeInfoParser.<Long>parse("Long"), env.getConfig())__		FlinkKafkaProducer011<Long> prod = new FlinkKafkaProducer011<>(topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, Optional.of(new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		}))__		prod.setWriteTimestampToKafka(true)___		streamWithTimestamps.addSink(prod).setParallelism(3)___		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer011<Long> kafkaSource = new FlinkKafkaConsumer011<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111173247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 11 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,11,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255115836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1110l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,type,info,parser,long,parse,long,env,get,config,flink,kafka,producer011,long,prod,new,flink,kafka,producer011,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,optional,of,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,write,timestamp,to,kafka,true,stream,with,timestamps,add,sink,prod,set,parallelism,3,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer011,long,kafka,source,new,flink,kafka,consumer011,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111173247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,11,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka011ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1525452496;Kafka 0.11 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255115836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1110L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(Types.LONG, env.getConfig())__		FlinkKafkaProducer011<Long> prod = new FlinkKafkaProducer011<>(topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, Optional.of(new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		}))__		prod.setWriteTimestampToKafka(true)___		streamWithTimestamps.addSink(prod).setParallelism(3)___		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer011<Long> kafkaSource = new FlinkKafkaConsumer011<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111173247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 11 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,11,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255115836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1110l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,types,long,env,get,config,flink,kafka,producer011,long,prod,new,flink,kafka,producer011,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,optional,of,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,write,timestamp,to,kafka,true,stream,with,timestamps,add,sink,prod,set,parallelism,3,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer011,long,kafka,source,new,flink,kafka,consumer011,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111173247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,11,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
Kafka011ITCase -> @Test(timeout = 60000) 	public void testTimestamps() throws Exception;1550834396;Kafka 0.11 specific test, ensuring Timestamps are properly written to and read from Kafka.;@Test(timeout = 60000)_	public void testTimestamps() throws Exception {__		final String topic = "tstopic"__		createTestTopic(topic, 3, 1)___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		DataStream<Long> streamWithTimestamps = env.addSource(new SourceFunction<Long>() {_			private static final long serialVersionUID = -2255115836471289626L__			boolean running = true___			@Override_			public void run(SourceContext<Long> ctx) throws Exception {_				long i = 0__				while (running) {_					ctx.collectWithTimestamp(i, i * 2)__					if (i++ == 1110L) {_						running = false__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		final TypeInformationSerializationSchema<Long> longSer = new TypeInformationSerializationSchema<>(Types.LONG, env.getConfig())__		FlinkKafkaProducer011<Long> prod = new FlinkKafkaProducer011<>(topic, new KeyedSerializationSchemaWrapper<>(longSer), standardProps, Optional.of(new FlinkKafkaPartitioner<Long>() {_			private static final long serialVersionUID = -6730989584364230617L___			@Override_			public int partition(Long next, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return (int) (next % 3)__			}_		}))__		prod.setWriteTimestampToKafka(true)___		streamWithTimestamps.addSink(prod).setParallelism(3)___		env.execute("Produce some")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		FlinkKafkaConsumer011<Long> kafkaSource = new FlinkKafkaConsumer011<>(topic, new LimitedLongDeserializer(), standardProps)__		kafkaSource.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Long>() {_			private static final long serialVersionUID = -4834111173247835189L___			@Nullable_			@Override_			public Watermark checkAndGetNextWatermark(Long lastElement, long extractedTimestamp) {_				if (lastElement % 11 == 0) {_					return new Watermark(lastElement)__				}_				return null__			}__			@Override_			public long extractTimestamp(Long element, long previousElementTimestamp) {_				return previousElementTimestamp__			}_		})___		DataStream<Long> stream = env.addSource(kafkaSource)__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		stream.transform("timestamp validating operator", objectTypeInfo, new TimestampValidatingOperator()).setParallelism(1)___		env.execute("Consume again")___		deleteTestTopic(topic)__	};kafka,0,11,specific,test,ensuring,timestamps,are,properly,written,to,and,read,from,kafka;test,timeout,60000,public,void,test,timestamps,throws,exception,final,string,topic,tstopic,create,test,topic,topic,3,1,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,data,stream,long,stream,with,timestamps,env,add,source,new,source,function,long,private,static,final,long,serial,version,uid,2255115836471289626l,boolean,running,true,override,public,void,run,source,context,long,ctx,throws,exception,long,i,0,while,running,ctx,collect,with,timestamp,i,i,2,if,i,1110l,running,false,override,public,void,cancel,running,false,final,type,information,serialization,schema,long,long,ser,new,type,information,serialization,schema,types,long,env,get,config,flink,kafka,producer011,long,prod,new,flink,kafka,producer011,topic,new,keyed,serialization,schema,wrapper,long,ser,standard,props,optional,of,new,flink,kafka,partitioner,long,private,static,final,long,serial,version,uid,6730989584364230617l,override,public,int,partition,long,next,byte,key,byte,value,string,target,topic,int,partitions,return,int,next,3,prod,set,write,timestamp,to,kafka,true,stream,with,timestamps,add,sink,prod,set,parallelism,3,env,execute,produce,some,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,set,stream,time,characteristic,time,characteristic,event,time,flink,kafka,consumer011,long,kafka,source,new,flink,kafka,consumer011,topic,new,limited,long,deserializer,standard,props,kafka,source,assign,timestamps,and,watermarks,new,assigner,with,punctuated,watermarks,long,private,static,final,long,serial,version,uid,4834111173247835189l,nullable,override,public,watermark,check,and,get,next,watermark,long,last,element,long,extracted,timestamp,if,last,element,11,0,return,new,watermark,last,element,return,null,override,public,long,extract,timestamp,long,element,long,previous,element,timestamp,return,previous,element,timestamp,data,stream,long,stream,env,add,source,kafka,source,generic,type,info,object,object,type,info,new,generic,type,info,object,class,stream,transform,timestamp,validating,operator,object,type,info,new,timestamp,validating,operator,set,parallelism,1,env,execute,consume,again,delete,test,topic,topic
