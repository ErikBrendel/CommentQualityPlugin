commented;modifiers;parameterAmount;loc;comment;code
true;public;1;3;/**  * If set to true, Flink will write the (event time) timestamp attached to each record into Kafka.  * Timestamps must be positive for Kafka to accept them.  *  * @param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.  */ ;// ---------------------------------- Properties -------------------------- /**  * If set to true, Flink will write the (event time) timestamp attached to each record into Kafka.  * Timestamps must be positive for Kafka to accept them.  *  * @param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.  */ public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {     this.writeTimestampToKafka = writeTimestampToKafka. }
true;public;1;3;/**  * Defines whether the producer should fail on errors, or only log them.  * If this is set to true, then exceptions will be only logged, if set to false,  * exceptions will be eventually thrown and cause the streaming program to  * fail (and enter recovery).  *  * @param logFailuresOnly The flag to indicate logging-only on exceptions.  */ ;/**  * Defines whether the producer should fail on errors, or only log them.  * If this is set to true, then exceptions will be only logged, if set to false,  * exceptions will be eventually thrown and cause the streaming program to  * fail (and enter recovery).  *  * @param logFailuresOnly The flag to indicate logging-only on exceptions.  */ public void setLogFailuresOnly(boolean logFailuresOnly) {     this.logFailuresOnly = logFailuresOnly. }
true;public;0;5;/**  * Disables the propagation of exceptions thrown when committing presumably timed out Kafka  * transactions during recovery of the job. If a Kafka transaction is timed out, a commit will  * never be successful. Hence, use this feature to avoid recovery loops of the Job. Exceptions  * will still be logged to inform the user that data loss might have occurred.  *  * <p>Note that we use {@link System#currentTimeMillis()} to track the age of a transaction.  * Moreover, only exceptions thrown during the recovery are caught, i.e., the producer will  * attempt at least one commit of the transaction before giving up.</p>  */ ;/**  * Disables the propagation of exceptions thrown when committing presumably timed out Kafka  * transactions during recovery of the job. If a Kafka transaction is timed out, a commit will  * never be successful. Hence, use this feature to avoid recovery loops of the Job. Exceptions  * will still be logged to inform the user that data loss might have occurred.  *  * <p>Note that we use {@link System#currentTimeMillis()} to track the age of a transaction.  * Moreover, only exceptions thrown during the recovery are caught, i.e., the producer will  * attempt at least one commit of the transaction before giving up.</p>  */ @Override public FlinkKafkaProducer011<IN> ignoreFailuresAfterTransactionTimeout() {     super.ignoreFailuresAfterTransactionTimeout().     return this. }
false;public;2;7;;@Override public void onCompletion(RecordMetadata metadata, Exception exception) {     if (exception != null && asyncException == null) {         asyncException = exception.     }     acknowledgeMessage(). }
false;public;2;7;;@Override public void onCompletion(RecordMetadata metadata, Exception e) {     if (e != null) {         LOG.error("Error while sending record to Kafka: " + e.getMessage(), e).     }     acknowledgeMessage(). }
true;public;1;27;/**  * Initializes the connection to Kafka.  */ ;// ----------------------------------- Utilities -------------------------- /**  * Initializes the connection to Kafka.  */ @Override public void open(Configuration configuration) throws Exception {     if (logFailuresOnly) {         callback = new Callback() {              @Override             public void onCompletion(RecordMetadata metadata, Exception e) {                 if (e != null) {                     LOG.error("Error while sending record to Kafka: " + e.getMessage(), e).                 }                 acknowledgeMessage().             }         }.     } else {         callback = new Callback() {              @Override             public void onCompletion(RecordMetadata metadata, Exception exception) {                 if (exception != null && asyncException == null) {                     asyncException = exception.                 }                 acknowledgeMessage().             }         }.     }     super.open(configuration). }
false;public;3;35;;@Override public void invoke(KafkaTransactionState transaction, IN next, Context context) throws FlinkKafka011Exception {     checkErroneous().     byte[] serializedKey = schema.serializeKey(next).     byte[] serializedValue = schema.serializeValue(next).     String targetTopic = schema.getTargetTopic(next).     if (targetTopic == null) {         targetTopic = defaultTopicId.     }     Long timestamp = null.     if (this.writeTimestampToKafka) {         timestamp = context.timestamp().     }     ProducerRecord<byte[], byte[]> record.     int[] partitions = topicPartitionsMap.get(targetTopic).     if (null == partitions) {         partitions = getPartitionsByTopic(targetTopic, transaction.producer).         topicPartitionsMap.put(targetTopic, partitions).     }     if (flinkKafkaPartitioner != null) {         record = new ProducerRecord<>(targetTopic, flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions), timestamp, serializedKey, serializedValue).     } else {         record = new ProducerRecord<>(targetTopic, null, timestamp, serializedKey, serializedValue).     }     pendingRecords.incrementAndGet().     transaction.producer.send(record, callback). }
false;public;0;30;;@Override public void close() throws FlinkKafka011Exception {     final KafkaTransactionState currentTransaction = currentTransaction().     if (currentTransaction != null) {         // to avoid exceptions on aborting transactions with some pending records         flush(currentTransaction).         // we need to close it manually         switch(semantic) {             case EXACTLY_ONCE:                 break.             case AT_LEAST_ONCE:             case NONE:                 currentTransaction.producer.close().                 break.         }     }     try {         super.close().     } catch (Exception e) {         asyncException = ExceptionUtils.firstOrSuppressed(e, asyncException).     }     // make sure we propagate pending errors     checkErroneous().     pendingTransactions().forEach(transaction -> IOUtils.closeQuietly(transaction.getValue().producer)). }
false;protected;0;19;;// ------------------- Logic for handling checkpoint flushing -------------------------- // @Override protected KafkaTransactionState beginTransaction() throws FlinkKafka011Exception {     switch(semantic) {         case EXACTLY_ONCE:             FlinkKafkaProducer<byte[], byte[]> producer = createTransactionalProducer().             producer.beginTransaction().             return new KafkaTransactionState(producer.getTransactionalId(), producer).         case AT_LEAST_ONCE:         case NONE:             // Do not create new producer on each beginTransaction() if it is not necessary             final KafkaTransactionState currentTransaction = currentTransaction().             if (currentTransaction != null && currentTransaction.producer != null) {                 return new KafkaTransactionState(currentTransaction.producer).             }             return new KafkaTransactionState(initNonTransactionalProducer(true)).         default:             throw new UnsupportedOperationException("Not implemented semantic").     } }
false;protected;1;14;;@Override protected void preCommit(KafkaTransactionState transaction) throws FlinkKafka011Exception {     switch(semantic) {         case EXACTLY_ONCE:         case AT_LEAST_ONCE:             flush(transaction).             break.         case NONE:             break.         default:             throw new UnsupportedOperationException("Not implemented semantic").     }     checkErroneous(). }
false;protected;1;10;;@Override protected void commit(KafkaTransactionState transaction) {     if (transaction.isTransactional()) {         try {             transaction.producer.commitTransaction().         } finally {             recycleTransactionalProducer(transaction.producer).         }     } }
false;protected;1;17;;@Override protected void recoverAndCommit(KafkaTransactionState transaction) {     if (transaction.isTransactional()) {         try (FlinkKafkaProducer<byte[], byte[]> producer = initTransactionalProducer(transaction.transactionalId, false)) {             producer.resumeTransaction(transaction.producerId, transaction.epoch).             producer.commitTransaction().         } catch (InvalidTxnStateException | ProducerFencedException ex) {             // That means we have committed this transaction before.             LOG.warn("Encountered error {} while recovering transaction {}. " + "Presumably this transaction has been already committed before", ex, transaction).         }     } }
false;protected;1;7;;@Override protected void abort(KafkaTransactionState transaction) {     if (transaction.isTransactional()) {         transaction.producer.abortTransaction().         recycleTransactionalProducer(transaction.producer).     } }
false;protected;1;10;;@Override protected void recoverAndAbort(KafkaTransactionState transaction) {     if (transaction.isTransactional()) {         try (FlinkKafkaProducer<byte[], byte[]> producer = initTransactionalProducer(transaction.transactionalId, false)) {             producer.initTransactions().         }     } }
false;private;0;3;;private void acknowledgeMessage() {     pendingRecords.decrementAndGet(). }
true;private;1;12;/**  * Flush pending records.  * @param transaction  */ ;/**  * Flush pending records.  * @param transaction  */ private void flush(KafkaTransactionState transaction) throws FlinkKafka011Exception {     if (transaction.producer != null) {         transaction.producer.flush().     }     long pendingRecordsCount = pendingRecords.get().     if (pendingRecordsCount != 0) {         throw new IllegalStateException("Pending record count must be zero at this point: " + pendingRecordsCount).     }     // if the flushed requests has errors, we should propagate it also and fail the checkpoint     checkErroneous(). }
false;public;1;23;;@Override public void snapshotState(FunctionSnapshotContext context) throws Exception {     super.snapshotState(context).     nextTransactionalIdHintState.clear().     // subtasks would write exactly same information.     if (getRuntimeContext().getIndexOfThisSubtask() == 0 && semantic == Semantic.EXACTLY_ONCE) {         checkState(nextTransactionalIdHint != null, "nextTransactionalIdHint must be set for EXACTLY_ONCE").         long nextFreeTransactionalId = nextTransactionalIdHint.nextFreeTransactionalId.         // scaling up.         if (getRuntimeContext().getNumberOfParallelSubtasks() > nextTransactionalIdHint.lastParallelism) {             nextFreeTransactionalId += getRuntimeContext().getNumberOfParallelSubtasks() * kafkaProducersPoolSize.         }         nextTransactionalIdHintState.add(new NextTransactionalIdHint(getRuntimeContext().getNumberOfParallelSubtasks(), nextFreeTransactionalId)).     } }
false;public;1;39;;@Override public void initializeState(FunctionInitializationContext context) throws Exception {     if (semantic != Semantic.NONE && !((StreamingRuntimeContext) this.getRuntimeContext()).isCheckpointingEnabled()) {         LOG.warn("Using {} semantic, but checkpointing is not enabled. Switching to {} semantic.", semantic, Semantic.NONE).         semantic = Semantic.NONE.     }     nextTransactionalIdHintState = context.getOperatorStateStore().getUnionListState(NEXT_TRANSACTIONAL_ID_HINT_DESCRIPTOR).     transactionalIdsGenerator = new TransactionalIdsGenerator(getRuntimeContext().getTaskName() + "-" + ((StreamingRuntimeContext) getRuntimeContext()).getOperatorUniqueID(), getRuntimeContext().getIndexOfThisSubtask(), getRuntimeContext().getNumberOfParallelSubtasks(), kafkaProducersPoolSize, SAFE_SCALE_DOWN_FACTOR).     if (semantic != Semantic.EXACTLY_ONCE) {         nextTransactionalIdHint = null.     } else {         ArrayList<NextTransactionalIdHint> transactionalIdHints = Lists.newArrayList(nextTransactionalIdHintState.get()).         if (transactionalIdHints.size() > 1) {             throw new IllegalStateException("There should be at most one next transactional id hint written by the first subtask").         } else if (transactionalIdHints.size() == 0) {             nextTransactionalIdHint = new NextTransactionalIdHint(0, 0).             // this means that this is either:             // (1) the first execution of this application             // (2) previous execution has failed before first checkpoint completed             //              // in case of (2) we have to abort all previous transactions             abortTransactions(transactionalIdsGenerator.generateIdsToAbort()).         } else {             nextTransactionalIdHint = transactionalIdHints.get(0).         }     }     super.initializeState(context). }
false;protected;0;10;;@Override protected Optional<KafkaTransactionContext> initializeUserContext() {     if (semantic != Semantic.EXACTLY_ONCE) {         return Optional.empty().     }     Set<String> transactionalIds = generateNewTransactionalIds().     resetAvailableTransactionalIdsPool(transactionalIds).     return Optional.of(new KafkaTransactionContext(transactionalIds)). }
false;private;0;7;;private Set<String> generateNewTransactionalIds() {     checkState(nextTransactionalIdHint != null, "nextTransactionalIdHint must be present for EXACTLY_ONCE").     Set<String> transactionalIds = transactionalIdsGenerator.generateIdsToUse(nextTransactionalIdHint.nextFreeTransactionalId).     LOG.info("Generated new transactionalIds {}", transactionalIds).     return transactionalIds. }
false;protected;0;6;;@Override protected void finishRecoveringContext() {     cleanUpUserContext().     resetAvailableTransactionalIdsPool(getUserContext().get().transactionalIds).     LOG.info("Recovered transactionalIds {}", getUserContext().get().transactionalIds). }
true;private;0;6;/**  * After initialization make sure that all previous transactions from the current user context have been completed.  */ ;/**  * After initialization make sure that all previous transactions from the current user context have been completed.  */ private void cleanUpUserContext() {     if (!getUserContext().isPresent()) {         return.     }     abortTransactions(getUserContext().get().transactionalIds). }
false;private;1;4;;private void resetAvailableTransactionalIdsPool(Collection<String> transactionalIds) {     availableTransactionalIds.clear().     availableTransactionalIds.addAll(transactionalIds). }
false;private;1;9;;// ----------------------------------- Utilities -------------------------- private void abortTransactions(Set<String> transactionalIds) {     for (String transactionalId : transactionalIds) {         try (FlinkKafkaProducer<byte[], byte[]> kafkaProducer = initTransactionalProducer(transactionalId, false)) {             // it suffice to call initTransactions - this will abort any lingering transactions             kafkaProducer.initTransactions().         }     } }
false;;0;7;;int getTransactionCoordinatorId() {     final KafkaTransactionState currentTransaction = currentTransaction().     if (currentTransaction == null || currentTransaction.producer == null) {         throw new IllegalArgumentException().     }     return currentTransaction.producer.getTransactionCoordinatorId(). }
true;private;0;11;/**  * For each checkpoint we create new {@link FlinkKafkaProducer} so that new transactions will not clash  * with transactions created during previous checkpoints ({@code producer.initTransactions()} assures that we  * obtain new producerId and epoch counters).  */ ;/**  * For each checkpoint we create new {@link FlinkKafkaProducer} so that new transactions will not clash  * with transactions created during previous checkpoints ({@code producer.initTransactions()} assures that we  * obtain new producerId and epoch counters).  */ private FlinkKafkaProducer<byte[], byte[]> createTransactionalProducer() throws FlinkKafka011Exception {     String transactionalId = availableTransactionalIds.poll().     if (transactionalId == null) {         throw new FlinkKafka011Exception(FlinkKafka011ErrorCode.PRODUCERS_POOL_EMPTY, "Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.").     }     FlinkKafkaProducer<byte[], byte[]> producer = initTransactionalProducer(transactionalId, true).     producer.initTransactions().     return producer. }
false;private;1;4;;private void recycleTransactionalProducer(FlinkKafkaProducer<byte[], byte[]> producer) {     availableTransactionalIds.add(producer.getTransactionalId()).     producer.close(). }
false;private;2;4;;private FlinkKafkaProducer<byte[], byte[]> initTransactionalProducer(String transactionalId, boolean registerMetrics) {     producerConfig.put("transactional.id", transactionalId).     return initProducer(registerMetrics). }
false;private;1;4;;private FlinkKafkaProducer<byte[], byte[]> initNonTransactionalProducer(boolean registerMetrics) {     producerConfig.remove("transactional.id").     return initProducer(registerMetrics). }
false;private;1;43;;private FlinkKafkaProducer<byte[], byte[]> initProducer(boolean registerMetrics) {     FlinkKafkaProducer<byte[], byte[]> producer = new FlinkKafkaProducer<>(this.producerConfig).     RuntimeContext ctx = getRuntimeContext().     if (flinkKafkaPartitioner != null) {         if (flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {             ((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(getPartitionsByTopic(this.defaultTopicId, producer)).         }         flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks()).     }     LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}", ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId).     // register Kafka metrics to Flink accumulators     if (registerMetrics && !Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {         Map<MetricName, ? extends Metric> metrics = producer.metrics().         if (metrics == null) {             // MapR's Kafka implementation returns null here.             LOG.info("Producer implementation does not support metrics").         } else {             final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer").             for (Map.Entry<MetricName, ? extends Metric> entry : metrics.entrySet()) {                 String name = entry.getKey().name().                 Metric metric = entry.getValue().                 KafkaMetricMutableWrapper wrapper = previouslyCreatedMetrics.get(name).                 if (wrapper != null) {                     wrapper.setKafkaMetric(metric).                 } else {                     // TODO: somehow merge metrics from all active producers?                     wrapper = new KafkaMetricMutableWrapper(metric).                     previouslyCreatedMetrics.put(name, wrapper).                     kafkaMetricGroup.gauge(name, wrapper).                 }             }         }     }     return producer. }
false;private;0;11;;private void checkErroneous() throws FlinkKafka011Exception {     Exception e = asyncException.     if (e != null) {         // prevent double throwing         asyncException = null.         throw new FlinkKafka011Exception(FlinkKafka011ErrorCode.EXTERNAL_ERROR, "Failed to send data to Kafka: " + e.getMessage(), e).     } }
false;private;1;3;;private void readObject(java.io.ObjectInputStream in) throws IOException, ClassNotFoundException {     in.defaultReadObject(). }
false;private,static;1;12;;private static Properties getPropertiesFromBrokerList(String brokerList) {     String[] elements = brokerList.split(",").     // validate the broker addresses     for (String broker : elements) {         NetUtils.getCorrectHostnamePort(broker).     }     Properties props = new Properties().     props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList).     return props. }
false;public;2;4;;@Override public int compare(PartitionInfo o1, PartitionInfo o2) {     return Integer.compare(o1.partition(), o2.partition()). }
false;private,static;2;19;;private static int[] getPartitionsByTopic(String topic, Producer<byte[], byte[]> producer) {     // the fetched list is immutable, so we're creating a mutable copy in order to sort it     List<PartitionInfo> partitionsList = new ArrayList<>(producer.partitionsFor(topic)).     // sort the partitions by partition id to make sure the fetched partition list is the same across subtasks     Collections.sort(partitionsList, new Comparator<PartitionInfo>() {          @Override         public int compare(PartitionInfo o1, PartitionInfo o2) {             return Integer.compare(o1.partition(), o2.partition()).         }     }).     int[] partitions = new int[partitionsList.size()].     for (int i = 0. i < partitions.length. i++) {         partitions[i] = partitionsList.get(i).partition().     }     return partitions. }
false;;0;3;;boolean isTransactional() {     return transactionalId != null. }
false;public;0;9;;@Override public String toString() {     return String.format("%s [transactionalId=%s, producerId=%s, epoch=%s]", this.getClass().getSimpleName(), transactionalId, producerId, epoch). }
false;public;1;19;;@Override public boolean equals(Object o) {     if (this == o) {         return true.     }     if (o == null || getClass() != o.getClass()) {         return false.     }     KafkaTransactionState that = (KafkaTransactionState) o.     if (producerId != that.producerId) {         return false.     }     if (epoch != that.epoch) {         return false.     }     return transactionalId != null ? transactionalId.equals(that.transactionalId) : that.transactionalId == null. }
false;public;0;7;;@Override public int hashCode() {     int result = transactionalId != null ? transactionalId.hashCode() : 0.     result = 31 * result + (int) (producerId ^ (producerId >>> 32)).     result = 31 * result + (int) epoch.     return result. }
false;public;1;13;;@Override public boolean equals(Object o) {     if (this == o) {         return true.     }     if (o == null || getClass() != o.getClass()) {         return false.     }     KafkaTransactionContext that = (KafkaTransactionContext) o.     return transactionalIds.equals(that.transactionalIds). }
false;public;0;4;;@Override public int hashCode() {     return transactionalIds.hashCode(). }
false;public;0;4;;@Override public boolean isImmutableType() {     return true. }
false;public;0;4;;@Override public KafkaTransactionState createInstance() {     return null. }
false;public;1;4;;@Override public KafkaTransactionState copy(KafkaTransactionState from) {     return from. }
false;public;2;6;;@Override public KafkaTransactionState copy(KafkaTransactionState from, KafkaTransactionState reuse) {     return from. }
false;public;0;4;;@Override public int getLength() {     return -1. }
false;public;2;13;;@Override public void serialize(KafkaTransactionState record, DataOutputView target) throws IOException {     if (record.transactionalId == null) {         target.writeBoolean(false).     } else {         target.writeBoolean(true).         target.writeUTF(record.transactionalId).     }     target.writeLong(record.producerId).     target.writeShort(record.epoch). }
false;public;1;10;;@Override public KafkaTransactionState deserialize(DataInputView source) throws IOException {     String transactionalId = null.     if (source.readBoolean()) {         transactionalId = source.readUTF().     }     long producerId = source.readLong().     short epoch = source.readShort().     return new KafkaTransactionState(transactionalId, producerId, epoch, null). }
false;public;2;6;;@Override public KafkaTransactionState deserialize(KafkaTransactionState reuse, DataInputView source) throws IOException {     return deserialize(source). }
false;public;2;11;;@Override public void copy(DataInputView source, DataOutputView target) throws IOException {     boolean hasTransactionalId = source.readBoolean().     target.writeBoolean(hasTransactionalId).     if (hasTransactionalId) {         target.writeUTF(source.readUTF()).     }     target.writeLong(source.readLong()).     target.writeShort(source.readShort()). }
false;public;0;4;;// ------------------------------------------------------------------------ @Override public TypeSerializerSnapshot<KafkaTransactionState> snapshotConfiguration() {     return new TransactionStateSerializerSnapshot(). }
false;public;0;4;;@Override public boolean isImmutableType() {     return true. }
false;public;0;4;;@Override public KafkaTransactionContext createInstance() {     return null. }
false;public;1;4;;@Override public KafkaTransactionContext copy(KafkaTransactionContext from) {     return from. }
false;public;2;6;;@Override public KafkaTransactionContext copy(KafkaTransactionContext from, KafkaTransactionContext reuse) {     return from. }
false;public;0;4;;@Override public int getLength() {     return -1. }
false;public;2;10;;@Override public void serialize(KafkaTransactionContext record, DataOutputView target) throws IOException {     int numIds = record.transactionalIds.size().     target.writeInt(numIds).     for (String id : record.transactionalIds) {         target.writeUTF(id).     } }
false;public;1;9;;@Override public KafkaTransactionContext deserialize(DataInputView source) throws IOException {     int numIds = source.readInt().     Set<String> ids = new HashSet<>(numIds).     for (int i = 0. i < numIds. i++) {         ids.add(source.readUTF()).     }     return new KafkaTransactionContext(ids). }
false;public;2;6;;@Override public KafkaTransactionContext deserialize(KafkaTransactionContext reuse, DataInputView source) throws IOException {     return deserialize(source). }
false;public;2;10;;@Override public void copy(DataInputView source, DataOutputView target) throws IOException {     int numIds = source.readInt().     target.writeInt(numIds).     for (int i = 0. i < numIds. i++) {         target.writeUTF(source.readUTF()).     } }
false;public;0;4;;// ------------------------------------------------------------------------ @Override public TypeSerializerSnapshot<KafkaTransactionContext> snapshotConfiguration() {     return new ContextStateSerializerSnapshot(). }
