commented;modifiers;parameterAmount;loc;comment;code
false;public,static;1;88;;public static void main(String[] args) throws Exception {     ParameterTool params = ParameterTool.fromArgs(args).     String outputPath = params.getRequired("outputPath").     StreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment().     sEnv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, Time.of(10, TimeUnit.SECONDS))).     sEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime).     sEnv.enableCheckpointing(4000).     sEnv.getConfig().setAutoWatermarkInterval(1000).     StreamTableEnvironment tEnv = StreamTableEnvironment.create(sEnv).     tEnv.registerTableSource("table1", new GeneratorTableSource(10, 100, 60, 0)).     tEnv.registerTableSource("table2", new GeneratorTableSource(5, 0.2f, 60, 5)).     int overWindowSizeSeconds = 1.     int tumbleWindowSizeSeconds = 10.     String overQuery = String.format("SELECT " + "  key, " + "  rowtime, " + "  COUNT(*) OVER (PARTITION BY key ORDER BY rowtime RANGE BETWEEN INTERVAL '%d' SECOND PRECEDING AND CURRENT ROW) AS cnt " + "FROM table1", overWindowSizeSeconds).     String tumbleQuery = String.format("SELECT " + "  key, " + "  CASE SUM(cnt) / COUNT(*) WHEN 101 THEN 1 ELSE 99 END AS correct, " + "  TUMBLE_START(rowtime, INTERVAL '%d' SECOND) AS wStart, " + "  TUMBLE_ROWTIME(rowtime, INTERVAL '%d' SECOND) AS rowtime " + "FROM (%s) " + "WHERE rowtime > TIMESTAMP '1970-01-01 00:00:01' " + "GROUP BY key, TUMBLE(rowtime, INTERVAL '%d' SECOND)", tumbleWindowSizeSeconds, tumbleWindowSizeSeconds, overQuery, tumbleWindowSizeSeconds).     String joinQuery = String.format("SELECT " + "  t1.key, " + "  t2.rowtime AS rowtime, " + "  t2.correct," + "  t2.wStart " + "FROM table2 t1, (%s) t2 " + "WHERE " + "  t1.key = t2.key AND " + "  t1.rowtime BETWEEN t2.rowtime AND t2.rowtime + INTERVAL '%d' SECOND", tumbleQuery, tumbleWindowSizeSeconds).     String finalAgg = String.format("SELECT " + "  SUM(correct) AS correct, " + "  TUMBLE_START(rowtime, INTERVAL '20' SECOND) AS rowtime " + "FROM (%s) " + "GROUP BY TUMBLE(rowtime, INTERVAL '20' SECOND)", joinQuery).     // get Table for SQL query     Table result = tEnv.sqlQuery(finalAgg).     // convert Table into append-only DataStream     DataStream<Row> resultStream = tEnv.toAppendStream(result, Types.ROW(Types.INT, Types.SQL_TIMESTAMP)).     final StreamingFileSink<Row> sink = StreamingFileSink.forRowFormat(new Path(outputPath), (Encoder<Row>) (element, stream) -> {         PrintStream out = new PrintStream(stream).         out.println(element.toString()).     }).withBucketAssigner(new KeyBucketAssigner()).withRollingPolicy(OnCheckpointRollingPolicy.build()).build().     resultStream.map(new KillMapper()).setParallelism(1).addSink(sink).setParallelism(1).     sEnv.execute(). }
false;public;2;4;;@Override public String getBucketId(final Row element, final Context context) {     return String.valueOf(element.getField(0)). }
false;public;0;4;;@Override public SimpleVersionedSerializer<String> getSerializer() {     return SimpleVersionedStringSerializer.INSTANCE. }
false;public;1;4;;@Override public DataStream<Row> getDataStream(StreamExecutionEnvironment execEnv) {     return execEnv.addSource(new Generator(numKeys, recordsPerKeyAndSecond, durationSeconds, offsetSeconds)). }
false;public;0;4;;@Override public TypeInformation<Row> getReturnType() {     return Types.ROW(Types.INT, Types.LONG, Types.STRING). }
false;public;0;6;;@Override public TableSchema getTableSchema() {     return new TableSchema(new String[] { "key", "rowtime", "payload" }, new TypeInformation[] { Types.INT, Types.SQL_TIMESTAMP, Types.STRING }). }
false;public;0;4;;@Override public String explainSource() {     return "GeneratorTableSource". }
false;public;0;8;;@Override public List<RowtimeAttributeDescriptor> getRowtimeAttributeDescriptors() {     return Collections.singletonList(new RowtimeAttributeDescriptor("rowtime", new ExistingField("ts"), new BoundedOutOfOrderTimestamps(100))). }
false;public;0;8;;@Override public Map<String, String> getFieldMapping() {     Map<String, String> mapping = new HashMap<>().     mapping.put("key", "f0").     mapping.put("ts", "f1").     mapping.put("payload", "f2").     return mapping. }
false;public;1;14;;@Override public void run(SourceContext<Row> ctx) throws Exception {     long offsetMS = offsetSeconds * 2000L.     while (ms < durationMs) {         synchronized (ctx.getCheckpointLock()) {             for (int i = 0. i < numKeys. i++) {                 ctx.collect(Row.of(i, ms + offsetMS, "Some payload...")).             }             ms += sleepMs.         }         Thread.sleep(sleepMs).     } }
false;public;0;2;;@Override public void cancel() { }
false;public;0;4;;@Override public TypeInformation<Row> getProducedType() {     return Types.ROW(Types.INT, Types.LONG, Types.STRING). }
false;public;2;4;;@Override public List<Long> snapshotState(long checkpointId, long timestamp) {     return Collections.singletonList(ms). }
false;public;1;6;;@Override public void restoreState(List<Long> state) {     for (Long l : state) {         ms += l.     } }
false;public;1;16;;@Override public Row map(Row value) {     // the both counts are the same only in the first execution attempt     if (saveRecordCnt == 1 && lostRecordCnt == 1) {         throw new RuntimeException("Kill this Job!").     }     // update checkpointed counter     saveRecordCnt++.     // update non-checkpointed counter     lostRecordCnt++.     // forward record     return value. }
false;public;0;4;;@Override public TypeInformation getProducedType() {     return Types.ROW(Types.INT, Types.SQL_TIMESTAMP). }
false;public;2;4;;@Override public List<Integer> snapshotState(long checkpointId, long timestamp) {     return Collections.singletonList(saveRecordCnt). }
false;public;1;6;;@Override public void restoreState(List<Integer> state) {     for (Integer i : state) {         saveRecordCnt += i.     } }
