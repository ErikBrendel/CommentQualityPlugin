commented;modifiers;parameterAmount;loc;comment;code
false;;0;3;;void start() {     executor.scheduleWithFixedDelay(fetcherTask, 0, refreshIntervalMillis, TimeUnit.MILLISECONDS). }
false;;0;11;;void stop() {     executor.shutdown().     try {         if (!executor.awaitTermination(1, TimeUnit.SECONDS)) {             executor.shutdownNow().         }     } catch (InterruptedException ignored) {         executor.shutdownNow().     } }
false;public;0;96;;@Override public void run() {     try {         for (HistoryServer.RefreshLocation refreshLocation : refreshDirs) {             Path refreshDir = refreshLocation.getPath().             FileSystem refreshFS = refreshLocation.getFs().             // contents of /:refreshDir             FileStatus[] jobArchives.             try {                 jobArchives = refreshFS.listStatus(refreshDir).             } catch (IOException e) {                 LOG.error("Failed to access job archive location for path {}.", refreshDir, e).                 continue.             }             if (jobArchives == null) {                 continue.             }             boolean updateOverview = false.             for (FileStatus jobArchive : jobArchives) {                 Path jobArchivePath = jobArchive.getPath().                 String jobID = jobArchivePath.getName().                 try {                     JobID.fromHexString(jobID).                 } catch (IllegalArgumentException iae) {                     LOG.debug("Archive directory {} contained file with unexpected name {}. Ignoring file.", refreshDir, jobID, iae).                     continue.                 }                 if (cachedArchives.add(jobID)) {                     try {                         for (ArchivedJson archive : FsJobArchivist.getArchivedJsons(jobArchive.getPath())) {                             String path = archive.getPath().                             String json = archive.getJson().                             File target.                             if (path.equals(JobsOverviewHeaders.URL)) {                                 target = new File(webOverviewDir, jobID + JSON_FILE_ENDING).                             } else if (path.equals("/joboverview")) {                                 // legacy path                                 json = convertLegacyJobOverview(json).                                 target = new File(webOverviewDir, jobID + JSON_FILE_ENDING).                             } else {                                 target = new File(webDir, path + JSON_FILE_ENDING).                             }                             java.nio.file.Path parent = target.getParentFile().toPath().                             try {                                 Files.createDirectories(parent).                             } catch (FileAlreadyExistsException ignored) {                             // there may be left-over directories from the previous attempt                             }                             java.nio.file.Path targetPath = target.toPath().                             // We overwrite existing files since this may be another attempt at fetching this archive.                             // Existing files may be incomplete/corrupt.                             Files.deleteIfExists(targetPath).                             Files.createFile(target.toPath()).                             try (FileWriter fw = new FileWriter(target)) {                                 fw.write(json).                                 fw.flush().                             }                         }                         updateOverview = true.                     } catch (IOException e) {                         LOG.error("Failure while fetching/processing job archive for job {}.", jobID, e).                         // Make sure we attempt to fetch the archive again                         cachedArchives.remove(jobID).                         // Make sure we do not include this job in the overview                         try {                             Files.delete(new File(webOverviewDir, jobID + JSON_FILE_ENDING).toPath()).                         } catch (IOException ioe) {                             LOG.debug("Could not delete file from overview directory.", ioe).                         }                         // Clean up job files we may have created                         File jobDirectory = new File(webJobDir, jobID).                         try {                             FileUtils.deleteDirectory(jobDirectory).                         } catch (IOException ioe) {                             LOG.debug("Could not clean up job directory.", ioe).                         }                     }                 }             }             if (updateOverview) {                 updateJobOverview(webOverviewDir, webDir).             }         }     } catch (Exception e) {         LOG.error("Critical failure while fetching/processing job archives.", e).     }     numFinishedPolls.countDown(). }
false;private,static;1;40;;private static String convertLegacyJobOverview(String legacyOverview) throws IOException {     JsonNode root = mapper.readTree(legacyOverview).     JsonNode finishedJobs = root.get("finished").     JsonNode job = finishedJobs.get(0).     JobID jobId = JobID.fromHexString(job.get("jid").asText()).     String name = job.get("name").asText().     JobStatus state = JobStatus.valueOf(job.get("state").asText()).     long startTime = job.get("start-time").asLong().     long endTime = job.get("end-time").asLong().     long duration = job.get("duration").asLong().     long lastMod = job.get("last-modification").asLong().     JsonNode tasks = job.get("tasks").     int numTasks = tasks.get("total").asInt().     int pending = tasks.get("pending").asInt().     int running = tasks.get("running").asInt().     int finished = tasks.get("finished").asInt().     int canceling = tasks.get("canceling").asInt().     int canceled = tasks.get("canceled").asInt().     int failed = tasks.get("failed").asInt().     int[] tasksPerState = new int[ExecutionState.values().length].     // pending is a mix of CREATED/SCHEDULED/DEPLOYING     // to maintain the correct number of task states we have to pick one of them     tasksPerState[ExecutionState.SCHEDULED.ordinal()] = pending.     tasksPerState[ExecutionState.RUNNING.ordinal()] = running.     tasksPerState[ExecutionState.FINISHED.ordinal()] = finished.     tasksPerState[ExecutionState.CANCELING.ordinal()] = canceling.     tasksPerState[ExecutionState.CANCELED.ordinal()] = canceled.     tasksPerState[ExecutionState.FAILED.ordinal()] = failed.     JobDetails jobDetails = new JobDetails(jobId, name, startTime, endTime, duration, state, lastMod, tasksPerState, numTasks).     MultipleJobsDetails multipleJobsDetails = new MultipleJobsDetails(Collections.singleton(jobDetails)).     StringWriter sw = new StringWriter().     mapper.writeValue(sw, multipleJobsDetails).     return sw.toString(). }
true;private,static;2;15;/**  * This method replicates the JSON response that would be given by the JobsOverviewHandler when  * listing both running and finished jobs.  *  * <p>Every job archive contains a joboverview.json file containing the same structure. Since jobs are archived on  * their own however the list of finished jobs only contains a single job.  *  * <p>For the display in the HistoryServer WebFrontend we have to combine these overviews.  */ ;/**  * This method replicates the JSON response that would be given by the JobsOverviewHandler when  * listing both running and finished jobs.  *  * <p>Every job archive contains a joboverview.json file containing the same structure. Since jobs are archived on  * their own however the list of finished jobs only contains a single job.  *  * <p>For the display in the HistoryServer WebFrontend we have to combine these overviews.  */ private static void updateJobOverview(File webOverviewDir, File webDir) {     try (JsonGenerator gen = jacksonFactory.createGenerator(HistoryServer.createOrGetFile(webDir, JobsOverviewHeaders.URL))) {         File[] overviews = new File(webOverviewDir.getPath()).listFiles().         if (overviews != null) {             Collection<JobDetails> allJobs = new ArrayList<>(overviews.length).             for (File overview : overviews) {                 MultipleJobsDetails subJobs = mapper.readValue(overview, MultipleJobsDetails.class).                 allJobs.addAll(subJobs.getJobs()).             }             mapper.writeValue(gen, new MultipleJobsDetails(allJobs)).         }     } catch (IOException ioe) {         LOG.error("Failed to update job overview.", ioe).     } }
