commented;modifiers;parameterAmount;loc;comment;code
false;public;1;12;;@Override public void configure(Configuration parameters) {     super.configure(parameters).     if (!this.skipWrongSchemaFileSplit) {         this.skipWrongSchemaFileSplit = parameters.getBoolean(PARQUET_SKIP_WRONG_SCHEMA_SPLITS, false).     }     if (this.skipCorruptedRecord) {         this.skipCorruptedRecord = parameters.getBoolean(PARQUET_SKIP_CORRUPTED_RECORD, false).     } }
true;public;1;15;/**  * Configures the fields to be read and returned by the ParquetInputFormat. Selected fields must be present  * in the configured schema.  *  * @param fieldNames Names of all selected fields.  */ ;/**  * Configures the fields to be read and returned by the ParquetInputFormat. Selected fields must be present  * in the configured schema.  *  * @param fieldNames Names of all selected fields.  */ public void selectFields(String[] fieldNames) {     checkNotNull(fieldNames, "fieldNames").     this.fieldNames = fieldNames.     RowTypeInfo rowTypeInfo = (RowTypeInfo) ParquetSchemaConverter.fromParquetType(expectedFileSchema).     TypeInformation[] selectFieldTypes = new TypeInformation[fieldNames.length].     for (int i = 0. i < fieldNames.length. i++) {         try {             selectFieldTypes[i] = rowTypeInfo.getTypeAt(fieldNames[i]).         } catch (IndexOutOfBoundsException e) {             throw new IllegalArgumentException(String.format("Fail to access Field %s , " + "which is not contained in the file schema", fieldNames[i]), e).         }     }     this.fieldTypes = selectFieldTypes. }
false;public;0;4;;@Override public Tuple2<Long, Long> getCurrentState() {     return parquetRecordReader.getCurrentReadPosition(). }
false;public;1;27;;@Override public void open(FileInputSplit split) throws IOException {     // reset the flag when open a new split     this.skipThisSplit = false.     org.apache.hadoop.conf.Configuration configuration = new org.apache.hadoop.conf.Configuration().     InputFile inputFile = HadoopInputFile.fromPath(new org.apache.hadoop.fs.Path(split.getPath().toUri()), configuration).     ParquetReadOptions options = ParquetReadOptions.builder().build().     ParquetFileReader fileReader = new ParquetFileReader(inputFile, options).     MessageType fileSchema = fileReader.getFileMetaData().getSchema().     MessageType readSchema = getReadSchema(fileSchema, split.getPath()).     if (skipThisSplit) {         LOG.warn(String.format("Escaped the file split [%s] due to mismatch of file schema to expected result schema", split.getPath().toString())).     } else {         this.parquetRecordReader = new ParquetRecordReader<>(new RowReadSupport(), readSchema, FilterCompat.NOOP).         this.parquetRecordReader.initialize(fileReader, configuration).         this.parquetRecordReader.setSkipCorruptedRecord(this.skipCorruptedRecord).         if (this.recordConsumed == null) {             this.recordConsumed = getRuntimeContext().getMetricGroup().counter("parquet-records-consumed").         }         LOG.debug(String.format("Open ParquetInputFormat with FileInputSplit [%s]", split.getPath().toString())).     } }
false;public;2;8;;@Override public void reopen(FileInputSplit split, Tuple2<Long, Long> state) throws IOException {     Preconditions.checkNotNull(split, "reopen() cannot be called on a null split.").     Preconditions.checkNotNull(state, "reopen() cannot be called with a null initial state.").     this.open(split).     // seek to the read position in the split that we were at when the checkpoint was taken.     parquetRecordReader.seek(state.f0, state.f1). }
true;protected;0;3;/**  * Get field names of read result.  *  * @return field names array  */ ;/**  * Get field names of read result.  *  * @return field names array  */ protected String[] getFieldNames() {     return fieldNames. }
true;protected;0;3;/**  * Get field types of read result.  *  * @return field types array  */ ;/**  * Get field types of read result.  *  * @return field types array  */ protected TypeInformation[] getFieldTypes() {     return fieldTypes. }
false;public;0;6;;@Override public void close() throws IOException {     if (parquetRecordReader != null) {         parquetRecordReader.close().     } }
false;public;0;8;;@Override public boolean reachedEnd() throws IOException {     if (skipThisSplit) {         return true.     }     return parquetRecordReader.reachEnd(). }
false;public;1;9;;@Override public E nextRecord(E e) throws IOException {     if (reachedEnd()) {         return null.     }     recordConsumed.inc().     return convert(parquetRecordReader.nextRecord()). }
true;protected,abstract;1;1;/**  * This ParquetInputFormat read parquet record as Row by default. Sub classes of it can extend this method  * to further convert row to other types, such as POJO, Map or Tuple.  *  * @param row row read from parquet file  * @return E target result type  */ ;/**  * This ParquetInputFormat read parquet record as Row by default. Sub classes of it can extend this method  * to further convert row to other types, such as POJO, Map or Tuple.  *  * @param row row read from parquet file  * @return E target result type  */ protected abstract E convert(Row row).
true;private;2;31;/**  * Generates and returns the read schema based on the projected fields for a given file.  *  * @param fileSchema The schema of the given file.  * @param filePath The path of the given file.  * @return The read schema based on the given file's schema and the projected fields.  */ ;/**  * Generates and returns the read schema based on the projected fields for a given file.  *  * @param fileSchema The schema of the given file.  * @param filePath The path of the given file.  * @return The read schema based on the given file's schema and the projected fields.  */ private MessageType getReadSchema(MessageType fileSchema, Path filePath) {     RowTypeInfo fileTypeInfo = (RowTypeInfo) ParquetSchemaConverter.fromParquetType(fileSchema).     List<Type> types = new ArrayList<>().     for (int i = 0. i < fieldNames.length. ++i) {         String readFieldName = fieldNames[i].         TypeInformation<?> readFieldType = fieldTypes[i].         if (fileTypeInfo.getFieldIndex(readFieldName) < 0) {             if (!skipWrongSchemaFileSplit) {                 throw new IllegalArgumentException("Field " + readFieldName + " cannot be found in schema of " + " Parquet file: " + filePath + ".").             } else {                 this.skipThisSplit = true.                 return fileSchema.             }         }         if (!readFieldType.equals(fileTypeInfo.getTypeAt(readFieldName))) {             if (!skipWrongSchemaFileSplit) {                 throw new IllegalArgumentException("Expecting type " + readFieldType + " for field " + readFieldName + " but found type " + fileTypeInfo.getTypeAt(readFieldName) + " in Parquet file: " + filePath + ".").             } else {                 this.skipThisSplit = true.                 return fileSchema.             }         }         types.add(fileSchema.getType(readFieldName)).     }     return new MessageType(fileSchema.getName(), types). }
