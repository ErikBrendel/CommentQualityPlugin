commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;9;;@BeforeClass public static void checkCredentialsAndSetup() throws IOException {     // check whether credentials exist     S3TestCredentials.assumeCredentialsAvailable().     skipTest = false.     setupCustomHadoopConfig(). }
false;public,static;0;4;;@AfterClass public static void resetFileSystemConfiguration() throws IOException {     FileSystem.initialize(new Configuration()). }
false;public,static;0;10;;@AfterClass public static void checkAtLeastOneTestRun() {     if (!skipTest) {         assertThat("No S3 filesystem upload test executed. Please activate the " + "'include_hadoop_aws' build profile or set '-Dinclude_hadoop_aws' during build " + "(Hadoop >= 2.6 moved S3 filesystems out of hadoop-common).", numRecursiveUploadTests, greaterThan(0)).     } }
true;private,static;0;32;/**  * Create a Hadoop config file containing S3 access credentials.  *  * <p>Note that we cannot use them as part of the URL since this may fail if the credentials  * contain a "/" (see <a href="https://issues.apache.org/jira/browse/HADOOP-3733">HADOOP-3733</a>).  */ ;/**  * Create a Hadoop config file containing S3 access credentials.  *  * <p>Note that we cannot use them as part of the URL since this may fail if the credentials  * contain a "/" (see <a href="https://issues.apache.org/jira/browse/HADOOP-3733">HADOOP-3733</a>).  */ private static void setupCustomHadoopConfig() throws IOException {     File hadoopConfig = TEMP_FOLDER.newFile().     Map<String, String> /* value */     parameters = new HashMap<>().     // set all different S3 fs implementation variants' configuration keys     parameters.put("fs.s3a.access.key", S3TestCredentials.getS3AccessKey()).     parameters.put("fs.s3a.secret.key", S3TestCredentials.getS3SecretKey()).     parameters.put("fs.s3.awsAccessKeyId", S3TestCredentials.getS3AccessKey()).     parameters.put("fs.s3.awsSecretAccessKey", S3TestCredentials.getS3SecretKey()).     parameters.put("fs.s3n.awsAccessKeyId", S3TestCredentials.getS3AccessKey()).     parameters.put("fs.s3n.awsSecretAccessKey", S3TestCredentials.getS3SecretKey()).     try (PrintStream out = new PrintStream(new FileOutputStream(hadoopConfig))) {         out.println("<?xml version=\"1.0\"?>").         out.println("<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>").         out.println("<configuration>").         for (Map.Entry<String, String> entry : parameters.entrySet()) {             out.println("\t<property>").             out.println("\t\t<name>" + entry.getKey() + "</name>").             out.println("\t\t<value>" + entry.getValue() + "</value>").             out.println("\t</property>").         }         out.println("</configuration>").     }     final Configuration conf = new Configuration().     conf.setString(ConfigConstants.HDFS_SITE_CONFIG, hadoopConfig.getAbsolutePath()).     FileSystem.initialize(conf). }
true;private;2;18;/**  * Verifies that nested directories are properly copied with to the given S3 path (using the  * appropriate file system) during resource uploads for YARN.  *  * @param scheme  * 		file system scheme  * @param pathSuffix  * 		test path suffix which will be the test's target path  */ ;/**  * Verifies that nested directories are properly copied with to the given S3 path (using the  * appropriate file system) during resource uploads for YARN.  *  * @param scheme  * 		file system scheme  * @param pathSuffix  * 		test path suffix which will be the test's target path  */ private void testRecursiveUploadForYarn(String scheme, String pathSuffix) throws Exception {     ++numRecursiveUploadTests.     final Path basePath = new Path(S3TestCredentials.getTestBucketUriWithScheme(scheme) + TEST_DATA_DIR).     final HadoopFileSystem fs = (HadoopFileSystem) basePath.getFileSystem().     assumeFalse(fs.exists(basePath)).     try {         final Path directory = new Path(basePath, pathSuffix).         YarnFileStageTest.testCopyFromLocalRecursive(fs.getHadoopFileSystem(), new org.apache.hadoop.fs.Path(directory.toUri()), tempFolder, true).     } finally {         // clean up         fs.delete(basePath, true).     } }
false;public;0;12;;@Test public void testRecursiveUploadForYarnS3n() throws Exception {     try {         Class.forName("org.apache.hadoop.fs.s3native.NativeS3FileSystem").     } catch (ClassNotFoundException e) {         // not in the classpath, cannot run this test         String msg = "Skipping test because NativeS3FileSystem is not in the class path".         log.info(msg).         assumeNoException(msg, e).     }     testRecursiveUploadForYarn("s3n", "testYarn-s3n"). }
false;public;0;12;;@Test public void testRecursiveUploadForYarnS3a() throws Exception {     try {         Class.forName("org.apache.hadoop.fs.s3a.S3AFileSystem").     } catch (ClassNotFoundException e) {         // not in the classpath, cannot run this test         String msg = "Skipping test because S3AFileSystem is not in the class path".         log.info(msg).         assumeNoException(msg, e).     }     testRecursiveUploadForYarn("s3a", "testYarn-s3a"). }
