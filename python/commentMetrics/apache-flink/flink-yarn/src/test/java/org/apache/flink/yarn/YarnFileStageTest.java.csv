commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;13;;// ------------------------------------------------------------------------ // Test setup and shutdown // ------------------------------------------------------------------------ @BeforeClass public static void createHDFS() throws Exception {     Assume.assumeTrue(!OperatingSystem.isWindows()).     final File tempDir = CLASS_TEMP_DIR.newFolder().     org.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration().     hdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, tempDir.getAbsolutePath()).     MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf).     hdfsCluster = builder.build().     hdfsRootPath = new Path(hdfsCluster.getURI()). }
false;public,static;0;8;;@AfterClass public static void destroyHDFS() {     if (hdfsCluster != null) {         hdfsCluster.shutdown().     }     hdfsCluster = null.     hdfsRootPath = null. }
false;public;0;5;;@Before public void initConfig() {     hadoopConfig = new org.apache.hadoop.conf.Configuration().     hadoopConfig.set(org.apache.hadoop.fs.FileSystem.FS_DEFAULT_NAME_KEY, hdfsRootPath.toString()). }
true;public;0;7;/**  * Verifies that nested directories are properly copied with a <tt>hdfs://</tt> file  * system (from a <tt>file:///absolute/path</tt> source path).  */ ;/**  * Verifies that nested directories are properly copied with a <tt>hdfs://</tt> file  * system (from a <tt>file:///absolute/path</tt> source path).  */ @Test public void testCopyFromLocalRecursiveWithScheme() throws Exception {     final FileSystem targetFileSystem = hdfsRootPath.getFileSystem(hadoopConfig).     final Path targetDir = targetFileSystem.getWorkingDirectory().     testCopyFromLocalRecursive(targetFileSystem, targetDir, tempFolder, true). }
true;public;0;7;/**  * Verifies that nested directories are properly copied with a <tt>hdfs://</tt> file  * system (from a <tt>/absolute/path</tt> source path).  */ ;/**  * Verifies that nested directories are properly copied with a <tt>hdfs://</tt> file  * system (from a <tt>/absolute/path</tt> source path).  */ @Test public void testCopyFromLocalRecursiveWithoutScheme() throws Exception {     final FileSystem targetFileSystem = hdfsRootPath.getFileSystem(hadoopConfig).     final Path targetDir = targetFileSystem.getWorkingDirectory().     testCopyFromLocalRecursive(targetFileSystem, targetDir, tempFolder, false). }
true;static;4;86;/**  * Verifies that nested directories are properly copied with the given filesystem and paths.  *  * @param targetFileSystem  * 		file system of the target path  * @param targetDir  * 		target path (URI like <tt>hdfs://...</tt>)  * @param tempFolder  * 		JUnit temporary folder rule to create the source directory with  * @param addSchemeToLocalPath  * 		whether add the <tt>file://</tt> scheme to the local path to copy from  */ ;/**  * Verifies that nested directories are properly copied with the given filesystem and paths.  *  * @param targetFileSystem  * 		file system of the target path  * @param targetDir  * 		target path (URI like <tt>hdfs://...</tt>)  * @param tempFolder  * 		JUnit temporary folder rule to create the source directory with  * @param addSchemeToLocalPath  * 		whether add the <tt>file://</tt> scheme to the local path to copy from  */ static void testCopyFromLocalRecursive(FileSystem targetFileSystem, Path targetDir, TemporaryFolder tempFolder, boolean addSchemeToLocalPath) throws Exception {     // directory must not yet exist     assertFalse(targetFileSystem.exists(targetDir)).     final File srcDir = tempFolder.newFolder().     final Path srcPath.     if (addSchemeToLocalPath) {         srcPath = new Path("file://" + srcDir.getAbsolutePath()).     } else {         srcPath = new Path(srcDir.getAbsolutePath()).     }     HashMap<String, String> /* contents */     srcFiles = new HashMap<>(4).     // create and fill source files     srcFiles.put("1", "Hello 1").     srcFiles.put("2", "Hello 2").     srcFiles.put("nested/3", "Hello nested/3").     srcFiles.put("nested/4/5", "Hello nested/4/5").     for (Map.Entry<String, String> src : srcFiles.entrySet()) {         File file = new File(srcDir, src.getKey()).         // noinspection ResultOfMethodCallIgnored         file.getParentFile().mkdirs().         try (DataOutputStream out = new DataOutputStream(new FileOutputStream(file))) {             out.writeUTF(src.getValue()).         }     }     // copy the created directory recursively:     try {         List<Path> remotePaths = new ArrayList<>().         HashMap<String, LocalResource> localResources = new HashMap<>().         AbstractYarnClusterDescriptor.uploadAndRegisterFiles(Collections.singletonList(new File(srcPath.toUri().getPath())), targetFileSystem, targetDir, ApplicationId.newInstance(0, 0), remotePaths, localResources, new StringBuilder()).         assertEquals(srcFiles.size(), localResources.size()).         Path workDir = ConverterUtils.getPathFromYarnURL(localResources.get(srcPath.getName() + "/1").getResource()).getParent().         RemoteIterator<LocatedFileStatus> targetFilesIterator = targetFileSystem.listFiles(workDir, true).         HashMap<String, String> /* contents */         targetFiles = new HashMap<>(4).         final int workDirPrefixLength = // one more for the concluding "/"         workDir.toString().length() + 1.         while (targetFilesIterator.hasNext()) {             LocatedFileStatus targetFile = targetFilesIterator.next().             int retries = 5.             do {                 try (FSDataInputStream in = targetFileSystem.open(targetFile.getPath())) {                     String absolutePathString = targetFile.getPath().toString().                     String relativePath = absolutePathString.substring(workDirPrefixLength).                     targetFiles.put(relativePath, in.readUTF()).                     assertEquals("extraneous data in file " + relativePath, -1, in.read()).                     break.                 } catch (FileNotFoundException e) {                     // For S3, read-after-write may be eventually consistent, i.e. when trying                     // to access the object before writing it. see                     // https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel                     // -> try again a bit later                     Thread.sleep(50).                 }             } while ((retries--) > 0).         }         assertThat(targetFiles, equalTo(srcFiles)).     } finally {         // clean up         targetFileSystem.delete(targetDir, true).     } }
