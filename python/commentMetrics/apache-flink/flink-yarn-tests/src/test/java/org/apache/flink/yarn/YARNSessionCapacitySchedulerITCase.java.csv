commented;modifiers;parameterAmount;loc;comment;code
false;public,static;0;12;;@BeforeClass public static void setup() throws Exception {     YARN_CONFIGURATION.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class, ResourceScheduler.class).     YARN_CONFIGURATION.set("yarn.scheduler.capacity.root.queues", "default,qa-team").     YARN_CONFIGURATION.setInt("yarn.scheduler.capacity.root.default.capacity", 40).     YARN_CONFIGURATION.setInt("yarn.scheduler.capacity.root.qa-team.capacity", 60).     YARN_CONFIGURATION.set(YarnTestBase.TEST_CLUSTER_NAME_KEY, "flink-yarn-tests-capacityscheduler").     startYARNWithConfig(YARN_CONFIGURATION).     restClientExecutor = Executors.newSingleThreadExecutor().     restClient = new RestClient(RestClientConfiguration.fromConfiguration(new Configuration()), restClientExecutor). }
false;public,static;0;14;;@AfterClass public static void tearDown() throws Exception {     try {         YarnTestBase.teardown().     } finally {         if (restClient != null) {             restClient.shutdown(Time.seconds(5)).         }         if (restClientExecutor != null) {             restClientExecutor.shutdownNow().         }     } }
true;public;0;10;/**  * Tests that a session cluster, that uses the resources from the <i>qa-team</i> queue,  * can be started from the command line.  */ ;/**  * Tests that a session cluster, that uses the resources from the <i>qa-team</i> queue,  * can be started from the command line.  */ @Test public void testStartYarnSessionClusterInQaTeamQueue() throws IOException {     runWithArgs(new String[] { "-j", flinkUberjar.getAbsolutePath(), "-t", flinkLibFolder.getAbsolutePath(), "-t", flinkShadedHadoopDir.getAbsolutePath(), "-jm", "768m", "-tm", "1024m", "-qu", "qa-team" }, "Flink JobManager is now running on ", null, RunTypes.YARN_SESSION, 0). }
true;public;0;21;/**  * Test per-job yarn cluster  *  * <p>This also tests the prefixed CliFrontend options for the YARN case  * We also test if the requested parallelism of 2 is passed through.  * The parallelism is requested at the YARN client (-ys).  */ ;/**  * Test per-job yarn cluster  *  * <p>This also tests the prefixed CliFrontend options for the YARN case  * We also test if the requested parallelism of 2 is passed through.  * The parallelism is requested at the YARN client (-ys).  */ @Test public void perJobYarnCluster() throws IOException {     LOG.info("Starting perJobYarnCluster()").     addTestAppender(CliFrontend.class, Level.INFO).     File exampleJarLocation = getTestJarPath("BatchWordCount.jar").     runWithArgs(new String[] { "run", "-m", "yarn-cluster", "-yj", flinkUberjar.getAbsolutePath(), "-yt", flinkLibFolder.getAbsolutePath(), "-yt", flinkShadedHadoopDir.getAbsolutePath(), "-yn", "1", // test that the job is executed with a DOP of 2     "-ys", // test that the job is executed with a DOP of 2     "2", "-yjm", "768m", "-ytm", "1024m", exampleJarLocation.getAbsolutePath() }, /* test succeeded after this string */     "Program execution finished", // (we should see "DataSink (...) (1/2)" and "DataSink (...) (2/2)" instead)     new String[] { "DataSink \\(.*\\) \\(1/1\\) switched to FINISHED" }, RunTypes.CLI_FRONTEND, 0, true).     LOG.info("Finished perJobYarnCluster()"). }
true;public;0;39;/**  * Test per-job yarn cluster and memory calculations for off-heap use (see FLINK-7400) with the  * same job as {@link #perJobYarnCluster()}.  *  * <p>This ensures that with (any) pre-allocated off-heap memory by us, there is some off-heap  * memory remaining for Flink's libraries. Creating task managers will thus fail if no off-heap  * memory remains.  */ ;/**  * Test per-job yarn cluster and memory calculations for off-heap use (see FLINK-7400) with the  * same job as {@link #perJobYarnCluster()}.  *  * <p>This ensures that with (any) pre-allocated off-heap memory by us, there is some off-heap  * memory remaining for Flink's libraries. Creating task managers will thus fail if no off-heap  * memory remains.  */ @Test public void perJobYarnClusterOffHeap() throws IOException {     LOG.info("Starting perJobYarnCluster()").     addTestAppender(CliFrontend.class, Level.INFO).     File exampleJarLocation = getTestJarPath("BatchWordCount.jar").     // set memory constraints (otherwise this is the same test as perJobYarnCluster() above)     final long taskManagerMemoryMB = 1024.     // noinspection NumericOverflow if the calculation of the total Java memory size overflows, default configuration parameters are wrong in the first place, so we can ignore this inspection     final long networkBuffersMB = TaskManagerServices.calculateNetworkBufferMemory((taskManagerMemoryMB - ResourceManagerOptions.CONTAINERIZED_HEAP_CUTOFF_MIN.defaultValue()) << 20, new Configuration()) >> 20.     final long offHeapMemory = taskManagerMemoryMB - ResourceManagerOptions.CONTAINERIZED_HEAP_CUTOFF_MIN.defaultValue() - // cutoff memory (will be added automatically)     networkBuffersMB - // reserve something for the Java heap space     100.     runWithArgs(new String[] { "run", "-m", "yarn-cluster", "-yj", flinkUberjar.getAbsolutePath(), "-yt", flinkLibFolder.getAbsolutePath(), "-yt", flinkShadedHadoopDir.getAbsolutePath(), "-yn", "1", // test that the job is executed with a DOP of 2     "-ys", // test that the job is executed with a DOP of 2     "2", "-yjm", "768m", "-ytm", taskManagerMemoryMB + "m", "-yD", "taskmanager.memory.off-heap=true", "-yD", "taskmanager.memory.size=" + offHeapMemory + "m", "-yD", "taskmanager.memory.preallocate=true", exampleJarLocation.getAbsolutePath() }, /* test succeeded after this string */     "Program execution finished", // (we should see "DataSink (...) (1/2)" and "DataSink (...) (2/2)" instead)     new String[] { "DataSink \\(.*\\) \\(1/1\\) switched to FINISHED" }, RunTypes.CLI_FRONTEND, 0, true).     LOG.info("Finished perJobYarnCluster()"). }
true;public;0;58;/**  * Starts a session cluster on YARN, and submits a streaming job.  *  * <p>Tests  * <ul>  * <li>if a custom YARN application name can be set from the command line,  * <li>if the number of TaskManager slots can be set from the command line,  * <li>if dynamic properties from the command line are set,  * <li>if the vcores are set correctly (FLINK-2213),  * <li>if jobmanager hostname/port are shown in web interface (FLINK-1902)  * </ul>  *  * <p><b>Hint: </b> If you think it is a good idea to add more assertions to this test, think again!  */ ;/**  * Starts a session cluster on YARN, and submits a streaming job.  *  * <p>Tests  * <ul>  * <li>if a custom YARN application name can be set from the command line,  * <li>if the number of TaskManager slots can be set from the command line,  * <li>if dynamic properties from the command line are set,  * <li>if the vcores are set correctly (FLINK-2213),  * <li>if jobmanager hostname/port are shown in web interface (FLINK-1902)  * </ul>  *  * <p><b>Hint: </b> If you think it is a good idea to add more assertions to this test, think again!  */ @Test(timeout = 100_000) public void testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots() throws Exception {     checkForProhibitedLogContents = false.     final Runner yarnSessionClusterRunner = startWithArgs(new String[] { "-j", flinkUberjar.getAbsolutePath(), "-t", flinkLibFolder.getAbsolutePath(), "-t", flinkShadedHadoopDir.getAbsolutePath(), "-jm", "768m", "-tm", "1024m", // set the slots 3 to check if the vCores are set properly!     "-s", // set the slots 3 to check if the vCores are set properly!     "3", "-nm", "customName", "-Dfancy-configuration-value=veryFancy", "-Dyarn.maximum-failed-containers=3", "-D" + YarnConfigOptions.VCORES.key() + "=2" }, "Flink JobManager is now running on ", RunTypes.YARN_SESSION).     final String logs = outContent.toString().     final HostAndPort hostAndPort = parseJobManagerHostname(logs).     final String host = hostAndPort.getHostText().     final int port = hostAndPort.getPort().     LOG.info("Extracted hostname:port: {}", host, port).     submitJob("WindowJoin.jar").     //      // Assert that custom YARN application name "customName" is set     //      final ApplicationReport applicationReport = getOnlyApplicationReport().     assertEquals("customName", applicationReport.getName()).     //      // Assert the number of TaskManager slots are set     //      waitForTaskManagerRegistration(host, port, Duration.ofMillis(30_000)).     assertNumberOfSlotsPerTask(host, port, 3).     final Map<String, String> flinkConfig = getFlinkConfig(host, port).     //      // Assert dynamic properties     //      assertThat(flinkConfig, hasEntry("fancy-configuration-value", "veryFancy")).     assertThat(flinkConfig, hasEntry("yarn.maximum-failed-containers", "3")).     //      // FLINK-2213: assert that vcores are set     //      assertThat(flinkConfig, hasEntry(YarnConfigOptions.VCORES.key(), "2")).     //      // FLINK-1902: check if jobmanager hostname is shown in web interface     //      assertThat(flinkConfig, hasEntry(JobManagerOptions.ADDRESS.key(), host)).     yarnSessionClusterRunner.sendStop().     yarnSessionClusterRunner.join(). }
false;private,static;1;16;;private static HostAndPort parseJobManagerHostname(final String logs) {     final Pattern p = Pattern.compile("Flink JobManager is now running on ([a-zA-Z0-9.-]+):([0-9]+)").     final Matcher matches = p.matcher(logs).     String hostname = null.     String port = null.     while (matches.find()) {         hostname = matches.group(1).toLowerCase().         port = matches.group(2).     }     checkState(hostname != null, "hostname not found in log").     checkState(port != null, "port not found in log").     return HostAndPort.fromParts(hostname, Integer.parseInt(port)). }
false;private;0;8;;private ApplicationReport getOnlyApplicationReport() throws IOException, YarnException {     final YarnClient yarnClient = getYarnClient().     checkState(yarnClient != null).     final List<ApplicationReport> apps = yarnClient.getApplications(EnumSet.of(YarnApplicationState.RUNNING)).     // Only one running     assertEquals(1, apps.size()).     return apps.get(0). }
false;private;1;6;;private void submitJob(final String jobFileName) throws IOException, InterruptedException {     Runner jobRunner = startWithArgs(new String[] { "run", "--detached", getTestJarPath(jobFileName).getAbsolutePath() }, "Job has been submitted with JobID", RunTypes.CLI_FRONTEND).     jobRunner.join(). }
false;private,static;3;6;;private static void waitForTaskManagerRegistration(final String host, final int port, final Duration waitDuration) throws Exception {     CommonTestUtils.waitUntilCondition(() -> getNumberOfTaskManagers(host, port) > 0, Deadline.fromNow(waitDuration)). }
false;private,static;3;11;;private static void assertNumberOfSlotsPerTask(final String host, final int port, final int slotsNumber) throws Exception {     try {         CommonTestUtils.waitUntilCondition(() -> getNumberOfSlotsPerTaskManager(host, port) == slotsNumber, Deadline.fromNow(Duration.ofSeconds(30))).     } catch (final TimeoutException e) {         final int currentNumberOfSlots = getNumberOfSlotsPerTaskManager(host, port).         fail(String.format("Expected slots per TM to be %d, was: %d", slotsNumber, currentNumberOfSlots)).     } }
false;private,static;2;8;;private static int getNumberOfTaskManagers(final String host, final int port) throws Exception {     final ClusterOverviewWithVersion clusterOverviewWithVersion = restClient.sendRequest(host, port, ClusterOverviewHeaders.getInstance()).get(30_000, TimeUnit.MILLISECONDS).     return clusterOverviewWithVersion.getNumTaskManagersConnected(). }
false;private,static;2;12;;private static int getNumberOfSlotsPerTaskManager(final String host, final int port) throws Exception {     final TaskManagersInfo taskManagersInfo = restClient.sendRequest(host, port, TaskManagersHeaders.getInstance()).get().     return taskManagersInfo.getTaskManagerInfos().stream().map(TaskManagerInfo::getNumberSlots).findFirst().orElse(0). }
false;private,static;2;10;;private static Map<String, String> getFlinkConfig(final String host, final int port) throws Exception {     final ClusterConfigurationInfo clusterConfigurationInfoEntries = restClient.sendRequest(host, port, ClusterConfigurationInfoHeaders.getInstance()).get().     return clusterConfigurationInfoEntries.stream().collect(Collectors.toMap(ClusterConfigurationInfoEntry::getKey, ClusterConfigurationInfoEntry::getValue)). }
true;public;0;18;/**  * Test deployment to non-existing queue & ensure that the system logs a WARN message  * for the user. (Users had unexpected behavior of Flink on YARN because they mistyped the  * target queue. With an error message, we can help users identifying the issue)  */ ;/**  * Test deployment to non-existing queue & ensure that the system logs a WARN message  * for the user. (Users had unexpected behavior of Flink on YARN because they mistyped the  * target queue. With an error message, we can help users identifying the issue)  */ @Test public void testNonexistingQueueWARNmessage() throws IOException {     LOG.info("Starting testNonexistingQueueWARNmessage()").     addTestAppender(AbstractYarnClusterDescriptor.class, Level.WARN).     try {         runWithArgs(new String[] { "-j", flinkUberjar.getAbsolutePath(), "-t", flinkLibFolder.getAbsolutePath(), "-t", flinkShadedHadoopDir.getAbsolutePath(), "-n", "1", "-jm", "768m", "-tm", "1024m", "-qu", "doesntExist" }, "to unknown queue: doesntExist", null, RunTypes.YARN_SESSION, 1).     } catch (Exception e) {         assertTrue(ExceptionUtils.findThrowableWithMessage(e, "to unknown queue: doesntExist").isPresent()).     }     checkForLogString("The specified queue 'doesntExist' does not exist. Available queues").     LOG.info("Finished testNonexistingQueueWARNmessage()"). }
true;public;0;24;/**  * Test per-job yarn cluster with the parallelism set at the CliFrontend instead of the YARN client.  */ ;/**  * Test per-job yarn cluster with the parallelism set at the CliFrontend instead of the YARN client.  */ @Test public void perJobYarnClusterWithParallelism() throws IOException {     LOG.info("Starting perJobYarnClusterWithParallelism()").     // write log messages to stdout as well, so that the runWithArgs() method     // is catching the log output     addTestAppender(CliFrontend.class, Level.INFO).     File exampleJarLocation = getTestJarPath("BatchWordCount.jar").     runWithArgs(new String[] { "run", // test that the job is executed with a DOP of 2     "-p", // test that the job is executed with a DOP of 2     "2", "-m", "yarn-cluster", "-yj", flinkUberjar.getAbsolutePath(), "-yt", flinkLibFolder.getAbsolutePath(), "-yt", flinkShadedHadoopDir.getAbsolutePath(), "-yn", "1", "-ys", "2", "-yjm", "768m", "-ytm", "1024m", exampleJarLocation.getAbsolutePath() }, /* test succeeded after this string */     "Program execution finished", /* prohibited strings: (we want to see "DataSink (...) (2/2) switched to FINISHED") */     new String[] { "DataSink \\(.*\\) \\(1/1\\) switched to FINISHED" }, RunTypes.CLI_FRONTEND, 0, true).     LOG.info("Finished perJobYarnClusterWithParallelism()"). }
true;public;0;10;/**  * Test a fire-and-forget job submission to a YARN cluster.  */ ;/**  * Test a fire-and-forget job submission to a YARN cluster.  */ @Test(timeout = 60000) public void testDetachedPerJobYarnCluster() throws Exception {     LOG.info("Starting testDetachedPerJobYarnCluster()").     File exampleJarLocation = getTestJarPath("BatchWordCount.jar").     testDetachedPerJobYarnClusterInternal(exampleJarLocation.getAbsolutePath()).     LOG.info("Finished testDetachedPerJobYarnCluster()"). }
true;public;0;10;/**  * Test a fire-and-forget job submission to a YARN cluster.  */ ;/**  * Test a fire-and-forget job submission to a YARN cluster.  */ @Test(timeout = 60000) public void testDetachedPerJobYarnClusterWithStreamingJob() throws Exception {     LOG.info("Starting testDetachedPerJobYarnClusterWithStreamingJob()").     File exampleJarLocation = getTestJarPath("StreamingWordCount.jar").     testDetachedPerJobYarnClusterInternal(exampleJarLocation.getAbsolutePath()).     LOG.info("Finished testDetachedPerJobYarnClusterWithStreamingJob()"). }
false;public;2;4;;@Override public int compare(ApplicationReport o1, ApplicationReport o2) {     return o1.getApplicationId().compareTo(o2.getApplicationId()) * -1. }
false;public;2;4;;@Override public boolean accept(File dir, String name) {     return name.contains("jobmanager.log") && dir.getAbsolutePath().contains(id.toString()). }
false;private;1;161;;private void testDetachedPerJobYarnClusterInternal(String job) throws Exception {     YarnClient yc = YarnClient.createYarnClient().     yc.init(YARN_CONFIGURATION).     yc.start().     // get temporary folder for writing output of wordcount example     File tmpOutFolder = null.     try {         tmpOutFolder = tmp.newFolder().     } catch (IOException e) {         throw new RuntimeException(e).     }     // get temporary file for reading input data for wordcount example     File tmpInFile.     try {         tmpInFile = tmp.newFile().         FileUtils.writeStringToFile(tmpInFile, WordCountData.TEXT).     } catch (IOException e) {         throw new RuntimeException(e).     }     Runner runner = startWithArgs(new String[] { "run", "-m", "yarn-cluster", "-yj", flinkUberjar.getAbsolutePath(), "-yt", flinkLibFolder.getAbsolutePath(), "-yt", flinkShadedHadoopDir.getAbsolutePath(), "-yn", "1", "-yjm", "768m", // of containerized.heap-cutoff-min (default: 600MB)     "-yD", "yarn.heap-cutoff-ratio=0.7", "-yD", "yarn.tags=test-tag", "-ytm", "1024m", // test requesting slots from YARN.     "-ys", // test requesting slots from YARN.     "2", "-p", "2", "--detached", job, "--input", tmpInFile.getAbsoluteFile().toString(), "--output", tmpOutFolder.getAbsoluteFile().toString() }, "Job has been submitted with JobID", RunTypes.CLI_FRONTEND).     // it should usually be 2, but on slow machines, the number varies     Assert.assertTrue("There should be at most 2 containers running", getRunningContainers() <= 2).     // give the runner some time to detach     for (int attempt = 0. runner.isAlive() && attempt < 5. attempt++) {         try {             Thread.sleep(500).         } catch (InterruptedException e) {         }     }     Assert.assertFalse("The runner should detach.", runner.isAlive()).     LOG.info("CLI Frontend has returned, so the job is running").     // find out the application id and wait until it has finished.     try {         List<ApplicationReport> apps = yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING)).         ApplicationId tmpAppId.         if (apps.size() == 1) {             // Better method to find the right appId. But sometimes the app is shutting down very fast             // Only one running             tmpAppId = apps.get(0).getApplicationId().             LOG.info("waiting for the job with appId {} to finish", tmpAppId).             // wait until the app has finished             while (yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING)).size() > 0) {                 sleep(500).             }         } else {             // get appId by finding the latest finished appid             apps = yc.getApplications().             Collections.sort(apps, new Comparator<ApplicationReport>() {                  @Override                 public int compare(ApplicationReport o1, ApplicationReport o2) {                     return o1.getApplicationId().compareTo(o2.getApplicationId()) * -1.                 }             }).             tmpAppId = apps.get(0).getApplicationId().             LOG.info("Selected {} as the last appId from {}", tmpAppId, Arrays.toString(apps.toArray())).         }         final ApplicationId id = tmpAppId.         // now it has finished.         // check the output files.         File[] listOfOutputFiles = tmpOutFolder.listFiles().         Assert.assertNotNull("Taskmanager output not found", listOfOutputFiles).         LOG.info("The job has finished. TaskManager output files found in {}", tmpOutFolder).         // read all output files in output folder to one output string         String content = "".         for (File f : listOfOutputFiles) {             if (f.isFile()) {                 content += FileUtils.readFileToString(f) + "\n".             }         }         // String content = FileUtils.readFileToString(taskmanagerOut).         // check for some of the wordcount outputs.         Assert.assertTrue("Expected string 'da 5' or '(all,2)' not found in string '" + content + "'", content.contains("da 5") || content.contains("(da,5)") || content.contains("(all,2)")).         Assert.assertTrue("Expected string 'der 29' or '(mind,1)' not found in string'" + content + "'", content.contains("der 29") || content.contains("(der,29)") || content.contains("(mind,1)")).         // check if the heap size for the TaskManager was set correctly         File jobmanagerLog = YarnTestBase.findFile("..", new FilenameFilter() {              @Override             public boolean accept(File dir, String name) {                 return name.contains("jobmanager.log") && dir.getAbsolutePath().contains(id.toString()).             }         }).         Assert.assertNotNull("Unable to locate JobManager log", jobmanagerLog).         content = FileUtils.readFileToString(jobmanagerLog).         // TM was started with 1024 but we cut off 70% (NOT THE DEFAULT VALUE)         String expected = "Starting TaskManagers".         Assert.assertTrue("Expected string '" + expected + "' not found in JobManager log: '" + jobmanagerLog + "'", content.contains(expected)).         expected = " (2/2) (attempt #0) to ".         Assert.assertTrue("Expected string '" + expected + "' not found in JobManager log." + "This string checks that the job has been started with a parallelism of 2. Log contents: '" + jobmanagerLog + "'", content.contains(expected)).         // make sure the detached app is really finished.         LOG.info("Checking again that app has finished").         ApplicationReport rep.         do {             sleep(500).             rep = yc.getApplicationReport(id).             LOG.info("Got report {}", rep).         } while (rep.getYarnApplicationState() == YarnApplicationState.RUNNING).         verifyApplicationTags(rep).     } finally {         // cleanup the yarn-properties file         String confDirPath = System.getenv("FLINK_CONF_DIR").         File configDirectory = new File(confDirPath).         LOG.info("testDetachedPerJobYarnClusterInternal: Using configuration directory " + configDirectory.getAbsolutePath()).         // load the configuration         LOG.info("testDetachedPerJobYarnClusterInternal: Trying to load configuration file").         Configuration configuration = GlobalConfiguration.loadConfiguration(configDirectory.getAbsolutePath()).         try {             File yarnPropertiesFile = FlinkYarnSessionCli.getYarnPropertiesLocation(configuration.getValue(YarnConfigOptions.PROPERTIES_FILE_LOCATION)).             if (yarnPropertiesFile.exists()) {                 LOG.info("testDetachedPerJobYarnClusterInternal: Cleaning up temporary Yarn address reference: {}", yarnPropertiesFile.getAbsolutePath()).                 yarnPropertiesFile.delete().             }         } catch (Exception e) {             LOG.warn("testDetachedPerJobYarnClusterInternal: Exception while deleting the JobManager address file", e).         }         try {             LOG.info("testDetachedPerJobYarnClusterInternal: Closing the yarn client").             yc.stop().         } catch (Exception e) {             LOG.warn("testDetachedPerJobYarnClusterInternal: Exception while close the yarn client", e).         }     } }
true;private;1;19;/**  * Ensures that the YARN application tags were set properly.  *  * <p>Since YARN application tags were only added in Hadoop 2.4, but Flink still supports Hadoop 2.3, reflection is  * required to invoke the methods. If the method does not exist, this test passes.  */ ;/**  * Ensures that the YARN application tags were set properly.  *  * <p>Since YARN application tags were only added in Hadoop 2.4, but Flink still supports Hadoop 2.3, reflection is  * required to invoke the methods. If the method does not exist, this test passes.  */ private void verifyApplicationTags(final ApplicationReport report) throws InvocationTargetException, IllegalAccessException {     final Method applicationTagsMethod.     Class<ApplicationReport> clazz = ApplicationReport.class.     try {         // this method is only supported by Hadoop 2.4.0 onwards         applicationTagsMethod = clazz.getMethod("getApplicationTags").     } catch (NoSuchMethodException e) {         // only verify the tags if the method exists         return.     }     @SuppressWarnings("unchecked")     Set<String> applicationTags = (Set<String>) applicationTagsMethod.invoke(report).     assertEquals(Collections.singleton("test-tag"), applicationTags). }
false;public;0;6;;@After public void checkForProhibitedLogContents() {     if (checkForProhibitedLogContents) {         ensureNoProhibitedStringInLogFiles(PROHIBITED_STRINGS, WHITELISTED_STRINGS).     } }
