# id;timestamp;commentText;codeText;commentWords;codeWords
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingLocalParallelism();1427097830;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 1st reduce and 2nd map, such that more tasks are on one instance._Expected to re-establish partitioning between map and reduce via a local hash.;@Test_	public void checkPropertyHandlingWithIncreasingLocalParallelism() {_		final int degOfPar = 2 * DEFAULT_PARALLELISM__		_		_		FileDataSource source = new FileDataSource(new DummyInputFormat(), IN_FILE, "Source")__		source.setParallelism(degOfPar)__		_		MapOperator map1 = MapOperator.builder(new IdentityMap()).name("Map1").build()__		map1.setParallelism(degOfPar)__		map1.setInput(source)__		_		ReduceOperator reduce1 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 1").build()__		reduce1.setParallelism(degOfPar)__		reduce1.setInput(map1)__		_		MapOperator map2 = MapOperator.builder(new IdentityMap()).name("Map2").build()__		map2.setParallelism(degOfPar * 2)__		map2.setInput(reduce1)__		_		ReduceOperator reduce2 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 2").build()__		reduce2.setParallelism(degOfPar * 2)__		reduce2.setInput(map2)__		_		FileDataSink sink = new FileDataSink(new DummyOutputFormat(), OUT_FILE, "Sink")__		sink.setParallelism(degOfPar * 2)__		sink.setInput(reduce2)__		_		Plan plan = new Plan(sink, "Test Increasing parallelism")__		_		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType reduceIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertTrue("Invalid ship strategy for an operator.", _				(ShipStrategyType.PARTITION_RANDOM ==  mapIn && ShipStrategyType.PARTITION_HASH == reduceIn) || _				(ShipStrategyType.PARTITION_HASH == mapIn && ShipStrategyType.FORWARD == reduceIn))__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,1st,reduce,and,2nd,map,such,that,more,tasks,are,on,one,instance,expected,to,re,establish,partitioning,between,map,and,reduce,via,a,local,hash;test,public,void,check,property,handling,with,increasing,local,parallelism,final,int,deg,of,par,2,file,data,source,source,new,file,data,source,new,dummy,input,format,source,source,set,parallelism,deg,of,par,map,operator,map1,map,operator,builder,new,identity,map,name,map1,build,map1,set,parallelism,deg,of,par,map1,set,input,source,reduce,operator,reduce1,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,1,build,reduce1,set,parallelism,deg,of,par,reduce1,set,input,map1,map,operator,map2,map,operator,builder,new,identity,map,name,map2,build,map2,set,parallelism,deg,of,par,2,map2,set,input,reduce1,reduce,operator,reduce2,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,2,build,reduce2,set,parallelism,deg,of,par,2,reduce2,set,input,map2,file,data,sink,sink,new,file,data,sink,new,dummy,output,format,sink,sink,set,parallelism,deg,of,par,2,sink,set,input,reduce2,plan,plan,new,plan,sink,test,increasing,parallelism,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,reduce,in,red2node,get,input,get,ship,strategy,assert,assert,true,invalid,ship,strategy,for,an,operator,ship,strategy,type,map,in,ship,strategy,type,reduce,in,ship,strategy,type,map,in,ship,strategy,type,forward,reduce,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingLocalParallelism();1427784999;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 1st reduce and 2nd map, such that more tasks are on one instance._Expected to re-establish partitioning between map and reduce via a local hash.;@Test_	public void checkPropertyHandlingWithIncreasingLocalParallelism() {_		final int degOfPar = 2 * DEFAULT_PARALLELISM__		_		_		FileDataSource source = new FileDataSource(new DummyInputFormat(), IN_FILE, "Source")__		source.setParallelism(degOfPar)__		_		MapOperator map1 = MapOperator.builder(new IdentityMap()).name("Map1").build()__		map1.setParallelism(degOfPar)__		map1.setInput(source)__		_		ReduceOperator reduce1 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 1").build()__		reduce1.setParallelism(degOfPar)__		reduce1.setInput(map1)__		_		MapOperator map2 = MapOperator.builder(new IdentityMap()).name("Map2").build()__		map2.setParallelism(degOfPar * 2)__		map2.setInput(reduce1)__		_		ReduceOperator reduce2 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 2").build()__		reduce2.setParallelism(degOfPar * 2)__		reduce2.setInput(map2)__		_		FileDataSink sink = new FileDataSink(new DummyOutputFormat(), OUT_FILE, "Sink")__		sink.setParallelism(degOfPar * 2)__		sink.setInput(reduce2)__		_		Plan plan = new Plan(sink, "Test Increasing parallelism")__		_		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType reduceIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertTrue("Invalid ship strategy for an operator.", _				(ShipStrategyType.PARTITION_RANDOM ==  mapIn && ShipStrategyType.PARTITION_HASH == reduceIn) || _				(ShipStrategyType.PARTITION_HASH == mapIn && ShipStrategyType.FORWARD == reduceIn))__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,1st,reduce,and,2nd,map,such,that,more,tasks,are,on,one,instance,expected,to,re,establish,partitioning,between,map,and,reduce,via,a,local,hash;test,public,void,check,property,handling,with,increasing,local,parallelism,final,int,deg,of,par,2,file,data,source,source,new,file,data,source,new,dummy,input,format,source,source,set,parallelism,deg,of,par,map,operator,map1,map,operator,builder,new,identity,map,name,map1,build,map1,set,parallelism,deg,of,par,map1,set,input,source,reduce,operator,reduce1,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,1,build,reduce1,set,parallelism,deg,of,par,reduce1,set,input,map1,map,operator,map2,map,operator,builder,new,identity,map,name,map2,build,map2,set,parallelism,deg,of,par,2,map2,set,input,reduce1,reduce,operator,reduce2,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,2,build,reduce2,set,parallelism,deg,of,par,2,reduce2,set,input,map2,file,data,sink,sink,new,file,data,sink,new,dummy,output,format,sink,sink,set,parallelism,deg,of,par,2,sink,set,input,reduce2,plan,plan,new,plan,sink,test,increasing,parallelism,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,reduce,in,red2node,get,input,get,ship,strategy,assert,assert,true,invalid,ship,strategy,for,an,operator,ship,strategy,type,map,in,ship,strategy,type,reduce,in,ship,strategy,type,map,in,ship,strategy,type,forward,reduce,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingLocalParallelism();1430859707;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 1st reduce and 2nd map, such that more tasks are on one instance._Expected to re-establish partitioning between map and reduce via a local hash.;@Test_	public void checkPropertyHandlingWithIncreasingLocalParallelism() {_		final int p = DEFAULT_PARALLELISM * 2___		_		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(p)__		DataSet<Long> set1 = env.generateSequence(0,1).setParallelism(p)___		set1.map(new IdentityMapper<Long>())_				.withForwardedFields("*").setParallelism(p).name("Map1")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_				.withForwardedFields("*").setParallelism(p).name("Reduce1")_				.map(new IdentityMapper<Long>())_				.withForwardedFields("*").setParallelism(p * 2).name("Map2")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_				.withForwardedFields("*").setParallelism(p * 2).name("Reduce2")_				.output(new DiscardingOutputFormat<Long>()).setParallelism(p * 2).name("Sink")___		JavaPlan plan = env.createProgramPlan()__		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType reduceIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertTrue("Invalid ship strategy for an operator.", _				(ShipStrategyType.PARTITION_RANDOM ==  mapIn && ShipStrategyType.PARTITION_HASH == reduceIn) || _				(ShipStrategyType.PARTITION_HASH == mapIn && ShipStrategyType.FORWARD == reduceIn))__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,1st,reduce,and,2nd,map,such,that,more,tasks,are,on,one,instance,expected,to,re,establish,partitioning,between,map,and,reduce,via,a,local,hash;test,public,void,check,property,handling,with,increasing,local,parallelism,final,int,p,2,execution,environment,env,execution,environment,get,execution,environment,env,set,parallelism,p,data,set,long,set1,env,generate,sequence,0,1,set,parallelism,p,set1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,name,map1,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,name,reduce1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,2,name,map2,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,2,name,reduce2,output,new,discarding,output,format,long,set,parallelism,p,2,name,sink,java,plan,plan,env,create,program,plan,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,reduce,in,red2node,get,input,get,ship,strategy,assert,assert,true,invalid,ship,strategy,for,an,operator,ship,strategy,type,map,in,ship,strategy,type,reduce,in,ship,strategy,type,map,in,ship,strategy,type,forward,reduce,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingLocalParallelism();1449526184;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 1st reduce and 2nd map, such that more tasks are on one instance._Expected to re-establish partitioning between map and reduce via a local hash.;@Test_	public void checkPropertyHandlingWithIncreasingLocalParallelism() {_		final int p = DEFAULT_PARALLELISM * 2___		_		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(p)__		DataSet<Long> set1 = env.generateSequence(0,1).setParallelism(p)___		set1.map(new IdentityMapper<Long>())_				.withForwardedFields("*").setParallelism(p).name("Map1")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_				.withForwardedFields("*").setParallelism(p).name("Reduce1")_				.map(new IdentityMapper<Long>())_				.withForwardedFields("*").setParallelism(p * 2).name("Map2")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_				.withForwardedFields("*").setParallelism(p * 2).name("Reduce2")_				.output(new DiscardingOutputFormat<Long>()).setParallelism(p * 2).name("Sink")___		Plan plan = env.createProgramPlan()__		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType reduceIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertTrue("Invalid ship strategy for an operator.", _				(ShipStrategyType.PARTITION_RANDOM ==  mapIn && ShipStrategyType.PARTITION_HASH == reduceIn) || _				(ShipStrategyType.PARTITION_HASH == mapIn && ShipStrategyType.FORWARD == reduceIn))__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,1st,reduce,and,2nd,map,such,that,more,tasks,are,on,one,instance,expected,to,re,establish,partitioning,between,map,and,reduce,via,a,local,hash;test,public,void,check,property,handling,with,increasing,local,parallelism,final,int,p,2,execution,environment,env,execution,environment,get,execution,environment,env,set,parallelism,p,data,set,long,set1,env,generate,sequence,0,1,set,parallelism,p,set1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,name,map1,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,name,reduce1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,2,name,map2,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,2,name,reduce2,output,new,discarding,output,format,long,set,parallelism,p,2,name,sink,plan,plan,env,create,program,plan,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,reduce,in,red2node,get,input,get,ship,strategy,assert,assert,true,invalid,ship,strategy,for,an,operator,ship,strategy,type,map,in,ship,strategy,type,reduce,in,ship,strategy,type,map,in,ship,strategy,type,forward,reduce,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingGlobalParallelism2();1427097830;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 2nd map and 2nd reduce, so the hash partitioning from 1st reduce is not reusable._Expected to re-establish partitioning between map and reduce (hash).;@Test_	public void checkPropertyHandlingWithIncreasingGlobalParallelism2() {_		final int degOfPar = DEFAULT_PARALLELISM__		_		_		FileDataSource source = new FileDataSource(new DummyInputFormat(), IN_FILE, "Source")__		source.setParallelism(degOfPar)__		_		MapOperator map1 = MapOperator.builder(new IdentityMap()).name("Map1").build()__		map1.setParallelism(degOfPar)__		map1.setInput(source)__		_		ReduceOperator reduce1 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 1").build()__		reduce1.setParallelism(degOfPar)__		reduce1.setInput(map1)__		_		MapOperator map2 = MapOperator.builder(new IdentityMap()).name("Map2").build()__		map2.setParallelism(degOfPar)__		map2.setInput(reduce1)__		_		ReduceOperator reduce2 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 2").build()__		reduce2.setParallelism(degOfPar * 2)__		reduce2.setInput(map2)__		_		FileDataSink sink = new FileDataSink(new DummyOutputFormat(), OUT_FILE, "Sink")__		sink.setParallelism(degOfPar * 2)__		sink.setInput(reduce2)__		_		Plan plan = new Plan(sink, "Test Increasing parallelism")__		_		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType reduceIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.FORWARD, mapIn)__		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.PARTITION_HASH, reduceIn)__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,2nd,map,and,2nd,reduce,so,the,hash,partitioning,from,1st,reduce,is,not,reusable,expected,to,re,establish,partitioning,between,map,and,reduce,hash;test,public,void,check,property,handling,with,increasing,global,parallelism2,final,int,deg,of,par,file,data,source,source,new,file,data,source,new,dummy,input,format,source,source,set,parallelism,deg,of,par,map,operator,map1,map,operator,builder,new,identity,map,name,map1,build,map1,set,parallelism,deg,of,par,map1,set,input,source,reduce,operator,reduce1,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,1,build,reduce1,set,parallelism,deg,of,par,reduce1,set,input,map1,map,operator,map2,map,operator,builder,new,identity,map,name,map2,build,map2,set,parallelism,deg,of,par,map2,set,input,reduce1,reduce,operator,reduce2,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,2,build,reduce2,set,parallelism,deg,of,par,2,reduce2,set,input,map2,file,data,sink,sink,new,file,data,sink,new,dummy,output,format,sink,sink,set,parallelism,deg,of,par,2,sink,set,input,reduce2,plan,plan,new,plan,sink,test,increasing,parallelism,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,reduce,in,red2node,get,input,get,ship,strategy,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,forward,map,in,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,reduce,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingGlobalParallelism2();1427784999;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 2nd map and 2nd reduce, so the hash partitioning from 1st reduce is not reusable._Expected to re-establish partitioning between map and reduce (hash).;@Test_	public void checkPropertyHandlingWithIncreasingGlobalParallelism2() {_		final int degOfPar = DEFAULT_PARALLELISM__		_		_		FileDataSource source = new FileDataSource(new DummyInputFormat(), IN_FILE, "Source")__		source.setParallelism(degOfPar)__		_		MapOperator map1 = MapOperator.builder(new IdentityMap()).name("Map1").build()__		map1.setParallelism(degOfPar)__		map1.setInput(source)__		_		ReduceOperator reduce1 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 1").build()__		reduce1.setParallelism(degOfPar)__		reduce1.setInput(map1)__		_		MapOperator map2 = MapOperator.builder(new IdentityMap()).name("Map2").build()__		map2.setParallelism(degOfPar)__		map2.setInput(reduce1)__		_		ReduceOperator reduce2 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 2").build()__		reduce2.setParallelism(degOfPar * 2)__		reduce2.setInput(map2)__		_		FileDataSink sink = new FileDataSink(new DummyOutputFormat(), OUT_FILE, "Sink")__		sink.setParallelism(degOfPar * 2)__		sink.setInput(reduce2)__		_		Plan plan = new Plan(sink, "Test Increasing parallelism")__		_		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType reduceIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.FORWARD, mapIn)__		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.PARTITION_HASH, reduceIn)__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,2nd,map,and,2nd,reduce,so,the,hash,partitioning,from,1st,reduce,is,not,reusable,expected,to,re,establish,partitioning,between,map,and,reduce,hash;test,public,void,check,property,handling,with,increasing,global,parallelism2,final,int,deg,of,par,file,data,source,source,new,file,data,source,new,dummy,input,format,source,source,set,parallelism,deg,of,par,map,operator,map1,map,operator,builder,new,identity,map,name,map1,build,map1,set,parallelism,deg,of,par,map1,set,input,source,reduce,operator,reduce1,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,1,build,reduce1,set,parallelism,deg,of,par,reduce1,set,input,map1,map,operator,map2,map,operator,builder,new,identity,map,name,map2,build,map2,set,parallelism,deg,of,par,map2,set,input,reduce1,reduce,operator,reduce2,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,2,build,reduce2,set,parallelism,deg,of,par,2,reduce2,set,input,map2,file,data,sink,sink,new,file,data,sink,new,dummy,output,format,sink,sink,set,parallelism,deg,of,par,2,sink,set,input,reduce2,plan,plan,new,plan,sink,test,increasing,parallelism,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,reduce,in,red2node,get,input,get,ship,strategy,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,forward,map,in,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,reduce,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingGlobalParallelism2();1430859707;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 2nd map and 2nd reduce, so the hash partitioning from 1st reduce is not reusable._Expected to re-establish partitioning between map and reduce (hash).;@Test_	public void checkPropertyHandlingWithIncreasingGlobalParallelism2() {_		final int p = DEFAULT_PARALLELISM___		_		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(p)__		DataSet<Long> set1 = env.generateSequence(0,1).setParallelism(p)___		set1.map(new IdentityMapper<Long>())_				.withForwardedFields("*").setParallelism(p).name("Map1")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_				.withForwardedFields("*").setParallelism(p).name("Reduce1")_				.map(new IdentityMapper<Long>())_				.withForwardedFields("*").setParallelism(p).name("Map2")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_				.withForwardedFields("*").setParallelism(p * 2).name("Reduce2")_				.output(new DiscardingOutputFormat<Long>()).setParallelism(p * 2).name("Sink")___		JavaPlan plan = env.createProgramPlan()__		_		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType reduceIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.FORWARD, mapIn)__		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.PARTITION_HASH, reduceIn)__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,2nd,map,and,2nd,reduce,so,the,hash,partitioning,from,1st,reduce,is,not,reusable,expected,to,re,establish,partitioning,between,map,and,reduce,hash;test,public,void,check,property,handling,with,increasing,global,parallelism2,final,int,p,execution,environment,env,execution,environment,get,execution,environment,env,set,parallelism,p,data,set,long,set1,env,generate,sequence,0,1,set,parallelism,p,set1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,name,map1,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,name,reduce1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,name,map2,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,2,name,reduce2,output,new,discarding,output,format,long,set,parallelism,p,2,name,sink,java,plan,plan,env,create,program,plan,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,reduce,in,red2node,get,input,get,ship,strategy,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,forward,map,in,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,reduce,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingGlobalParallelism2();1449526184;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 2nd map and 2nd reduce, so the hash partitioning from 1st reduce is not reusable._Expected to re-establish partitioning between map and reduce (hash).;@Test_	public void checkPropertyHandlingWithIncreasingGlobalParallelism2() {_		final int p = DEFAULT_PARALLELISM___		_		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(p)__		DataSet<Long> set1 = env.generateSequence(0,1).setParallelism(p)___		set1.map(new IdentityMapper<Long>())_				.withForwardedFields("*").setParallelism(p).name("Map1")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_				.withForwardedFields("*").setParallelism(p).name("Reduce1")_				.map(new IdentityMapper<Long>())_				.withForwardedFields("*").setParallelism(p).name("Map2")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_				.withForwardedFields("*").setParallelism(p * 2).name("Reduce2")_				.output(new DiscardingOutputFormat<Long>()).setParallelism(p * 2).name("Sink")___		Plan plan = env.createProgramPlan()__		_		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType reduceIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.FORWARD, mapIn)__		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.PARTITION_HASH, reduceIn)__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,2nd,map,and,2nd,reduce,so,the,hash,partitioning,from,1st,reduce,is,not,reusable,expected,to,re,establish,partitioning,between,map,and,reduce,hash;test,public,void,check,property,handling,with,increasing,global,parallelism2,final,int,p,execution,environment,env,execution,environment,get,execution,environment,env,set,parallelism,p,data,set,long,set1,env,generate,sequence,0,1,set,parallelism,p,set1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,name,map1,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,name,reduce1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,name,map2,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,2,name,reduce2,output,new,discarding,output,format,long,set,parallelism,p,2,name,sink,plan,plan,env,create,program,plan,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,reduce,in,red2node,get,input,get,ship,strategy,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,forward,map,in,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,reduce,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingGlobalParallelism1();1427097830;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 1st reduce and 2nd map, so the hash partitioning from 1st reduce is not reusable._Expected to re-establish partitioning between reduce and map, via hash, because random is a full network_transit as well.;@Test_	public void checkPropertyHandlingWithIncreasingGlobalParallelism1() {_		final int degOfPar = DEFAULT_PARALLELISM__		_		_		FileDataSource source = new FileDataSource(new DummyInputFormat(), IN_FILE, "Source")__		source.setParallelism(degOfPar)__		_		MapOperator map1 = MapOperator.builder(new IdentityMap()).name("Map1").build()__		map1.setParallelism(degOfPar)__		map1.setInput(source)__		_		ReduceOperator reduce1 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 1").build()__		reduce1.setParallelism(degOfPar)__		reduce1.setInput(map1)__		_		MapOperator map2 = MapOperator.builder(new IdentityMap()).name("Map2").build()__		map2.setParallelism(degOfPar * 2)__		map2.setInput(reduce1)__		_		ReduceOperator reduce2 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 2").build()__		reduce2.setParallelism(degOfPar * 2)__		reduce2.setInput(map2)__		_		FileDataSink sink = new FileDataSink(new DummyOutputFormat(), OUT_FILE, "Sink")__		sink.setParallelism(degOfPar * 2)__		sink.setInput(reduce2)__		_		Plan plan = new Plan(sink, "Test Increasing parallelism")__		_		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType redIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.PARTITION_HASH, mapIn)__		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.FORWARD, redIn)__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,1st,reduce,and,2nd,map,so,the,hash,partitioning,from,1st,reduce,is,not,reusable,expected,to,re,establish,partitioning,between,reduce,and,map,via,hash,because,random,is,a,full,network,transit,as,well;test,public,void,check,property,handling,with,increasing,global,parallelism1,final,int,deg,of,par,file,data,source,source,new,file,data,source,new,dummy,input,format,source,source,set,parallelism,deg,of,par,map,operator,map1,map,operator,builder,new,identity,map,name,map1,build,map1,set,parallelism,deg,of,par,map1,set,input,source,reduce,operator,reduce1,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,1,build,reduce1,set,parallelism,deg,of,par,reduce1,set,input,map1,map,operator,map2,map,operator,builder,new,identity,map,name,map2,build,map2,set,parallelism,deg,of,par,2,map2,set,input,reduce1,reduce,operator,reduce2,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,2,build,reduce2,set,parallelism,deg,of,par,2,reduce2,set,input,map2,file,data,sink,sink,new,file,data,sink,new,dummy,output,format,sink,sink,set,parallelism,deg,of,par,2,sink,set,input,reduce2,plan,plan,new,plan,sink,test,increasing,parallelism,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,red,in,red2node,get,input,get,ship,strategy,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,map,in,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,forward,red,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingGlobalParallelism1();1427784999;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 1st reduce and 2nd map, so the hash partitioning from 1st reduce is not reusable._Expected to re-establish partitioning between reduce and map, via hash, because random is a full network_transit as well.;@Test_	public void checkPropertyHandlingWithIncreasingGlobalParallelism1() {_		final int degOfPar = DEFAULT_PARALLELISM__		_		_		FileDataSource source = new FileDataSource(new DummyInputFormat(), IN_FILE, "Source")__		source.setParallelism(degOfPar)__		_		MapOperator map1 = MapOperator.builder(new IdentityMap()).name("Map1").build()__		map1.setParallelism(degOfPar)__		map1.setInput(source)__		_		ReduceOperator reduce1 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 1").build()__		reduce1.setParallelism(degOfPar)__		reduce1.setInput(map1)__		_		MapOperator map2 = MapOperator.builder(new IdentityMap()).name("Map2").build()__		map2.setParallelism(degOfPar * 2)__		map2.setInput(reduce1)__		_		ReduceOperator reduce2 = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0).name("Reduce 2").build()__		reduce2.setParallelism(degOfPar * 2)__		reduce2.setInput(map2)__		_		FileDataSink sink = new FileDataSink(new DummyOutputFormat(), OUT_FILE, "Sink")__		sink.setParallelism(degOfPar * 2)__		sink.setInput(reduce2)__		_		Plan plan = new Plan(sink, "Test Increasing parallelism")__		_		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType redIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.PARTITION_HASH, mapIn)__		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.FORWARD, redIn)__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,1st,reduce,and,2nd,map,so,the,hash,partitioning,from,1st,reduce,is,not,reusable,expected,to,re,establish,partitioning,between,reduce,and,map,via,hash,because,random,is,a,full,network,transit,as,well;test,public,void,check,property,handling,with,increasing,global,parallelism1,final,int,deg,of,par,file,data,source,source,new,file,data,source,new,dummy,input,format,source,source,set,parallelism,deg,of,par,map,operator,map1,map,operator,builder,new,identity,map,name,map1,build,map1,set,parallelism,deg,of,par,map1,set,input,source,reduce,operator,reduce1,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,1,build,reduce1,set,parallelism,deg,of,par,reduce1,set,input,map1,map,operator,map2,map,operator,builder,new,identity,map,name,map2,build,map2,set,parallelism,deg,of,par,2,map2,set,input,reduce1,reduce,operator,reduce2,reduce,operator,builder,new,identity,reduce,int,value,class,0,name,reduce,2,build,reduce2,set,parallelism,deg,of,par,2,reduce2,set,input,map2,file,data,sink,sink,new,file,data,sink,new,dummy,output,format,sink,sink,set,parallelism,deg,of,par,2,sink,set,input,reduce2,plan,plan,new,plan,sink,test,increasing,parallelism,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,red,in,red2node,get,input,get,ship,strategy,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,map,in,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,forward,red,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingGlobalParallelism1();1430859707;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 1st reduce and 2nd map, so the hash partitioning from 1st reduce is not reusable._Expected to re-establish partitioning between reduce and map, via hash, because random is a full network_transit as well.;@Test_	public void checkPropertyHandlingWithIncreasingGlobalParallelism1() {_		final int p = DEFAULT_PARALLELISM___		_		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(p)__		DataSet<Long> set1 = env.generateSequence(0,1).setParallelism(p)___		set1.map(new IdentityMapper<Long>())_					.withForwardedFields("*").setParallelism(p).name("Map1")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_					.withForwardedFields("*").setParallelism(p).name("Reduce1")_				.map(new IdentityMapper<Long>())_					.withForwardedFields("*").setParallelism(p * 2).name("Map2")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_					.withForwardedFields("*").setParallelism(p * 2).name("Reduce2")_				.output(new DiscardingOutputFormat<Long>()).setParallelism(p * 2).name("Sink")___		JavaPlan plan = env.createProgramPlan()__		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType redIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.PARTITION_HASH, mapIn)__		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.FORWARD, redIn)__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,1st,reduce,and,2nd,map,so,the,hash,partitioning,from,1st,reduce,is,not,reusable,expected,to,re,establish,partitioning,between,reduce,and,map,via,hash,because,random,is,a,full,network,transit,as,well;test,public,void,check,property,handling,with,increasing,global,parallelism1,final,int,p,execution,environment,env,execution,environment,get,execution,environment,env,set,parallelism,p,data,set,long,set1,env,generate,sequence,0,1,set,parallelism,p,set1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,name,map1,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,name,reduce1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,2,name,map2,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,2,name,reduce2,output,new,discarding,output,format,long,set,parallelism,p,2,name,sink,java,plan,plan,env,create,program,plan,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,red,in,red2node,get,input,get,ship,strategy,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,map,in,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,forward,red,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithIncreasingGlobalParallelism1();1449526184;Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties).__Increases parallelism between 1st reduce and 2nd map, so the hash partitioning from 1st reduce is not reusable._Expected to re-establish partitioning between reduce and map, via hash, because random is a full network_transit as well.;@Test_	public void checkPropertyHandlingWithIncreasingGlobalParallelism1() {_		final int p = DEFAULT_PARALLELISM___		_		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(p)__		DataSet<Long> set1 = env.generateSequence(0,1).setParallelism(p)___		set1.map(new IdentityMapper<Long>())_					.withForwardedFields("*").setParallelism(p).name("Map1")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_					.withForwardedFields("*").setParallelism(p).name("Reduce1")_				.map(new IdentityMapper<Long>())_					.withForwardedFields("*").setParallelism(p * 2).name("Map2")_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_					.withForwardedFields("*").setParallelism(p * 2).name("Reduce2")_				.output(new DiscardingOutputFormat<Long>()).setParallelism(p * 2).name("Sink")___		Plan plan = env.createProgramPlan()__		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		_		_		_		_		SinkPlanNode sinkNode = oPlan.getDataSinks().iterator().next()__		SingleInputPlanNode red2Node = (SingleInputPlanNode) sinkNode.getPredecessor()__		SingleInputPlanNode map2Node = (SingleInputPlanNode) red2Node.getPredecessor()__		_		ShipStrategyType mapIn = map2Node.getInput().getShipStrategy()__		ShipStrategyType redIn = red2Node.getInput().getShipStrategy()__		_		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.PARTITION_HASH, mapIn)__		Assert.assertEquals("Invalid ship strategy for an operator.", ShipStrategyType.FORWARD, redIn)__	};simple,job,map,reduce,map,reduce,all,functions,preserve,all,fields,hence,all,properties,increases,parallelism,between,1st,reduce,and,2nd,map,so,the,hash,partitioning,from,1st,reduce,is,not,reusable,expected,to,re,establish,partitioning,between,reduce,and,map,via,hash,because,random,is,a,full,network,transit,as,well;test,public,void,check,property,handling,with,increasing,global,parallelism1,final,int,p,execution,environment,env,execution,environment,get,execution,environment,env,set,parallelism,p,data,set,long,set1,env,generate,sequence,0,1,set,parallelism,p,set1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,name,map1,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,name,reduce1,map,new,identity,mapper,long,with,forwarded,fields,set,parallelism,p,2,name,map2,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,p,2,name,reduce2,output,new,discarding,output,format,long,set,parallelism,p,2,name,sink,plan,plan,env,create,program,plan,optimized,plan,o,plan,compile,no,stats,plan,sink,plan,node,sink,node,o,plan,get,data,sinks,iterator,next,single,input,plan,node,red2node,single,input,plan,node,sink,node,get,predecessor,single,input,plan,node,map2node,single,input,plan,node,red2node,get,predecessor,ship,strategy,type,map,in,map2node,get,input,get,ship,strategy,ship,strategy,type,red,in,red2node,get,input,get,ship,strategy,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,map,in,assert,assert,equals,invalid,ship,strategy,for,an,operator,ship,strategy,type,forward,red,in
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithTwoInputs();1427097830;Checks that re-partitioning happens when the inputs of a two-input contract have different parallelisms.__Test Plan:_<pre>__(source) -> reduce -\_Match -> (sink)_(source) -> reduce -/__</pre>;@Test_	public void checkPropertyHandlingWithTwoInputs() {_		__		FileDataSource sourceA = new FileDataSource(new DummyInputFormat(), IN_FILE)__		FileDataSource sourceB = new FileDataSource(new DummyInputFormat(), IN_FILE)__		_		ReduceOperator redA = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0)_			.input(sourceA)_			.build()__		ReduceOperator redB = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0)_			.input(sourceB)_			.build()__		_		JoinOperator mat = JoinOperator.builder(new DummyMatchStub(), IntValue.class, 0, 0)_			.input1(redA)_			.input2(redB)_			.build()__		_		FileDataSink sink = new FileDataSink(new DummyOutputFormat(), OUT_FILE, mat)__		_		sourceA.setParallelism(5)__		sourceB.setParallelism(7)__		redA.setParallelism(5)__		redB.setParallelism(7)__		_		mat.setParallelism(5)__		_		sink.setParallelism(5)__		_		_		_		Plan plan = new Plan(sink, "Partition on DoP Change")__		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		JobGraphGenerator jobGen = new JobGraphGenerator()__		_		_		jobGen.compileJobGraph(oPlan)__		_		oPlan.accept(new Visitor<PlanNode>() {_			_			@Override_			public boolean preVisit(PlanNode visitable) {_				if (visitable instanceof DualInputPlanNode) {_					DualInputPlanNode node = (DualInputPlanNode) visitable__					Channel c1 = node.getInput1()__					Channel c2 = node.getInput2()__					_					Assert.assertEquals("Incompatible shipping strategy chosen for match", ShipStrategyType.FORWARD, c1.getShipStrategy())__					Assert.assertEquals("Incompatible shipping strategy chosen for match", ShipStrategyType.PARTITION_HASH, c2.getShipStrategy())__					return false__				}_				return true__			}_			_			@Override_			public void postVisit(PlanNode visitable) {_				_			}_		})__	};checks,that,re,partitioning,happens,when,the,inputs,of,a,two,input,contract,have,different,parallelisms,test,plan,pre,source,reduce,match,sink,source,reduce,pre;test,public,void,check,property,handling,with,two,inputs,file,data,source,source,a,new,file,data,source,new,dummy,input,format,file,data,source,source,b,new,file,data,source,new,dummy,input,format,reduce,operator,red,a,reduce,operator,builder,new,identity,reduce,int,value,class,0,input,source,a,build,reduce,operator,red,b,reduce,operator,builder,new,identity,reduce,int,value,class,0,input,source,b,build,join,operator,mat,join,operator,builder,new,dummy,match,stub,int,value,class,0,0,input1,red,a,input2,red,b,build,file,data,sink,sink,new,file,data,sink,new,dummy,output,format,mat,source,a,set,parallelism,5,source,b,set,parallelism,7,red,a,set,parallelism,5,red,b,set,parallelism,7,mat,set,parallelism,5,sink,set,parallelism,5,plan,plan,new,plan,sink,partition,on,do,p,change,optimized,plan,o,plan,compile,no,stats,plan,job,graph,generator,job,gen,new,job,graph,generator,job,gen,compile,job,graph,o,plan,o,plan,accept,new,visitor,plan,node,override,public,boolean,pre,visit,plan,node,visitable,if,visitable,instanceof,dual,input,plan,node,dual,input,plan,node,node,dual,input,plan,node,visitable,channel,c1,node,get,input1,channel,c2,node,get,input2,assert,assert,equals,incompatible,shipping,strategy,chosen,for,match,ship,strategy,type,forward,c1,get,ship,strategy,assert,assert,equals,incompatible,shipping,strategy,chosen,for,match,ship,strategy,type,c2,get,ship,strategy,return,false,return,true,override,public,void,post,visit,plan,node,visitable
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithTwoInputs();1427784999;Checks that re-partitioning happens when the inputs of a two-input contract have different parallelisms.__Test Plan:_<pre>__(source) -> reduce -\_Match -> (sink)_(source) -> reduce -/__</pre>;@Test_	public void checkPropertyHandlingWithTwoInputs() {_		__		FileDataSource sourceA = new FileDataSource(new DummyInputFormat(), IN_FILE)__		FileDataSource sourceB = new FileDataSource(new DummyInputFormat(), IN_FILE)__		_		ReduceOperator redA = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0)_			.input(sourceA)_			.build()__		ReduceOperator redB = ReduceOperator.builder(new IdentityReduce(), IntValue.class, 0)_			.input(sourceB)_			.build()__		_		JoinOperator mat = JoinOperator.builder(new DummyMatchStub(), IntValue.class, 0, 0)_			.input1(redA)_			.input2(redB)_			.build()__		_		FileDataSink sink = new FileDataSink(new DummyOutputFormat(), OUT_FILE, mat)__		_		sourceA.setParallelism(5)__		sourceB.setParallelism(7)__		redA.setParallelism(5)__		redB.setParallelism(7)__		_		mat.setParallelism(5)__		_		sink.setParallelism(5)__		_		_		_		Plan plan = new Plan(sink, "Partition on DoP Change")__		_		OptimizedPlan oPlan = compileNoStats(plan)__		_		JobGraphGenerator jobGen = new JobGraphGenerator()__		_		_		jobGen.compileJobGraph(oPlan)__		_		oPlan.accept(new Visitor<PlanNode>() {_			_			@Override_			public boolean preVisit(PlanNode visitable) {_				if (visitable instanceof DualInputPlanNode) {_					DualInputPlanNode node = (DualInputPlanNode) visitable__					Channel c1 = node.getInput1()__					Channel c2 = node.getInput2()__					_					Assert.assertEquals("Incompatible shipping strategy chosen for match", ShipStrategyType.FORWARD, c1.getShipStrategy())__					Assert.assertEquals("Incompatible shipping strategy chosen for match", ShipStrategyType.PARTITION_HASH, c2.getShipStrategy())__					return false__				}_				return true__			}_			_			@Override_			public void postVisit(PlanNode visitable) {_				_			}_		})__	};checks,that,re,partitioning,happens,when,the,inputs,of,a,two,input,contract,have,different,parallelisms,test,plan,pre,source,reduce,match,sink,source,reduce,pre;test,public,void,check,property,handling,with,two,inputs,file,data,source,source,a,new,file,data,source,new,dummy,input,format,file,data,source,source,b,new,file,data,source,new,dummy,input,format,reduce,operator,red,a,reduce,operator,builder,new,identity,reduce,int,value,class,0,input,source,a,build,reduce,operator,red,b,reduce,operator,builder,new,identity,reduce,int,value,class,0,input,source,b,build,join,operator,mat,join,operator,builder,new,dummy,match,stub,int,value,class,0,0,input1,red,a,input2,red,b,build,file,data,sink,sink,new,file,data,sink,new,dummy,output,format,mat,source,a,set,parallelism,5,source,b,set,parallelism,7,red,a,set,parallelism,5,red,b,set,parallelism,7,mat,set,parallelism,5,sink,set,parallelism,5,plan,plan,new,plan,sink,partition,on,do,p,change,optimized,plan,o,plan,compile,no,stats,plan,job,graph,generator,job,gen,new,job,graph,generator,job,gen,compile,job,graph,o,plan,o,plan,accept,new,visitor,plan,node,override,public,boolean,pre,visit,plan,node,visitable,if,visitable,instanceof,dual,input,plan,node,dual,input,plan,node,node,dual,input,plan,node,visitable,channel,c1,node,get,input1,channel,c2,node,get,input2,assert,assert,equals,incompatible,shipping,strategy,chosen,for,match,ship,strategy,type,forward,c1,get,ship,strategy,assert,assert,equals,incompatible,shipping,strategy,chosen,for,match,ship,strategy,type,c2,get,ship,strategy,return,false,return,true,override,public,void,post,visit,plan,node,visitable
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithTwoInputs();1430859707;Checks that re-partitioning happens when the inputs of a two-input contract have different parallelisms.__Test Plan:_<pre>__(source) -> reduce -\_Match -> (sink)_(source) -> reduce -/__</pre>;@Test_	public void checkPropertyHandlingWithTwoInputs() {__		_		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(DEFAULT_PARALLELISM)___		DataSet<Long> set1 = env.generateSequence(0,1).setParallelism(5)__		DataSet<Long> set2 = env.generateSequence(0,1).setParallelism(7)___		DataSet<Long> reduce1 = set1_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_					.withForwardedFields("*").setParallelism(5)__		DataSet<Long> reduce2 = set2_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_					.withForwardedFields("*").setParallelism(7)___		reduce1.join(reduce2).where("*").equalTo("*")_					.with(new IdentityJoiner<Long>()).setParallelism(5)_				.output(new DiscardingOutputFormat<Long>()).setParallelism(5)___		JavaPlan plan = env.createProgramPlan()__		_		OptimizedPlan oPlan = compileNoStats(plan)___		JobGraphGenerator jobGen = new JobGraphGenerator()__		_		_		jobGen.compileJobGraph(oPlan)__		_		oPlan.accept(new Visitor<PlanNode>() {_			_			@Override_			public boolean preVisit(PlanNode visitable) {_				if (visitable instanceof DualInputPlanNode) {_					DualInputPlanNode node = (DualInputPlanNode) visitable__					Channel c1 = node.getInput1()__					Channel c2 = node.getInput2()__					_					Assert.assertEquals("Incompatible shipping strategy chosen for match", ShipStrategyType.FORWARD, c1.getShipStrategy())__					Assert.assertEquals("Incompatible shipping strategy chosen for match", ShipStrategyType.PARTITION_HASH, c2.getShipStrategy())__					return false__				}_				return true__			}_			_			@Override_			public void postVisit(PlanNode visitable) {_				_			}_		})__	};checks,that,re,partitioning,happens,when,the,inputs,of,a,two,input,contract,have,different,parallelisms,test,plan,pre,source,reduce,match,sink,source,reduce,pre;test,public,void,check,property,handling,with,two,inputs,execution,environment,env,execution,environment,get,execution,environment,env,set,parallelism,data,set,long,set1,env,generate,sequence,0,1,set,parallelism,5,data,set,long,set2,env,generate,sequence,0,1,set,parallelism,7,data,set,long,reduce1,set1,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,5,data,set,long,reduce2,set2,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,7,reduce1,join,reduce2,where,equal,to,with,new,identity,joiner,long,set,parallelism,5,output,new,discarding,output,format,long,set,parallelism,5,java,plan,plan,env,create,program,plan,optimized,plan,o,plan,compile,no,stats,plan,job,graph,generator,job,gen,new,job,graph,generator,job,gen,compile,job,graph,o,plan,o,plan,accept,new,visitor,plan,node,override,public,boolean,pre,visit,plan,node,visitable,if,visitable,instanceof,dual,input,plan,node,dual,input,plan,node,node,dual,input,plan,node,visitable,channel,c1,node,get,input1,channel,c2,node,get,input2,assert,assert,equals,incompatible,shipping,strategy,chosen,for,match,ship,strategy,type,forward,c1,get,ship,strategy,assert,assert,equals,incompatible,shipping,strategy,chosen,for,match,ship,strategy,type,c2,get,ship,strategy,return,false,return,true,override,public,void,post,visit,plan,node,visitable
ParallelismChangeTest -> @Test 	public void checkPropertyHandlingWithTwoInputs();1449526184;Checks that re-partitioning happens when the inputs of a two-input contract have different parallelisms.__Test Plan:_<pre>__(source) -> reduce -\_Match -> (sink)_(source) -> reduce -/__</pre>;@Test_	public void checkPropertyHandlingWithTwoInputs() {__		_		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(DEFAULT_PARALLELISM)___		DataSet<Long> set1 = env.generateSequence(0,1).setParallelism(5)__		DataSet<Long> set2 = env.generateSequence(0,1).setParallelism(7)___		DataSet<Long> reduce1 = set1_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_					.withForwardedFields("*").setParallelism(5)__		DataSet<Long> reduce2 = set2_				.groupBy("*").reduceGroup(new IdentityGroupReducer<Long>())_					.withForwardedFields("*").setParallelism(7)___		reduce1.join(reduce2).where("*").equalTo("*")_					.with(new IdentityJoiner<Long>()).setParallelism(5)_				.output(new DiscardingOutputFormat<Long>()).setParallelism(5)___		Plan plan = env.createProgramPlan()__		_		OptimizedPlan oPlan = compileNoStats(plan)___		JobGraphGenerator jobGen = new JobGraphGenerator()__		_		_		jobGen.compileJobGraph(oPlan)__		_		oPlan.accept(new Visitor<PlanNode>() {_			_			@Override_			public boolean preVisit(PlanNode visitable) {_				if (visitable instanceof DualInputPlanNode) {_					DualInputPlanNode node = (DualInputPlanNode) visitable__					Channel c1 = node.getInput1()__					Channel c2 = node.getInput2()__					_					Assert.assertEquals("Incompatible shipping strategy chosen for match", ShipStrategyType.FORWARD, c1.getShipStrategy())__					Assert.assertEquals("Incompatible shipping strategy chosen for match", ShipStrategyType.PARTITION_HASH, c2.getShipStrategy())__					return false__				}_				return true__			}_			_			@Override_			public void postVisit(PlanNode visitable) {_				_			}_		})__	};checks,that,re,partitioning,happens,when,the,inputs,of,a,two,input,contract,have,different,parallelisms,test,plan,pre,source,reduce,match,sink,source,reduce,pre;test,public,void,check,property,handling,with,two,inputs,execution,environment,env,execution,environment,get,execution,environment,env,set,parallelism,data,set,long,set1,env,generate,sequence,0,1,set,parallelism,5,data,set,long,set2,env,generate,sequence,0,1,set,parallelism,7,data,set,long,reduce1,set1,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,5,data,set,long,reduce2,set2,group,by,reduce,group,new,identity,group,reducer,long,with,forwarded,fields,set,parallelism,7,reduce1,join,reduce2,where,equal,to,with,new,identity,joiner,long,set,parallelism,5,output,new,discarding,output,format,long,set,parallelism,5,plan,plan,env,create,program,plan,optimized,plan,o,plan,compile,no,stats,plan,job,graph,generator,job,gen,new,job,graph,generator,job,gen,compile,job,graph,o,plan,o,plan,accept,new,visitor,plan,node,override,public,boolean,pre,visit,plan,node,visitable,if,visitable,instanceof,dual,input,plan,node,dual,input,plan,node,node,dual,input,plan,node,visitable,channel,c1,node,get,input1,channel,c2,node,get,input2,assert,assert,equals,incompatible,shipping,strategy,chosen,for,match,ship,strategy,type,forward,c1,get,ship,strategy,assert,assert,equals,incompatible,shipping,strategy,chosen,for,match,ship,strategy,type,c2,get,ship,strategy,return,false,return,true,override,public,void,post,visit,plan,node,visitable
