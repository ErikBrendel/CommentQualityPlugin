commented;modifiers;parameterAmount;loc;comment;code
false;;1;7;;RecoverableMultiPartUpload getNewRecoverableUpload(Path path) throws IOException {     return RecoverableMultiPartUploadImpl.newUpload(s3AccessHelper, limitedExecutor(), pathToObjectName(path)). }
false;;1;12;;RecoverableMultiPartUpload recoverRecoverableUpload(S3Recoverable recoverable) throws IOException {     final Optional<File> incompletePart = recoverInProgressPart(recoverable).     return RecoverableMultiPartUploadImpl.recoverUpload(s3AccessHelper, limitedExecutor(), recoverable.uploadId(), recoverable.getObjectName(), recoverable.parts(), recoverable.numBytesInParts(), incompletePart). }
false;private;1;21;;private Optional<File> recoverInProgressPart(S3Recoverable recoverable) throws IOException {     final String objectKey = recoverable.incompleteObjectName().     if (objectKey == null) {         return Optional.empty().     }     // download the file (simple way)     final RefCountedFile refCountedFile = tmpFileSupplier.apply(null).     final File file = refCountedFile.getFile().     final long numBytes = s3AccessHelper.getObject(objectKey, file).     if (numBytes != recoverable.incompleteObjectLength()) {         throw new IOException(String.format("Error recovering writer: " + "Downloading the last data chunk file gives incorrect length." + "File length is %d bytes, RecoveryData indicates %d bytes", numBytes, recoverable.incompleteObjectLength())).     }     return Optional.of(file). }
false;private;1;10;;private String pathToObjectName(final Path path) {     org.apache.hadoop.fs.Path hadoopPath = HadoopFileSystem.toHadoopPath(path).     if (!hadoopPath.isAbsolute()) {         hadoopPath = new org.apache.hadoop.fs.Path(fs.getWorkingDirectory(), hadoopPath).     }     return hadoopPath.toUri().getScheme() != null && hadoopPath.toUri().getPath().isEmpty() ? "" : hadoopPath.toUri().getPath().substring(1). }
false;private;0;5;;private Executor limitedExecutor() {     return maxConcurrentUploadsPerStream <= 0 ? executor : new BackPressuringExecutor(executor, maxConcurrentUploadsPerStream). }
