commented;modifiers;parameterAmount;loc;comment;code
true;public;0;3;/**  * Gets the underlying Hadoop FileSystem.  * @return The underlying Hadoop FileSystem.  */ ;/**  * Gets the underlying Hadoop FileSystem.  * @return The underlying Hadoop FileSystem.  */ public org.apache.hadoop.fs.FileSystem getHadoopFileSystem() {     return this.fs. }
false;public;0;4;;// ------------------------------------------------------------------------ // file system methods // ------------------------------------------------------------------------ @Override public Path getWorkingDirectory() {     return new Path(this.fs.getWorkingDirectory().toUri()). }
false;public;0;3;;public Path getHomeDirectory() {     return new Path(this.fs.getHomeDirectory().toUri()). }
false;public;0;4;;@Override public URI getUri() {     return fs.getUri(). }
false;public;1;5;;@Override public FileStatus getFileStatus(final Path f) throws IOException {     org.apache.hadoop.fs.FileStatus status = this.fs.getFileStatus(toHadoopPath(f)).     return new HadoopFileStatus(status). }
false;public;3;20;;@Override public BlockLocation[] getFileBlockLocations(final FileStatus file, final long start, final long len) throws IOException {     if (!(file instanceof HadoopFileStatus)) {         throw new IOException("file is not an instance of DistributedFileStatus").     }     final HadoopFileStatus f = (HadoopFileStatus) file.     final org.apache.hadoop.fs.BlockLocation[] blkLocations = fs.getFileBlockLocations(f.getInternalFileStatus(), start, len).     // Wrap up HDFS specific block location objects     final HadoopBlockLocation[] distBlkLocations = new HadoopBlockLocation[blkLocations.length].     for (int i = 0. i < distBlkLocations.length. i++) {         distBlkLocations[i] = new HadoopBlockLocation(blkLocations[i]).     }     return distBlkLocations. }
false;public;2;6;;@Override public HadoopDataInputStream open(final Path f, final int bufferSize) throws IOException {     final org.apache.hadoop.fs.Path path = toHadoopPath(f).     final org.apache.hadoop.fs.FSDataInputStream fdis = this.fs.open(path, bufferSize).     return new HadoopDataInputStream(fdis). }
false;public;1;6;;@Override public HadoopDataInputStream open(final Path f) throws IOException {     final org.apache.hadoop.fs.Path path = toHadoopPath(f).     final org.apache.hadoop.fs.FSDataInputStream fdis = fs.open(path).     return new HadoopDataInputStream(fdis). }
false;public;5;13;;@Override @SuppressWarnings("deprecation") public HadoopDataOutputStream create(final Path f, final boolean overwrite, final int bufferSize, final short replication, final long blockSize) throws IOException {     final org.apache.hadoop.fs.FSDataOutputStream fdos = this.fs.create(toHadoopPath(f), overwrite, bufferSize, replication, blockSize).     return new HadoopDataOutputStream(fdos). }
false;public;2;6;;@Override public HadoopDataOutputStream create(final Path f, final WriteMode overwrite) throws IOException {     final org.apache.hadoop.fs.FSDataOutputStream fsDataOutputStream = this.fs.create(toHadoopPath(f), overwrite == WriteMode.OVERWRITE).     return new HadoopDataOutputStream(fsDataOutputStream). }
false;public;2;4;;@Override public boolean delete(final Path f, final boolean recursive) throws IOException {     return this.fs.delete(toHadoopPath(f), recursive). }
false;public;1;4;;@Override public boolean exists(Path f) throws IOException {     return this.fs.exists(toHadoopPath(f)). }
false;public;1;12;;@Override public FileStatus[] listStatus(final Path f) throws IOException {     final org.apache.hadoop.fs.FileStatus[] hadoopFiles = this.fs.listStatus(toHadoopPath(f)).     final FileStatus[] files = new FileStatus[hadoopFiles.length].     // Convert types     for (int i = 0. i < files.length. i++) {         files[i] = new HadoopFileStatus(hadoopFiles[i]).     }     return files. }
false;public;1;4;;@Override public boolean mkdirs(final Path f) throws IOException {     return this.fs.mkdirs(toHadoopPath(f)). }
false;public;2;4;;@Override public boolean rename(final Path src, final Path dst) throws IOException {     return this.fs.rename(toHadoopPath(src), toHadoopPath(dst)). }
false;public;0;5;;@SuppressWarnings("deprecation") @Override public long getDefaultBlockSize() {     return this.fs.getDefaultBlockSize(). }
false;public;0;4;;@Override public boolean isDistributedFS() {     return true. }
false;public;0;7;;@Override public FileSystemKind getKind() {     if (fsKind == null) {         fsKind = getKindForScheme(this.fs.getUri().getScheme()).     }     return fsKind. }
false;public;0;7;;@Override public RecoverableWriter createRecoverableWriter() throws IOException {     // messages in the constructor of the writer.     return new HadoopRecoverableWriter(fs). }
false;public,static;1;3;;// ------------------------------------------------------------------------ // Utilities // ------------------------------------------------------------------------ public static org.apache.hadoop.fs.Path toHadoopPath(Path path) {     return new org.apache.hadoop.fs.Path(path.toUri()). }
true;static;1;19;/**  * Gets the kind of the file system from its scheme.  *  * <p>Implementation note: Initially, especially within the Flink 1.3.x line  * (in order to not break backwards compatibility), we must only label file systems  * as 'inconsistent' or as 'not proper filesystems' if we are sure about it.  * Otherwise, we cause regression for example in the performance and cleanup handling  * of checkpoints.  * For that reason, we initially mark some filesystems as 'eventually consistent' or  * as 'object stores', and leave the others as 'consistent file systems'.  */ ;/**  * Gets the kind of the file system from its scheme.  *  * <p>Implementation note: Initially, especially within the Flink 1.3.x line  * (in order to not break backwards compatibility), we must only label file systems  * as 'inconsistent' or as 'not proper filesystems' if we are sure about it.  * Otherwise, we cause regression for example in the performance and cleanup handling  * of checkpoints.  * For that reason, we initially mark some filesystems as 'eventually consistent' or  * as 'object stores', and leave the others as 'consistent file systems'.  */ static FileSystemKind getKindForScheme(String scheme) {     scheme = scheme.toLowerCase(Locale.US).     if (scheme.startsWith("s3") || scheme.startsWith("emr") || scheme.startsWith("oss")) {         // the Amazon S3 storage or Aliyun OSS storage         return FileSystemKind.OBJECT_STORE.     } else if (scheme.startsWith("http") || scheme.startsWith("ftp")) {         // currently to rely on that         return FileSystemKind.OBJECT_STORE.     } else {         // this also includes federated HDFS (viewfs).         return FileSystemKind.FILE_SYSTEM.     } }
