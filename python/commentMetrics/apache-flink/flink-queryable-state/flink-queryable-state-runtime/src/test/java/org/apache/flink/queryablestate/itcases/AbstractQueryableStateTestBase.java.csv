commented;modifiers;parameterAmount;loc;comment;code
false;public;0;9;;@Before public void setUp() throws Exception {     // NOTE: do not use a shared instance for all tests as the tests may break     this.stateBackend = createStateBackend().     Assert.assertNotNull(clusterClient).     maxParallelism = 4. }
true;protected,abstract;0;1;/**  * Creates a state backend instance which is used in the {@link #setUp()} method before each  * test case.  *  * @return a state backend instance for each unit test  */ ;/**  * Creates a state backend instance which is used in the {@link #setUp()} method before each  * test case.  *  * @return a state backend instance for each unit test  */ protected abstract StateBackend createStateBackend() throws Exception.
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;95;/**  * Runs a simple topology producing random (key, 1) pairs at the sources (where  * number of keys is in fixed in range 0...numKeys). The records are keyed and  * a reducing queryable state instance is created, which sums up the records.  *  * <p>After submitting the job in detached mode, the QueryableStateCLient is used  * to query the counts of each key in rounds until all keys have non-zero counts.  */ ;/**  * Runs a simple topology producing random (key, 1) pairs at the sources (where  * number of keys is in fixed in range 0...numKeys). The records are keyed and  * a reducing queryable state instance is created, which sums up the records.  *  * <p>After submitting the job in detached mode, the QueryableStateCLient is used  * to query the counts of each key in rounds until all keys have non-zero counts.  */ @Test public void testQueryableState() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final int numKeys = 256.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestKeyRangeSource(numKeys)).     ReducingStateDescriptor<Tuple2<Integer, Long>> reducingState = new ReducingStateDescriptor<>("any-name", new SumReduce(), source.getType()).     final String queryName = "hakuna-matata".     source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 7143749578983540352L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).asQueryableState(queryName, reducingState).     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         final AtomicLongArray counts = new AtomicLongArray(numKeys).         final List<CompletableFuture<ReducingState<Tuple2<Integer, Long>>>> futures = new ArrayList<>(numKeys).         boolean allNonZero = false.         while (!allNonZero && deadline.hasTimeLeft()) {             allNonZero = true.             futures.clear().             for (int i = 0. i < numKeys. i++) {                 final int key = i.                 if (counts.get(key) > 0L) {                     // Skip this one                     continue.                 } else {                     allNonZero = false.                 }                 CompletableFuture<ReducingState<Tuple2<Integer, Long>>> result = getKvState(deadline, client, jobId, queryName, key, BasicTypeInfo.INT_TYPE_INFO, reducingState, false, executor).                 result.thenAccept(response -> {                     try {                         Tuple2<Integer, Long> res = response.get().                         counts.set(key, res.f1).                         assertEquals("Key mismatch", key, res.f0.intValue()).                     } catch (Exception e) {                         Assert.fail(e.getMessage()).                     }                 }).                 futures.add(result).             }             // wait for all the futures to complete             CompletableFuture.allOf(futures.toArray(new CompletableFuture<?>[futures.size()])).get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).         }         assertTrue("Not all keys are non-zero", allNonZero).         // All should be non-zero         for (int i = 0. i < numKeys. i++) {             long count = counts.get(i).             assertTrue("Count at position " + i + " is " + count, count > 0).         }     } }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;58;/**  * Tests that duplicate query registrations fail the job at the JobManager.  */ ;/**  * Tests that duplicate query registrations fail the job at the JobManager.  */ @Test(timeout = 60_000) public void testDuplicateRegistrationFailsJob() throws Exception {     final int numKeys = 256.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestKeyRangeSource(numKeys)).     // Reducing state     ReducingStateDescriptor<Tuple2<Integer, Long>> reducingState = new ReducingStateDescriptor<>("any-name", new SumReduce(), source.getType()).     final String queryName = "duplicate-me".     final QueryableStateStream<Integer, Tuple2<Integer, Long>> queryableState = source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = -4126824763829132959L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).asQueryableState(queryName, reducingState).     final QueryableStateStream<Integer, Tuple2<Integer, Long>> duplicate = source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = -6265024000462809436L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).asQueryableState(queryName).     // Submit the job graph     final JobGraph jobGraph = env.getStreamGraph().getJobGraph().     clusterClient.setDetached(false).     boolean caughtException = false.     try {         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).     } catch (ProgramInvocationException e) {         String failureCause = ExceptionUtils.stringifyException(e).         assertThat(failureCause, containsString("KvState with name '" + queryName + "' has already been registered by another operator")).         caughtException = true.     }     assertTrue(caughtException). }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;38;/**  * Tests simple value state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The tests succeeds after each subtask index is queried with  * value numElements (the latest element updated the state).  */ ;/**  * Tests simple value state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The tests succeeds after each subtask index is queried with  * value numElements (the latest element updated the state).  */ @Test public void testValueState() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final long numElements = 1024L.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     // Value state     ValueStateDescriptor<Tuple2<Integer, Long>> valueState = new ValueStateDescriptor<>("any", source.getType()).     source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 7662520075515707428L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).asQueryableState("hakuna", valueState).     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         executeValueQuery(deadline, client, jobId, "hakuna", valueState, numElements).     } }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;79;/**  * Tests that the correct exception is thrown if the query  * contains a wrong jobId or wrong queryable state name.  */ ;/**  * Tests that the correct exception is thrown if the query  * contains a wrong jobId or wrong queryable state name.  */ @Test @Ignore public void testWrongJobIdAndWrongQueryableStateName() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final long numElements = 1024L.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     ValueStateDescriptor<Tuple2<Integer, Long>> valueState = new ValueStateDescriptor<>("any", source.getType()).     source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 7662520075515707428L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).asQueryableState("hakuna", valueState).     try (AutoCancellableJob closableJobGraph = new AutoCancellableJob(deadline, clusterClient, env)) {         clusterClient.setDetached(true).         clusterClient.submitJob(closableJobGraph.getJobGraph(), AbstractQueryableStateTestBase.class.getClassLoader()).         CompletableFuture<JobStatus> jobStatusFuture = clusterClient.getJobStatus(closableJobGraph.getJobId()).         while (deadline.hasTimeLeft() && !jobStatusFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).equals(JobStatus.RUNNING)) {             Thread.sleep(50).             jobStatusFuture = clusterClient.getJobStatus(closableJobGraph.getJobId()).         }         assertEquals(JobStatus.RUNNING, jobStatusFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS)).         final JobID wrongJobId = new JobID().         CompletableFuture<ValueState<Tuple2<Integer, Long>>> unknownJobFuture = client.getKvState(// this is the wrong job id         wrongJobId, "hakuna", 0, BasicTypeInfo.INT_TYPE_INFO, valueState).         try {             unknownJobFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).             // by now the request must have failed.             fail().         } catch (ExecutionException e) {             Assert.assertTrue("GOT: " + e.getCause().getMessage(), e.getCause() instanceof RuntimeException).             Assert.assertTrue("GOT: " + e.getCause().getMessage(), e.getCause().getMessage().contains("FlinkJobNotFoundException: Could not find Flink job (" + wrongJobId + ")")).         } catch (Exception f) {             fail("Unexpected type of exception: " + f.getMessage()).         }         CompletableFuture<ValueState<Tuple2<Integer, Long>>> unknownQSName = client.getKvState(closableJobGraph.getJobId(), // this is the wrong name.         "wrong-hakuna", 0, BasicTypeInfo.INT_TYPE_INFO, valueState).         try {             unknownQSName.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).             // by now the request must have failed.             fail().         } catch (ExecutionException e) {             Assert.assertTrue("GOT: " + e.getCause().getMessage(), e.getCause() instanceof RuntimeException).             Assert.assertTrue("GOT: " + e.getCause().getMessage(), e.getCause().getMessage().contains("UnknownKvStateLocation: No KvStateLocation found for KvState instance with name 'wrong-hakuna'.")).         } catch (Exception f) {             fail("Unexpected type of exception: " + f.getMessage()).         }     } }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;50;/**  * Similar tests as {@link #testValueState()} but before submitting the  * job, we already issue one request which fails.  */ ;/**  * Similar tests as {@link #testValueState()} but before submitting the  * job, we already issue one request which fails.  */ @Test public void testQueryNonStartedJobState() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final long numElements = 1024L.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because clusterClient is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     ValueStateDescriptor<Tuple2<Integer, Long>> valueState = new ValueStateDescriptor<>("any", source.getType(), null).     QueryableStateStream<Integer, Tuple2<Integer, Long>> queryableState = source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 7480503339992214681L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).asQueryableState("hakuna", valueState).     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         long expected = numElements.         // query once         client.getKvState(autoCancellableJob.getJobId(), queryableState.getQueryableStateName(), 0, BasicTypeInfo.INT_TYPE_INFO, valueState).         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         executeValueQuery(deadline, client, jobId, "hakuna", valueState, expected).     } }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return 1. }
true;public;0;59;/**  * Tests simple value state queryable state instance with a default value  * set. Each source emits (subtaskIndex, 0)..(subtaskIndex, numElements)  * tuples, the key is mapped to 1 but key 0 is queried which should throw  * a {@link UnknownKeyOrNamespaceException} exception.  *  * @throws UnknownKeyOrNamespaceException thrown due querying a non-existent key  */ ;/**  * Tests simple value state queryable state instance with a default value  * set. Each source emits (subtaskIndex, 0)..(subtaskIndex, numElements)  * tuples, the key is mapped to 1 but key 0 is queried which should throw  * a {@link UnknownKeyOrNamespaceException} exception.  *  * @throws UnknownKeyOrNamespaceException thrown due querying a non-existent key  */ @Test(expected = UnknownKeyOrNamespaceException.class) public void testValueStateDefault() throws Throwable {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final long numElements = 1024L.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     ValueStateDescriptor<Tuple2<Integer, Long>> valueState = new ValueStateDescriptor<>("any", source.getType(), Tuple2.of(0, 1337L)).     // only expose key "1"     QueryableStateStream<Integer, Tuple2<Integer, Long>> queryableState = source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 4509274556892655887L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return 1.         }     }).asQueryableState("hakuna", valueState).     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         // Now query         int key = 0.         CompletableFuture<ValueState<Tuple2<Integer, Long>>> future = getKvState(deadline, client, jobId, queryableState.getQueryableStateName(), key, BasicTypeInfo.INT_TYPE_INFO, valueState, true, executor).         try {             future.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).         } catch (ExecutionException | CompletionException e) {             // exception in an ExecutionException.             throw e.getCause().         }     } }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;41;/**  * Tests simple value state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The tests succeeds after each subtask index is queried with  * value numElements (the latest element updated the state).  *  * <p>This is the same as the simple value state test, but uses the API shortcut.  */ ;/**  * Tests simple value state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The tests succeeds after each subtask index is queried with  * value numElements (the latest element updated the state).  *  * <p>This is the same as the simple value state test, but uses the API shortcut.  */ @Test public void testValueStateShortcut() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final long numElements = 1024L.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     // Value state shortcut     final QueryableStateStream<Integer, Tuple2<Integer, Long>> queryableState = source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 9168901838808830068L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).asQueryableState("matata").     @SuppressWarnings("unchecked")     final ValueStateDescriptor<Tuple2<Integer, Long>> stateDesc = (ValueStateDescriptor<Tuple2<Integer, Long>>) queryableState.getStateDescriptor().     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         executeValueQuery(deadline, client, jobId, "matata", stateDesc, numElements).     } }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;66;/**  * Tests simple folding state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The folding state sums these up and maps them to Strings. The  * test succeeds after each subtask index is queried with result n*(n+1)/2  * (as a String).  */ ;/**  * Tests simple folding state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The folding state sums these up and maps them to Strings. The  * test succeeds after each subtask index is queried with result n*(n+1)/2  * (as a String).  */ @Test public void testFoldingState() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final int numElements = 1024.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     FoldingStateDescriptor<Tuple2<Integer, Long>, String> foldingState = new FoldingStateDescriptor<>("any", "0", new SumFold(), StringSerializer.INSTANCE).     source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = -842809958106747539L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).asQueryableState("pumba", foldingState).     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         final String expected = Integer.toString(numElements * (numElements + 1) / 2).         for (int key = 0. key < maxParallelism. key++) {             boolean success = false.             while (deadline.hasTimeLeft() && !success) {                 CompletableFuture<FoldingState<Tuple2<Integer, Long>, String>> future = getKvState(deadline, client, jobId, "pumba", key, BasicTypeInfo.INT_TYPE_INFO, foldingState, false, executor).                 String value = future.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).get().                 // assertEquals("Key mismatch", key, value.f0.intValue()).                 if (expected.equals(value)) {                     success = true.                 } else {                     // Retry                     Thread.sleep(RETRY_TIMEOUT).                 }             }             assertTrue("Did not succeed query", success).         }     } }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;66;/**  * Tests simple reducing state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The reducing state instance sums these up. The test succeeds  * after each subtask index is queried with result n*(n+1)/2.  */ ;/**  * Tests simple reducing state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The reducing state instance sums these up. The test succeeds  * after each subtask index is queried with result n*(n+1)/2.  */ @Test public void testReducingState() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final long numElements = 1024L.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     ReducingStateDescriptor<Tuple2<Integer, Long>> reducingState = new ReducingStateDescriptor<>("any", new SumReduce(), source.getType()).     source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 8470749712274833552L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).asQueryableState("jungle", reducingState).     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         final long expected = numElements * (numElements + 1L) / 2L.         for (int key = 0. key < maxParallelism. key++) {             boolean success = false.             while (deadline.hasTimeLeft() && !success) {                 CompletableFuture<ReducingState<Tuple2<Integer, Long>>> future = getKvState(deadline, client, jobId, "jungle", key, BasicTypeInfo.INT_TYPE_INFO, reducingState, false, executor).                 Tuple2<Integer, Long> value = future.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).get().                 assertEquals("Key mismatch", key, value.f0.intValue()).                 if (expected == value.f1) {                     success = true.                 } else {                     // Retry                     Thread.sleep(RETRY_TIMEOUT).                 }             }             assertTrue("Did not succeed query", success).         }     } }
false;public;1;5;;@Override public void open(Configuration parameters) throws Exception {     super.open(parameters).     mapState = getRuntimeContext().getMapState(mapStateDescriptor). }
false;public;3;8;;@Override public void processElement(Tuple2<Integer, Long> value, Context ctx, Collector<Object> out) throws Exception {     Tuple2<Integer, Long> v = mapState.get(value.f0).     if (v == null) {         v = new Tuple2<>(value.f0, 0L).     }     mapState.put(value.f0, new Tuple2<>(v.f0, v.f1 + value.f1)). }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;87;/**  * Tests simple map state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The map state instance sums the values up. The test succeeds  * after each subtask index is queried with result n*(n+1)/2.  */ ;/**  * Tests simple map state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The map state instance sums the values up. The test succeeds  * after each subtask index is queried with result n*(n+1)/2.  */ @Test public void testMapState() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final long numElements = 1024L.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     final MapStateDescriptor<Integer, Tuple2<Integer, Long>> mapStateDescriptor = new MapStateDescriptor<>("timon", BasicTypeInfo.INT_TYPE_INFO, source.getType()).     mapStateDescriptor.setQueryable("timon-queryable").     source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 8470749712274833552L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).process(new ProcessFunction<Tuple2<Integer, Long>, Object>() {          private static final long serialVersionUID = -805125545438296619L.          private transient MapState<Integer, Tuple2<Integer, Long>> mapState.          @Override         public void open(Configuration parameters) throws Exception {             super.open(parameters).             mapState = getRuntimeContext().getMapState(mapStateDescriptor).         }          @Override         public void processElement(Tuple2<Integer, Long> value, Context ctx, Collector<Object> out) throws Exception {             Tuple2<Integer, Long> v = mapState.get(value.f0).             if (v == null) {                 v = new Tuple2<>(value.f0, 0L).             }             mapState.put(value.f0, new Tuple2<>(v.f0, v.f1 + value.f1)).         }     }).     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         final long expected = numElements * (numElements + 1L) / 2L.         for (int key = 0. key < maxParallelism. key++) {             boolean success = false.             while (deadline.hasTimeLeft() && !success) {                 CompletableFuture<MapState<Integer, Tuple2<Integer, Long>>> future = getKvState(deadline, client, jobId, "timon-queryable", key, BasicTypeInfo.INT_TYPE_INFO, mapStateDescriptor, false, executor).                 Tuple2<Integer, Long> value = future.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).get(key).                 if (value != null && value.f0 != null && expected == value.f1) {                     assertEquals("Key mismatch", key, value.f0.intValue()).                     success = true.                 } else {                     // Retry                     Thread.sleep(RETRY_TIMEOUT).                 }             }             assertTrue("Did not succeed query", success).         }     } }
false;public;1;5;;@Override public void open(Configuration parameters) throws Exception {     super.open(parameters).     listState = getRuntimeContext().getListState(listStateDescriptor). }
false;public;3;4;;@Override public void processElement(Tuple2<Integer, Long> value, Context ctx, Collector<Object> out) throws Exception {     listState.add(value.f1). }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
true;public;0;96;/**  * Tests simple list state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The list state instance add the values to the list. The test  * succeeds after each subtask index is queried and the list contains  * the correct number of distinct elements.  */ ;/**  * Tests simple list state queryable state instance. Each source emits  * (subtaskIndex, 0)..(subtaskIndex, numElements) tuples, which are then  * queried. The list state instance add the values to the list. The test  * succeeds after each subtask index is queried and the list contains  * the correct number of distinct elements.  */ @Test public void testListState() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final long numElements = 1024L.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     final ListStateDescriptor<Long> listStateDescriptor = new ListStateDescriptor<Long>("list", BasicTypeInfo.LONG_TYPE_INFO).     listStateDescriptor.setQueryable("list-queryable").     source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 8470749712274833552L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).process(new ProcessFunction<Tuple2<Integer, Long>, Object>() {          private static final long serialVersionUID = -805125545438296619L.          private transient ListState<Long> listState.          @Override         public void open(Configuration parameters) throws Exception {             super.open(parameters).             listState = getRuntimeContext().getListState(listStateDescriptor).         }          @Override         public void processElement(Tuple2<Integer, Long> value, Context ctx, Collector<Object> out) throws Exception {             listState.add(value.f1).         }     }).     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         final Map<Integer, Set<Long>> results = new HashMap<>().         for (int key = 0. key < maxParallelism. key++) {             boolean success = false.             while (deadline.hasTimeLeft() && !success) {                 final CompletableFuture<ListState<Long>> future = getKvState(deadline, client, jobId, "list-queryable", key, BasicTypeInfo.INT_TYPE_INFO, listStateDescriptor, false, executor).                 Iterable<Long> value = future.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).get().                 Set<Long> res = new HashSet<>().                 for (Long v : value) {                     res.add(v).                 }                 // the source starts at 0, so +1                 if (res.size() == numElements + 1L) {                     success = true.                     results.put(key, res).                 } else {                     // Retry                     Thread.sleep(RETRY_TIMEOUT).                 }             }             assertTrue("Did not succeed query", success).         }         for (int key = 0. key < maxParallelism. key++) {             Set<Long> values = results.get(key).             for (long i = 0L. i <= numElements. i++) {                 assertTrue(values.contains(i)).             }         }     } }
false;public;1;4;;@Override public Integer getKey(Tuple2<Integer, Long> value) {     return value.f0. }
false;public;0;68;;@Test public void testAggregatingState() throws Exception {     final Deadline deadline = Deadline.now().plus(TEST_TIMEOUT).     final long numElements = 1024L.     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().     env.setStateBackend(stateBackend).     env.setParallelism(maxParallelism).     // Very important, because cluster is shared between tests and we     // don't explicitly check that all slots are available before     // submitting.     env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 1000L)).     DataStream<Tuple2<Integer, Long>> source = env.addSource(new TestAscendingValueSource(numElements)).     final AggregatingStateDescriptor<Tuple2<Integer, Long>, String, String> aggrStateDescriptor = new AggregatingStateDescriptor<>("aggregates", new SumAggr(), String.class).     aggrStateDescriptor.setQueryable("aggr-queryable").     source.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {          private static final long serialVersionUID = 8470749712274833552L.          @Override         public Integer getKey(Tuple2<Integer, Long> value) {             return value.f0.         }     }).transform("TestAggregatingOperator", BasicTypeInfo.STRING_TYPE_INFO, new AggregatingTestOperator(aggrStateDescriptor)).     try (AutoCancellableJob autoCancellableJob = new AutoCancellableJob(deadline, clusterClient, env)) {         final JobID jobId = autoCancellableJob.getJobId().         final JobGraph jobGraph = autoCancellableJob.getJobGraph().         clusterClient.setDetached(true).         clusterClient.submitJob(jobGraph, AbstractQueryableStateTestBase.class.getClassLoader()).         for (int key = 0. key < maxParallelism. key++) {             boolean success = false.             while (deadline.hasTimeLeft() && !success) {                 CompletableFuture<AggregatingState<Tuple2<Integer, Long>, String>> future = getKvState(deadline, client, jobId, "aggr-queryable", key, BasicTypeInfo.INT_TYPE_INFO, aggrStateDescriptor, false, executor).                 String value = future.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).get().                 if (Long.parseLong(value) == numElements * (numElements + 1L) / 2L) {                     success = true.                 } else {                     // Retry                     Thread.sleep(RETRY_TIMEOUT).                 }             }             assertTrue("Did not succeed query", success).         }     } }
false;public;1;4;;@Override public void open(Configuration parameters) throws Exception {     super.open(parameters). }
false;public;1;22;;@Override public void run(SourceContext<Tuple2<Integer, Long>> ctx) throws Exception {     // f0 => key     int key = getRuntimeContext().getIndexOfThisSubtask().     Tuple2<Integer, Long> record = new Tuple2<>(key, 0L).     long currentValue = 0.     while (isRunning && currentValue <= maxValue) {         synchronized (ctx.getCheckpointLock()) {             record.f1 = currentValue.             ctx.collect(record).         }         currentValue++.     }     while (isRunning) {         synchronized (this) {             wait().         }     } }
false;public;0;8;;@Override public void cancel() {     isRunning = false.     synchronized (this) {         notifyAll().     } }
false;public;1;7;;@Override public void open(Configuration parameters) throws Exception {     super.open(parameters).     if (getRuntimeContext().getIndexOfThisSubtask() == 0) {         LATEST_CHECKPOINT_ID.set(0L).     } }
false;public;1;18;;@Override public void run(SourceContext<Tuple2<Integer, Long>> ctx) throws Exception {     // f0 => key     Tuple2<Integer, Long> record = new Tuple2<>(0, 1L).     while (isRunning) {         synchronized (ctx.getCheckpointLock()) {             record.f0 = random.nextInt(numKeys).             ctx.collect(record).             counter++.         }         if (counter % 50 == 0) {             // mild slow down             Thread.sleep(1L).         }     } }
false;public;0;4;;@Override public void cancel() {     isRunning = false. }
false;public;1;6;;@Override public void notifyCheckpointComplete(long checkpointId) throws Exception {     if (getRuntimeContext().getIndexOfThisSubtask() == 0) {         LATEST_CHECKPOINT_ID.set(checkpointId).     } }
false;public;0;8;;@Override public void open() throws Exception {     super.open().     this.state = getKeyedStateBackend().getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor). }
false;public;1;4;;@Override public void processElement(StreamRecord<Tuple2<Integer, Long>> element) throws Exception {     state.add(element.getValue()). }
false;public;0;4;;@Override public String createAccumulator() {     return "0". }
false;public;2;6;;@Override public String add(Tuple2<Integer, Long> value, String accumulator) {     long acc = Long.valueOf(accumulator).     acc += value.f1.     return Long.toString(acc). }
false;public;1;4;;@Override public String getResult(String accumulator) {     return accumulator. }
false;public;2;4;;@Override public String merge(String a, String b) {     return Long.toString(Long.valueOf(a) + Long.valueOf(b)). }
false;public;2;6;;@Override public String fold(String accumulator, Tuple2<Integer, Long> value) throws Exception {     long acc = Long.valueOf(accumulator).     acc += value.f1.     return Long.toString(acc). }
false;public;2;5;;@Override public Tuple2<Integer, Long> reduce(Tuple2<Integer, Long> value1, Tuple2<Integer, Long> value2) throws Exception {     value1.f1 += value2.f1.     return value1. }
false;;0;3;;JobGraph getJobGraph() {     return jobGraph. }
false;;0;3;;JobID getJobId() {     return jobId. }
false;public;0;15;;@Override public void close() throws Exception {     // Free cluster resources     clusterClient.cancel(jobId).     // cancel() is non-blocking so do this to make sure the job finished     CompletableFuture<JobStatus> jobStatusFuture = FutureUtils.retrySuccessfulWithDelay(() -> clusterClient.getJobStatus(jobId), Time.milliseconds(50), deadline, (jobStatus) -> jobStatus.equals(JobStatus.CANCELED), TestingUtils.defaultScheduledExecutor()).     assertEquals(JobStatus.CANCELED, jobStatusFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS)). }
false;private,static;9;17;;private static <K, S extends State, V> CompletableFuture<S> getKvState(final Deadline deadline, final QueryableStateClient client, final JobID jobId, final String queryName, final K key, final TypeInformation<K> keyTypeInfo, final StateDescriptor<S, V> stateDescriptor, final boolean failForUnknownKeyOrNamespace, final ScheduledExecutor executor) {     final CompletableFuture<S> resultFuture = new CompletableFuture<>().     getKvStateIgnoringCertainExceptions(deadline, resultFuture, client, jobId, queryName, key, keyTypeInfo, stateDescriptor, failForUnknownKeyOrNamespace, executor).     return resultFuture. }
false;private,static;10;35;;private static <K, S extends State, V> void getKvStateIgnoringCertainExceptions(final Deadline deadline, final CompletableFuture<S> resultFuture, final QueryableStateClient client, final JobID jobId, final String queryName, final K key, final TypeInformation<K> keyTypeInfo, final StateDescriptor<S, V> stateDescriptor, final boolean failForUnknownKeyOrNamespace, final ScheduledExecutor executor) {     if (!resultFuture.isDone()) {         CompletableFuture<S> expected = client.getKvState(jobId, queryName, key, keyTypeInfo, stateDescriptor).         expected.whenCompleteAsync((result, throwable) -> {             if (throwable != null) {                 if (throwable.getCause() instanceof CancellationException || throwable.getCause() instanceof AssertionError || (failForUnknownKeyOrNamespace && throwable.getCause() instanceof UnknownKeyOrNamespaceException)) {                     resultFuture.completeExceptionally(throwable.getCause()).                 } else if (deadline.hasTimeLeft()) {                     getKvStateIgnoringCertainExceptions(deadline, resultFuture, client, jobId, queryName, key, keyTypeInfo, stateDescriptor, failForUnknownKeyOrNamespace, executor).                 }             } else {                 resultFuture.complete(result).             }         }, executor).         resultFuture.whenComplete((result, throwable) -> expected.cancel(false)).     } }
true;private;6;36;/**  * Retry a query for state for keys between 0 and {@link #maxParallelism} until  * <tt>expected</tt> equals the value of the result tuple's second field.  */ ;/**  * Retry a query for state for keys between 0 and {@link #maxParallelism} until  * <tt>expected</tt> equals the value of the result tuple's second field.  */ private void executeValueQuery(final Deadline deadline, final QueryableStateClient client, final JobID jobId, final String queryableStateName, final ValueStateDescriptor<Tuple2<Integer, Long>> stateDescriptor, final long expected) throws Exception {     for (int key = 0. key < maxParallelism. key++) {         boolean success = false.         while (deadline.hasTimeLeft() && !success) {             CompletableFuture<ValueState<Tuple2<Integer, Long>>> future = getKvState(deadline, client, jobId, queryableStateName, key, BasicTypeInfo.INT_TYPE_INFO, stateDescriptor, false, executor).             Tuple2<Integer, Long> value = future.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS).value().             assertEquals("Key mismatch", key, value.f0.intValue()).             if (expected == value.f1) {                 success = true.             } else {                 // Retry                 Thread.sleep(RETRY_TIMEOUT).             }         }         assertTrue("Did not succeed query", success).     } }
