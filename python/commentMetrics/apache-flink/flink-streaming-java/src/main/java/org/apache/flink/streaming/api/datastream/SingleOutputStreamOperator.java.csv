commented;modifiers;parameterAmount;loc;comment;code
true;public;0;3;/**  * Gets the name of the current data stream. This name is  * used by the visualization and logging during runtime.  *  * @return Name of the stream.  */ ;/**  * Gets the name of the current data stream. This name is  * used by the visualization and logging during runtime.  *  * @return Name of the stream.  */ public String getName() {     return transformation.getName(). }
true;public;1;4;/**  * Sets the name of the current data stream. This name is  * used by the visualization and logging during runtime.  *  * @return The named operator.  */ ;/**  * Sets the name of the current data stream. This name is  * used by the visualization and logging during runtime.  *  * @return The named operator.  */ public SingleOutputStreamOperator<T> name(String name) {     transformation.setName(name).     return this. }
true;public;1;5;/**  * Sets an ID for this operator.  *  * <p>The specified ID is used to assign the same operator ID across job  * submissions (for example when starting a job from a savepoint).  *  * <p><strong>Important</strong>: this ID needs to be unique per  * transformation and job. Otherwise, job submission will fail.  *  * @param uid The unique user-specified ID of this transformation.  * @return The operator with the specified ID.  */ ;/**  * Sets an ID for this operator.  *  * <p>The specified ID is used to assign the same operator ID across job  * submissions (for example when starting a job from a savepoint).  *  * <p><strong>Important</strong>: this ID needs to be unique per  * transformation and job. Otherwise, job submission will fail.  *  * @param uid The unique user-specified ID of this transformation.  * @return The operator with the specified ID.  */ @PublicEvolving public SingleOutputStreamOperator<T> uid(String uid) {     transformation.setUid(uid).     return this. }
true;public;1;5;/**  * Sets an user provided hash for this operator. This will be used AS IS the create the  * JobVertexID.  *  * <p>The user provided hash is an alternative to the generated hashes, that is considered when  * identifying an operator through the default hash mechanics fails (e.g. because of changes  * between Flink versions).  *  * <p><strong>Important</strong>: this should be used as a workaround or for trouble shooting.  * The provided hash needs to be unique per transformation and job. Otherwise, job submission  * will fail. Furthermore, you cannot assign user-specified hash to intermediate nodes in an  * operator chain and trying so will let your job fail.  *  * <p>A use case for this is in migration between Flink versions or changing the jobs in a way  * that changes the automatically generated hashes. In this case, providing the previous hashes  * directly through this method (e.g. obtained from old logs) can help to reestablish a lost  * mapping from states to their target operator.  * <p/>  *  * @param uidHash The user provided hash for this operator. This will become the JobVertexID,  *                  which is shown in the logs and web ui.  * @return The operator with the user provided hash.  */ ;/**  * Sets an user provided hash for this operator. This will be used AS IS the create the  * JobVertexID.  *  * <p>The user provided hash is an alternative to the generated hashes, that is considered when  * identifying an operator through the default hash mechanics fails (e.g. because of changes  * between Flink versions).  *  * <p><strong>Important</strong>: this should be used as a workaround or for trouble shooting.  * The provided hash needs to be unique per transformation and job. Otherwise, job submission  * will fail. Furthermore, you cannot assign user-specified hash to intermediate nodes in an  * operator chain and trying so will let your job fail.  *  * <p>A use case for this is in migration between Flink versions or changing the jobs in a way  * that changes the automatically generated hashes. In this case, providing the previous hashes  * directly through this method (e.g. obtained from old logs) can help to reestablish a lost  * mapping from states to their target operator.  * <p/>  *  * @param uidHash The user provided hash for this operator. This will become the JobVertexID,  *                  which is shown in the logs and web ui.  * @return The operator with the user provided hash.  */ @PublicEvolving public SingleOutputStreamOperator<T> setUidHash(String uidHash) {     transformation.setUidHash(uidHash).     return this. }
true;public;1;8;/**  * Sets the parallelism for this operator.  *  * @param parallelism  *            The parallelism for this operator.  * @return The operator with set parallelism.  */ ;/**  * Sets the parallelism for this operator.  *  * @param parallelism  *            The parallelism for this operator.  * @return The operator with set parallelism.  */ public SingleOutputStreamOperator<T> setParallelism(int parallelism) {     Preconditions.checkArgument(canBeParallel() || parallelism == 1, "The parallelism of non parallel operator must be 1.").     transformation.setParallelism(parallelism).     return this. }
true;public;1;12;/**  * Sets the maximum parallelism of this operator.  *  * <p>The maximum parallelism specifies the upper bound for dynamic scaling. It also defines the  * number of key groups used for partitioned state.  *  * @param maxParallelism Maximum parallelism  * @return The operator with set maximum parallelism  */ ;/**  * Sets the maximum parallelism of this operator.  *  * <p>The maximum parallelism specifies the upper bound for dynamic scaling. It also defines the  * number of key groups used for partitioned state.  *  * @param maxParallelism Maximum parallelism  * @return The operator with set maximum parallelism  */ @PublicEvolving public SingleOutputStreamOperator<T> setMaxParallelism(int maxParallelism) {     Preconditions.checkArgument(maxParallelism > 0, "The maximum parallelism must be greater than 0.").     Preconditions.checkArgument(canBeParallel() || maxParallelism == 1, "The maximum parallelism of non parallel operator must be 1.").     transformation.setMaxParallelism(maxParallelism).     return this. }
true;private;2;10;/**  * Sets the minimum and preferred resources for this operator, and the lower and upper resource limits will  * be considered in dynamic resource resize feature for future plan.  *  * @param minResources The minimum resources for this operator.  * @param preferredResources The preferred resources for this operator.  * @return The operator with set minimum and preferred resources.  */ ;// --------------------------------------------------------------------------- // Fine-grained resource profiles are an incomplete work-in-progress feature // The setters are hence private at this point. // --------------------------------------------------------------------------- /**  * Sets the minimum and preferred resources for this operator, and the lower and upper resource limits will  * be considered in dynamic resource resize feature for future plan.  *  * @param minResources The minimum resources for this operator.  * @param preferredResources The preferred resources for this operator.  * @return The operator with set minimum and preferred resources.  */ private SingleOutputStreamOperator<T> setResources(ResourceSpec minResources, ResourceSpec preferredResources) {     Preconditions.checkNotNull(minResources, "The min resources must be not null.").     Preconditions.checkNotNull(preferredResources, "The preferred resources must be not null.").     Preconditions.checkArgument(minResources.isValid() && preferredResources.isValid() && minResources.lessThanOrEqual(preferredResources), "The values in resources must be not less than 0 and the preferred resources must be greater than the min resources.").     transformation.setResources(minResources, preferredResources).     return this. }
true;private;1;8;/**  * Sets the resources for this operator, the minimum and preferred resources are the same by default.  *  * @param resources The resources for this operator.  * @return The operator with set minimum and preferred resources.  */ ;/**  * Sets the resources for this operator, the minimum and preferred resources are the same by default.  *  * @param resources The resources for this operator.  * @return The operator with set minimum and preferred resources.  */ private SingleOutputStreamOperator<T> setResources(ResourceSpec resources) {     Preconditions.checkNotNull(resources, "The resources must be not null.").     Preconditions.checkArgument(resources.isValid(), "The values in resources must be not less than 0.").     transformation.setResources(resources, resources).     return this. }
false;private;0;3;;private boolean canBeParallel() {     return !nonParallel. }
true;public;0;7;/**  * Sets the parallelism and maximum parallelism of this operator to one.  * And mark this operator cannot set a non-1 degree of parallelism.  *  * @return The operator with only one parallelism.  */ ;/**  * Sets the parallelism and maximum parallelism of this operator to one.  * And mark this operator cannot set a non-1 degree of parallelism.  *  * @return The operator with only one parallelism.  */ @PublicEvolving public SingleOutputStreamOperator<T> forceNonParallel() {     transformation.setParallelism(1).     transformation.setMaxParallelism(1).     nonParallel = true.     return this. }
true;public;1;5;/**  * Sets the buffering timeout for data produced by this operation.  * The timeout defines how long data may linger in a partially full buffer  * before being sent over the network.  *  * <p>Lower timeouts lead to lower tail latencies, but may affect throughput.  * Timeouts of 1 ms still sustain high throughput, even for jobs with high parallelism.  *  * <p>A value of '-1' means that the default buffer timeout should be used. A value  * of '0' indicates that no buffering should happen, and all records/events should be  * immediately sent through the network, without additional buffering.  *  * @param timeoutMillis  *            The maximum time between two output flushes.  * @return The operator with buffer timeout set.  */ ;/**  * Sets the buffering timeout for data produced by this operation.  * The timeout defines how long data may linger in a partially full buffer  * before being sent over the network.  *  * <p>Lower timeouts lead to lower tail latencies, but may affect throughput.  * Timeouts of 1 ms still sustain high throughput, even for jobs with high parallelism.  *  * <p>A value of '-1' means that the default buffer timeout should be used. A value  * of '0' indicates that no buffering should happen, and all records/events should be  * immediately sent through the network, without additional buffering.  *  * @param timeoutMillis  *            The maximum time between two output flushes.  * @return The operator with buffer timeout set.  */ public SingleOutputStreamOperator<T> setBufferTimeout(long timeoutMillis) {     checkArgument(timeoutMillis >= -1, "timeout must be >= -1").     transformation.setBufferTimeout(timeoutMillis).     return this. }
true;private;1;5;/**  * Sets the {@link ChainingStrategy} for the given operator affecting the  * way operators will possibly be co-located on the same thread for  * increased performance.  *  * @param strategy  *            The selected {@link ChainingStrategy}  * @return The operator with the modified chaining strategy  */ ;/**  * Sets the {@link ChainingStrategy} for the given operator affecting the  * way operators will possibly be co-located on the same thread for  * increased performance.  *  * @param strategy  *            The selected {@link ChainingStrategy}  * @return The operator with the modified chaining strategy  */ @PublicEvolving private SingleOutputStreamOperator<T> setChainingStrategy(ChainingStrategy strategy) {     this.transformation.setChainingStrategy(strategy).     return this. }
true;public;0;4;/**  * Turns off chaining for this operator so thread co-location will not be used as an  * optimization.  *  * <p>Chaining can be turned off for the whole job by  * {@link StreamExecutionEnvironment#disableOperatorChaining()} however it is not advised for  * performance considerations.  *  * @return The operator with chaining disabled  */ ;/**  * Turns off chaining for this operator so thread co-location will not be used as an  * optimization.  *  * <p>Chaining can be turned off for the whole job by  * {@link StreamExecutionEnvironment#disableOperatorChaining()} however it is not advised for  * performance considerations.  *  * @return The operator with chaining disabled  */ @PublicEvolving public SingleOutputStreamOperator<T> disableChaining() {     return setChainingStrategy(ChainingStrategy.NEVER). }
true;public;0;4;/**  * Starts a new task chain beginning at this operator. This operator will  * not be chained (thread co-located for increased performance) to any  * previous tasks even if possible.  *  * @return The operator with chaining set.  */ ;/**  * Starts a new task chain beginning at this operator. This operator will  * not be chained (thread co-located for increased performance) to any  * previous tasks even if possible.  *  * @return The operator with chaining set.  */ @PublicEvolving public SingleOutputStreamOperator<T> startNewChain() {     return setChainingStrategy(ChainingStrategy.HEAD). }
true;public;1;12;/**  * Adds a type information hint about the return type of this operator. This method  * can be used in cases where Flink cannot determine automatically what the produced  * type of a function is. That can be the case if the function uses generic type variables  * in the return type that cannot be inferred from the input type.  *  * <p>Classes can be used as type hints for non-generic types (classes without generic parameters),  * but not for generic types like for example Tuples. For those generic types, please  * use the {@link #returns(TypeHint)} method.  *  * @param typeClass The class of the returned data type.  * @return This operator with the type information corresponding to the given type class.  */ ;// ------------------------------------------------------------------------ // Type hinting // ------------------------------------------------------------------------ /**  * Adds a type information hint about the return type of this operator. This method  * can be used in cases where Flink cannot determine automatically what the produced  * type of a function is. That can be the case if the function uses generic type variables  * in the return type that cannot be inferred from the input type.  *  * <p>Classes can be used as type hints for non-generic types (classes without generic parameters),  * but not for generic types like for example Tuples. For those generic types, please  * use the {@link #returns(TypeHint)} method.  *  * @param typeClass The class of the returned data type.  * @return This operator with the type information corresponding to the given type class.  */ public SingleOutputStreamOperator<T> returns(Class<T> typeClass) {     requireNonNull(typeClass, "type class must not be null.").     try {         return returns(TypeInformation.of(typeClass)).     } catch (InvalidTypesException e) {         throw new InvalidTypesException("Cannot infer the type information from the class alone." + "This is most likely because the class represents a generic type. In that case," + "please use the 'returns(TypeHint)' method instead.").     } }
true;public;1;11;/**  * Adds a type information hint about the return type of this operator. This method  * can be used in cases where Flink cannot determine automatically what the produced  * type of a function is. That can be the case if the function uses generic type variables  * in the return type that cannot be inferred from the input type.  *  * <p>Use this method the following way:  * <pre>{@code  *     DataStream<Tuple2<String, Double>> result =  *         stream.flatMap(new FunctionWithNonInferrableReturnType())  *               .returns(new TypeHint<Tuple2<String, Double>>(){}).  * }</pre>  *  * @param typeHint The type hint for the returned data type.  * @return This operator with the type information corresponding to the given type hint.  */ ;/**  * Adds a type information hint about the return type of this operator. This method  * can be used in cases where Flink cannot determine automatically what the produced  * type of a function is. That can be the case if the function uses generic type variables  * in the return type that cannot be inferred from the input type.  *  * <p>Use this method the following way:  * <pre>{@code  *     DataStream<Tuple2<String, Double>> result =  *         stream.flatMap(new FunctionWithNonInferrableReturnType())  *               .returns(new TypeHint<Tuple2<String, Double>>(){}).  * }</pre>  *  * @param typeHint The type hint for the returned data type.  * @return This operator with the type information corresponding to the given type hint.  */ public SingleOutputStreamOperator<T> returns(TypeHint<T> typeHint) {     requireNonNull(typeHint, "TypeHint must not be null").     try {         return returns(TypeInformation.of(typeHint)).     } catch (InvalidTypesException e) {         throw new InvalidTypesException("Cannot infer the type information from the type hint. " + "Make sure that the TypeHint does not use any generic type variables.").     } }
true;public;1;6;/**  * Adds a type information hint about the return type of this operator. This method  * can be used in cases where Flink cannot determine automatically what the produced  * type of a function is. That can be the case if the function uses generic type variables  * in the return type that cannot be inferred from the input type.  *  * <p>In most cases, the methods {@link #returns(Class)} and {@link #returns(TypeHint)}  * are preferable.  *  * @param typeInfo type information as a return type hint  * @return This operator with a given return type hint.  */ ;/**  * Adds a type information hint about the return type of this operator. This method  * can be used in cases where Flink cannot determine automatically what the produced  * type of a function is. That can be the case if the function uses generic type variables  * in the return type that cannot be inferred from the input type.  *  * <p>In most cases, the methods {@link #returns(Class)} and {@link #returns(TypeHint)}  * are preferable.  *  * @param typeInfo type information as a return type hint  * @return This operator with a given return type hint.  */ public SingleOutputStreamOperator<T> returns(TypeInformation<T> typeInfo) {     requireNonNull(typeInfo, "TypeInformation must not be null").     transformation.setOutputType(typeInfo).     return this. }
true;public;1;5;/**  * Sets the slot sharing group of this operation. Parallel instances of  * operations that are in the same slot sharing group will be co-located in the same  * TaskManager slot, if possible.  *  * <p>Operations inherit the slot sharing group of input operations if all input operations  * are in the same slot sharing group and no slot sharing group was explicitly specified.  *  * <p>Initially an operation is in the default slot sharing group. An operation can be put into  * the default group explicitly by setting the slot sharing group to {@code "default"}.  *  * @param slotSharingGroup The slot sharing group name.  */ ;// ------------------------------------------------------------------------ // Miscellaneous // ------------------------------------------------------------------------ /**  * Sets the slot sharing group of this operation. Parallel instances of  * operations that are in the same slot sharing group will be co-located in the same  * TaskManager slot, if possible.  *  * <p>Operations inherit the slot sharing group of input operations if all input operations  * are in the same slot sharing group and no slot sharing group was explicitly specified.  *  * <p>Initially an operation is in the default slot sharing group. An operation can be put into  * the default group explicitly by setting the slot sharing group to {@code "default"}.  *  * @param slotSharingGroup The slot sharing group name.  */ @PublicEvolving public SingleOutputStreamOperator<T> slotSharingGroup(String slotSharingGroup) {     transformation.setSlotSharingGroup(slotSharingGroup).     return this. }
false;public;1;10;;@Override public SplitStream<T> split(OutputSelector<T> outputSelector) {     if (requestedSideOutputs.isEmpty()) {         wasSplitApplied = true.         return super.split(outputSelector).     } else {         throw new UnsupportedOperationException("getSideOutput() and split() may not be called on the same DataStream. " + "As a work-around, please add a no-op map function before the split() call.").     } }
true;public;1;23;/**  * Gets the {@link DataStream} that contains the elements that are emitted from an operation  * into the side output with the given {@link OutputTag}.  *  * @see org.apache.flink.streaming.api.functions.ProcessFunction.Context#output(OutputTag, Object)  */ ;/**  * Gets the {@link DataStream} that contains the elements that are emitted from an operation  * into the side output with the given {@link OutputTag}.  *  * @see org.apache.flink.streaming.api.functions.ProcessFunction.Context#output(OutputTag, Object)  */ public <X> DataStream<X> getSideOutput(OutputTag<X> sideOutputTag) {     if (wasSplitApplied) {         throw new UnsupportedOperationException("getSideOutput() and split() may not be called on the same DataStream. " + "As a work-around, please add a no-op map function before the split() call.").     }     sideOutputTag = clean(requireNonNull(sideOutputTag)).     // make a defensive copy     sideOutputTag = new OutputTag<X>(sideOutputTag.getId(), sideOutputTag.getTypeInfo()).     TypeInformation<?> type = requestedSideOutputs.get(sideOutputTag).     if (type != null && !type.equals(sideOutputTag.getTypeInfo())) {         throw new UnsupportedOperationException("A side output with a matching id was " + "already requested with a different type. This is not allowed, side output " + "ids need to be unique.").     }     requestedSideOutputs.put(sideOutputTag, sideOutputTag.getTypeInfo()).     SideOutputTransformation<X> sideOutputTransformation = new SideOutputTransformation<>(this.getTransformation(), sideOutputTag).     return new DataStream<>(this.getExecutionEnvironment(), sideOutputTransformation). }
