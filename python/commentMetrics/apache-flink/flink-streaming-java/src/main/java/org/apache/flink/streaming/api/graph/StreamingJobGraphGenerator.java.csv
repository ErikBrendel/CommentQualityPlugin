commented;modifiers;parameterAmount;loc;comment;code
false;public,static;1;3;;// ------------------------------------------------------------------------ public static JobGraph createJobGraph(StreamGraph streamGraph) {     return createJobGraph(streamGraph, null). }
false;public,static;2;3;;public static JobGraph createJobGraph(StreamGraph streamGraph, @Nullable JobID jobID) {     return new StreamingJobGraphGenerator(streamGraph, jobID).createJobGraph(). }
false;private;0;38;;private JobGraph createJobGraph() {     // make sure that all vertices start immediately     jobGraph.setScheduleMode(ScheduleMode.EAGER).     // Generate deterministic hashes for the nodes in order to identify them across     // submission iff they didn't change.     Map<Integer, byte[]> hashes = defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph).     // Generate legacy version hashes for backwards compatibility     List<Map<Integer, byte[]>> legacyHashes = new ArrayList<>(legacyStreamGraphHashers.size()).     for (StreamGraphHasher hasher : legacyStreamGraphHashers) {         legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph)).     }     Map<Integer, List<Tuple2<byte[], byte[]>>> chainedOperatorHashes = new HashMap<>().     setChaining(hashes, legacyHashes, chainedOperatorHashes).     setPhysicalEdges().     setSlotSharingAndCoLocation().     configureCheckpointing().     JobGraphGenerator.addUserArtifactEntries(streamGraph.getEnvironment().getCachedFiles(), jobGraph).     // set the ExecutionConfig last when it has been finalized     try {         jobGraph.setExecutionConfig(streamGraph.getExecutionConfig()).     } catch (IOException e) {         throw new IllegalConfigurationException("Could not serialize the ExecutionConfig." + "This indicates that non-serializable types (like custom serializers) were registered").     }     return jobGraph. }
false;private;0;24;;private void setPhysicalEdges() {     Map<Integer, List<StreamEdge>> physicalInEdgesInOrder = new HashMap<Integer, List<StreamEdge>>().     for (StreamEdge edge : physicalEdgesInOrder) {         int target = edge.getTargetId().         List<StreamEdge> inEdges = physicalInEdgesInOrder.get(target).         // create if not set         if (inEdges == null) {             inEdges = new ArrayList<>().             physicalInEdgesInOrder.put(target, inEdges).         }         inEdges.add(edge).     }     for (Map.Entry<Integer, List<StreamEdge>> inEdges : physicalInEdgesInOrder.entrySet()) {         int vertex = inEdges.getKey().         List<StreamEdge> edgeList = inEdges.getValue().         vertexConfigs.get(vertex).setInPhysicalEdges(edgeList).     } }
true;private;3;5;/**  * Sets up task chains from the source {@link StreamNode} instances.  *  * <p>This will recursively create all {@link JobVertex} instances.  */ ;/**  * Sets up task chains from the source {@link StreamNode} instances.  *  * <p>This will recursively create all {@link JobVertex} instances.  */ private void setChaining(Map<Integer, byte[]> hashes, List<Map<Integer, byte[]>> legacyHashes, Map<Integer, List<Tuple2<byte[], byte[]>>> chainedOperatorHashes) {     for (Integer sourceNodeId : streamGraph.getSourceIDs()) {         createChain(sourceNodeId, sourceNodeId, hashes, legacyHashes, 0, chainedOperatorHashes).     } }
false;private;6;90;;private List<StreamEdge> createChain(Integer startNodeId, Integer currentNodeId, Map<Integer, byte[]> hashes, List<Map<Integer, byte[]>> legacyHashes, int chainIndex, Map<Integer, List<Tuple2<byte[], byte[]>>> chainedOperatorHashes) {     if (!builtVertices.contains(startNodeId)) {         List<StreamEdge> transitiveOutEdges = new ArrayList<StreamEdge>().         List<StreamEdge> chainableOutputs = new ArrayList<StreamEdge>().         List<StreamEdge> nonChainableOutputs = new ArrayList<StreamEdge>().         for (StreamEdge outEdge : streamGraph.getStreamNode(currentNodeId).getOutEdges()) {             if (isChainable(outEdge, streamGraph)) {                 chainableOutputs.add(outEdge).             } else {                 nonChainableOutputs.add(outEdge).             }         }         for (StreamEdge chainable : chainableOutputs) {             transitiveOutEdges.addAll(createChain(startNodeId, chainable.getTargetId(), hashes, legacyHashes, chainIndex + 1, chainedOperatorHashes)).         }         for (StreamEdge nonChainable : nonChainableOutputs) {             transitiveOutEdges.add(nonChainable).             createChain(nonChainable.getTargetId(), nonChainable.getTargetId(), hashes, legacyHashes, 0, chainedOperatorHashes).         }         List<Tuple2<byte[], byte[]>> operatorHashes = chainedOperatorHashes.computeIfAbsent(startNodeId, k -> new ArrayList<>()).         byte[] primaryHashBytes = hashes.get(currentNodeId).         for (Map<Integer, byte[]> legacyHash : legacyHashes) {             operatorHashes.add(new Tuple2<>(primaryHashBytes, legacyHash.get(currentNodeId))).         }         chainedNames.put(currentNodeId, createChainedName(currentNodeId, chainableOutputs)).         chainedMinResources.put(currentNodeId, createChainedMinResources(currentNodeId, chainableOutputs)).         chainedPreferredResources.put(currentNodeId, createChainedPreferredResources(currentNodeId, chainableOutputs)).         StreamConfig config = currentNodeId.equals(startNodeId) ? createJobVertex(startNodeId, hashes, legacyHashes, chainedOperatorHashes) : new StreamConfig(new Configuration()).         setVertexConfig(currentNodeId, config, chainableOutputs, nonChainableOutputs).         if (currentNodeId.equals(startNodeId)) {             config.setChainStart().             config.setChainIndex(0).             config.setOperatorName(streamGraph.getStreamNode(currentNodeId).getOperatorName()).             config.setOutEdgesInOrder(transitiveOutEdges).             config.setOutEdges(streamGraph.getStreamNode(currentNodeId).getOutEdges()).             for (StreamEdge edge : transitiveOutEdges) {                 connect(startNodeId, edge).             }             config.setTransitiveChainedTaskConfigs(chainedConfigs.get(startNodeId)).         } else {             Map<Integer, StreamConfig> chainedConfs = chainedConfigs.get(startNodeId).             if (chainedConfs == null) {                 chainedConfigs.put(startNodeId, new HashMap<Integer, StreamConfig>()).             }             config.setChainIndex(chainIndex).             StreamNode node = streamGraph.getStreamNode(currentNodeId).             config.setOperatorName(node.getOperatorName()).             chainedConfigs.get(startNodeId).put(currentNodeId, config).         }         config.setOperatorID(new OperatorID(primaryHashBytes)).         if (chainableOutputs.isEmpty()) {             config.setChainEnd().         }         return transitiveOutEdges.     } else {         return new ArrayList<>().     } }
false;private;2;14;;private String createChainedName(Integer vertexID, List<StreamEdge> chainedOutputs) {     String operatorName = streamGraph.getStreamNode(vertexID).getOperatorName().     if (chainedOutputs.size() > 1) {         List<String> outputChainedNames = new ArrayList<>().         for (StreamEdge chainable : chainedOutputs) {             outputChainedNames.add(chainedNames.get(chainable.getTargetId())).         }         return operatorName + " -> (" + StringUtils.join(outputChainedNames, ", ") + ")".     } else if (chainedOutputs.size() == 1) {         return operatorName + " -> " + chainedNames.get(chainedOutputs.get(0).getTargetId()).     } else {         return operatorName.     } }
false;private;2;7;;private ResourceSpec createChainedMinResources(Integer vertexID, List<StreamEdge> chainedOutputs) {     ResourceSpec minResources = streamGraph.getStreamNode(vertexID).getMinResources().     for (StreamEdge chainable : chainedOutputs) {         minResources = minResources.merge(chainedMinResources.get(chainable.getTargetId())).     }     return minResources. }
false;private;2;7;;private ResourceSpec createChainedPreferredResources(Integer vertexID, List<StreamEdge> chainedOutputs) {     ResourceSpec preferredResources = streamGraph.getStreamNode(vertexID).getPreferredResources().     for (StreamEdge chainable : chainedOutputs) {         preferredResources = preferredResources.merge(chainedPreferredResources.get(chainable.getTargetId())).     }     return preferredResources. }
false;private;4;81;;private StreamConfig createJobVertex(Integer streamNodeId, Map<Integer, byte[]> hashes, List<Map<Integer, byte[]>> legacyHashes, Map<Integer, List<Tuple2<byte[], byte[]>>> chainedOperatorHashes) {     JobVertex jobVertex.     StreamNode streamNode = streamGraph.getStreamNode(streamNodeId).     byte[] hash = hashes.get(streamNodeId).     if (hash == null) {         throw new IllegalStateException("Cannot find node hash. " + "Did you generate them before calling this method?").     }     JobVertexID jobVertexId = new JobVertexID(hash).     List<JobVertexID> legacyJobVertexIds = new ArrayList<>(legacyHashes.size()).     for (Map<Integer, byte[]> legacyHash : legacyHashes) {         hash = legacyHash.get(streamNodeId).         if (null != hash) {             legacyJobVertexIds.add(new JobVertexID(hash)).         }     }     List<Tuple2<byte[], byte[]>> chainedOperators = chainedOperatorHashes.get(streamNodeId).     List<OperatorID> chainedOperatorVertexIds = new ArrayList<>().     List<OperatorID> userDefinedChainedOperatorVertexIds = new ArrayList<>().     if (chainedOperators != null) {         for (Tuple2<byte[], byte[]> chainedOperator : chainedOperators) {             chainedOperatorVertexIds.add(new OperatorID(chainedOperator.f0)).             userDefinedChainedOperatorVertexIds.add(chainedOperator.f1 != null ? new OperatorID(chainedOperator.f1) : null).         }     }     if (streamNode.getInputFormat() != null) {         jobVertex = new InputFormatVertex(chainedNames.get(streamNodeId), jobVertexId, legacyJobVertexIds, chainedOperatorVertexIds, userDefinedChainedOperatorVertexIds).         TaskConfig taskConfig = new TaskConfig(jobVertex.getConfiguration()).         taskConfig.setStubWrapper(new UserCodeObjectWrapper<Object>(streamNode.getInputFormat())).     } else {         jobVertex = new JobVertex(chainedNames.get(streamNodeId), jobVertexId, legacyJobVertexIds, chainedOperatorVertexIds, userDefinedChainedOperatorVertexIds).     }     jobVertex.setResources(chainedMinResources.get(streamNodeId), chainedPreferredResources.get(streamNodeId)).     jobVertex.setInvokableClass(streamNode.getJobVertexClass()).     int parallelism = streamNode.getParallelism().     if (parallelism > 0) {         jobVertex.setParallelism(parallelism).     } else {         parallelism = jobVertex.getParallelism().     }     jobVertex.setMaxParallelism(streamNode.getMaxParallelism()).     if (LOG.isDebugEnabled()) {         LOG.debug("Parallelism set: {} for {}", parallelism, streamNodeId).     }     // TODO: inherit InputDependencyConstraint from the head operator     jobVertex.setInputDependencyConstraint(streamGraph.getExecutionConfig().getDefaultInputDependencyConstraint()).     jobVertices.put(streamNodeId, jobVertex).     builtVertices.add(streamNodeId).     jobGraph.addVertex(jobVertex).     return new StreamConfig(jobVertex.getConfiguration()). }
false;private;4;69;;@SuppressWarnings("unchecked") private void setVertexConfig(Integer vertexID, StreamConfig config, List<StreamEdge> chainableOutputs, List<StreamEdge> nonChainableOutputs) {     StreamNode vertex = streamGraph.getStreamNode(vertexID).     config.setVertexID(vertexID).     config.setBufferTimeout(vertex.getBufferTimeout()).     config.setTypeSerializerIn1(vertex.getTypeSerializerIn1()).     config.setTypeSerializerIn2(vertex.getTypeSerializerIn2()).     config.setTypeSerializerOut(vertex.getTypeSerializerOut()).     // iterate edges, find sideOutput edges create and save serializers for each outputTag type     for (StreamEdge edge : chainableOutputs) {         if (edge.getOutputTag() != null) {             config.setTypeSerializerSideOut(edge.getOutputTag(), edge.getOutputTag().getTypeInfo().createSerializer(streamGraph.getExecutionConfig())).         }     }     for (StreamEdge edge : nonChainableOutputs) {         if (edge.getOutputTag() != null) {             config.setTypeSerializerSideOut(edge.getOutputTag(), edge.getOutputTag().getTypeInfo().createSerializer(streamGraph.getExecutionConfig())).         }     }     config.setStreamOperator(vertex.getOperator()).     config.setOutputSelectors(vertex.getOutputSelectors()).     config.setNumberOfOutputs(nonChainableOutputs.size()).     config.setNonChainedOutputs(nonChainableOutputs).     config.setChainedOutputs(chainableOutputs).     config.setTimeCharacteristic(streamGraph.getEnvironment().getStreamTimeCharacteristic()).     final CheckpointConfig checkpointCfg = streamGraph.getCheckpointConfig().     config.setStateBackend(streamGraph.getStateBackend()).     config.setCheckpointingEnabled(checkpointCfg.isCheckpointingEnabled()).     if (checkpointCfg.isCheckpointingEnabled()) {         config.setCheckpointMode(checkpointCfg.getCheckpointingMode()).     } else {         // the "at-least-once" input handler is slightly cheaper (in the absence of checkpoints),         // so we use that one if checkpointing is not enabled         config.setCheckpointMode(CheckpointingMode.AT_LEAST_ONCE).     }     config.setStatePartitioner(0, vertex.getStatePartitioner1()).     config.setStatePartitioner(1, vertex.getStatePartitioner2()).     config.setStateKeySerializer(vertex.getStateKeySerializer()).     Class<? extends AbstractInvokable> vertexClass = vertex.getJobVertexClass().     if (vertexClass.equals(StreamIterationHead.class) || vertexClass.equals(StreamIterationTail.class)) {         config.setIterationId(streamGraph.getBrokerID(vertexID)).         config.setIterationWaitTime(streamGraph.getLoopTimeout(vertexID)).     }     List<StreamEdge> allOutputs = new ArrayList<StreamEdge>(chainableOutputs).     allOutputs.addAll(nonChainableOutputs).     vertexConfigs.put(vertexID, config). }
false;private;2;34;;private void connect(Integer headOfChain, StreamEdge edge) {     physicalEdgesInOrder.add(edge).     Integer downStreamvertexID = edge.getTargetId().     JobVertex headVertex = jobVertices.get(headOfChain).     JobVertex downStreamVertex = jobVertices.get(downStreamvertexID).     StreamConfig downStreamConfig = new StreamConfig(downStreamVertex.getConfiguration()).     downStreamConfig.setNumberOfInputs(downStreamConfig.getNumberOfInputs() + 1).     StreamPartitioner<?> partitioner = edge.getPartitioner().     JobEdge jobEdge.     if (partitioner instanceof ForwardPartitioner || partitioner instanceof RescalePartitioner) {         jobEdge = downStreamVertex.connectNewDataSetAsInput(headVertex, DistributionPattern.POINTWISE, ResultPartitionType.PIPELINED_BOUNDED).     } else {         jobEdge = downStreamVertex.connectNewDataSetAsInput(headVertex, DistributionPattern.ALL_TO_ALL, ResultPartitionType.PIPELINED_BOUNDED).     }     // set strategy name so that web interface can show it.     jobEdge.setShipStrategyName(partitioner.toString()).     if (LOG.isDebugEnabled()) {         LOG.debug("CONNECTED: {} - {} -> {}", partitioner.getClass().getSimpleName(), headOfChain, downStreamvertexID).     } }
false;public,static;2;18;;public static boolean isChainable(StreamEdge edge, StreamGraph streamGraph) {     StreamNode upStreamVertex = streamGraph.getSourceVertex(edge).     StreamNode downStreamVertex = streamGraph.getTargetVertex(edge).     StreamOperator<?> headOperator = upStreamVertex.getOperator().     StreamOperator<?> outOperator = downStreamVertex.getOperator().     return downStreamVertex.getInEdges().size() == 1 && outOperator != null && headOperator != null && upStreamVertex.isSameSlotSharingGroup(downStreamVertex) && outOperator.getChainingStrategy() == ChainingStrategy.ALWAYS && (headOperator.getChainingStrategy() == ChainingStrategy.HEAD || headOperator.getChainingStrategy() == ChainingStrategy.ALWAYS) && (edge.getPartitioner() instanceof ForwardPartitioner) && upStreamVertex.getParallelism() == downStreamVertex.getParallelism() && streamGraph.isChainingEnabled(). }
false;private;0;53;;private void setSlotSharingAndCoLocation() {     final HashMap<String, SlotSharingGroup> slotSharingGroups = new HashMap<>().     final HashMap<String, Tuple2<SlotSharingGroup, CoLocationGroup>> coLocationGroups = new HashMap<>().     for (Entry<Integer, JobVertex> entry : jobVertices.entrySet()) {         final StreamNode node = streamGraph.getStreamNode(entry.getKey()).         final JobVertex vertex = entry.getValue().         // configure slot sharing group         final String slotSharingGroupKey = node.getSlotSharingGroup().         final SlotSharingGroup sharingGroup.         if (slotSharingGroupKey != null) {             sharingGroup = slotSharingGroups.computeIfAbsent(slotSharingGroupKey, (k) -> new SlotSharingGroup()).             vertex.setSlotSharingGroup(sharingGroup).         } else {             sharingGroup = null.         }         // configure co-location constraint         final String coLocationGroupKey = node.getCoLocationGroup().         if (coLocationGroupKey != null) {             if (sharingGroup == null) {                 throw new IllegalStateException("Cannot use a co-location constraint without a slot sharing group").             }             Tuple2<SlotSharingGroup, CoLocationGroup> constraint = coLocationGroups.computeIfAbsent(coLocationGroupKey, (k) -> new Tuple2<>(sharingGroup, new CoLocationGroup())).             if (constraint.f0 != sharingGroup) {                 throw new IllegalStateException("Cannot co-locate operators from different slot sharing groups").             }             vertex.updateCoLocationGroup(constraint.f1).         }     }     for (Tuple2<StreamNode, StreamNode> pair : streamGraph.getIterationSourceSinkPairs()) {         CoLocationGroup ccg = new CoLocationGroup().         JobVertex source = jobVertices.get(pair.f0.getId()).         JobVertex sink = jobVertices.get(pair.f1.getId()).         ccg.addVertex(source).         ccg.addVertex(sink).         source.updateCoLocationGroup(ccg).         sink.updateCoLocationGroup(ccg).     } }
false;private;0;128;;private void configureCheckpointing() {     CheckpointConfig cfg = streamGraph.getCheckpointConfig().     long interval = cfg.getCheckpointInterval().     if (interval > 0) {         ExecutionConfig executionConfig = streamGraph.getExecutionConfig().         // propagate the expected behaviour for checkpoint errors to task.         executionConfig.setFailTaskOnCheckpointError(cfg.isFailOnCheckpointingErrors()).     } else {         // interval of max value means disable periodic checkpoint         interval = Long.MAX_VALUE.     }     // --- configure the participating vertices ---     // collect the vertices that receive "trigger checkpoint" messages.     // currently, these are all the sources     List<JobVertexID> triggerVertices = new ArrayList<>().     // collect the vertices that need to acknowledge the checkpoint     // currently, these are all vertices     List<JobVertexID> ackVertices = new ArrayList<>(jobVertices.size()).     // collect the vertices that receive "commit checkpoint" messages     // currently, these are all vertices     List<JobVertexID> commitVertices = new ArrayList<>(jobVertices.size()).     for (JobVertex vertex : jobVertices.values()) {         if (vertex.isInputVertex()) {             triggerVertices.add(vertex.getID()).         }         commitVertices.add(vertex.getID()).         ackVertices.add(vertex.getID()).     }     // --- configure options ---     CheckpointRetentionPolicy retentionAfterTermination.     if (cfg.isExternalizedCheckpointsEnabled()) {         CheckpointConfig.ExternalizedCheckpointCleanup cleanup = cfg.getExternalizedCheckpointCleanup().         // Sanity check         if (cleanup == null) {             throw new IllegalStateException("Externalized checkpoints enabled, but no cleanup mode configured.").         }         retentionAfterTermination = cleanup.deleteOnCancellation() ? CheckpointRetentionPolicy.RETAIN_ON_FAILURE : CheckpointRetentionPolicy.RETAIN_ON_CANCELLATION.     } else {         retentionAfterTermination = CheckpointRetentionPolicy.NEVER_RETAIN_AFTER_TERMINATION.     }     CheckpointingMode mode = cfg.getCheckpointingMode().     boolean isExactlyOnce.     if (mode == CheckpointingMode.EXACTLY_ONCE) {         isExactlyOnce = true.     } else if (mode == CheckpointingMode.AT_LEAST_ONCE) {         isExactlyOnce = false.     } else {         throw new IllegalStateException("Unexpected checkpointing mode. " + "Did not expect there to be another checkpointing mode besides " + "exactly-once or at-least-once.").     }     // --- configure the master-side checkpoint hooks ---     final ArrayList<MasterTriggerRestoreHook.Factory> hooks = new ArrayList<>().     for (StreamNode node : streamGraph.getStreamNodes()) {         StreamOperator<?> op = node.getOperator().         if (op instanceof AbstractUdfStreamOperator) {             Function f = ((AbstractUdfStreamOperator<?, ?>) op).getUserFunction().             if (f instanceof WithMasterCheckpointHook) {                 hooks.add(new FunctionMasterCheckpointHookFactory((WithMasterCheckpointHook<?>) f)).             }         }     }     // because the hooks can have user-defined code, they need to be stored as     // eagerly serialized values     final SerializedValue<MasterTriggerRestoreHook.Factory[]> serializedHooks.     if (hooks.isEmpty()) {         serializedHooks = null.     } else {         try {             MasterTriggerRestoreHook.Factory[] asArray = hooks.toArray(new MasterTriggerRestoreHook.Factory[hooks.size()]).             serializedHooks = new SerializedValue<>(asArray).         } catch (IOException e) {             throw new FlinkRuntimeException("Trigger/restore hook is not serializable", e).         }     }     // because the state backend can have user-defined code, it needs to be stored as     // eagerly serialized value     final SerializedValue<StateBackend> serializedStateBackend.     if (streamGraph.getStateBackend() == null) {         serializedStateBackend = null.     } else {         try {             serializedStateBackend = new SerializedValue<StateBackend>(streamGraph.getStateBackend()).         } catch (IOException e) {             throw new FlinkRuntimeException("State backend is not serializable", e).         }     }     // --- done, put it all together ---     JobCheckpointingSettings settings = new JobCheckpointingSettings(triggerVertices, ackVertices, commitVertices, new CheckpointCoordinatorConfiguration(interval, cfg.getCheckpointTimeout(), cfg.getMinPauseBetweenCheckpoints(), cfg.getMaxConcurrentCheckpoints(), retentionAfterTermination, isExactlyOnce), serializedStateBackend, serializedHooks).     jobGraph.setSnapshotSettings(settings). }
