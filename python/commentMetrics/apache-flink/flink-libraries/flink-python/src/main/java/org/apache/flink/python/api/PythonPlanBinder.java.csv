commented;modifiers;parameterAmount;loc;comment;code
true;public,static;1;10;/**  * Entry point for the execution of a python plan.  *  * @param args planPath[ package1[ packageX[ - parameter1[ parameterX]]]]  * @throws Exception  */ ;/**  * Entry point for the execution of a python plan.  *  * @param args planPath[ package1[ packageX[ - parameter1[ parameterX]]]]  * @throws Exception  */ public static void main(String[] args) throws Exception {     Configuration globalConfig = GlobalConfiguration.loadConfiguration().     PythonPlanBinder binder = new PythonPlanBinder(globalConfig).     try {         binder.runPlan(args).     } catch (Exception e) {         System.out.println("Failed to run plan: " + e.getMessage()).         LOG.error("Failed to run plan.", e).     } }
false;;1;86;;void runPlan(String[] args) throws Exception {     if (args.length < 1) {         throw new IllegalArgumentException("Missing script file argument. Usage: ./bin/pyflink.[sh/bat] <pathToScript>[ <pathToPackage1>[ <pathToPackageX]][ - <parameter1>[ <parameterX>]]").     }     int split = 0.     for (int x = 0. x < args.length. x++) {         if (args[x].equals("-")) {             split = x.             break.         }     }     try {         String planFile = args[0].         String[] filesToCopy = Arrays.copyOfRange(args, 1, split == 0 ? args.length : split).         String[] planArgumentsArray = Arrays.copyOfRange(args, split == 0 ? args.length : split + 1, args.length).         StringBuilder planArgumentsBuilder = new StringBuilder().         for (String arg : planArgumentsArray) {             planArgumentsBuilder.append(" ").append(arg).         }         String planArguments = planArgumentsBuilder.toString().         operatorConfig.setString(PLAN_ARGUMENTS_KEY, planArguments).         Path planPath = new Path(planFile).         if (!FileSystem.getUnguardedFileSystem(planPath.toUri()).exists(planPath)) {             throw new FileNotFoundException("Plan file " + planFile + " does not exist.").         }         for (String file : filesToCopy) {             Path filePath = new Path(file).             if (!FileSystem.getUnguardedFileSystem(filePath.toUri()).exists(filePath)) {                 throw new FileNotFoundException("Additional file " + file + " does not exist.").             }         }         // setup temporary local directory for flink python library and user files         Path targetDir = new Path(tmpPlanFilesDir).         deleteIfExists(targetDir).         targetDir.getFileSystem().mkdirs(targetDir).         // extract and unzip flink library to temporary location         unzipPythonLibrary(new Path(tmpPlanFilesDir)).         // copy user files to temporary location         Path tmpPlanFilesPath = new Path(tmpPlanFilesDir).         copyFile(planPath, tmpPlanFilesPath, FLINK_PYTHON_PLAN_NAME).         for (String file : filesToCopy) {             Path source = new Path(file).             copyFile(source, tmpPlanFilesPath, source.getName()).         }         // start python process         streamer = new PythonPlanStreamer(operatorConfig).         streamer.open(tmpPlanFilesDir, planArguments).         // Python process should terminate itself when all jobs have been run         while (streamer.preparePlanMode()) {             ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment().             receivePlan(env).             env.registerCachedFile(tmpPlanFilesPath.toUri().toString(), FLINK_PYTHON_DC_ID, true).             JobExecutionResult jer = env.execute().             long runtime = jer.getNetRuntime().             streamer.sendRecord(runtime).             streamer.finishPlanMode().             sets.reset().         }     } finally {         try {             // clean up created files             FileSystem local = FileSystem.getLocalFileSystem().             local.delete(new Path(tmpPlanFilesDir), true).         } catch (IOException ioe) {             LOG.error("PythonAPI file cleanup failed. {}", ioe.getMessage()).         } finally {             if (streamer != null) {                 streamer.close().             }         }     } }
false;private,static;1;27;;private static void unzipPythonLibrary(Path targetDir) throws IOException {     FileSystem targetFs = targetDir.getFileSystem().     ClassLoader classLoader = PythonPlanBinder.class.getClassLoader().     try (ZipInputStream zis = new ZipInputStream(classLoader.getResourceAsStream("python-source.zip"))) {         ZipEntry entry = zis.getNextEntry().         while (entry != null) {             String fileName = entry.getName().             Path newFile = new Path(targetDir, fileName).             if (entry.isDirectory()) {                 targetFs.mkdirs(newFile).             } else {                 try {                     LOG.debug("Unzipping to {}.", newFile).                     FSDataOutputStream fsDataOutputStream = targetFs.create(newFile, FileSystem.WriteMode.NO_OVERWRITE).                     IOUtils.copyBytes(zis, fsDataOutputStream, false).                 } catch (Exception e) {                     zis.closeEntry().                     throw new IOException("Failed to unzip flink python library.", e).                 }             }             zis.closeEntry().             entry = zis.getNextEntry().         }         zis.closeEntry().     } }
false;private,static;1;6;;// =====Setup======================================================================================================== private static void deleteIfExists(Path path) throws IOException {     FileSystem fs = path.getFileSystem().     if (fs.exists(path)) {         fs.delete(path, true).     } }
false;private,static;3;5;;private static void copyFile(Path source, Path targetDirectory, String name) throws IOException {     Path targetFilePath = new Path(targetDirectory, name).     deleteIfExists(targetFilePath).     FileUtils.copy(source, targetFilePath, true). }
true;private;1;5;// ====Plan========================================================================================================== ;// ====Plan========================================================================================================== private void receivePlan(ExecutionEnvironment env) throws IOException {     // IDs used in HashMap of sets are only unique for each environment     receiveParameters(env).     receiveOperations(env). }
false;private;1;21;;private void receiveParameters(ExecutionEnvironment env) throws IOException {     for (int x = 0. x < Parameters.values().length. x++) {         Tuple value = (Tuple) streamer.getRecord(true).         switch(Parameters.valueOf(((String) value.getField(0)).toUpperCase())) {             case DOP:                 Integer dop = value.<Integer>getField(1).                 env.setParallelism(dop).                 break.             case RETRY:                 int retry = value.<Integer>getField(1).                 env.setRestartStrategy(RestartStrategies.fixedDelayRestart(retry, 10000L)).                 break.             case ID:                 currentEnvironmentID = value.<Integer>getField(1).                 break.         }     }     if (env.getParallelism() < 0) {         env.setParallelism(1).     } }
false;private;1;93;;private void receiveOperations(ExecutionEnvironment env) throws IOException {     Integer operationCount = (Integer) streamer.getRecord(true).     for (int x = 0. x < operationCount. x++) {         PythonOperationInfo info = new PythonOperationInfo(streamer, currentEnvironmentID).         Operation op = Operation.valueOf(info.identifier.toUpperCase()).         switch(op) {             case SOURCE_CSV:                 createCsvSource(env, info).                 break.             case SOURCE_TEXT:                 createTextSource(env, info).                 break.             case SOURCE_VALUE:                 createValueSource(env, info).                 break.             case SOURCE_SEQ:                 createSequenceSource(env, info).                 break.             case SINK_CSV:                 createCsvSink(info).                 break.             case SINK_TEXT:                 createTextSink(info).                 break.             case SINK_PRINT:                 createPrintSink(info).                 break.             case BROADCAST:                 createBroadcastVariable(info).                 break.             case DISTINCT:                 createDistinctOperation(info).                 break.             case FIRST:                 createFirstOperation(info).                 break.             case PARTITION_HASH:                 createHashPartitionOperation(info).                 break.             case REBALANCE:                 createRebalanceOperation(info).                 break.             case GROUPBY:                 createGroupOperation(info).                 break.             case SORT:                 createSortOperation(info).                 break.             case UNION:                 createUnionOperation(info).                 break.             case COGROUP:                 createCoGroupOperation(info, info.types).                 break.             case CROSS:                 createCrossOperation(NONE, info, info.types).                 break.             case CROSS_H:                 createCrossOperation(HUGE, info, info.types).                 break.             case CROSS_T:                 createCrossOperation(TINY, info, info.types).                 break.             case FILTER:                 createFilterOperation(info, info.types).                 break.             case FLATMAP:                 createFlatMapOperation(info, info.types).                 break.             case GROUPREDUCE:                 createGroupReduceOperation(info).                 break.             case JOIN:                 createJoinOperation(NONE, info, info.types).                 break.             case JOIN_H:                 createJoinOperation(HUGE, info, info.types).                 break.             case JOIN_T:                 createJoinOperation(TINY, info, info.types).                 break.             case MAP:                 createMapOperation(info, info.types).                 break.             case MAPPARTITION:                 createMapPartitionOperation(info, info.types).                 break.             case REDUCE:                 createReduceOperation(info).                 break.         }     } }
false;private;2;12;;@SuppressWarnings("unchecked") private <T extends Tuple> void createCsvSource(ExecutionEnvironment env, PythonOperationInfo info) {     if (!(info.types instanceof TupleTypeInfo)) {         throw new RuntimeException("The output type of a csv source has to be a tuple. The derived type is " + info).     }     Path path = new Path(info.path).     String lineD = info.lineDelimiter.     String fieldD = info.fieldDelimiter.     TupleTypeInfo<T> types = (TupleTypeInfo<T>) info.types.     sets.add(info.setID, env.createInput(new TupleCsvInputFormat<>(path, lineD, fieldD, types), types).setParallelism(info.parallelism).name("CsvSource").map(new SerializerMap<T>()).setParallelism(info.parallelism).name("CsvSourcePostStep")). }
false;private;2;4;;private void createTextSource(ExecutionEnvironment env, PythonOperationInfo info) {     sets.add(info.setID, env.readTextFile(info.path).setParallelism(info.parallelism).name("TextSource").map(new SerializerMap<String>()).setParallelism(info.parallelism).name("TextSourcePostStep")). }
false;private;2;4;;private void createValueSource(ExecutionEnvironment env, PythonOperationInfo info) {     sets.add(info.setID, env.fromCollection(info.values).setParallelism(info.parallelism).name("ValueSource").map(new SerializerMap<>()).setParallelism(info.parallelism).name("ValueSourcePostStep")). }
false;private;2;4;;private void createSequenceSource(ExecutionEnvironment env, PythonOperationInfo info) {     sets.add(info.setID, env.generateSequence(info.frm, info.to).setParallelism(info.parallelism).name("SequenceSource").map(new SerializerMap<Long>()).setParallelism(info.parallelism).name("SequenceSourcePostStep")). }
false;private;1;5;;private void createCsvSink(PythonOperationInfo info) {     DataSet<byte[]> parent = sets.getDataSet(info.parentID).     parent.map(new StringTupleDeserializerMap()).setParallelism(info.parallelism).name("CsvSinkPreStep").writeAsCsv(info.path, info.lineDelimiter, info.fieldDelimiter, info.writeMode).setParallelism(info.parallelism).name("CsvSink"). }
false;private;1;5;;private void createTextSink(PythonOperationInfo info) {     DataSet<byte[]> parent = sets.getDataSet(info.parentID).     parent.map(new StringDeserializerMap()).setParallelism(info.parallelism).writeAsText(info.path, info.writeMode).setParallelism(info.parallelism).name("TextSink"). }
false;private;1;5;;private void createPrintSink(PythonOperationInfo info) {     DataSet<byte[]> parent = sets.getDataSet(info.parentID).     parent.map(new StringDeserializerMap()).setParallelism(info.parallelism).name("PrintSinkPreStep").output(new PrintingOutputFormat<String>(info.toError)).setParallelism(info.parallelism). }
false;private;1;17;;private void createBroadcastVariable(PythonOperationInfo info) {     UdfOperator<?> op1 = (UdfOperator<?>) sets.getDataSet(info.parentID).     DataSet<?> op2 = sets.getDataSet(info.otherID).     op1.withBroadcastSet(op2, info.name).     Configuration c = op1.getParameters().     if (c == null) {         c = new Configuration().     }     int count = c.getInteger(PLANBINDER_CONFIG_BCVAR_COUNT, 0).     c.setInteger(PLANBINDER_CONFIG_BCVAR_COUNT, count + 1).     c.setString(PLANBINDER_CONFIG_BCVAR_NAME_PREFIX + count, info.name).     op1.withParameters(c). }
false;private;1;7;;private <K extends Tuple> void createDistinctOperation(PythonOperationInfo info) {     DataSet<Tuple2<K, byte[]>> op = sets.getDataSet(info.parentID).     DataSet<byte[]> result = op.distinct(info.keys.toArray(new String[info.keys.size()])).setParallelism(info.parallelism).name("Distinct").map(new KeyDiscarder<K>()).setParallelism(info.parallelism).name("DistinctPostStep").     sets.add(info.setID, result). }
false;private;1;17;;private <K extends Tuple> void createFirstOperation(PythonOperationInfo info) {     if (sets.isDataSet(info.parentID)) {         DataSet<byte[]> op = sets.getDataSet(info.parentID).         sets.add(info.setID, op.first(info.count).setParallelism(info.parallelism).name("First")).     } else if (sets.isUnsortedGrouping(info.parentID)) {         UnsortedGrouping<Tuple2<K, byte[]>> op = sets.getUnsortedGrouping(info.parentID).         sets.add(info.setID, op.first(info.count).setParallelism(info.parallelism).name("First").map(new KeyDiscarder<K>()).setParallelism(info.parallelism).name("FirstPostStep")).     } else if (sets.isSortedGrouping(info.parentID)) {         SortedGrouping<Tuple2<K, byte[]>> op = sets.getSortedGrouping(info.parentID).         sets.add(info.setID, op.first(info.count).setParallelism(info.parallelism).name("First").map(new KeyDiscarder<K>()).setParallelism(info.parallelism).name("FirstPostStep")).     } }
false;private;1;4;;private void createGroupOperation(PythonOperationInfo info) {     DataSet<?> op1 = sets.getDataSet(info.parentID).     sets.add(info.setID, op1.groupBy(info.keys.toArray(new String[info.keys.size()]))). }
false;private;1;7;;private <K extends Tuple> void createHashPartitionOperation(PythonOperationInfo info) {     DataSet<Tuple2<K, byte[]>> op1 = sets.getDataSet(info.parentID).     DataSet<byte[]> result = op1.partitionByHash(info.keys.toArray(new String[info.keys.size()])).setParallelism(info.parallelism).map(new KeyDiscarder<K>()).setParallelism(info.parallelism).name("HashPartitionPostStep").     sets.add(info.setID, result). }
false;private;1;4;;private void createRebalanceOperation(PythonOperationInfo info) {     DataSet<?> op = sets.getDataSet(info.parentID).     sets.add(info.setID, op.rebalance().setParallelism(info.parallelism).name("Rebalance")). }
false;private;1;9;;private void createSortOperation(PythonOperationInfo info) {     if (sets.isDataSet(info.parentID)) {         throw new IllegalArgumentException("sort() can not be applied on a DataSet.").     } else if (sets.isUnsortedGrouping(info.parentID)) {         sets.add(info.setID, sets.getUnsortedGrouping(info.parentID).sortGroup(info.field, info.order)).     } else if (sets.isSortedGrouping(info.parentID)) {         sets.add(info.setID, sets.getSortedGrouping(info.parentID).sortGroup(info.field, info.order)).     } }
false;private;1;5;;private <IN> void createUnionOperation(PythonOperationInfo info) {     DataSet<IN> op1 = sets.getDataSet(info.parentID).     DataSet<IN> op2 = sets.getDataSet(info.otherID).     sets.add(info.setID, op1.union(op2).name("Union")). }
false;private;2;8;;private <IN1, IN2, OUT> void createCoGroupOperation(PythonOperationInfo info, TypeInformation<OUT> type) {     DataSet<IN1> op1 = sets.getDataSet(info.parentID).     DataSet<IN2> op2 = sets.getDataSet(info.otherID).     Keys.ExpressionKeys<IN1> key1 = new Keys.ExpressionKeys<>(info.keys1.toArray(new String[info.keys1.size()]), op1.getType()).     Keys.ExpressionKeys<IN2> key2 = new Keys.ExpressionKeys<>(info.keys2.toArray(new String[info.keys2.size()]), op2.getType()).     PythonCoGroup<IN1, IN2, OUT> pcg = new PythonCoGroup<>(operatorConfig, info.envID, info.setID, type).     sets.add(info.setID, new CoGroupRawOperator<>(op1, op2, key1, key2, pcg, type, info.name).setParallelism(info.parallelism)). }
false;private;3;28;;private <IN1, IN2, OUT> void createCrossOperation(DatasizeHint mode, PythonOperationInfo info, TypeInformation<OUT> type) {     DataSet<IN1> op1 = sets.getDataSet(info.parentID).     DataSet<IN2> op2 = sets.getDataSet(info.otherID).     DefaultCross<IN1, IN2> defaultResult.     switch(mode) {         case NONE:             defaultResult = op1.cross(op2).             break.         case HUGE:             defaultResult = op1.crossWithHuge(op2).             break.         case TINY:             defaultResult = op1.crossWithTiny(op2).             break.         default:             throw new IllegalArgumentException("Invalid Cross mode specified: " + mode).     }     defaultResult.setParallelism(info.parallelism).     if (info.usesUDF) {         sets.add(info.setID, defaultResult.mapPartition(new PythonMapPartition<Tuple2<IN1, IN2>, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name)).     } else {         sets.add(info.setID, defaultResult.name("DefaultCross")).     } }
false;private;2;6;;private <IN, OUT> void createFilterOperation(PythonOperationInfo info, TypeInformation<OUT> type) {     DataSet<IN> op1 = sets.getDataSet(info.parentID).     sets.add(info.setID, op1.mapPartition(new PythonMapPartition<IN, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name)). }
false;private;2;6;;private <IN, OUT> void createFlatMapOperation(PythonOperationInfo info, TypeInformation<OUT> type) {     DataSet<IN> op1 = sets.getDataSet(info.parentID).     sets.add(info.setID, op1.mapPartition(new PythonMapPartition<IN, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name)). }
false;private;1;9;;private void createGroupReduceOperation(PythonOperationInfo info) {     if (sets.isDataSet(info.parentID)) {         sets.add(info.setID, applyGroupReduceOperation(sets.getDataSet(info.parentID), info, info.types)).     } else if (sets.isUnsortedGrouping(info.parentID)) {         sets.add(info.setID, applyGroupReduceOperation(sets.getUnsortedGrouping(info.parentID), info, info.types)).     } else if (sets.isSortedGrouping(info.parentID)) {         sets.add(info.setID, applyGroupReduceOperation(sets.getSortedGrouping(info.parentID), info, info.types)).     } }
false;private;3;6;;private <IN, OUT> DataSet<OUT> applyGroupReduceOperation(DataSet<IN> op1, PythonOperationInfo info, TypeInformation<OUT> type) {     return op1.reduceGroup(new IdentityGroupReduce<IN>()).setCombinable(false).name("PythonGroupReducePreStep").setParallelism(info.parallelism).mapPartition(new PythonMapPartition<IN, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name). }
false;private;3;6;;private <IN, OUT> DataSet<OUT> applyGroupReduceOperation(UnsortedGrouping<IN> op1, PythonOperationInfo info, TypeInformation<OUT> type) {     return op1.reduceGroup(new IdentityGroupReduce<IN>()).setCombinable(false).setParallelism(info.parallelism).name("PythonGroupReducePreStep").mapPartition(new PythonMapPartition<IN, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name). }
false;private;3;6;;private <IN, OUT> DataSet<OUT> applyGroupReduceOperation(SortedGrouping<IN> op1, PythonOperationInfo info, TypeInformation<OUT> type) {     return op1.reduceGroup(new IdentityGroupReduce<IN>()).setCombinable(false).setParallelism(info.parallelism).name("PythonGroupReducePreStep").mapPartition(new PythonMapPartition<IN, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name). }
false;private;3;12;;private <IN1, IN2, OUT> void createJoinOperation(DatasizeHint mode, PythonOperationInfo info, TypeInformation<OUT> type) {     DataSet<IN1> op1 = sets.getDataSet(info.parentID).     DataSet<IN2> op2 = sets.getDataSet(info.otherID).     if (info.usesUDF) {         sets.add(info.setID, createDefaultJoin(op1, op2, info.keys1, info.keys2, mode, info.parallelism).mapPartition(new PythonMapPartition<Tuple2<byte[], byte[]>, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name)).     } else {         sets.add(info.setID, createDefaultJoin(op1, op2, info.keys1, info.keys2, mode, info.parallelism)).     } }
false;private;6;20;;private <IN1, IN2> DataSet<Tuple2<byte[], byte[]>> createDefaultJoin(DataSet<IN1> op1, DataSet<IN2> op2, List<String> firstKeys, List<String> secondKeys, DatasizeHint mode, int parallelism) {     String[] firstKeysArray = firstKeys.toArray(new String[firstKeys.size()]).     String[] secondKeysArray = secondKeys.toArray(new String[secondKeys.size()]).     switch(mode) {         case NONE:             return op1.join(op2).where(firstKeysArray).equalTo(secondKeysArray).setParallelism(parallelism).map(new NestedKeyDiscarder<Tuple2<IN1, IN2>>()).setParallelism(parallelism).name("DefaultJoinPostStep").         case HUGE:             return op1.joinWithHuge(op2).where(firstKeysArray).equalTo(secondKeysArray).setParallelism(parallelism).map(new NestedKeyDiscarder<Tuple2<IN1, IN2>>()).setParallelism(parallelism).name("DefaultJoinPostStep").         case TINY:             return op1.joinWithTiny(op2).where(firstKeysArray).equalTo(secondKeysArray).setParallelism(parallelism).map(new NestedKeyDiscarder<Tuple2<IN1, IN2>>()).setParallelism(parallelism).name("DefaultJoinPostStep").         default:             throw new IllegalArgumentException("Invalid join mode specified.").     } }
false;private;2;6;;private <IN, OUT> void createMapOperation(PythonOperationInfo info, TypeInformation<OUT> type) {     DataSet<IN> op1 = sets.getDataSet(info.parentID).     sets.add(info.setID, op1.mapPartition(new PythonMapPartition<IN, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name)). }
false;private;2;6;;private <IN, OUT> void createMapPartitionOperation(PythonOperationInfo info, TypeInformation<OUT> type) {     DataSet<IN> op1 = sets.getDataSet(info.parentID).     sets.add(info.setID, op1.mapPartition(new PythonMapPartition<IN, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name)). }
false;private;1;9;;private void createReduceOperation(PythonOperationInfo info) {     if (sets.isDataSet(info.parentID)) {         sets.add(info.setID, applyReduceOperation(sets.getDataSet(info.parentID), info, info.types)).     } else if (sets.isUnsortedGrouping(info.parentID)) {         sets.add(info.setID, applyReduceOperation(sets.getUnsortedGrouping(info.parentID), info, info.types)).     } else if (sets.isSortedGrouping(info.parentID)) {         throw new IllegalArgumentException("Reduce cannot be applied on a SortedGrouping.").     } }
false;private;3;6;;private <IN, OUT> DataSet<OUT> applyReduceOperation(DataSet<IN> op1, PythonOperationInfo info, TypeInformation<OUT> type) {     return op1.reduceGroup(new IdentityGroupReduce<IN>()).setCombinable(false).setParallelism(info.parallelism).name("PythonReducePreStep").mapPartition(new PythonMapPartition<IN, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name). }
false;private;3;6;;private <IN, OUT> DataSet<OUT> applyReduceOperation(UnsortedGrouping<IN> op1, PythonOperationInfo info, TypeInformation<OUT> type) {     return op1.reduceGroup(new IdentityGroupReduce<IN>()).setCombinable(false).setParallelism(info.parallelism).name("PythonReducePreStep").mapPartition(new PythonMapPartition<IN, OUT>(operatorConfig, info.envID, info.setID, type)).setParallelism(info.parallelism).name(info.name). }
