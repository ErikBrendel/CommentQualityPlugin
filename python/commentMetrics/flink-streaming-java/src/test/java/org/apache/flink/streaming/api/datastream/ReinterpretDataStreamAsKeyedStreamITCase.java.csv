# id;timestamp;commentText;codeText;commentWords;codeWords
ReinterpretDataStreamAsKeyedStreamITCase -> @Test 	public void testReinterpretAsKeyedStream() throws Exception;1518197017;This test checks that reinterpreting a data stream to a keyed stream works as expected. This test consists of_two jobs. The first job materializes a keyBy into files, one files per partition. The second job opens the_files created by the first jobs as sources (doing the correct assignment of files to partitions) and_reinterprets the sources as keyed, because we know they have been partitioned in a keyBy from the first job.;@Test_	public void testReinterpretAsKeyedStream() throws Exception {__		final int numEventsPerInstance = 100__		final int maxParallelism = 8__		final int parallelism = 3__		final int numUniqueKeys = 12___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)__		env.setMaxParallelism(maxParallelism)__		env.setParallelism(parallelism)___		final List<File> partitionFiles = new ArrayList<>(parallelism)__		for (int i = 0_ i < parallelism_ ++i) {_			File partitionFile = temporaryFolder.newFile()__			partitionFiles.add(i, partitionFile)__		}__		env.addSource(new RandomTupleSource(numEventsPerInstance, numUniqueKeys))_			.keyBy(0)_			.addSink(new ToPartitionFileSink(partitionFiles))___		env.execute()___		DataStreamUtils.reinterpretAsKeyedStream(_			env.addSource(new FromPartitionFileSource(partitionFiles)),_			(KeySelector<Tuple2<Integer, Integer>, Integer>) value -> value.f0,_			TypeInformation.of(Integer.class))_			.timeWindow(Time.seconds(1)) _			.reduce((ReduceFunction<Tuple2<Integer, Integer>>) (value1, value2) ->_				new Tuple2<>(value1.f0, value1.f1 + value2.f1))_			.addSink(new ValidatingSink(numEventsPerInstance * parallelism)).setParallelism(1)___		env.execute()__	};this,test,checks,that,reinterpreting,a,data,stream,to,a,keyed,stream,works,as,expected,this,test,consists,of,two,jobs,the,first,job,materializes,a,key,by,into,files,one,files,per,partition,the,second,job,opens,the,files,created,by,the,first,jobs,as,sources,doing,the,correct,assignment,of,files,to,partitions,and,reinterprets,the,sources,as,keyed,because,we,know,they,have,been,partitioned,in,a,key,by,from,the,first,job;test,public,void,test,reinterpret,as,keyed,stream,throws,exception,final,int,num,events,per,instance,100,final,int,max,parallelism,8,final,int,parallelism,3,final,int,num,unique,keys,12,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,stream,time,characteristic,time,characteristic,ingestion,time,env,set,max,parallelism,max,parallelism,env,set,parallelism,parallelism,final,list,file,partition,files,new,array,list,parallelism,for,int,i,0,i,parallelism,i,file,partition,file,temporary,folder,new,file,partition,files,add,i,partition,file,env,add,source,new,random,tuple,source,num,events,per,instance,num,unique,keys,key,by,0,add,sink,new,to,partition,file,sink,partition,files,env,execute,data,stream,utils,reinterpret,as,keyed,stream,env,add,source,new,from,partition,file,source,partition,files,key,selector,tuple2,integer,integer,integer,value,value,f0,type,information,of,integer,class,time,window,time,seconds,1,reduce,reduce,function,tuple2,integer,integer,value1,value2,new,tuple2,value1,f0,value1,f1,value2,f1,add,sink,new,validating,sink,num,events,per,instance,parallelism,set,parallelism,1,env,execute
ReinterpretDataStreamAsKeyedStreamITCase -> @Test 	public void testReinterpretAsKeyedStream() throws Exception;1542043292;This test checks that reinterpreting a data stream to a keyed stream works as expected. This test consists of_two jobs. The first job materializes a keyBy into files, one files per partition. The second job opens the_files created by the first jobs as sources (doing the correct assignment of files to partitions) and_reinterprets the sources as keyed, because we know they have been partitioned in a keyBy from the first job.;@Test_	public void testReinterpretAsKeyedStream() throws Exception {__		final int numEventsPerInstance = 100__		final int maxParallelism = 8__		final int parallelism = 3__		final int numUniqueKeys = 12___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)__		env.setMaxParallelism(maxParallelism)__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0L))___		final List<File> partitionFiles = new ArrayList<>(parallelism)__		for (int i = 0_ i < parallelism_ ++i) {_			File partitionFile = temporaryFolder.newFile()__			partitionFiles.add(i, partitionFile)__		}__		env.addSource(new RandomTupleSource(numEventsPerInstance, numUniqueKeys))_			.keyBy(0)_			.addSink(new ToPartitionFileSink(partitionFiles))___		env.execute()___		DataStreamUtils.reinterpretAsKeyedStream(_			env.addSource(new FromPartitionFileSource(partitionFiles)),_			(KeySelector<Tuple2<Integer, Integer>, Integer>) value -> value.f0,_			TypeInformation.of(Integer.class))_			.timeWindow(Time.seconds(1)) _			.reduce((ReduceFunction<Tuple2<Integer, Integer>>) (value1, value2) ->_				new Tuple2<>(value1.f0, value1.f1 + value2.f1))_			.addSink(new ValidatingSink(numEventsPerInstance * parallelism)).setParallelism(1)___		env.execute()__	};this,test,checks,that,reinterpreting,a,data,stream,to,a,keyed,stream,works,as,expected,this,test,consists,of,two,jobs,the,first,job,materializes,a,key,by,into,files,one,files,per,partition,the,second,job,opens,the,files,created,by,the,first,jobs,as,sources,doing,the,correct,assignment,of,files,to,partitions,and,reinterprets,the,sources,as,keyed,because,we,know,they,have,been,partitioned,in,a,key,by,from,the,first,job;test,public,void,test,reinterpret,as,keyed,stream,throws,exception,final,int,num,events,per,instance,100,final,int,max,parallelism,8,final,int,parallelism,3,final,int,num,unique,keys,12,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,stream,time,characteristic,time,characteristic,ingestion,time,env,set,max,parallelism,max,parallelism,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0l,final,list,file,partition,files,new,array,list,parallelism,for,int,i,0,i,parallelism,i,file,partition,file,temporary,folder,new,file,partition,files,add,i,partition,file,env,add,source,new,random,tuple,source,num,events,per,instance,num,unique,keys,key,by,0,add,sink,new,to,partition,file,sink,partition,files,env,execute,data,stream,utils,reinterpret,as,keyed,stream,env,add,source,new,from,partition,file,source,partition,files,key,selector,tuple2,integer,integer,integer,value,value,f0,type,information,of,integer,class,time,window,time,seconds,1,reduce,reduce,function,tuple2,integer,integer,value1,value2,new,tuple2,value1,f0,value1,f1,value2,f1,add,sink,new,validating,sink,num,events,per,instance,parallelism,set,parallelism,1,env,execute
ReinterpretDataStreamAsKeyedStreamITCase -> @Test 	public void testReinterpretAsKeyedStream() throws Exception;1544525981;This test checks that reinterpreting a data stream to a keyed stream works as expected. This test consists of_two jobs. The first job materializes a keyBy into files, one files per partition. The second job opens the_files created by the first jobs as sources (doing the correct assignment of files to partitions) and_reinterprets the sources as keyed, because we know they have been partitioned in a keyBy from the first job.;@Test_	public void testReinterpretAsKeyedStream() throws Exception {__		final int numEventsPerInstance = 100__		final int maxParallelism = 8__		final int parallelism = 3__		final int numUniqueKeys = 12___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)__		env.setMaxParallelism(maxParallelism)__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0L))___		final List<File> partitionFiles = new ArrayList<>(parallelism)__		for (int i = 0_ i < parallelism_ ++i) {_			File partitionFile = temporaryFolder.newFile()__			partitionFiles.add(i, partitionFile)__		}__		env.addSource(new RandomTupleSource(numEventsPerInstance, numUniqueKeys))_			.keyBy(0)_			.addSink(new ToPartitionFileSink(partitionFiles))___		env.execute()___		DataStreamUtils.reinterpretAsKeyedStream(_			env.addSource(new FromPartitionFileSource(partitionFiles)),_			(KeySelector<Tuple2<Integer, Integer>, Integer>) value -> value.f0,_			TypeInformation.of(Integer.class))_			.timeWindow(Time.seconds(1)) _			.reduce((ReduceFunction<Tuple2<Integer, Integer>>) (value1, value2) ->_				new Tuple2<>(value1.f0, value1.f1 + value2.f1))_			.addSink(new ValidatingSink(numEventsPerInstance * parallelism)).setParallelism(1)___		env.execute()__	};this,test,checks,that,reinterpreting,a,data,stream,to,a,keyed,stream,works,as,expected,this,test,consists,of,two,jobs,the,first,job,materializes,a,key,by,into,files,one,files,per,partition,the,second,job,opens,the,files,created,by,the,first,jobs,as,sources,doing,the,correct,assignment,of,files,to,partitions,and,reinterprets,the,sources,as,keyed,because,we,know,they,have,been,partitioned,in,a,key,by,from,the,first,job;test,public,void,test,reinterpret,as,keyed,stream,throws,exception,final,int,num,events,per,instance,100,final,int,max,parallelism,8,final,int,parallelism,3,final,int,num,unique,keys,12,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,stream,time,characteristic,time,characteristic,ingestion,time,env,set,max,parallelism,max,parallelism,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0l,final,list,file,partition,files,new,array,list,parallelism,for,int,i,0,i,parallelism,i,file,partition,file,temporary,folder,new,file,partition,files,add,i,partition,file,env,add,source,new,random,tuple,source,num,events,per,instance,num,unique,keys,key,by,0,add,sink,new,to,partition,file,sink,partition,files,env,execute,data,stream,utils,reinterpret,as,keyed,stream,env,add,source,new,from,partition,file,source,partition,files,key,selector,tuple2,integer,integer,integer,value,value,f0,type,information,of,integer,class,time,window,time,seconds,1,reduce,reduce,function,tuple2,integer,integer,value1,value2,new,tuple2,value1,f0,value1,f1,value2,f1,add,sink,new,validating,sink,num,events,per,instance,parallelism,set,parallelism,1,env,execute
ReinterpretDataStreamAsKeyedStreamITCase -> @Test 	public void testReinterpretAsKeyedStream() throws Exception;1550744171;This test checks that reinterpreting a data stream to a keyed stream works as expected. This test consists of_two jobs. The first job materializes a keyBy into files, one files per partition. The second job opens the_files created by the first jobs as sources (doing the correct assignment of files to partitions) and_reinterprets the sources as keyed, because we know they have been partitioned in a keyBy from the first job.;@Test_	public void testReinterpretAsKeyedStream() throws Exception {__		final int maxParallelism = 8__		final int numEventsPerInstance = 100__		final int parallelism = 3__		final int numTotalEvents = numEventsPerInstance * parallelism__		final int numUniqueKeys = 100___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)__		env.setMaxParallelism(maxParallelism)__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0L))___		final List<File> partitionFiles = new ArrayList<>(parallelism)__		for (int i = 0_ i < parallelism_ ++i) {_			File partitionFile = temporaryFolder.newFile()__			partitionFiles.add(i, partitionFile)__		}__		env.addSource(new RandomTupleSource(numEventsPerInstance, numUniqueKeys))_			.keyBy(0)_			.addSink(new ToPartitionFileSink(partitionFiles))___		env.execute()___		DataStreamUtils.reinterpretAsKeyedStream(_			env.addSource(new FromPartitionFileSource(partitionFiles)),_			(KeySelector<Tuple2<Integer, Integer>, Integer>) value -> value.f0,_			TypeInformation.of(Integer.class))_			.timeWindow(Time.seconds(1)) _			.reduce((ReduceFunction<Tuple2<Integer, Integer>>) (value1, value2) ->_				new Tuple2<>(value1.f0, value1.f1 + value2.f1))_			.addSink(new ValidatingSink(numTotalEvents)).setParallelism(1)___		env.execute()__	};this,test,checks,that,reinterpreting,a,data,stream,to,a,keyed,stream,works,as,expected,this,test,consists,of,two,jobs,the,first,job,materializes,a,key,by,into,files,one,files,per,partition,the,second,job,opens,the,files,created,by,the,first,jobs,as,sources,doing,the,correct,assignment,of,files,to,partitions,and,reinterprets,the,sources,as,keyed,because,we,know,they,have,been,partitioned,in,a,key,by,from,the,first,job;test,public,void,test,reinterpret,as,keyed,stream,throws,exception,final,int,max,parallelism,8,final,int,num,events,per,instance,100,final,int,parallelism,3,final,int,num,total,events,num,events,per,instance,parallelism,final,int,num,unique,keys,100,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,stream,time,characteristic,time,characteristic,ingestion,time,env,set,max,parallelism,max,parallelism,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0l,final,list,file,partition,files,new,array,list,parallelism,for,int,i,0,i,parallelism,i,file,partition,file,temporary,folder,new,file,partition,files,add,i,partition,file,env,add,source,new,random,tuple,source,num,events,per,instance,num,unique,keys,key,by,0,add,sink,new,to,partition,file,sink,partition,files,env,execute,data,stream,utils,reinterpret,as,keyed,stream,env,add,source,new,from,partition,file,source,partition,files,key,selector,tuple2,integer,integer,integer,value,value,f0,type,information,of,integer,class,time,window,time,seconds,1,reduce,reduce,function,tuple2,integer,integer,value1,value2,new,tuple2,value1,f0,value1,f1,value2,f1,add,sink,new,validating,sink,num,total,events,set,parallelism,1,env,execute
