# id;timestamp;commentText;codeText;commentWords;codeWords
LegacyStatefulJobSavepointMigrationITCase -> @Test 	@Ignore 	public void writeSavepoint() throws Exception;1519232893;Manually run this to write binary snapshot data.;@Test_	@Ignore_	public void writeSavepoint() throws Exception {__		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		switch (flinkGenerateSavepointBackendType) {_			case StateBackendLoader.ROCKSDB_STATE_BACKEND_NAME:_				env.setStateBackend(new RocksDBStateBackend(new MemoryStateBackend()))__				break__			case StateBackendLoader.MEMORY_STATE_BACKEND_NAME:_				env.setStateBackend(new MemoryStateBackend())__				break__			default:_				throw new UnsupportedOperationException()__		}__		env.enableCheckpointing(500)__		env.setParallelism(4)__		env.setMaxParallelism(4)___		env_			.addSource(new LegacyCheckpointedSource(NUM_SOURCE_ELEMENTS)).setMaxParallelism(1).uid("LegacyCheckpointedSource")_			.flatMap(new LegacyCheckpointedFlatMap()).startNewChain().uid("LegacyCheckpointedFlatMap")_			.keyBy(0)_			.flatMap(new LegacyCheckpointedFlatMapWithKeyedState()).startNewChain().uid("LegacyCheckpointedFlatMapWithKeyedState")_			.keyBy(0)_			.flatMap(new KeyedStateSettingFlatMap()).startNewChain().uid("KeyedStateSettingFlatMap")_			.keyBy(0)_			.transform(_				"custom_operator",_				new TypeHint<Tuple2<Long, Long>>() {}.getTypeInfo(),_				new CheckpointedUdfOperator(new LegacyCheckpointedFlatMapWithKeyedState())).uid("LegacyCheckpointedOperator")_			.keyBy(0)_			.transform(_				"timely_stateful_operator",_				new TypeHint<Tuple2<Long, Long>>() {}.getTypeInfo(),_				new TimelyStatefulOperator()).uid("TimelyStatefulOperator")_			.addSink(new AccumulatorCountingSink<Tuple2<Long, Long>>())___		executeAndSavepoint(_			env,_			"src/test/resources/" + getSavepointPath(flinkGenerateSavepointVersion, flinkGenerateSavepointBackendType),_			new Tuple2<>(AccumulatorCountingSink.NUM_ELEMENTS_ACCUMULATOR, NUM_SOURCE_ELEMENTS))__	};manually,run,this,to,write,binary,snapshot,data;test,ignore,public,void,write,savepoint,throws,exception,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,stream,time,characteristic,time,characteristic,event,time,switch,flink,generate,savepoint,backend,type,case,state,backend,loader,env,set,state,backend,new,rocks,dbstate,backend,new,memory,state,backend,break,case,state,backend,loader,env,set,state,backend,new,memory,state,backend,break,default,throw,new,unsupported,operation,exception,env,enable,checkpointing,500,env,set,parallelism,4,env,set,max,parallelism,4,env,add,source,new,legacy,checkpointed,source,set,max,parallelism,1,uid,legacy,checkpointed,source,flat,map,new,legacy,checkpointed,flat,map,start,new,chain,uid,legacy,checkpointed,flat,map,key,by,0,flat,map,new,legacy,checkpointed,flat,map,with,keyed,state,start,new,chain,uid,legacy,checkpointed,flat,map,with,keyed,state,key,by,0,flat,map,new,keyed,state,setting,flat,map,start,new,chain,uid,keyed,state,setting,flat,map,key,by,0,transform,new,type,hint,tuple2,long,long,get,type,info,new,checkpointed,udf,operator,new,legacy,checkpointed,flat,map,with,keyed,state,uid,legacy,checkpointed,operator,key,by,0,transform,new,type,hint,tuple2,long,long,get,type,info,new,timely,stateful,operator,uid,timely,stateful,operator,add,sink,new,accumulator,counting,sink,tuple2,long,long,execute,and,savepoint,env,src,test,resources,get,savepoint,path,flink,generate,savepoint,version,flink,generate,savepoint,backend,type,new,tuple2,accumulator,counting,sink
LegacyStatefulJobSavepointMigrationITCase -> @Test 	@Ignore 	public void writeSavepoint() throws Exception;1519653247;Manually run this to write binary snapshot data.;@Test_	@Ignore_	public void writeSavepoint() throws Exception {__		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		switch (flinkGenerateSavepointBackendType) {_			case StateBackendLoader.ROCKSDB_STATE_BACKEND_NAME:_				env.setStateBackend(new RocksDBStateBackend(new MemoryStateBackend()))__				break__			case StateBackendLoader.MEMORY_STATE_BACKEND_NAME:_				env.setStateBackend(new MemoryStateBackend())__				break__			default:_				throw new UnsupportedOperationException()__		}__		env.enableCheckpointing(500)__		env.setParallelism(4)__		env.setMaxParallelism(4)___		env_			.addSource(new LegacyCheckpointedSource(NUM_SOURCE_ELEMENTS)).setMaxParallelism(1).uid("LegacyCheckpointedSource")_			.flatMap(new LegacyCheckpointedFlatMap()).startNewChain().uid("LegacyCheckpointedFlatMap")_			.keyBy(0)_			.flatMap(new LegacyCheckpointedFlatMapWithKeyedState()).startNewChain().uid("LegacyCheckpointedFlatMapWithKeyedState")_			.keyBy(0)_			.flatMap(new KeyedStateSettingFlatMap()).startNewChain().uid("KeyedStateSettingFlatMap")_			.keyBy(0)_			.transform(_				"custom_operator",_				new TypeHint<Tuple2<Long, Long>>() {}.getTypeInfo(),_				new CheckpointedUdfOperator(new LegacyCheckpointedFlatMapWithKeyedState())).uid("LegacyCheckpointedOperator")_			.keyBy(0)_			.transform(_				"timely_stateful_operator",_				new TypeHint<Tuple2<Long, Long>>() {}.getTypeInfo(),_				new TimelyStatefulOperator()).uid("TimelyStatefulOperator")_			.addSink(new AccumulatorCountingSink<Tuple2<Long, Long>>())___		executeAndSavepoint(_			env,_			"src/test/resources/" + getSavepointPath(flinkGenerateSavepointVersion, flinkGenerateSavepointBackendType),_			new Tuple2<>(AccumulatorCountingSink.NUM_ELEMENTS_ACCUMULATOR, NUM_SOURCE_ELEMENTS))__	};manually,run,this,to,write,binary,snapshot,data;test,ignore,public,void,write,savepoint,throws,exception,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,stream,time,characteristic,time,characteristic,event,time,switch,flink,generate,savepoint,backend,type,case,state,backend,loader,env,set,state,backend,new,rocks,dbstate,backend,new,memory,state,backend,break,case,state,backend,loader,env,set,state,backend,new,memory,state,backend,break,default,throw,new,unsupported,operation,exception,env,enable,checkpointing,500,env,set,parallelism,4,env,set,max,parallelism,4,env,add,source,new,legacy,checkpointed,source,set,max,parallelism,1,uid,legacy,checkpointed,source,flat,map,new,legacy,checkpointed,flat,map,start,new,chain,uid,legacy,checkpointed,flat,map,key,by,0,flat,map,new,legacy,checkpointed,flat,map,with,keyed,state,start,new,chain,uid,legacy,checkpointed,flat,map,with,keyed,state,key,by,0,flat,map,new,keyed,state,setting,flat,map,start,new,chain,uid,keyed,state,setting,flat,map,key,by,0,transform,new,type,hint,tuple2,long,long,get,type,info,new,checkpointed,udf,operator,new,legacy,checkpointed,flat,map,with,keyed,state,uid,legacy,checkpointed,operator,key,by,0,transform,new,type,hint,tuple2,long,long,get,type,info,new,timely,stateful,operator,uid,timely,stateful,operator,add,sink,new,accumulator,counting,sink,tuple2,long,long,execute,and,savepoint,env,src,test,resources,get,savepoint,path,flink,generate,savepoint,version,flink,generate,savepoint,backend,type,new,tuple2,accumulator,counting,sink
LegacyStatefulJobSavepointMigrationITCase -> @Test 	@Ignore 	public void writeSavepoint() throws Exception;1545190012;Manually run this to write binary snapshot data.;@Test_	@Ignore_	public void writeSavepoint() throws Exception {__		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		switch (flinkGenerateSavepointBackendType) {_			case StateBackendLoader.ROCKSDB_STATE_BACKEND_NAME:_				env.setStateBackend(new RocksDBStateBackend(new MemoryStateBackend()))__				break__			case StateBackendLoader.MEMORY_STATE_BACKEND_NAME:_				env.setStateBackend(new MemoryStateBackend())__				break__			default:_				throw new UnsupportedOperationException()__		}__		env.enableCheckpointing(500)__		env.setParallelism(4)__		env.setMaxParallelism(4)___		env_			.addSource(new LegacyCheckpointedSource(NUM_SOURCE_ELEMENTS)).setMaxParallelism(1).uid("LegacyCheckpointedSource")_			.flatMap(new LegacyCheckpointedFlatMap()).startNewChain().uid("LegacyCheckpointedFlatMap")_			.keyBy(0)_			.flatMap(new LegacyCheckpointedFlatMapWithKeyedState()).startNewChain().uid("LegacyCheckpointedFlatMapWithKeyedState")_			.keyBy(0)_			.flatMap(new KeyedStateSettingFlatMap()).startNewChain().uid("KeyedStateSettingFlatMap")_			.keyBy(0)_			.transform(_				"custom_operator",_				new TypeHint<Tuple2<Long, Long>>() {}.getTypeInfo(),_				new CheckpointedUdfOperator(new LegacyCheckpointedFlatMapWithKeyedState())).uid("LegacyCheckpointedOperator")_			.keyBy(0)_			.transform(_				"timely_stateful_operator",_				new TypeHint<Tuple2<Long, Long>>() {}.getTypeInfo(),_				new TimelyStatefulOperator()).uid("TimelyStatefulOperator")_			.addSink(new AccumulatorCountingSink<Tuple2<Long, Long>>())___		executeAndSavepoint(_			env,_			"src/test/resources/" + getSavepointPath(flinkGenerateSavepointVersion, flinkGenerateSavepointBackendType),_			new Tuple2<>(AccumulatorCountingSink.NUM_ELEMENTS_ACCUMULATOR, NUM_SOURCE_ELEMENTS))__	};manually,run,this,to,write,binary,snapshot,data;test,ignore,public,void,write,savepoint,throws,exception,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,stream,time,characteristic,time,characteristic,event,time,switch,flink,generate,savepoint,backend,type,case,state,backend,loader,env,set,state,backend,new,rocks,dbstate,backend,new,memory,state,backend,break,case,state,backend,loader,env,set,state,backend,new,memory,state,backend,break,default,throw,new,unsupported,operation,exception,env,enable,checkpointing,500,env,set,parallelism,4,env,set,max,parallelism,4,env,add,source,new,legacy,checkpointed,source,set,max,parallelism,1,uid,legacy,checkpointed,source,flat,map,new,legacy,checkpointed,flat,map,start,new,chain,uid,legacy,checkpointed,flat,map,key,by,0,flat,map,new,legacy,checkpointed,flat,map,with,keyed,state,start,new,chain,uid,legacy,checkpointed,flat,map,with,keyed,state,key,by,0,flat,map,new,keyed,state,setting,flat,map,start,new,chain,uid,keyed,state,setting,flat,map,key,by,0,transform,new,type,hint,tuple2,long,long,get,type,info,new,checkpointed,udf,operator,new,legacy,checkpointed,flat,map,with,keyed,state,uid,legacy,checkpointed,operator,key,by,0,transform,new,type,hint,tuple2,long,long,get,type,info,new,timely,stateful,operator,uid,timely,stateful,operator,add,sink,new,accumulator,counting,sink,tuple2,long,long,execute,and,savepoint,env,src,test,resources,get,savepoint,path,flink,generate,savepoint,version,flink,generate,savepoint,backend,type,new,tuple2,accumulator,counting,sink
LegacyStatefulJobSavepointMigrationITCase -> @Test 	@Ignore 	public void writeSavepoint() throws Exception;1547221798;Manually run this to write binary snapshot data.;@Test_	@Ignore_	public void writeSavepoint() throws Exception {__		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)___		switch (flinkGenerateSavepointBackendType) {_			case StateBackendLoader.ROCKSDB_STATE_BACKEND_NAME:_				env.setStateBackend(new RocksDBStateBackend(new MemoryStateBackend()))__				break__			case StateBackendLoader.MEMORY_STATE_BACKEND_NAME:_				env.setStateBackend(new MemoryStateBackend())__				break__			default:_				throw new UnsupportedOperationException()__		}__		env.enableCheckpointing(500)__		env.setParallelism(4)__		env.setMaxParallelism(4)___		env_			.addSource(new LegacyCheckpointedSource(NUM_SOURCE_ELEMENTS)).setMaxParallelism(1).uid("LegacyCheckpointedSource")_			.flatMap(new LegacyCheckpointedFlatMap()).startNewChain().uid("LegacyCheckpointedFlatMap")_			.keyBy(0)_			.flatMap(new LegacyCheckpointedFlatMapWithKeyedState()).startNewChain().uid("LegacyCheckpointedFlatMapWithKeyedState")_			.keyBy(0)_			.flatMap(new KeyedStateSettingFlatMap()).startNewChain().uid("KeyedStateSettingFlatMap")_			.keyBy(0)_			.transform(_				"custom_operator",_				new TypeHint<Tuple2<Long, Long>>() {}.getTypeInfo(),_				new CheckpointedUdfOperator(new LegacyCheckpointedFlatMapWithKeyedState())).uid("LegacyCheckpointedOperator")_			.keyBy(0)_			.transform(_				"timely_stateful_operator",_				new TypeHint<Tuple2<Long, Long>>() {}.getTypeInfo(),_				new TimelyStatefulOperator()).uid("TimelyStatefulOperator")_			.addSink(new AccumulatorCountingSink<Tuple2<Long, Long>>())___		executeAndSavepoint(_			env,_			"src/test/resources/" + getSavepointPath(flinkGenerateSavepointVersion, flinkGenerateSavepointBackendType),_			new Tuple2<>(AccumulatorCountingSink.NUM_ELEMENTS_ACCUMULATOR, NUM_SOURCE_ELEMENTS))__	};manually,run,this,to,write,binary,snapshot,data;test,ignore,public,void,write,savepoint,throws,exception,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,stream,time,characteristic,time,characteristic,event,time,switch,flink,generate,savepoint,backend,type,case,state,backend,loader,env,set,state,backend,new,rocks,dbstate,backend,new,memory,state,backend,break,case,state,backend,loader,env,set,state,backend,new,memory,state,backend,break,default,throw,new,unsupported,operation,exception,env,enable,checkpointing,500,env,set,parallelism,4,env,set,max,parallelism,4,env,add,source,new,legacy,checkpointed,source,set,max,parallelism,1,uid,legacy,checkpointed,source,flat,map,new,legacy,checkpointed,flat,map,start,new,chain,uid,legacy,checkpointed,flat,map,key,by,0,flat,map,new,legacy,checkpointed,flat,map,with,keyed,state,start,new,chain,uid,legacy,checkpointed,flat,map,with,keyed,state,key,by,0,flat,map,new,keyed,state,setting,flat,map,start,new,chain,uid,keyed,state,setting,flat,map,key,by,0,transform,new,type,hint,tuple2,long,long,get,type,info,new,checkpointed,udf,operator,new,legacy,checkpointed,flat,map,with,keyed,state,uid,legacy,checkpointed,operator,key,by,0,transform,new,type,hint,tuple2,long,long,get,type,info,new,timely,stateful,operator,uid,timely,stateful,operator,add,sink,new,accumulator,counting,sink,tuple2,long,long,execute,and,savepoint,env,src,test,resources,get,savepoint,path,flink,generate,savepoint,version,flink,generate,savepoint,backend,type,new,tuple2,accumulator,counting,sink
