commented;modifiers;parameterAmount;loc;comment;code
false;protected;0;4;;@Override protected Collection<Class<? extends Plugin>> getPlugins() {     return pluginList(InternalSettingsPlugin.class, MapperExtrasPlugin.class). }
false;public;0;31;;public void testMerge() throws IOException {     String stage1Mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("person").startObject("properties").startObject("tc").field("type", "token_count").field("analyzer", "keyword").endObject().endObject().endObject().endObject()).     MapperService mapperService = createIndex("test").mapperService().     DocumentMapper stage1 = mapperService.merge("person", new CompressedXContent(stage1Mapping), MapperService.MergeReason.MAPPING_UPDATE).     String stage2Mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("person").startObject("properties").startObject("tc").field("type", "token_count").field("analyzer", "standard").endObject().endObject().endObject().endObject()).     DocumentMapper stage2 = mapperService.merge("person", new CompressedXContent(stage2Mapping), MapperService.MergeReason.MAPPING_UPDATE).     // previous mapper has not been modified     assertThat(((TokenCountFieldMapper) stage1.mappers().getMapper("tc")).analyzer(), equalTo("keyword")).     // but the new one has the change     assertThat(((TokenCountFieldMapper) stage2.mappers().getMapper("tc")).analyzer(), equalTo("standard")). }
true;public;0;4;/**  *  When position increments are counted, we're looking to make sure that we:  *        - don't count tokens without an increment  *        - count normal tokens with one increment  *        - count funny tokens with more than one increment  *        - count the final token increments on the rare token streams that have them  */ ;/**  *  When position increments are counted, we're looking to make sure that we:  *        - don't count tokens without an increment  *        - count normal tokens with one increment  *        - count funny tokens with more than one increment  *        - count the final token increments on the rare token streams that have them  */ public void testCountPositionsWithIncrements() throws IOException {     Analyzer analyzer = createMockAnalyzer().     assertThat(TokenCountFieldMapper.countPositions(analyzer, "", "", true), equalTo(7)). }
true;public;0;4;/**  *  When position increments are not counted (only positions are counted), we're looking to make sure that we:  *        - don't count tokens without an increment  *        - count normal tokens with one increment  *        - count funny tokens with more than one increment as only one  *        - don't count the final token increments on the rare token streams that have them  */ ;/**  *  When position increments are not counted (only positions are counted), we're looking to make sure that we:  *        - don't count tokens without an increment  *        - count normal tokens with one increment  *        - count funny tokens with more than one increment as only one  *        - don't count the final token increments on the rare token streams that have them  */ public void testCountPositionsWithoutIncrements() throws IOException {     Analyzer analyzer = createMockAnalyzer().     assertThat(TokenCountFieldMapper.countPositions(analyzer, "", "", false), equalTo(2)). }
false;public;1;4;;@Override public TokenStreamComponents createComponents(String fieldName) {     return new TokenStreamComponents(new MockTokenizer(), tokenStream). }
false;private;0;20;;private Analyzer createMockAnalyzer() {     // Token without an increment     Token t1 = new Token().     t1.setPositionIncrement(0).     Token t2 = new Token().     // Normal token with one increment     t2.setPositionIncrement(1).     Token t3 = new Token().     // Funny token with more than one increment     t2.setPositionIncrement(2).     // Final token increment     int finalTokenIncrement = 4.     Token[] tokens = new Token[] { t1, t2, t3 }.     Collections.shuffle(Arrays.asList(tokens), random()).     final TokenStream tokenStream = new CannedTokenStream(finalTokenIncrement, 0, tokens).     // TODO: we have no CannedAnalyzer?     Analyzer analyzer = new Analyzer() {          @Override         public TokenStreamComponents createComponents(String fieldName) {             return new TokenStreamComponents(new MockTokenizer(), tokenStream).         }     }.     return analyzer. }
false;public;0;19;;public void testEmptyName() throws IOException {     IndexService indexService = createIndex("test").     DocumentMapperParser parser = indexService.mapperService().documentMapperParser().     String mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type").startObject("properties").startObject("").field("type", "token_count").field("analyzer", "standard").endObject().endObject().endObject().endObject()).     // Empty name not allowed in index created after 5.0     IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> parser.parse("type", new CompressedXContent(mapping))).     assertThat(e.getMessage(), containsString("name cannot be empty string")). }
false;public;0;5;;public void testParseNullValue() throws Exception {     DocumentMapper mapper = createIndexWithTokenCountField().     ParseContext.Document doc = parseDocument(mapper, createDocument(null)).     assertNull(doc.getField("test.tc")). }
false;public;0;5;;public void testParseEmptyValue() throws Exception {     DocumentMapper mapper = createIndexWithTokenCountField().     ParseContext.Document doc = parseDocument(mapper, createDocument("")).     assertEquals(0, doc.getField("test.tc").numericValue()). }
false;public;0;5;;public void testParseNotNullValue() throws Exception {     DocumentMapper mapper = createIndexWithTokenCountField().     ParseContext.Document doc = parseDocument(mapper, createDocument("three tokens string")).     assertEquals(3, doc.getField("test.tc").numericValue()). }
false;private;0;18;;private DocumentMapper createIndexWithTokenCountField() throws IOException {     final String content = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("person").startObject("properties").startObject("test").field("type", "text").startObject("fields").startObject("tc").field("type", "token_count").field("analyzer", "standard").endObject().endObject().endObject().endObject().endObject().endObject()).     return createIndex("test").mapperService().documentMapperParser().parse("person", new CompressedXContent(content)). }
false;private;1;8;;private SourceToParse createDocument(String fieldValue) throws Exception {     BytesReference request = BytesReference.bytes(XContentFactory.jsonBuilder().startObject().field("test", fieldValue).endObject()).     return new SourceToParse("test", "person", "1", request, XContentType.JSON). }
false;private;2;4;;private ParseContext.Document parseDocument(DocumentMapper mapper, SourceToParse request) {     return mapper.parse(request).docs().stream().findFirst().orElseThrow(() -> new IllegalStateException("Test object not parsed")). }
