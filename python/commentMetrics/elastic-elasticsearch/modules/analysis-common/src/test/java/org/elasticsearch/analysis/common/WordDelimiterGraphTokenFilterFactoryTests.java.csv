commented;modifiers;parameterAmount;loc;comment;code
false;public;0;24;;public void testMultiTerms() throws IOException {     ESTestCase.TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).put("index.analysis.filter.my_word_delimiter.type", type).put("index.analysis.filter.my_word_delimiter.catenate_all", "true").put("index.analysis.filter.my_word_delimiter.preserve_original", "true").build(), new CommonAnalysisPlugin()).     TokenFilterFactory tokenFilter = analysis.tokenFilter.get("my_word_delimiter").     String source = "PowerShot 500-42 wi-fi wi-fi-4000 j2se O'Neil's".     String[] expected = new String[] { "PowerShot", "PowerShot", "Power", "Shot", "50042", "500-42", "500", "42", "wifi", "wi-fi", "wi", "fi", "wifi4000", "wi-fi-4000", "wi", "fi", "4000", "j2se", "j2se", "j", "2", "se", "ONeil", "O'Neil's", "O", "Neil" }.     Tokenizer tokenizer = new WhitespaceTokenizer().     tokenizer.setReader(new StringReader(source)).     int[] expectedIncr = new int[] { 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1 }.     int[] expectedPosLen = new int[] { 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 3, 3, 1, 1, 1, 3, 3, 1, 1, 1, 2, 2, 1, 1 }.     assertTokenStreamContents(tokenFilter.create(tokenizer), expected, null, null, null, expectedIncr, expectedPosLen, null). }
true;public;0;21;/**  * Correct offset order when doing both parts and concatenation: PowerShot is a synonym of Power  */ ;/**  * Correct offset order when doing both parts and concatenation: PowerShot is a synonym of Power  */ public void testPartsAndCatenate() throws IOException {     ESTestCase.TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).put("index.analysis.filter.my_word_delimiter.type", type).put("index.analysis.filter.my_word_delimiter.catenate_words", "true").put("index.analysis.filter.my_word_delimiter.generate_word_parts", "true").build(), new CommonAnalysisPlugin()).     TokenFilterFactory tokenFilter = analysis.tokenFilter.get("my_word_delimiter").     String source = "PowerShot".     int[] expectedIncr = new int[] { 1, 0, 1 }.     int[] expectedPosLen = new int[] { 2, 1, 1 }.     int[] expectedStartOffsets = new int[] { 0, 0, 5 }.     int[] expectedEndOffsets = new int[] { 9, 5, 9 }.     String[] expected = new String[] { "PowerShot", "Power", "Shot" }.     Tokenizer tokenizer = new WhitespaceTokenizer().     tokenizer.setReader(new StringReader(source)).     assertTokenStreamContents(tokenFilter.create(tokenizer), expected, expectedStartOffsets, expectedEndOffsets, null, expectedIncr, expectedPosLen, null). }
false;public;0;22;;public void testAdjustingOffsets() throws IOException {     ESTestCase.TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).put("index.analysis.filter.my_word_delimiter.type", type).put("index.analysis.filter.my_word_delimiter.catenate_words", "true").put("index.analysis.filter.my_word_delimiter.generate_word_parts", "true").put("index.analysis.filter.my_word_delimiter.adjust_offsets", "false").build(), new CommonAnalysisPlugin()).     TokenFilterFactory tokenFilter = analysis.tokenFilter.get("my_word_delimiter").     String source = "PowerShot".     int[] expectedIncr = new int[] { 1, 0, 1 }.     int[] expectedPosLen = new int[] { 2, 1, 1 }.     int[] expectedStartOffsets = new int[] { 0, 0, 0 }.     int[] expectedEndOffsets = new int[] { 9, 9, 9 }.     String[] expected = new String[] { "PowerShot", "Power", "Shot" }.     Tokenizer tokenizer = new WhitespaceTokenizer().     tokenizer.setReader(new StringReader(source)).     assertTokenStreamContents(tokenFilter.create(tokenizer), expected, expectedStartOffsets, expectedEndOffsets, null, expectedIncr, expectedPosLen, null). }
