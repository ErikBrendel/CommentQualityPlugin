commented;modifiers;parameterAmount;loc;comment;code
false;public;0;4;;@Before public void setup() throws IOException {     analysis = AnalysisTestsHelper.createTestAnalysisFromClassPath(createTempDir(), RESOURCE, new CommonAnalysisPlugin()). }
false;public;0;8;;public void testDefault() throws IOException {     TokenFilterFactory tokenFilter = analysis.tokenFilter.get("cjk_bigram").     String source = "多くの学生が試験に落ちた。".     String[] expected = new String[] { "多く", "くの", "の学", "学生", "生が", "が試", "試験", "験に", "に落", "落ち", "ちた" }.     Tokenizer tokenizer = new StandardTokenizer().     tokenizer.setReader(new StringReader(source)).     assertTokenStreamContents(tokenFilter.create(tokenizer), expected). }
false;public;0;8;;public void testNoFlags() throws IOException {     TokenFilterFactory tokenFilter = analysis.tokenFilter.get("cjk_no_flags").     String source = "多くの学生が試験に落ちた。".     String[] expected = new String[] { "多く", "くの", "の学", "学生", "生が", "が試", "試験", "験に", "に落", "落ち", "ちた" }.     Tokenizer tokenizer = new StandardTokenizer().     tokenizer.setReader(new StringReader(source)).     assertTokenStreamContents(tokenFilter.create(tokenizer), expected). }
false;public;0;8;;public void testHanOnly() throws IOException {     TokenFilterFactory tokenFilter = analysis.tokenFilter.get("cjk_han_only").     String source = "多くの学生が試験に落ちた。".     String[] expected = new String[] { "多", "く", "の", "学生", "が", "試験", "に", "落", "ち", "た" }.     Tokenizer tokenizer = new StandardTokenizer().     tokenizer.setReader(new StringReader(source)).     assertTokenStreamContents(tokenFilter.create(tokenizer), expected). }
false;public;0;8;;public void testHanUnigramOnly() throws IOException {     TokenFilterFactory tokenFilter = analysis.tokenFilter.get("cjk_han_unigram_only").     String source = "多くの学生が試験に落ちた。".     String[] expected = new String[] { "多", "く", "の", "学", "学生", "生", "が", "試", "試験", "験", "に", "落", "ち", "た" }.     Tokenizer tokenizer = new StandardTokenizer().     tokenizer.setReader(new StringReader(source)).     assertTokenStreamContents(tokenFilter.create(tokenizer), expected). }
false;public;0;19;;public void testDisableGraph() throws IOException {     TokenFilterFactory allFlagsFactory = analysis.tokenFilter.get("cjk_all_flags").     TokenFilterFactory hanOnlyFactory = analysis.tokenFilter.get("cjk_han_only").     String source = "多くの学生が試験に落ちた。".     Tokenizer tokenizer = new StandardTokenizer().     tokenizer.setReader(new StringReader(source)).     try (TokenStream tokenStream = allFlagsFactory.create(tokenizer)) {         // This config outputs different size of ngrams so graph analysis is disabled         assertTrue(tokenStream.hasAttribute(DisableGraphAttribute.class)).     }     tokenizer = new StandardTokenizer().     tokenizer.setReader(new StringReader(source)).     try (TokenStream tokenStream = hanOnlyFactory.create(tokenizer)) {         // This config uses only bigrams so graph analysis is enabled         assertFalse(tokenStream.hasAttribute(DisableGraphAttribute.class)).     } }
