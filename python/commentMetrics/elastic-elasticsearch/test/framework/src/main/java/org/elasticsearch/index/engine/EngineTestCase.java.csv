commented;modifiers;parameterAmount;loc;comment;code
false;protected,static;2;3;;protected static void assertVisibleCount(Engine engine, int numDocs) throws IOException {     assertVisibleCount(engine, numDocs, true). }
false;protected,static;3;10;;protected static void assertVisibleCount(Engine engine, int numDocs, boolean refresh) throws IOException {     if (refresh) {         engine.refresh("test").     }     try (Engine.Searcher searcher = engine.acquireSearcher("test")) {         final TotalHitCountCollector collector = new TotalHitCountCollector().         searcher.searcher().search(new MatchAllDocsQuery(), collector).         assertThat(collector.getTotalHits(), equalTo(numDocs)).     } }
false;protected;0;13;;protected Settings indexSettings() {     // TODO randomize more settings     return Settings.builder().put(IndexSettings.INDEX_GC_DELETES_SETTING.getKey(), // make sure this doesn't kick in on us     "1h").put(EngineConfig.INDEX_CODEC_SETTING.getKey(), codecName).put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).put(IndexSettings.MAX_REFRESH_LISTENERS_PER_SHARD.getKey(), between(10, 10 * IndexSettings.MAX_REFRESH_LISTENERS_PER_SHARD.get(Settings.EMPTY))).put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), randomBoolean()).put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), randomBoolean() ? IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.get(Settings.EMPTY) : between(0, 1000)).build(). }
false;public;0;40;;@Override @Before public void setUp() throws Exception {     super.setUp().     primaryTerm.set(randomLongBetween(1, Long.MAX_VALUE)).     CodecService codecService = new CodecService(null, logger).     String name = Codec.getDefault().getName().     if (Arrays.asList(codecService.availableCodecs()).contains(name)) {         // some codecs are read only so we only take the ones that we have in the service and randomly         // selected by lucene test case.         codecName = name.     } else {         codecName = "default".     }     defaultSettings = IndexSettingsModule.newIndexSettings("test", indexSettings()).     threadPool = new TestThreadPool(getClass().getName()).     store = createStore().     storeReplica = createStore().     Lucene.cleanLuceneIndex(store.directory()).     Lucene.cleanLuceneIndex(storeReplica.directory()).     primaryTranslogDir = createTempDir("translog-primary").     translogHandler = createTranslogHandler(defaultSettings).     engine = createEngine(store, primaryTranslogDir).     LiveIndexWriterConfig currentIndexWriterConfig = engine.getCurrentIndexWriterConfig().     assertEquals(engine.config().getCodec().getName(), codecService.codec(codecName).getName()).     assertEquals(currentIndexWriterConfig.getCodec().getName(), codecService.codec(codecName).getName()).     if (randomBoolean()) {         engine.config().setEnableGcDeletes(false).     }     replicaTranslogDir = createTempDir("translog-replica").     replicaEngine = createEngine(storeReplica, replicaTranslogDir).     currentIndexWriterConfig = replicaEngine.getCurrentIndexWriterConfig().     assertEquals(replicaEngine.config().getCodec().getName(), codecService.codec(codecName).getName()).     assertEquals(currentIndexWriterConfig.getCodec().getName(), codecService.codec(codecName).getName()).     if (randomBoolean()) {         engine.config().setEnableGcDeletes(false).     } }
false;public;2;9;;public EngineConfig copy(EngineConfig config, LongSupplier globalCheckpointSupplier) {     return new EngineConfig(config.getShardId(), config.getAllocationId(), config.getThreadPool(), config.getIndexSettings(), config.getWarmer(), config.getStore(), config.getMergePolicy(), config.getAnalyzer(), config.getSimilarity(), new CodecService(null, logger), config.getEventListener(), config.getQueryCache(), config.getQueryCachingPolicy(), config.getTranslogConfig(), config.getFlushMergesAfter(), config.getExternalRefreshListener(), Collections.emptyList(), config.getIndexSort(), config.getCircuitBreakerService(), globalCheckpointSupplier, config.retentionLeasesSupplier(), config.getPrimaryTermSupplier(), tombstoneDocSupplier()). }
false;public;2;9;;public EngineConfig copy(EngineConfig config, Analyzer analyzer) {     return new EngineConfig(config.getShardId(), config.getAllocationId(), config.getThreadPool(), config.getIndexSettings(), config.getWarmer(), config.getStore(), config.getMergePolicy(), analyzer, config.getSimilarity(), new CodecService(null, logger), config.getEventListener(), config.getQueryCache(), config.getQueryCachingPolicy(), config.getTranslogConfig(), config.getFlushMergesAfter(), config.getExternalRefreshListener(), Collections.emptyList(), config.getIndexSort(), config.getCircuitBreakerService(), config.getGlobalCheckpointSupplier(), config.retentionLeasesSupplier(), config.getPrimaryTermSupplier(), config.getTombstoneDocSupplier()). }
false;public;2;9;;public EngineConfig copy(EngineConfig config, MergePolicy mergePolicy) {     return new EngineConfig(config.getShardId(), config.getAllocationId(), config.getThreadPool(), config.getIndexSettings(), config.getWarmer(), config.getStore(), mergePolicy, config.getAnalyzer(), config.getSimilarity(), new CodecService(null, logger), config.getEventListener(), config.getQueryCache(), config.getQueryCachingPolicy(), config.getTranslogConfig(), config.getFlushMergesAfter(), config.getExternalRefreshListener(), Collections.emptyList(), config.getIndexSort(), config.getCircuitBreakerService(), config.getGlobalCheckpointSupplier(), config.retentionLeasesSupplier(), config.getPrimaryTermSupplier(), config.getTombstoneDocSupplier()). }
false;public;0;19;;@Override @After public void tearDown() throws Exception {     super.tearDown().     try {         if (engine != null && engine.isClosed.get() == false) {             engine.getTranslog().getDeletionPolicy().assertNoOpenTranslogRefs().             assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, createMapperService("test")).             assertMaxSeqNoInCommitUserData(engine).         }         if (replicaEngine != null && replicaEngine.isClosed.get() == false) {             replicaEngine.getTranslog().getDeletionPolicy().assertNoOpenTranslogRefs().             assertConsistentHistoryBetweenTranslogAndLuceneIndex(replicaEngine, createMapperService("test")).             assertMaxSeqNoInCommitUserData(replicaEngine).         }     } finally {         IOUtils.close(replicaEngine, storeReplica, engine, store, () -> terminate(threadPool)).     } }
false;protected,static;0;3;;protected static ParseContext.Document testDocumentWithTextField() {     return testDocumentWithTextField("test"). }
false;protected,static;1;5;;protected static ParseContext.Document testDocumentWithTextField(String value) {     ParseContext.Document document = testDocument().     document.add(new TextField("value", value, Field.Store.YES)).     return document. }
false;protected,static;0;3;;protected static ParseContext.Document testDocument() {     return new ParseContext.Document(). }
false;public,static;2;3;;public static ParsedDocument createParsedDoc(String id, String routing) {     return testParsedDocument(id, routing, testDocumentWithTextField(), new BytesArray("{ \"value\" : \"test\" }"), null). }
false;public,static;3;4;;public static ParsedDocument createParsedDoc(String id, String routing, boolean recoverySource) {     return testParsedDocument(id, routing, testDocumentWithTextField(), new BytesArray("{ \"value\" : \"test\" }"), null, recoverySource). }
false;protected,static;5;4;;protected static ParsedDocument testParsedDocument(String id, String routing, ParseContext.Document document, BytesReference source, Mapping mappingUpdate) {     return testParsedDocument(id, routing, document, source, mappingUpdate, false). }
false;protected,static;6;21;;protected static ParsedDocument testParsedDocument(String id, String routing, ParseContext.Document document, BytesReference source, Mapping mappingUpdate, boolean recoverySource) {     Field uidField = new Field("_id", Uid.encodeId(id), IdFieldMapper.Defaults.FIELD_TYPE).     Field versionField = new NumericDocValuesField("_version", 0).     SeqNoFieldMapper.SequenceIDFields seqID = SeqNoFieldMapper.SequenceIDFields.emptySeqID().     document.add(uidField).     document.add(versionField).     document.add(seqID.seqNo).     document.add(seqID.seqNoDocValue).     document.add(seqID.primaryTerm).     BytesRef ref = source.toBytesRef().     if (recoverySource) {         document.add(new StoredField(SourceFieldMapper.RECOVERY_SOURCE_NAME, ref.bytes, ref.offset, ref.length)).         document.add(new NumericDocValuesField(SourceFieldMapper.RECOVERY_SOURCE_NAME, 1)).     } else {         document.add(new StoredField(SourceFieldMapper.NAME, ref.bytes, ref.offset, ref.length)).     }     return new ParsedDocument(versionField, seqID, id, "test", routing, Arrays.asList(document), source, XContentType.JSON, mappingUpdate). }
false;public,static;0;19;;public static CheckedBiFunction<String, Integer, ParsedDocument, IOException> nestedParsedDocFactory() throws Exception {     final MapperService mapperService = createMapperService("type").     final String nestedMapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type").startObject("properties").startObject("nested_field").field("type", "nested").endObject().endObject().endObject().endObject()).     final DocumentMapper nestedMapper = mapperService.documentMapperParser().parse("type", new CompressedXContent(nestedMapping)).     return (docId, nestedFieldValues) -> {         final XContentBuilder source = XContentFactory.jsonBuilder().startObject().field("field", "value").         if (nestedFieldValues > 0) {             XContentBuilder nestedField = source.startObject("nested_field").             for (int i = 0. i < nestedFieldValues. i++) {                 nestedField.field("field-" + i, "value-" + i).             }             source.endObject().         }         source.endObject().         return nestedMapper.parse(new SourceToParse("test", "type", docId, BytesReference.bytes(source), XContentType.JSON)).     }. }
false;public;2;16;;@Override public ParsedDocument newDeleteTombstoneDoc(String type, String id) {     final ParseContext.Document doc = new ParseContext.Document().     Field uidField = new Field(IdFieldMapper.NAME, Uid.encodeId(id), IdFieldMapper.Defaults.FIELD_TYPE).     doc.add(uidField).     Field versionField = new NumericDocValuesField(VersionFieldMapper.NAME, 0).     doc.add(versionField).     SeqNoFieldMapper.SequenceIDFields seqID = SeqNoFieldMapper.SequenceIDFields.emptySeqID().     doc.add(seqID.seqNo).     doc.add(seqID.seqNoDocValue).     doc.add(seqID.primaryTerm).     seqID.tombstoneField.setLongValue(1).     doc.add(seqID.tombstoneField).     return new ParsedDocument(versionField, seqID, id, type, null, Collections.singletonList(doc), new BytesArray("{}"), XContentType.JSON, null). }
false;public;1;16;;@Override public ParsedDocument newNoopTombstoneDoc(String reason) {     final ParseContext.Document doc = new ParseContext.Document().     SeqNoFieldMapper.SequenceIDFields seqID = SeqNoFieldMapper.SequenceIDFields.emptySeqID().     doc.add(seqID.seqNo).     doc.add(seqID.seqNoDocValue).     doc.add(seqID.primaryTerm).     seqID.tombstoneField.setLongValue(1).     doc.add(seqID.tombstoneField).     Field versionField = new NumericDocValuesField(VersionFieldMapper.NAME, 0).     doc.add(versionField).     BytesRef byteRef = new BytesRef(reason).     doc.add(new StoredField(SourceFieldMapper.NAME, byteRef.bytes, byteRef.offset, byteRef.length)).     return new ParsedDocument(versionField, seqID, null, null, null, Collections.singletonList(doc), null, XContentType.JSON, null). }
true;public,static;0;37;/**  * Creates a tombstone document that only includes uid, seq#, term and version fields.  */ ;/**  * Creates a tombstone document that only includes uid, seq#, term and version fields.  */ public static EngineConfig.TombstoneDocSupplier tombstoneDocSupplier() {     return new EngineConfig.TombstoneDocSupplier() {          @Override         public ParsedDocument newDeleteTombstoneDoc(String type, String id) {             final ParseContext.Document doc = new ParseContext.Document().             Field uidField = new Field(IdFieldMapper.NAME, Uid.encodeId(id), IdFieldMapper.Defaults.FIELD_TYPE).             doc.add(uidField).             Field versionField = new NumericDocValuesField(VersionFieldMapper.NAME, 0).             doc.add(versionField).             SeqNoFieldMapper.SequenceIDFields seqID = SeqNoFieldMapper.SequenceIDFields.emptySeqID().             doc.add(seqID.seqNo).             doc.add(seqID.seqNoDocValue).             doc.add(seqID.primaryTerm).             seqID.tombstoneField.setLongValue(1).             doc.add(seqID.tombstoneField).             return new ParsedDocument(versionField, seqID, id, type, null, Collections.singletonList(doc), new BytesArray("{}"), XContentType.JSON, null).         }          @Override         public ParsedDocument newNoopTombstoneDoc(String reason) {             final ParseContext.Document doc = new ParseContext.Document().             SeqNoFieldMapper.SequenceIDFields seqID = SeqNoFieldMapper.SequenceIDFields.emptySeqID().             doc.add(seqID.seqNo).             doc.add(seqID.seqNoDocValue).             doc.add(seqID.primaryTerm).             seqID.tombstoneField.setLongValue(1).             doc.add(seqID.tombstoneField).             Field versionField = new NumericDocValuesField(VersionFieldMapper.NAME, 0).             doc.add(versionField).             BytesRef byteRef = new BytesRef(reason).             doc.add(new StoredField(SourceFieldMapper.NAME, byteRef.bytes, byteRef.offset, byteRef.length)).             return new ParsedDocument(versionField, seqID, null, null, null, Collections.singletonList(doc), null, XContentType.JSON, null).         }     }. }
false;protected;0;3;;protected Store createStore() throws IOException {     return createStore(newDirectory()). }
false;protected;1;3;;protected Store createStore(final Directory directory) throws IOException {     return createStore(INDEX_SETTINGS, directory). }
false;protected;2;3;;protected Store createStore(final IndexSettings indexSettings, final Directory directory) throws IOException {     return new Store(shardId, indexSettings, directory, new DummyShardLock(shardId)). }
false;protected;1;3;;protected Translog createTranslog(LongSupplier primaryTermSupplier) throws IOException {     return createTranslog(primaryTranslogDir, primaryTermSupplier). }
false;protected;2;7;;protected Translog createTranslog(Path translogPath, LongSupplier primaryTermSupplier) throws IOException {     TranslogConfig translogConfig = new TranslogConfig(shardId, translogPath, INDEX_SETTINGS, BigArrays.NON_RECYCLING_INSTANCE).     String translogUUID = Translog.createEmptyTranslog(translogPath, SequenceNumbers.NO_OPS_PERFORMED, shardId, primaryTermSupplier.getAsLong()).     return new Translog(translogConfig, translogUUID, createTranslogDeletionPolicy(INDEX_SETTINGS), () -> SequenceNumbers.NO_OPS_PERFORMED, primaryTermSupplier). }
false;protected;1;3;;protected TranslogHandler createTranslogHandler(IndexSettings indexSettings) {     return new TranslogHandler(xContentRegistry(), indexSettings). }
false;protected;2;3;;protected InternalEngine createEngine(Store store, Path translogPath) throws IOException {     return createEngine(defaultSettings, store, translogPath, newMergePolicy(), null). }
false;protected;3;3;;protected InternalEngine createEngine(Store store, Path translogPath, LongSupplier globalCheckpointSupplier) throws IOException {     return createEngine(defaultSettings, store, translogPath, newMergePolicy(), null, null, globalCheckpointSupplier). }
false;protected;3;6;;protected InternalEngine createEngine(Store store, Path translogPath, BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier) throws IOException {     return createEngine(defaultSettings, store, translogPath, newMergePolicy(), null, localCheckpointTrackerSupplier, null). }
false;protected;4;8;;protected InternalEngine createEngine(Store store, Path translogPath, BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier, ToLongBiFunction<Engine, Engine.Operation> seqNoForOperation) throws IOException {     return createEngine(defaultSettings, store, translogPath, newMergePolicy(), null, localCheckpointTrackerSupplier, null, seqNoForOperation). }
false;protected;4;5;;protected InternalEngine createEngine(IndexSettings indexSettings, Store store, Path translogPath, MergePolicy mergePolicy) throws IOException {     return createEngine(indexSettings, store, translogPath, mergePolicy, null). }
false;protected;5;4;;protected InternalEngine createEngine(IndexSettings indexSettings, Store store, Path translogPath, MergePolicy mergePolicy, @Nullable IndexWriterFactory indexWriterFactory) throws IOException {     return createEngine(indexSettings, store, translogPath, mergePolicy, indexWriterFactory, null, null). }
false;protected;7;12;;protected InternalEngine createEngine(IndexSettings indexSettings, Store store, Path translogPath, MergePolicy mergePolicy, @Nullable IndexWriterFactory indexWriterFactory, @Nullable BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier, @Nullable LongSupplier globalCheckpointSupplier) throws IOException {     return createEngine(indexSettings, store, translogPath, mergePolicy, indexWriterFactory, localCheckpointTrackerSupplier, null, null, globalCheckpointSupplier). }
false;protected;8;20;;protected InternalEngine createEngine(IndexSettings indexSettings, Store store, Path translogPath, MergePolicy mergePolicy, @Nullable IndexWriterFactory indexWriterFactory, @Nullable BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier, @Nullable LongSupplier globalCheckpointSupplier, @Nullable ToLongBiFunction<Engine, Engine.Operation> seqNoForOperation) throws IOException {     return createEngine(indexSettings, store, translogPath, mergePolicy, indexWriterFactory, localCheckpointTrackerSupplier, seqNoForOperation, null, globalCheckpointSupplier). }
false;protected;9;13;;protected InternalEngine createEngine(IndexSettings indexSettings, Store store, Path translogPath, MergePolicy mergePolicy, @Nullable IndexWriterFactory indexWriterFactory, @Nullable BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier, @Nullable ToLongBiFunction<Engine, Engine.Operation> seqNoForOperation, @Nullable Sort indexSort, @Nullable LongSupplier globalCheckpointSupplier) throws IOException {     EngineConfig config = config(indexSettings, store, translogPath, mergePolicy, null, indexSort, globalCheckpointSupplier).     return createEngine(indexWriterFactory, localCheckpointTrackerSupplier, seqNoForOperation, config). }
false;protected;1;3;;protected InternalEngine createEngine(EngineConfig config) throws IOException {     return createEngine(null, null, null, config). }
false;private;4;18;;private InternalEngine createEngine(@Nullable IndexWriterFactory indexWriterFactory, @Nullable BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier, @Nullable ToLongBiFunction<Engine, Engine.Operation> seqNoForOperation, EngineConfig config) throws IOException {     final Store store = config.getStore().     final Directory directory = store.directory().     if (Lucene.indexExists(directory) == false) {         store.createEmpty(config.getIndexSettings().getIndexVersionCreated().luceneVersion).         final String translogUuid = Translog.createEmptyTranslog(config.getTranslogConfig().getTranslogPath(), SequenceNumbers.NO_OPS_PERFORMED, shardId, primaryTerm.get()).         store.associateIndexWithNewTranslog(translogUuid).     }     InternalEngine internalEngine = createInternalEngine(indexWriterFactory, localCheckpointTrackerSupplier, seqNoForOperation, config).     internalEngine.initializeMaxSeqNoOfUpdatesOrDeletes().     internalEngine.recoverFromTranslog(translogHandler, Long.MAX_VALUE).     return internalEngine. }
false;;2;1;;IndexWriter createWriter(Directory directory, IndexWriterConfig iwc) throws IOException.
true;public,static;1;5;/**  * Generate a new sequence number and return it. Only works on InternalEngines  */ ;/**  * Generate a new sequence number and return it. Only works on InternalEngines  */ public static long generateNewSeqNo(final Engine engine) {     assert engine instanceof InternalEngine : "expected InternalEngine, got: " + engine.getClass().     InternalEngine internalEngine = (InternalEngine) engine.     return internalEngine.getLocalCheckpointTracker().generateSeqNo(). }
false;;2;6;;@Override IndexWriter createWriter(Directory directory, IndexWriterConfig iwc) throws IOException {     return (indexWriterFactory != null) ? indexWriterFactory.createWriter(directory, iwc) : super.createWriter(directory, iwc). }
false;protected;1;6;;@Override protected long doGenerateSeqNoForOperation(final Operation operation) {     return seqNoForOperation != null ? seqNoForOperation.applyAsLong(this, operation) : super.doGenerateSeqNoForOperation(operation). }
false;;2;6;;@Override IndexWriter createWriter(Directory directory, IndexWriterConfig iwc) throws IOException {     return (indexWriterFactory != null) ? indexWriterFactory.createWriter(directory, iwc) : super.createWriter(directory, iwc). }
false;protected;1;6;;@Override protected long doGenerateSeqNoForOperation(final Operation operation) {     return seqNoForOperation != null ? seqNoForOperation.applyAsLong(this, operation) : super.doGenerateSeqNoForOperation(operation). }
false;public,static;4;40;;public static InternalEngine createInternalEngine(@Nullable final IndexWriterFactory indexWriterFactory, @Nullable final BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier, @Nullable final ToLongBiFunction<Engine, Engine.Operation> seqNoForOperation, final EngineConfig config) {     if (localCheckpointTrackerSupplier == null) {         return new InternalTestEngine(config) {              @Override             IndexWriter createWriter(Directory directory, IndexWriterConfig iwc) throws IOException {                 return (indexWriterFactory != null) ? indexWriterFactory.createWriter(directory, iwc) : super.createWriter(directory, iwc).             }              @Override             protected long doGenerateSeqNoForOperation(final Operation operation) {                 return seqNoForOperation != null ? seqNoForOperation.applyAsLong(this, operation) : super.doGenerateSeqNoForOperation(operation).             }         }.     } else {         return new InternalTestEngine(config, localCheckpointTrackerSupplier) {              @Override             IndexWriter createWriter(Directory directory, IndexWriterConfig iwc) throws IOException {                 return (indexWriterFactory != null) ? indexWriterFactory.createWriter(directory, iwc) : super.createWriter(directory, iwc).             }              @Override             protected long doGenerateSeqNoForOperation(final Operation operation) {                 return seqNoForOperation != null ? seqNoForOperation.applyAsLong(this, operation) : super.doGenerateSeqNoForOperation(operation).             }         }.     } }
false;public;5;4;;public EngineConfig config(IndexSettings indexSettings, Store store, Path translogPath, MergePolicy mergePolicy, ReferenceManager.RefreshListener refreshListener) {     return config(indexSettings, store, translogPath, mergePolicy, refreshListener, null, () -> SequenceNumbers.NO_OPS_PERFORMED). }
false;public;7;12;;public EngineConfig config(IndexSettings indexSettings, Store store, Path translogPath, MergePolicy mergePolicy, ReferenceManager.RefreshListener refreshListener, Sort indexSort, LongSupplier globalCheckpointSupplier) {     return config(indexSettings, store, translogPath, mergePolicy, refreshListener, indexSort, globalCheckpointSupplier, globalCheckpointSupplier == null ? null : () -> RetentionLeases.EMPTY). }
false;public;8;21;;public EngineConfig config(final IndexSettings indexSettings, final Store store, final Path translogPath, final MergePolicy mergePolicy, final ReferenceManager.RefreshListener refreshListener, final Sort indexSort, final LongSupplier globalCheckpointSupplier, final Supplier<RetentionLeases> retentionLeasesSupplier) {     return config(indexSettings, store, translogPath, mergePolicy, refreshListener, null, indexSort, globalCheckpointSupplier, retentionLeasesSupplier, new NoneCircuitBreakerService()). }
false;public;9;17;;public EngineConfig config(IndexSettings indexSettings, Store store, Path translogPath, MergePolicy mergePolicy, ReferenceManager.RefreshListener externalRefreshListener, ReferenceManager.RefreshListener internalRefreshListener, Sort indexSort, @Nullable LongSupplier maybeGlobalCheckpointSupplier, CircuitBreakerService breakerService) {     return config(indexSettings, store, translogPath, mergePolicy, externalRefreshListener, internalRefreshListener, indexSort, maybeGlobalCheckpointSupplier, maybeGlobalCheckpointSupplier == null ? null : () -> RetentionLeases.EMPTY, breakerService). }
false;public;10;63;;public EngineConfig config(final IndexSettings indexSettings, final Store store, final Path translogPath, final MergePolicy mergePolicy, final ReferenceManager.RefreshListener externalRefreshListener, final ReferenceManager.RefreshListener internalRefreshListener, final Sort indexSort, @Nullable final LongSupplier maybeGlobalCheckpointSupplier, @Nullable final Supplier<RetentionLeases> maybeRetentionLeasesSupplier, final CircuitBreakerService breakerService) {     final IndexWriterConfig iwc = newIndexWriterConfig().     final TranslogConfig translogConfig = new TranslogConfig(shardId, translogPath, indexSettings, BigArrays.NON_RECYCLING_INSTANCE).     // we don't need to notify anybody in this test     final Engine.EventListener eventListener = new Engine.EventListener() {     }.     final List<ReferenceManager.RefreshListener> extRefreshListenerList = externalRefreshListener == null ? emptyList() : Collections.singletonList(externalRefreshListener).     final List<ReferenceManager.RefreshListener> intRefreshListenerList = internalRefreshListener == null ? emptyList() : Collections.singletonList(internalRefreshListener).     final LongSupplier globalCheckpointSupplier.     final Supplier<RetentionLeases> retentionLeasesSupplier.     if (maybeGlobalCheckpointSupplier == null) {         assert maybeRetentionLeasesSupplier == null.         final ReplicationTracker replicationTracker = new ReplicationTracker(shardId, allocationId.getId(), indexSettings, randomNonNegativeLong(), SequenceNumbers.NO_OPS_PERFORMED, update -> {         }, () -> 0L, (leases, listener) -> {         }).         globalCheckpointSupplier = replicationTracker.         retentionLeasesSupplier = replicationTracker::getRetentionLeases.     } else {         assert maybeRetentionLeasesSupplier != null.         globalCheckpointSupplier = maybeGlobalCheckpointSupplier.         retentionLeasesSupplier = maybeRetentionLeasesSupplier.     }     return new EngineConfig(shardId, allocationId.getId(), threadPool, indexSettings, null, store, mergePolicy, iwc.getAnalyzer(), iwc.getSimilarity(), new CodecService(null, logger), eventListener, IndexSearcher.getDefaultQueryCache(), IndexSearcher.getDefaultQueryCachingPolicy(), translogConfig, TimeValue.timeValueMinutes(5), extRefreshListenerList, intRefreshListenerList, indexSort, breakerService, globalCheckpointSupplier, retentionLeasesSupplier, primaryTerm, tombstoneDocSupplier()). }
false;protected;3;3;;protected EngineConfig noOpConfig(IndexSettings indexSettings, Store store, Path translogPath) {     return noOpConfig(indexSettings, store, translogPath, null). }
false;protected;4;3;;protected EngineConfig noOpConfig(IndexSettings indexSettings, Store store, Path translogPath, LongSupplier globalCheckpointSupplier) {     return config(indexSettings, store, translogPath, newMergePolicy(), null, null, globalCheckpointSupplier). }
false;protected,static;1;3;;protected static BytesArray bytesArray(String string) {     return new BytesArray(string.getBytes(Charset.defaultCharset())). }
false;public,static;1;3;;public static Term newUid(String id) {     return new Term("_id", Uid.encodeId(id)). }
false;public,static;1;3;;public static Term newUid(ParsedDocument doc) {     return newUid(doc.id()). }
false;protected;2;3;;protected Engine.Get newGet(boolean realtime, ParsedDocument doc) {     return new Engine.Get(realtime, false, doc.type(), doc.id(), newUid(doc)). }
false;protected;1;3;;protected Engine.Index indexForDoc(ParsedDocument doc) {     return new Engine.Index(newUid(doc), primaryTerm.get(), doc). }
false;protected;4;5;;protected Engine.Index replicaIndexForDoc(ParsedDocument doc, long version, long seqNo, boolean isRetry) {     return new Engine.Index(newUid(doc), doc, seqNo, primaryTerm.get(), version, null, Engine.Operation.Origin.REPLICA, System.nanoTime(), IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, isRetry, SequenceNumbers.UNASSIGNED_SEQ_NO, 0). }
false;protected;4;4;;protected Engine.Delete replicaDeleteForDoc(String id, long version, long seqNo, long startTime) {     return new Engine.Delete("test", id, newUid(id), seqNo, 1, version, null, Engine.Operation.Origin.REPLICA, startTime, SequenceNumbers.UNASSIGNED_SEQ_NO, 0). }
false;protected,static;2;3;;protected static void assertVisibleCount(InternalEngine engine, int numDocs) throws IOException {     assertVisibleCount(engine, numDocs, true). }
false;protected,static;3;10;;protected static void assertVisibleCount(InternalEngine engine, int numDocs, boolean refresh) throws IOException {     if (refresh) {         engine.refresh("test").     }     try (Engine.Searcher searcher = engine.acquireSearcher("test")) {         final TotalHitCountCollector collector = new TotalHitCountCollector().         searcher.searcher().search(new MatchAllDocsQuery(), collector).         assertThat(collector.getTotalHits(), equalTo(numDocs)).     } }
false;public,static;6;49;;public static List<Engine.Operation> generateSingleDocHistory(boolean forReplica, VersionType versionType, long primaryTerm, int minOpCount, int maxOpCount, String docId) {     final int numOfOps = randomIntBetween(minOpCount, maxOpCount).     final List<Engine.Operation> ops = new ArrayList<>().     final Term id = newUid(docId).     final int startWithSeqNo = 0.     final String valuePrefix = (forReplica ? "r_" : "p_") + docId + "_".     final boolean incrementTermWhenIntroducingSeqNo = randomBoolean().     for (int i = 0. i < numOfOps. i++) {         final Engine.Operation op.         final long version.         switch(versionType) {             case INTERNAL:                 version = forReplica ? i : Versions.MATCH_ANY.                 break.             case EXTERNAL:                 version = i.                 break.             case EXTERNAL_GTE:                 version = randomBoolean() ? Math.max(i - 1, 0) : i.                 break.             case FORCE:                 version = randomNonNegativeLong().                 break.             default:                 throw new UnsupportedOperationException("unknown version type: " + versionType).         }         if (randomBoolean()) {             op = new Engine.Index(id, testParsedDocument(docId, null, testDocumentWithTextField(valuePrefix + i), SOURCE, null), forReplica && i >= startWithSeqNo ? i * 2 : SequenceNumbers.UNASSIGNED_SEQ_NO, forReplica && i >= startWithSeqNo && incrementTermWhenIntroducingSeqNo ? primaryTerm + 1 : primaryTerm, version, forReplica ? null : versionType, forReplica ? REPLICA : PRIMARY, System.currentTimeMillis(), -1, false, SequenceNumbers.UNASSIGNED_SEQ_NO, 0).         } else {             op = new Engine.Delete("test", docId, id, forReplica && i >= startWithSeqNo ? i * 2 : SequenceNumbers.UNASSIGNED_SEQ_NO, forReplica && i >= startWithSeqNo && incrementTermWhenIntroducingSeqNo ? primaryTerm + 1 : primaryTerm, version, forReplica ? null : versionType, forReplica ? REPLICA : PRIMARY, System.currentTimeMillis(), SequenceNumbers.UNASSIGNED_SEQ_NO, 0).         }         ops.add(op).     }     return ops. }
false;public;4;39;;public List<Engine.Operation> generateHistoryOnReplica(int numOps, boolean allowGapInSeqNo, boolean allowDuplicate, boolean includeNestedDocs) throws Exception {     long seqNo = 0.     final int maxIdValue = randomInt(numOps * 2).     final List<Engine.Operation> operations = new ArrayList<>(numOps).     CheckedBiFunction<String, Integer, ParsedDocument, IOException> nestedParsedDocFactory = nestedParsedDocFactory().     for (int i = 0. i < numOps. i++) {         final String id = Integer.toString(randomInt(maxIdValue)).         final Engine.Operation.TYPE opType = randomFrom(Engine.Operation.TYPE.values()).         final boolean isNestedDoc = includeNestedDocs && opType == Engine.Operation.TYPE.INDEX && randomBoolean().         final int nestedValues = between(0, 3).         final long startTime = threadPool.relativeTimeInMillis().         final int copies = allowDuplicate && rarely() ? between(2, 4) : 1.         for (int copy = 0. copy < copies. copy++) {             final ParsedDocument doc = isNestedDoc ? nestedParsedDocFactory.apply(id, nestedValues) : createParsedDoc(id, null).             switch(opType) {                 case INDEX:                     operations.add(new Engine.Index(EngineTestCase.newUid(doc), doc, seqNo, primaryTerm.get(), i, null, Engine.Operation.Origin.REPLICA, startTime, -1, true, SequenceNumbers.UNASSIGNED_SEQ_NO, 0)).                     break.                 case DELETE:                     operations.add(new Engine.Delete(doc.type(), doc.id(), EngineTestCase.newUid(doc), seqNo, primaryTerm.get(), i, null, Engine.Operation.Origin.REPLICA, startTime, SequenceNumbers.UNASSIGNED_SEQ_NO, 0)).                     break.                 case NO_OP:                     operations.add(new Engine.NoOp(seqNo, primaryTerm.get(), Engine.Operation.Origin.REPLICA, startTime, "test-" + i)).                     break.                 default:                     throw new IllegalStateException("Unknown operation type [" + opType + "]").             }         }         seqNo++.         if (allowGapInSeqNo && rarely()) {             seqNo++.         }     }     Randomness.shuffle(operations).     return operations. }
false;public,static;4;68;;public static void assertOpsOnReplica(final List<Engine.Operation> ops, final InternalEngine replicaEngine, boolean shuffleOps, final Logger logger) throws IOException {     final Engine.Operation lastOp = ops.get(ops.size() - 1).     final String lastFieldValue.     if (lastOp instanceof Engine.Index) {         Engine.Index index = (Engine.Index) lastOp.         lastFieldValue = index.docs().get(0).get("value").     } else {         // delete         lastFieldValue = null.     }     if (shuffleOps) {         int firstOpWithSeqNo = 0.         while (firstOpWithSeqNo < ops.size() && ops.get(firstOpWithSeqNo).seqNo() < 0) {             firstOpWithSeqNo++.         }         // shuffle ops but make sure legacy ops are first         shuffle(ops.subList(0, firstOpWithSeqNo), random()).         shuffle(ops.subList(firstOpWithSeqNo, ops.size()), random()).     }     boolean firstOp = true.     for (Engine.Operation op : ops) {         logger.info("performing [{}], v [{}], seq# [{}], term [{}]", op.operationType().name().charAt(0), op.version(), op.seqNo(), op.primaryTerm()).         if (op instanceof Engine.Index) {             Engine.IndexResult result = replicaEngine.index((Engine.Index) op).             // replicas don't really care to about creation status of documents             // this allows to ignore the case where a document was found in the live version maps in             // a delete state and return false for the created flag in favor of code simplicity             // as deleted or not. This check is just signal regression so a decision can be made if it's             // intentional             assertThat(result.isCreated(), equalTo(firstOp)).             assertThat(result.getVersion(), equalTo(op.version())).             assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS)).         } else {             Engine.DeleteResult result = replicaEngine.delete((Engine.Delete) op).             // Replicas don't really care to about found status of documents             // this allows to ignore the case where a document was found in the live version maps in             // a delete state and return true for the found flag in favor of code simplicity             // his check is just signal regression so a decision can be made if it's             // intentional             assertThat(result.isFound(), equalTo(firstOp == false)).             assertThat(result.getVersion(), equalTo(op.version())).             assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS)).         }         if (randomBoolean()) {             replicaEngine.refresh("test").         }         if (randomBoolean()) {             replicaEngine.flush().             replicaEngine.refresh("test").         }         firstOp = false.     }     assertVisibleCount(replicaEngine, lastFieldValue == null ? 0 : 1).     if (lastFieldValue != null) {         try (Engine.Searcher searcher = replicaEngine.acquireSearcher("test")) {             final TotalHitCountCollector collector = new TotalHitCountCollector().             searcher.searcher().search(new TermQuery(new Term("value", lastFieldValue)), collector).             assertThat(collector.getTotalHits(), equalTo(1)).         }     } }
false;public,static;2;33;;public static void concurrentlyApplyOps(List<Engine.Operation> ops, InternalEngine engine) throws InterruptedException {     Thread[] thread = new Thread[randomIntBetween(3, 5)].     CountDownLatch startGun = new CountDownLatch(thread.length).     AtomicInteger offset = new AtomicInteger(-1).     for (int i = 0. i < thread.length. i++) {         thread[i] = new Thread(() -> {             startGun.countDown().             try {                 startGun.await().             } catch (InterruptedException e) {                 throw new AssertionError(e).             }             int docOffset.             while ((docOffset = offset.incrementAndGet()) < ops.size()) {                 try {                     applyOperation(engine, ops.get(docOffset)).                     if ((docOffset + 1) % 4 == 0) {                         engine.refresh("test").                     }                     if (rarely()) {                         engine.flush().                     }                 } catch (IOException e) {                     throw new AssertionError(e).                 }             }         }).         thread[i].start().     }     for (int i = 0. i < thread.length. i++) {         thread[i].join().     } }
false;public,static;2;11;;public static void applyOperations(Engine engine, List<Engine.Operation> operations) throws IOException {     for (Engine.Operation operation : operations) {         applyOperation(engine, operation).         if (randomInt(100) < 10) {             engine.refresh("test").         }         if (rarely()) {             engine.flush().         }     } }
false;public,static;2;17;;public static Engine.Result applyOperation(Engine engine, Engine.Operation operation) throws IOException {     final Engine.Result result.     switch(operation.operationType()) {         case INDEX:             result = engine.index((Engine.Index) operation).             break.         case DELETE:             result = engine.delete((Engine.Delete) operation).             break.         case NO_OP:             result = engine.noOp((Engine.NoOp) operation).             break.         default:             throw new IllegalStateException("No operation defined for [" + operation + "]").     }     return result. }
true;public,static;2;35;/**  * Gets a collection of tuples of docId, sequence number, and primary term of all live documents in the provided engine.  */ ;/**  * Gets a collection of tuples of docId, sequence number, and primary term of all live documents in the provided engine.  */ public static List<DocIdSeqNoAndTerm> getDocIds(Engine engine, boolean refresh) throws IOException {     if (refresh) {         engine.refresh("test_get_doc_ids").     }     try (Engine.Searcher searcher = engine.acquireSearcher("test_get_doc_ids")) {         List<DocIdSeqNoAndTerm> docs = new ArrayList<>().         for (LeafReaderContext leafContext : searcher.reader().leaves()) {             LeafReader reader = leafContext.reader().             NumericDocValues seqNoDocValues = reader.getNumericDocValues(SeqNoFieldMapper.NAME).             NumericDocValues primaryTermDocValues = reader.getNumericDocValues(SeqNoFieldMapper.PRIMARY_TERM_NAME).             Bits liveDocs = reader.getLiveDocs().             for (int i = 0. i < reader.maxDoc(). i++) {                 if (liveDocs == null || liveDocs.get(i)) {                     if (primaryTermDocValues.advanceExact(i) == false) {                         // We have to skip non-root docs because its _id field is not stored (indexed only).                         continue.                     }                     final long primaryTerm = primaryTermDocValues.longValue().                     Document uuid = reader.document(i, Collections.singleton(IdFieldMapper.NAME)).                     BytesRef binaryID = uuid.getBinaryValue(IdFieldMapper.NAME).                     String id = Uid.decodeId(Arrays.copyOfRange(binaryID.bytes, binaryID.offset, binaryID.offset + binaryID.length)).                     if (seqNoDocValues.advanceExact(i) == false) {                         throw new AssertionError("seqNoDocValues not found for doc[" + i + "] id[" + id + "]").                     }                     final long seqNo = seqNoDocValues.longValue().                     docs.add(new DocIdSeqNoAndTerm(id, seqNo, primaryTerm)).                 }             }         }         docs.sort(Comparator.comparingLong(DocIdSeqNoAndTerm::getSeqNo).thenComparingLong(DocIdSeqNoAndTerm::getPrimaryTerm).thenComparing((DocIdSeqNoAndTerm::getId))).         return docs.     } }
true;public,static;2;11;/**  * Reads all engine operations that have been processed by the engine from Lucene index.  * The returned operations are sorted and de-duplicated, thus each sequence number will be have at most one operation.  */ ;/**  * Reads all engine operations that have been processed by the engine from Lucene index.  * The returned operations are sorted and de-duplicated, thus each sequence number will be have at most one operation.  */ public static List<Translog.Operation> readAllOperationsInLucene(Engine engine, MapperService mapper) throws IOException {     final List<Translog.Operation> operations = new ArrayList<>().     long maxSeqNo = Math.max(0, ((InternalEngine) engine).getLocalCheckpointTracker().getMaxSeqNo()).     try (Translog.Snapshot snapshot = engine.newChangesSnapshot("test", mapper, 0, maxSeqNo, false)) {         Translog.Operation op.         while ((op = snapshot.next()) != null) {             operations.add(op).         }     }     return operations. }
true;public,static;2;43;/**  * Asserts the provided engine has a consistent document history between translog and Lucene index.  */ ;/**  * Asserts the provided engine has a consistent document history between translog and Lucene index.  */ public static void assertConsistentHistoryBetweenTranslogAndLuceneIndex(Engine engine, MapperService mapper) throws IOException {     if (mapper.documentMapper() == null || engine.config().getIndexSettings().isSoftDeleteEnabled() == false || (engine instanceof InternalEngine) == false) {         return.     }     final long maxSeqNo = ((InternalEngine) engine).getLocalCheckpointTracker().getMaxSeqNo().     if (maxSeqNo < 0) {         // nothing to check         return.     }     final Map<Long, Translog.Operation> translogOps = new HashMap<>().     try (Translog.Snapshot snapshot = EngineTestCase.getTranslog(engine).newSnapshot()) {         Translog.Operation op.         while ((op = snapshot.next()) != null) {             translogOps.put(op.seqNo(), op).         }     }     final Map<Long, Translog.Operation> luceneOps = readAllOperationsInLucene(engine, mapper).stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity())).     final long globalCheckpoint = EngineTestCase.getTranslog(engine).getLastSyncedGlobalCheckpoint().     final long retainedOps = engine.config().getIndexSettings().getSoftDeleteRetentionOperations().     final long seqNoForRecovery.     try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {         seqNoForRecovery = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY)) + 1.     }     final long minSeqNoToRetain = Math.min(seqNoForRecovery, globalCheckpoint + 1 - retainedOps).     for (Translog.Operation translogOp : translogOps.values()) {         final Translog.Operation luceneOp = luceneOps.get(translogOp.seqNo()).         if (luceneOp == null) {             if (minSeqNoToRetain <= translogOp.seqNo() && translogOp.seqNo() <= maxSeqNo) {                 fail("Operation not found seq# [" + translogOp.seqNo() + "], global checkpoint [" + globalCheckpoint + "], " + "retention policy [" + retainedOps + "], maxSeqNo [" + maxSeqNo + "], translog op [" + translogOp + "]").             } else {                 continue.             }         }         assertThat(luceneOp, notNullValue()).         assertThat(luceneOp.toString(), luceneOp.primaryTerm(), equalTo(translogOp.primaryTerm())).         assertThat(luceneOp.opType(), equalTo(translogOp.opType())).         if (luceneOp.opType() == Translog.Operation.Type.INDEX) {             assertThat(luceneOp.getSource().source, equalTo(translogOp.getSource().source)).         }     } }
true;public,static;1;11;/**  * Asserts that the max_seq_no stored in the commit's user_data is never smaller than seq_no of any document in the commit.  */ ;/**  * Asserts that the max_seq_no stored in the commit's user_data is never smaller than seq_no of any document in the commit.  */ public static void assertMaxSeqNoInCommitUserData(Engine engine) throws Exception {     List<IndexCommit> commits = DirectoryReader.listCommits(engine.store.directory()).     for (IndexCommit commit : commits) {         try (DirectoryReader reader = DirectoryReader.open(commit)) {             AtomicLong maxSeqNoFromDocs = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED).             Lucene.scanSeqNosInReader(reader, 0, Long.MAX_VALUE, n -> maxSeqNoFromDocs.set(Math.max(n, maxSeqNoFromDocs.get()))).             assertThat(Long.parseLong(commit.getUserData().get(SequenceNumbers.MAX_SEQ_NO)), greaterThanOrEqualTo(maxSeqNoFromDocs.get())).         }     } }
false;public,static;1;12;;public static MapperService createMapperService(String type) throws IOException {     IndexMetaData indexMetaData = IndexMetaData.builder("test").settings(Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)).putMapping(type, "{\"properties\": {}}").build().     MapperService mapperService = MapperTestUtils.newMapperService(new NamedXContentRegistry(ClusterModule.getNamedXWriteables()), createTempDir(), Settings.EMPTY, "test").     mapperService.merge(indexMetaData, MapperService.MergeReason.MAPPING_UPDATE).     return mapperService. }
true;public,static;1;5;/**  * Exposes a translog associated with the given engine for testing purpose.  */ ;/**  * Exposes a translog associated with the given engine for testing purpose.  */ public static Translog getTranslog(Engine engine) {     assert engine instanceof InternalEngine : "only InternalEngines have translogs, got: " + engine.getClass().     InternalEngine internalEngine = (InternalEngine) engine.     return internalEngine.getTranslog(). }
true;public,static;2;3;/**  * Waits for all operations up to the provided sequence number to complete in the given internal engine.  *  * @param seqNo the sequence number that the checkpoint must advance to before this method returns  * @throws InterruptedException if the thread was interrupted while blocking on the condition  */ ;/**  * Waits for all operations up to the provided sequence number to complete in the given internal engine.  *  * @param seqNo the sequence number that the checkpoint must advance to before this method returns  * @throws InterruptedException if the thread was interrupted while blocking on the condition  */ public static void waitForOpsToComplete(InternalEngine engine, long seqNo) throws InterruptedException {     engine.getLocalCheckpointTracker().waitForOpsToComplete(seqNo). }
false;public,static;1;5;;public static boolean hasSnapshottedCommits(Engine engine) {     assert engine instanceof InternalEngine : "only InternalEngines have snapshotted commits, got: " + engine.getClass().     InternalEngine internalEngine = (InternalEngine) engine.     return internalEngine.hasSnapshottedCommits(). }
false;public;0;3;;public long get() {     return term.get(). }
false;public;1;3;;public void set(long newTerm) {     this.term.set(newTerm). }
false;public;0;4;;@Override public long getAsLong() {     return get(). }
