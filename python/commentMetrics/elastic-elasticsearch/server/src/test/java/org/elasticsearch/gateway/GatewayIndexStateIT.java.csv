# id;timestamp;commentText;codeText;commentWords;codeWords
GatewayIndexStateIT -> public void testRecoverMissingAnalyzer() throws Exception;1524684173;This test really tests worst case scenario where we have a missing analyzer setting._In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService._This also includes plugins etc.;public void testRecoverMissingAnalyzer() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        prepareCreate("test").setSettings(Settings.builder()_            .put("index.analysis.analyzer.test.tokenizer", "keyword")_            .put("index.number_of_shards", "1"))_            .addMapping("type1", "{\n" +_                "    \"type1\": {\n" +_                "      \"properties\": {\n" +_                "        \"field1\": {\n" +_                "          \"type\": \"text\",\n" +_                "          \"analyzer\": \"test\"\n" +_                "        }\n" +_                "      }\n" +_                "    }\n" +_                "  }}", XContentType.JSON).get()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings()_                .filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())___        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(MapperParsingException.class, ex.getCause().getClass())__        assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]"))__    };this,test,really,tests,worst,case,scenario,where,we,have,a,missing,analyzer,setting,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,missing,analyzer,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,prepare,create,test,set,settings,settings,builder,put,index,analysis,analyzer,test,tokenizer,keyword,put,index,1,add,mapping,type1,n,type1,n,properties,n,field1,n,type,text,n,analyzer,test,n,n,n,n,xcontent,type,json,get,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value,one,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,meta,data,get,settings,filter,s,index,analysis,analyzer,test,tokenizer,equals,s,false,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,mapper,parsing,exception,class,ex,get,cause,get,class,assert,that,ex,get,cause,get,message,contains,string,analyzer,test,not,found,for,field,field1
GatewayIndexStateIT -> public void testRecoverMissingAnalyzer() throws Exception;1527622193;This test really tests worst case scenario where we have a missing analyzer setting._In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService._This also includes plugins etc.;public void testRecoverMissingAnalyzer() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        prepareCreate("test").setSettings(Settings.builder()_            .put("index.analysis.analyzer.test.tokenizer", "standard")_            .put("index.number_of_shards", "1"))_            .addMapping("type1", "{\n" +_                "    \"type1\": {\n" +_                "      \"properties\": {\n" +_                "        \"field1\": {\n" +_                "          \"type\": \"text\",\n" +_                "          \"analyzer\": \"test\"\n" +_                "        }\n" +_                "      }\n" +_                "    }\n" +_                "  }}", XContentType.JSON).get()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings()_                .filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())___        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(MapperParsingException.class, ex.getCause().getClass())__        assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]"))__    };this,test,really,tests,worst,case,scenario,where,we,have,a,missing,analyzer,setting,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,missing,analyzer,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,prepare,create,test,set,settings,settings,builder,put,index,analysis,analyzer,test,tokenizer,standard,put,index,1,add,mapping,type1,n,type1,n,properties,n,field1,n,type,text,n,analyzer,test,n,n,n,n,xcontent,type,json,get,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value,one,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,meta,data,get,settings,filter,s,index,analysis,analyzer,test,tokenizer,equals,s,false,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,mapper,parsing,exception,class,ex,get,cause,get,class,assert,that,ex,get,cause,get,message,contains,string,analyzer,test,not,found,for,field,field1
GatewayIndexStateIT -> public void testRecoverMissingAnalyzer() throws Exception;1528762805;This test really tests worst case scenario where we have a missing analyzer setting._In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService._This also includes plugins etc.;public void testRecoverMissingAnalyzer() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        prepareCreate("test").setSettings(Settings.builder()_            .put("index.analysis.analyzer.test.tokenizer", "standard")_            .put("index.number_of_shards", "1"))_            .addMapping("type1", "{\n" +_                "    \"type1\": {\n" +_                "      \"properties\": {\n" +_                "        \"field1\": {\n" +_                "          \"type\": \"text\",\n" +_                "          \"analyzer\": \"test\"\n" +_                "        }\n" +_                "      }\n" +_                "    }\n" +_                "  }}", XContentType.JSON).get()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings()_                .filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())___        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(MapperParsingException.class, ex.getCause().getClass())__        assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]"))__    };this,test,really,tests,worst,case,scenario,where,we,have,a,missing,analyzer,setting,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,missing,analyzer,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,prepare,create,test,set,settings,settings,builder,put,index,analysis,analyzer,test,tokenizer,standard,put,index,1,add,mapping,type1,n,type1,n,properties,n,field1,n,type,text,n,analyzer,test,n,n,n,n,xcontent,type,json,get,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value,one,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,meta,data,get,settings,filter,s,index,analysis,analyzer,test,tokenizer,equals,s,false,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,mapper,parsing,exception,class,ex,get,cause,get,class,assert,that,ex,get,cause,get,message,contains,string,analyzer,test,not,found,for,field,field1
GatewayIndexStateIT -> public void testRecoverMissingAnalyzer() throws Exception;1540486836;This test really tests worst case scenario where we have a missing analyzer setting._In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService._This also includes plugins etc.;public void testRecoverMissingAnalyzer() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        prepareCreate("test").setSettings(Settings.builder()_            .put("index.analysis.analyzer.test.tokenizer", "standard")_            .put("index.number_of_shards", "1"))_            .addMapping("type1", "{\n" +_                "    \"type1\": {\n" +_                "      \"properties\": {\n" +_                "        \"field1\": {\n" +_                "          \"type\": \"text\",\n" +_                "          \"analyzer\": \"test\"\n" +_                "        }\n" +_                "      }\n" +_                "    }\n" +_                "  }}", XContentType.JSON).get()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings()_                .filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())___        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(MapperParsingException.class, ex.getCause().getClass())__        assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]"))__    };this,test,really,tests,worst,case,scenario,where,we,have,a,missing,analyzer,setting,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,missing,analyzer,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,prepare,create,test,set,settings,settings,builder,put,index,analysis,analyzer,test,tokenizer,standard,put,index,1,add,mapping,type1,n,type1,n,properties,n,field1,n,type,text,n,analyzer,test,n,n,n,n,xcontent,type,json,get,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value,one,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,meta,data,get,settings,filter,s,index,analysis,analyzer,test,tokenizer,equals,s,false,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,mapper,parsing,exception,class,ex,get,cause,get,class,assert,that,ex,get,cause,get,message,contains,string,analyzer,test,not,found,for,field,field1
GatewayIndexStateIT -> public void testRecoverMissingAnalyzer() throws Exception;1543947737;This test really tests worst case scenario where we have a missing analyzer setting._In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService._This also includes plugins etc.;public void testRecoverMissingAnalyzer() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        prepareCreate("test").setSettings(Settings.builder()_            .put("index.analysis.analyzer.test.tokenizer", "standard")_            .put("index.number_of_shards", "1"))_            .addMapping("type1", "{\n" +_                "    \"type1\": {\n" +_                "      \"properties\": {\n" +_                "        \"field1\": {\n" +_                "          \"type\": \"text\",\n" +_                "          \"analyzer\": \"test\"\n" +_                "        }\n" +_                "      }\n" +_                "    }\n" +_                "  }}", XContentType.JSON).get()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings()_                .filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())___        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(MapperParsingException.class, ex.getCause().getClass())__        assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]"))__    };this,test,really,tests,worst,case,scenario,where,we,have,a,missing,analyzer,setting,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,missing,analyzer,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,prepare,create,test,set,settings,settings,builder,put,index,analysis,analyzer,test,tokenizer,standard,put,index,1,add,mapping,type1,n,type1,n,properties,n,field1,n,type,text,n,analyzer,test,n,n,n,n,xcontent,type,json,get,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value,one,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,meta,data,get,settings,filter,s,index,analysis,analyzer,test,tokenizer,equals,s,false,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,mapper,parsing,exception,class,ex,get,cause,get,class,assert,that,ex,get,cause,get,message,contains,string,analyzer,test,not,found,for,field,field1
GatewayIndexStateIT -> public void testRecoverMissingAnalyzer() throws Exception;1544081506;This test really tests worst case scenario where we have a missing analyzer setting._In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService._This also includes plugins etc.;public void testRecoverMissingAnalyzer() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        prepareCreate("test").setSettings(Settings.builder()_            .put("index.analysis.analyzer.test.tokenizer", "standard")_            .put("index.number_of_shards", "1"))_            .addMapping("type1", "{\n" +_                "    \"type1\": {\n" +_                "      \"properties\": {\n" +_                "        \"field1\": {\n" +_                "          \"type\": \"text\",\n" +_                "          \"analyzer\": \"test\"\n" +_                "        }\n" +_                "      }\n" +_                "    }\n" +_                "  }}", XContentType.JSON).get()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()___        final IndexMetaData metaData = state.getMetaData().index("test")__        final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings()_                .filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build()__        internalCluster().fullRestart(new RestartCallback(){_            @Override_            public Settings onNodeStopped(String nodeName) throws Exception {_                final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName)__                metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta)__                return super.onNodeStopped(nodeName)__            }_        })___        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())___        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(MapperParsingException.class, ex.getCause().getClass())__        assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]"))__    };this,test,really,tests,worst,case,scenario,where,we,have,a,missing,analyzer,setting,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,missing,analyzer,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,prepare,create,test,set,settings,settings,builder,put,index,analysis,analyzer,test,tokenizer,standard,put,index,1,add,mapping,type1,n,type1,n,properties,n,field1,n,type,text,n,analyzer,test,n,n,n,n,xcontent,type,json,get,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value,one,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,final,index,meta,data,meta,data,state,get,meta,data,index,test,final,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,meta,data,get,settings,filter,s,index,analysis,analyzer,test,tokenizer,equals,s,false,build,internal,cluster,full,restart,new,restart,callback,override,public,settings,on,node,stopped,string,node,name,throws,exception,final,meta,state,service,meta,state,service,internal,cluster,get,instance,meta,state,service,class,node,name,meta,state,service,write,index,and,update,manifest,broken,metadata,broken,meta,return,super,on,node,stopped,node,name,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,mapper,parsing,exception,class,ex,get,cause,get,class,assert,that,ex,get,cause,get,message,contains,string,analyzer,test,not,found,for,field,field1
GatewayIndexStateIT -> public void testRecoverMissingAnalyzer() throws Exception;1544544737;This test really tests worst case scenario where we have a missing analyzer setting._In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService._This also includes plugins etc.;public void testRecoverMissingAnalyzer() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        prepareCreate("test").setSettings(Settings.builder()_            .put("index.analysis.analyzer.test.tokenizer", "standard")_            .put("index.number_of_shards", "1"))_            .addMapping("type1", "{\n" +_                "    \"type1\": {\n" +_                "      \"properties\": {\n" +_                "        \"field1\": {\n" +_                "          \"type\": \"text\",\n" +_                "          \"analyzer\": \"test\"\n" +_                "        }\n" +_                "      }\n" +_                "    }\n" +_                "  }}", XContentType.JSON).get()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()___        final IndexMetaData metaData = state.getMetaData().index("test")__        final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings()_                .filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build()__        internalCluster().fullRestart(new RestartCallback(){_            @Override_            public Settings onNodeStopped(String nodeName) throws Exception {_                final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName)__                metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta)__                return super.onNodeStopped(nodeName)__            }_        })___        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())___        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(MapperParsingException.class, ex.getCause().getClass())__        assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]"))__    };this,test,really,tests,worst,case,scenario,where,we,have,a,missing,analyzer,setting,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,missing,analyzer,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,prepare,create,test,set,settings,settings,builder,put,index,analysis,analyzer,test,tokenizer,standard,put,index,1,add,mapping,type1,n,type1,n,properties,n,field1,n,type,text,n,analyzer,test,n,n,n,n,xcontent,type,json,get,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value,one,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,final,index,meta,data,meta,data,state,get,meta,data,index,test,final,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,meta,data,get,settings,filter,s,index,analysis,analyzer,test,tokenizer,equals,s,false,build,internal,cluster,full,restart,new,restart,callback,override,public,settings,on,node,stopped,string,node,name,throws,exception,final,meta,state,service,meta,state,service,internal,cluster,get,instance,meta,state,service,class,node,name,meta,state,service,write,index,and,update,manifest,broken,metadata,broken,meta,return,super,on,node,stopped,node,name,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,mapper,parsing,exception,class,ex,get,cause,get,class,assert,that,ex,get,cause,get,message,contains,string,analyzer,test,not,found,for,field,field1
GatewayIndexStateIT -> public void testRecoverMissingAnalyzer() throws Exception;1549029235;This test really tests worst case scenario where we have a missing analyzer setting._In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService._This also includes plugins etc.;public void testRecoverMissingAnalyzer() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        prepareCreate("test").setSettings(Settings.builder()_            .put("index.analysis.analyzer.test.tokenizer", "standard")_            .put("index.number_of_shards", "1"))_            .addMapping("type1", "{\n" +_                "    \"type1\": {\n" +_                "      \"properties\": {\n" +_                "        \"field1\": {\n" +_                "          \"type\": \"text\",\n" +_                "          \"analyzer\": \"test\"\n" +_                "        }\n" +_                "      }\n" +_                "    }\n" +_                "  }}", XContentType.JSON).get()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()___        final IndexMetaData metaData = state.getMetaData().index("test")__        final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings()_                .filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build()__        internalCluster().fullRestart(new RestartCallback(){_            @Override_            public Settings onNodeStopped(String nodeName) throws Exception {_                final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName)__                metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta)__                return super.onNodeStopped(nodeName)__            }_        })___        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())___        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(MapperParsingException.class, ex.getCause().getClass())__        assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]"))__    };this,test,really,tests,worst,case,scenario,where,we,have,a,missing,analyzer,setting,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,missing,analyzer,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,prepare,create,test,set,settings,settings,builder,put,index,analysis,analyzer,test,tokenizer,standard,put,index,1,add,mapping,type1,n,type1,n,properties,n,field1,n,type,text,n,analyzer,test,n,n,n,n,xcontent,type,json,get,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value,one,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,final,index,meta,data,meta,data,state,get,meta,data,index,test,final,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,meta,data,get,settings,filter,s,index,analysis,analyzer,test,tokenizer,equals,s,false,build,internal,cluster,full,restart,new,restart,callback,override,public,settings,on,node,stopped,string,node,name,throws,exception,final,meta,state,service,meta,state,service,internal,cluster,get,instance,meta,state,service,class,node,name,meta,state,service,write,index,and,update,manifest,broken,metadata,broken,meta,return,super,on,node,stopped,node,name,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,mapper,parsing,exception,class,ex,get,cause,get,class,assert,that,ex,get,cause,get,message,contains,string,analyzer,test,not,found,for,field,field1
GatewayIndexStateIT -> public void testRecoverMissingAnalyzer() throws Exception;1549031403;This test really tests worst case scenario where we have a missing analyzer setting._In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService._This also includes plugins etc.;public void testRecoverMissingAnalyzer() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        prepareCreate("test").setSettings(Settings.builder()_            .put("index.analysis.analyzer.test.tokenizer", "standard")_            .put("index.number_of_shards", "1"))_            .addMapping("type1", "{\n" +_                "    \"type1\": {\n" +_                "      \"properties\": {\n" +_                "        \"field1\": {\n" +_                "          \"type\": \"text\",\n" +_                "          \"analyzer\": \"test\"\n" +_                "        }\n" +_                "      }\n" +_                "    }\n" +_                "  }}", XContentType.JSON).get()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()___        final IndexMetaData metaData = state.getMetaData().index("test")__        final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings()_                .filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build()__        internalCluster().fullRestart(new RestartCallback(){_            @Override_            public Settings onNodeStopped(String nodeName) throws Exception {_                final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName)__                metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta)__                return super.onNodeStopped(nodeName)__            }_        })___        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())___        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(MapperParsingException.class, ex.getCause().getClass())__        assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]"))__    };this,test,really,tests,worst,case,scenario,where,we,have,a,missing,analyzer,setting,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,missing,analyzer,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,prepare,create,test,set,settings,settings,builder,put,index,analysis,analyzer,test,tokenizer,standard,put,index,1,add,mapping,type1,n,type1,n,properties,n,field1,n,type,text,n,analyzer,test,n,n,n,n,xcontent,type,json,get,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value,one,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,final,index,meta,data,meta,data,state,get,meta,data,index,test,final,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,meta,data,get,settings,filter,s,index,analysis,analyzer,test,tokenizer,equals,s,false,build,internal,cluster,full,restart,new,restart,callback,override,public,settings,on,node,stopped,string,node,name,throws,exception,final,meta,state,service,meta,state,service,internal,cluster,get,instance,meta,state,service,class,node,name,meta,state,service,write,index,and,update,manifest,broken,metadata,broken,meta,return,super,on,node,stopped,node,name,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,mapper,parsing,exception,class,ex,get,cause,get,class,assert,that,ex,get,cause,get,message,contains,string,analyzer,test,not,found,for,field,field1
GatewayIndexStateIT -> public void testRecoverBrokenIndexMetadata() throws Exception;1524684173;This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being_allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService. This also includes plugins etc.;public void testRecoverBrokenIndexMetadata() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings())_                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id)_                 _                .put("index.similarity.BM25.type", "classic")_                 _                .put("index.analysis.filter.myCollator.type", "icu_collation")_            ).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())__        assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type"))__        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(IllegalArgumentException.class, ex.getCause().getClass())__        assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]")__    };this,test,really,tests,worst,case,scenario,where,we,have,a,broken,setting,or,any,setting,that,prevents,an,index,from,being,allocated,in,our,metadata,that,we,recover,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,broken,index,metadata,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value1,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,settings,builder,put,meta,data,get,settings,put,index,meta,data,version,current,minimum,index,compatibility,version,id,put,index,similarity,bm25,type,classic,put,index,analysis,filter,my,collator,type,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,assert,equals,classic,state,get,meta,data,index,meta,data,get,index,get,settings,get,archived,index,similarity,bm25,type,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,illegal,argument,exception,class,ex,get,cause,get,class,assert,equals,ex,get,cause,get,message,unknown,filter,type,for,my,collator
GatewayIndexStateIT -> public void testRecoverBrokenIndexMetadata() throws Exception;1527622193;This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being_allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService. This also includes plugins etc.;public void testRecoverBrokenIndexMetadata() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings())_                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id)_                 _                .put("index.similarity.BM25.type", "classic")_                 _                .put("index.analysis.filter.myCollator.type", "icu_collation")_            ).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())__        assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type"))__        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(IllegalArgumentException.class, ex.getCause().getClass())__        assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]")__    };this,test,really,tests,worst,case,scenario,where,we,have,a,broken,setting,or,any,setting,that,prevents,an,index,from,being,allocated,in,our,metadata,that,we,recover,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,broken,index,metadata,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value1,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,settings,builder,put,meta,data,get,settings,put,index,meta,data,version,current,minimum,index,compatibility,version,id,put,index,similarity,bm25,type,classic,put,index,analysis,filter,my,collator,type,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,assert,equals,classic,state,get,meta,data,index,meta,data,get,index,get,settings,get,archived,index,similarity,bm25,type,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,illegal,argument,exception,class,ex,get,cause,get,class,assert,equals,ex,get,cause,get,message,unknown,filter,type,for,my,collator
GatewayIndexStateIT -> public void testRecoverBrokenIndexMetadata() throws Exception;1528762805;This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being_allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService. This also includes plugins etc.;public void testRecoverBrokenIndexMetadata() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings())_                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id)_                 _                .put("index.similarity.BM25.type", "classic")_                 _                .put("index.analysis.filter.myCollator.type", "icu_collation")_            ).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())__        assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type"))__        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(IllegalArgumentException.class, ex.getCause().getClass())__        assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]")__    };this,test,really,tests,worst,case,scenario,where,we,have,a,broken,setting,or,any,setting,that,prevents,an,index,from,being,allocated,in,our,metadata,that,we,recover,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,broken,index,metadata,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value1,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,settings,builder,put,meta,data,get,settings,put,index,meta,data,version,current,minimum,index,compatibility,version,id,put,index,similarity,bm25,type,classic,put,index,analysis,filter,my,collator,type,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,assert,equals,classic,state,get,meta,data,index,meta,data,get,index,get,settings,get,archived,index,similarity,bm25,type,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,illegal,argument,exception,class,ex,get,cause,get,class,assert,equals,ex,get,cause,get,message,unknown,filter,type,for,my,collator
GatewayIndexStateIT -> public void testRecoverBrokenIndexMetadata() throws Exception;1540486836;This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being_allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService. This also includes plugins etc.;public void testRecoverBrokenIndexMetadata() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings())_                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id)_                 _                .put("index.similarity.BM25.type", "classic")_                 _                .put("index.analysis.filter.myCollator.type", "icu_collation")_            ).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())__        assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type"))__        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(IllegalArgumentException.class, ex.getCause().getClass())__        assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]")__    };this,test,really,tests,worst,case,scenario,where,we,have,a,broken,setting,or,any,setting,that,prevents,an,index,from,being,allocated,in,our,metadata,that,we,recover,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,broken,index,metadata,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value1,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,settings,builder,put,meta,data,get,settings,put,index,meta,data,version,current,minimum,index,compatibility,version,id,put,index,similarity,bm25,type,classic,put,index,analysis,filter,my,collator,type,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,assert,equals,classic,state,get,meta,data,index,meta,data,get,index,get,settings,get,archived,index,similarity,bm25,type,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,illegal,argument,exception,class,ex,get,cause,get,class,assert,equals,ex,get,cause,get,message,unknown,filter,type,for,my,collator
GatewayIndexStateIT -> public void testRecoverBrokenIndexMetadata() throws Exception;1543947737;This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being_allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService. This also includes plugins etc.;public void testRecoverBrokenIndexMetadata() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()__        IndexMetaData metaData = state.getMetaData().index("test")__        for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {_            IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings())_                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id)_                 _                .put("index.similarity.BM25.type", "classic")_                 _                .put("index.analysis.filter.myCollator.type", "icu_collation")_            ).build()__            IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()))__        }_        internalCluster().fullRestart()__        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())__        assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type"))__        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(IllegalArgumentException.class, ex.getCause().getClass())__        assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]")__    };this,test,really,tests,worst,case,scenario,where,we,have,a,broken,setting,or,any,setting,that,prevents,an,index,from,being,allocated,in,our,metadata,that,we,recover,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,broken,index,metadata,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value1,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,index,meta,data,meta,data,state,get,meta,data,index,test,for,node,environment,services,internal,cluster,get,instances,node,environment,class,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,settings,builder,put,meta,data,get,settings,put,index,meta,data,version,current,minimum,index,compatibility,version,id,put,index,similarity,bm25,type,classic,put,index,analysis,filter,my,collator,type,build,index,meta,data,format,write,broken,meta,services,index,paths,broken,meta,get,index,internal,cluster,full,restart,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,assert,equals,classic,state,get,meta,data,index,meta,data,get,index,get,settings,get,archived,index,similarity,bm25,type,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,illegal,argument,exception,class,ex,get,cause,get,class,assert,equals,ex,get,cause,get,message,unknown,filter,type,for,my,collator
GatewayIndexStateIT -> public void testRecoverBrokenIndexMetadata() throws Exception;1544081506;This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being_allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService. This also includes plugins etc.;public void testRecoverBrokenIndexMetadata() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()___        final IndexMetaData metaData = state.getMetaData().index("test")__        final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings())_                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id)_                _                .put("index.similarity.BM25.type", "classic")_                _                .put("index.analysis.filter.myCollator.type", "icu_collation")_        ).build()__        internalCluster().fullRestart(new RestartCallback(){_            @Override_            public Settings onNodeStopped(String nodeName) throws Exception {_                final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName)__                metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta)__                return super.onNodeStopped(nodeName)__            }_        })___        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())__        assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type"))__        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(IllegalArgumentException.class, ex.getCause().getClass())__        assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]")__    };this,test,really,tests,worst,case,scenario,where,we,have,a,broken,setting,or,any,setting,that,prevents,an,index,from,being,allocated,in,our,metadata,that,we,recover,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,broken,index,metadata,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value1,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,final,index,meta,data,meta,data,state,get,meta,data,index,test,final,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,settings,builder,put,meta,data,get,settings,put,index,meta,data,version,current,minimum,index,compatibility,version,id,put,index,similarity,bm25,type,classic,put,index,analysis,filter,my,collator,type,build,internal,cluster,full,restart,new,restart,callback,override,public,settings,on,node,stopped,string,node,name,throws,exception,final,meta,state,service,meta,state,service,internal,cluster,get,instance,meta,state,service,class,node,name,meta,state,service,write,index,and,update,manifest,broken,metadata,broken,meta,return,super,on,node,stopped,node,name,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,assert,equals,classic,state,get,meta,data,index,meta,data,get,index,get,settings,get,archived,index,similarity,bm25,type,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,illegal,argument,exception,class,ex,get,cause,get,class,assert,equals,ex,get,cause,get,message,unknown,filter,type,for,my,collator
GatewayIndexStateIT -> public void testRecoverBrokenIndexMetadata() throws Exception;1544544737;This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being_allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService. This also includes plugins etc.;public void testRecoverBrokenIndexMetadata() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()___        final IndexMetaData metaData = state.getMetaData().index("test")__        final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings())_                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id)_                _                .put("index.similarity.BM25.type", "classic")_                _                .put("index.analysis.filter.myCollator.type", "icu_collation")_        ).build()__        internalCluster().fullRestart(new RestartCallback(){_            @Override_            public Settings onNodeStopped(String nodeName) throws Exception {_                final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName)__                metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta)__                return super.onNodeStopped(nodeName)__            }_        })___        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())__        assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type"))__        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(IllegalArgumentException.class, ex.getCause().getClass())__        assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]")__    };this,test,really,tests,worst,case,scenario,where,we,have,a,broken,setting,or,any,setting,that,prevents,an,index,from,being,allocated,in,our,metadata,that,we,recover,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,broken,index,metadata,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value1,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,final,index,meta,data,meta,data,state,get,meta,data,index,test,final,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,settings,builder,put,meta,data,get,settings,put,index,meta,data,version,current,minimum,index,compatibility,version,id,put,index,similarity,bm25,type,classic,put,index,analysis,filter,my,collator,type,build,internal,cluster,full,restart,new,restart,callback,override,public,settings,on,node,stopped,string,node,name,throws,exception,final,meta,state,service,meta,state,service,internal,cluster,get,instance,meta,state,service,class,node,name,meta,state,service,write,index,and,update,manifest,broken,metadata,broken,meta,return,super,on,node,stopped,node,name,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,assert,equals,classic,state,get,meta,data,index,meta,data,get,index,get,settings,get,archived,index,similarity,bm25,type,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,illegal,argument,exception,class,ex,get,cause,get,class,assert,equals,ex,get,cause,get,message,unknown,filter,type,for,my,collator
GatewayIndexStateIT -> public void testRecoverBrokenIndexMetadata() throws Exception;1549029235;This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being_allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService. This also includes plugins etc.;public void testRecoverBrokenIndexMetadata() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()___        final IndexMetaData metaData = state.getMetaData().index("test")__        final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings())_                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id)_                _                .put("index.similarity.BM25.type", "classic")_                _                .put("index.analysis.filter.myCollator.type", "icu_collation")_        ).build()__        internalCluster().fullRestart(new RestartCallback(){_            @Override_            public Settings onNodeStopped(String nodeName) throws Exception {_                final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName)__                metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta)__                return super.onNodeStopped(nodeName)__            }_        })___        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())__        assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type"))__        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(IllegalArgumentException.class, ex.getCause().getClass())__        assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]")__    };this,test,really,tests,worst,case,scenario,where,we,have,a,broken,setting,or,any,setting,that,prevents,an,index,from,being,allocated,in,our,metadata,that,we,recover,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,broken,index,metadata,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value1,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,final,index,meta,data,meta,data,state,get,meta,data,index,test,final,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,settings,builder,put,meta,data,get,settings,put,index,meta,data,version,current,minimum,index,compatibility,version,id,put,index,similarity,bm25,type,classic,put,index,analysis,filter,my,collator,type,build,internal,cluster,full,restart,new,restart,callback,override,public,settings,on,node,stopped,string,node,name,throws,exception,final,meta,state,service,meta,state,service,internal,cluster,get,instance,meta,state,service,class,node,name,meta,state,service,write,index,and,update,manifest,broken,metadata,broken,meta,return,super,on,node,stopped,node,name,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,assert,equals,classic,state,get,meta,data,index,meta,data,get,index,get,settings,get,archived,index,similarity,bm25,type,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,illegal,argument,exception,class,ex,get,cause,get,class,assert,equals,ex,get,cause,get,message,unknown,filter,type,for,my,collator
GatewayIndexStateIT -> public void testRecoverBrokenIndexMetadata() throws Exception;1549031403;This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being_allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk_if it is sane and if we can successfully create an IndexService. This also includes plugins etc.;public void testRecoverBrokenIndexMetadata() throws Exception {_        logger.info("--> starting one node")__        internalCluster().startNode()__        logger.info("--> indexing a simple document")__        client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get()__        logger.info("--> waiting for green status")__        if (usually()) {_            ensureYellow()__        } else {_            internalCluster().startNode()__            client().admin().cluster()_                .health(Requests.clusterHealthRequest()_                    .waitForGreenStatus()_                    .waitForEvents(Priority.LANGUID)_                    .waitForNoRelocatingShards(true).waitForNodes("2")).actionGet()__        }_        ClusterState state = client().admin().cluster().prepareState().get().getState()___        final IndexMetaData metaData = state.getMetaData().index("test")__        final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings())_                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id)_                _                .put("index.similarity.BM25.type", "classic")_                _                .put("index.analysis.filter.myCollator.type", "icu_collation")_        ).build()__        internalCluster().fullRestart(new RestartCallback(){_            @Override_            public Settings onNodeStopped(String nodeName) throws Exception {_                final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName)__                metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta)__                return super.onNodeStopped(nodeName)__            }_        })___        _        _        ensureGreen(metaData.getIndex().getName())__        state = client().admin().cluster().prepareState().get().getState()__        assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState())__        assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type"))__        _        ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get())__        assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex())__        assertNotNull(ex.getCause())__        assertEquals(IllegalArgumentException.class, ex.getCause().getClass())__        assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]")__    };this,test,really,tests,worst,case,scenario,where,we,have,a,broken,setting,or,any,setting,that,prevents,an,index,from,being,allocated,in,our,metadata,that,we,recover,in,that,case,we,now,have,the,ability,to,check,the,index,on,local,recovery,from,disk,if,it,is,sane,and,if,we,can,successfully,create,an,index,service,this,also,includes,plugins,etc;public,void,test,recover,broken,index,metadata,throws,exception,logger,info,starting,one,node,internal,cluster,start,node,logger,info,indexing,a,simple,document,client,prepare,index,test,type1,1,set,source,field1,value1,set,refresh,policy,immediate,get,logger,info,waiting,for,green,status,if,usually,ensure,yellow,else,internal,cluster,start,node,client,admin,cluster,health,requests,cluster,health,request,wait,for,green,status,wait,for,events,priority,languid,wait,for,no,relocating,shards,true,wait,for,nodes,2,action,get,cluster,state,state,client,admin,cluster,prepare,state,get,get,state,final,index,meta,data,meta,data,state,get,meta,data,index,test,final,index,meta,data,broken,meta,index,meta,data,builder,meta,data,settings,settings,builder,put,meta,data,get,settings,put,index,meta,data,version,current,minimum,index,compatibility,version,id,put,index,similarity,bm25,type,classic,put,index,analysis,filter,my,collator,type,build,internal,cluster,full,restart,new,restart,callback,override,public,settings,on,node,stopped,string,node,name,throws,exception,final,meta,state,service,meta,state,service,internal,cluster,get,instance,meta,state,service,class,node,name,meta,state,service,write,index,and,update,manifest,broken,metadata,broken,meta,return,super,on,node,stopped,node,name,ensure,green,meta,data,get,index,get,name,state,client,admin,cluster,prepare,state,get,get,state,assert,equals,index,meta,data,state,close,state,get,meta,data,index,meta,data,get,index,get,state,assert,equals,classic,state,get,meta,data,index,meta,data,get,index,get,settings,get,archived,index,similarity,bm25,type,elasticsearch,exception,ex,expect,throws,elasticsearch,exception,class,client,admin,indices,prepare,open,test,get,assert,equals,ex,get,message,failed,to,verify,index,meta,data,get,index,assert,not,null,ex,get,cause,assert,equals,illegal,argument,exception,class,ex,get,cause,get,class,assert,equals,ex,get,cause,get,message,unknown,filter,type,for,my,collator
GatewayIndexStateIT -> public void testIndexDeletionWhenNodeRejoins() throws Exception;1524684173;This test ensures that when an index deletion takes place while a node is offline, when that_node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.;public void testIndexDeletionWhenNodeRejoins() throws Exception {_        final String indexName = "test-index-del-on-node-rejoin-idx"__        final int numNodes = 2___        final List<String> nodes__        logger.info("--> starting a cluster with " + numNodes + " nodes")__        nodes = internalCluster().startNodes(numNodes,_            Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build())__        logger.info("--> create an index")__        createIndex(indexName)___        logger.info("--> waiting for green status")__        ensureGreen()__        final String indexUUID = resolveIndex(indexName).getUUID()___        logger.info("--> restart a random date node, deleting the index in between stopping and restarting")__        internalCluster().restartRandomDataNode(new RestartCallback() {_            @Override_            public Settings onNodeStopped(final String nodeName) throws Exception {_                nodes.remove(nodeName)__                logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes)__                assert nodes.size() > 0__                final String otherNode = nodes.get(0)__                logger.info("--> delete index and verify it is deleted")__                final Client client = client(otherNode)__                client.admin().indices().prepareDelete(indexName).execute().actionGet()__                assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists())__                return super.onNodeStopped(nodeName)__            }_        })___        logger.info("--> wait until all nodes are back online")__        client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID)_                                              .waitForNodes(Integer.toString(numNodes))).actionGet()___        logger.info("--> waiting for green status")__        ensureGreen()___        logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node")__        assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists())__        assertBusy(() -> {_            final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class)__            try {_                assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID))__            } catch (IOException e) {_                logger.error("Unable to retrieve available index folders from the node", e)__                fail("Unable to retrieve available index folders from the node")__            }_        })__    };this,test,ensures,that,when,an,index,deletion,takes,place,while,a,node,is,offline,when,that,node,rejoins,the,cluster,it,deletes,the,index,locally,instead,of,importing,it,as,a,dangling,index;public,void,test,index,deletion,when,node,rejoins,throws,exception,final,string,index,name,test,index,del,on,node,rejoin,idx,final,int,num,nodes,2,final,list,string,nodes,logger,info,starting,a,cluster,with,num,nodes,nodes,nodes,internal,cluster,start,nodes,num,nodes,settings,builder,put,index,graveyard,get,key,random,int,between,10,100,build,logger,info,create,an,index,create,index,index,name,logger,info,waiting,for,green,status,ensure,green,final,string,index,uuid,resolve,index,index,name,get,uuid,logger,info,restart,a,random,date,node,deleting,the,index,in,between,stopping,and,restarting,internal,cluster,restart,random,data,node,new,restart,callback,override,public,settings,on,node,stopped,final,string,node,name,throws,exception,nodes,remove,node,name,logger,info,stopped,node,remaining,nodes,node,name,nodes,assert,nodes,size,0,final,string,other,node,nodes,get,0,logger,info,delete,index,and,verify,it,is,deleted,final,client,client,client,other,node,client,admin,indices,prepare,delete,index,name,execute,action,get,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,return,super,on,node,stopped,node,name,logger,info,wait,until,all,nodes,are,back,online,client,admin,cluster,health,requests,cluster,health,request,wait,for,events,priority,languid,wait,for,nodes,integer,to,string,num,nodes,action,get,logger,info,waiting,for,green,status,ensure,green,logger,info,verify,that,the,deleted,index,is,removed,from,the,cluster,and,not,reimported,as,dangling,by,the,restarted,node,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,assert,busy,final,node,environment,node,env,internal,cluster,get,instance,node,environment,class,try,assert,false,index,folder,index,uuid,should,be,deleted,node,env,available,index,folders,contains,index,uuid,catch,ioexception,e,logger,error,unable,to,retrieve,available,index,folders,from,the,node,e,fail,unable,to,retrieve,available,index,folders,from,the,node
GatewayIndexStateIT -> public void testIndexDeletionWhenNodeRejoins() throws Exception;1527622193;This test ensures that when an index deletion takes place while a node is offline, when that_node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.;public void testIndexDeletionWhenNodeRejoins() throws Exception {_        final String indexName = "test-index-del-on-node-rejoin-idx"__        final int numNodes = 2___        final List<String> nodes__        logger.info("--> starting a cluster with " + numNodes + " nodes")__        nodes = internalCluster().startNodes(numNodes,_            Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build())__        logger.info("--> create an index")__        createIndex(indexName)___        logger.info("--> waiting for green status")__        ensureGreen()__        final String indexUUID = resolveIndex(indexName).getUUID()___        logger.info("--> restart a random date node, deleting the index in between stopping and restarting")__        internalCluster().restartRandomDataNode(new RestartCallback() {_            @Override_            public Settings onNodeStopped(final String nodeName) throws Exception {_                nodes.remove(nodeName)__                logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes)__                assert nodes.size() > 0__                final String otherNode = nodes.get(0)__                logger.info("--> delete index and verify it is deleted")__                final Client client = client(otherNode)__                client.admin().indices().prepareDelete(indexName).execute().actionGet()__                assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists())__                return super.onNodeStopped(nodeName)__            }_        })___        logger.info("--> wait until all nodes are back online")__        client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID)_                                              .waitForNodes(Integer.toString(numNodes))).actionGet()___        logger.info("--> waiting for green status")__        ensureGreen()___        logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node")__        assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists())__        assertBusy(() -> {_            final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class)__            try {_                assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID))__            } catch (IOException e) {_                logger.error("Unable to retrieve available index folders from the node", e)__                fail("Unable to retrieve available index folders from the node")__            }_        })__    };this,test,ensures,that,when,an,index,deletion,takes,place,while,a,node,is,offline,when,that,node,rejoins,the,cluster,it,deletes,the,index,locally,instead,of,importing,it,as,a,dangling,index;public,void,test,index,deletion,when,node,rejoins,throws,exception,final,string,index,name,test,index,del,on,node,rejoin,idx,final,int,num,nodes,2,final,list,string,nodes,logger,info,starting,a,cluster,with,num,nodes,nodes,nodes,internal,cluster,start,nodes,num,nodes,settings,builder,put,index,graveyard,get,key,random,int,between,10,100,build,logger,info,create,an,index,create,index,index,name,logger,info,waiting,for,green,status,ensure,green,final,string,index,uuid,resolve,index,index,name,get,uuid,logger,info,restart,a,random,date,node,deleting,the,index,in,between,stopping,and,restarting,internal,cluster,restart,random,data,node,new,restart,callback,override,public,settings,on,node,stopped,final,string,node,name,throws,exception,nodes,remove,node,name,logger,info,stopped,node,remaining,nodes,node,name,nodes,assert,nodes,size,0,final,string,other,node,nodes,get,0,logger,info,delete,index,and,verify,it,is,deleted,final,client,client,client,other,node,client,admin,indices,prepare,delete,index,name,execute,action,get,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,return,super,on,node,stopped,node,name,logger,info,wait,until,all,nodes,are,back,online,client,admin,cluster,health,requests,cluster,health,request,wait,for,events,priority,languid,wait,for,nodes,integer,to,string,num,nodes,action,get,logger,info,waiting,for,green,status,ensure,green,logger,info,verify,that,the,deleted,index,is,removed,from,the,cluster,and,not,reimported,as,dangling,by,the,restarted,node,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,assert,busy,final,node,environment,node,env,internal,cluster,get,instance,node,environment,class,try,assert,false,index,folder,index,uuid,should,be,deleted,node,env,available,index,folders,contains,index,uuid,catch,ioexception,e,logger,error,unable,to,retrieve,available,index,folders,from,the,node,e,fail,unable,to,retrieve,available,index,folders,from,the,node
GatewayIndexStateIT -> public void testIndexDeletionWhenNodeRejoins() throws Exception;1528762805;This test ensures that when an index deletion takes place while a node is offline, when that_node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.;public void testIndexDeletionWhenNodeRejoins() throws Exception {_        final String indexName = "test-index-del-on-node-rejoin-idx"__        final int numNodes = 2___        final List<String> nodes__        logger.info("--> starting a cluster with " + numNodes + " nodes")__        nodes = internalCluster().startNodes(numNodes,_            Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build())__        logger.info("--> create an index")__        createIndex(indexName)___        logger.info("--> waiting for green status")__        ensureGreen()__        final String indexUUID = resolveIndex(indexName).getUUID()___        logger.info("--> restart a random date node, deleting the index in between stopping and restarting")__        internalCluster().restartRandomDataNode(new RestartCallback() {_            @Override_            public Settings onNodeStopped(final String nodeName) throws Exception {_                nodes.remove(nodeName)__                logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes)__                assert nodes.size() > 0__                final String otherNode = nodes.get(0)__                logger.info("--> delete index and verify it is deleted")__                final Client client = client(otherNode)__                client.admin().indices().prepareDelete(indexName).execute().actionGet()__                assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists())__                return super.onNodeStopped(nodeName)__            }_        })___        logger.info("--> wait until all nodes are back online")__        client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID)_                                              .waitForNodes(Integer.toString(numNodes))).actionGet()___        logger.info("--> waiting for green status")__        ensureGreen()___        logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node")__        assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists())__        assertBusy(() -> {_            final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class)__            try {_                assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID))__            } catch (IOException e) {_                logger.error("Unable to retrieve available index folders from the node", e)__                fail("Unable to retrieve available index folders from the node")__            }_        })__    };this,test,ensures,that,when,an,index,deletion,takes,place,while,a,node,is,offline,when,that,node,rejoins,the,cluster,it,deletes,the,index,locally,instead,of,importing,it,as,a,dangling,index;public,void,test,index,deletion,when,node,rejoins,throws,exception,final,string,index,name,test,index,del,on,node,rejoin,idx,final,int,num,nodes,2,final,list,string,nodes,logger,info,starting,a,cluster,with,num,nodes,nodes,nodes,internal,cluster,start,nodes,num,nodes,settings,builder,put,index,graveyard,get,key,random,int,between,10,100,build,logger,info,create,an,index,create,index,index,name,logger,info,waiting,for,green,status,ensure,green,final,string,index,uuid,resolve,index,index,name,get,uuid,logger,info,restart,a,random,date,node,deleting,the,index,in,between,stopping,and,restarting,internal,cluster,restart,random,data,node,new,restart,callback,override,public,settings,on,node,stopped,final,string,node,name,throws,exception,nodes,remove,node,name,logger,info,stopped,node,remaining,nodes,node,name,nodes,assert,nodes,size,0,final,string,other,node,nodes,get,0,logger,info,delete,index,and,verify,it,is,deleted,final,client,client,client,other,node,client,admin,indices,prepare,delete,index,name,execute,action,get,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,return,super,on,node,stopped,node,name,logger,info,wait,until,all,nodes,are,back,online,client,admin,cluster,health,requests,cluster,health,request,wait,for,events,priority,languid,wait,for,nodes,integer,to,string,num,nodes,action,get,logger,info,waiting,for,green,status,ensure,green,logger,info,verify,that,the,deleted,index,is,removed,from,the,cluster,and,not,reimported,as,dangling,by,the,restarted,node,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,assert,busy,final,node,environment,node,env,internal,cluster,get,instance,node,environment,class,try,assert,false,index,folder,index,uuid,should,be,deleted,node,env,available,index,folders,contains,index,uuid,catch,ioexception,e,logger,error,unable,to,retrieve,available,index,folders,from,the,node,e,fail,unable,to,retrieve,available,index,folders,from,the,node
GatewayIndexStateIT -> public void testIndexDeletionWhenNodeRejoins() throws Exception;1540486836;This test ensures that when an index deletion takes place while a node is offline, when that_node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.;public void testIndexDeletionWhenNodeRejoins() throws Exception {_        final String indexName = "test-index-del-on-node-rejoin-idx"__        final int numNodes = 2___        final List<String> nodes__        logger.info("--> starting a cluster with " + numNodes + " nodes")__        nodes = internalCluster().startNodes(numNodes,_            Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build())__        logger.info("--> create an index")__        createIndex(indexName)___        logger.info("--> waiting for green status")__        ensureGreen()__        final String indexUUID = resolveIndex(indexName).getUUID()___        logger.info("--> restart a random date node, deleting the index in between stopping and restarting")__        internalCluster().restartRandomDataNode(new RestartCallback() {_            @Override_            public Settings onNodeStopped(final String nodeName) throws Exception {_                nodes.remove(nodeName)__                logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes)__                assert nodes.size() > 0__                final String otherNode = nodes.get(0)__                logger.info("--> delete index and verify it is deleted")__                final Client client = client(otherNode)__                client.admin().indices().prepareDelete(indexName).execute().actionGet()__                assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists())__                return super.onNodeStopped(nodeName)__            }_        })___        logger.info("--> wait until all nodes are back online")__        client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID)_                                              .waitForNodes(Integer.toString(numNodes))).actionGet()___        logger.info("--> waiting for green status")__        ensureGreen()___        logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node")__        assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists())__        assertBusy(() -> {_            final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class)__            try {_                assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID))__            } catch (IOException e) {_                logger.error("Unable to retrieve available index folders from the node", e)__                fail("Unable to retrieve available index folders from the node")__            }_        })__    };this,test,ensures,that,when,an,index,deletion,takes,place,while,a,node,is,offline,when,that,node,rejoins,the,cluster,it,deletes,the,index,locally,instead,of,importing,it,as,a,dangling,index;public,void,test,index,deletion,when,node,rejoins,throws,exception,final,string,index,name,test,index,del,on,node,rejoin,idx,final,int,num,nodes,2,final,list,string,nodes,logger,info,starting,a,cluster,with,num,nodes,nodes,nodes,internal,cluster,start,nodes,num,nodes,settings,builder,put,index,graveyard,get,key,random,int,between,10,100,build,logger,info,create,an,index,create,index,index,name,logger,info,waiting,for,green,status,ensure,green,final,string,index,uuid,resolve,index,index,name,get,uuid,logger,info,restart,a,random,date,node,deleting,the,index,in,between,stopping,and,restarting,internal,cluster,restart,random,data,node,new,restart,callback,override,public,settings,on,node,stopped,final,string,node,name,throws,exception,nodes,remove,node,name,logger,info,stopped,node,remaining,nodes,node,name,nodes,assert,nodes,size,0,final,string,other,node,nodes,get,0,logger,info,delete,index,and,verify,it,is,deleted,final,client,client,client,other,node,client,admin,indices,prepare,delete,index,name,execute,action,get,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,return,super,on,node,stopped,node,name,logger,info,wait,until,all,nodes,are,back,online,client,admin,cluster,health,requests,cluster,health,request,wait,for,events,priority,languid,wait,for,nodes,integer,to,string,num,nodes,action,get,logger,info,waiting,for,green,status,ensure,green,logger,info,verify,that,the,deleted,index,is,removed,from,the,cluster,and,not,reimported,as,dangling,by,the,restarted,node,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,assert,busy,final,node,environment,node,env,internal,cluster,get,instance,node,environment,class,try,assert,false,index,folder,index,uuid,should,be,deleted,node,env,available,index,folders,contains,index,uuid,catch,ioexception,e,logger,error,unable,to,retrieve,available,index,folders,from,the,node,e,fail,unable,to,retrieve,available,index,folders,from,the,node
GatewayIndexStateIT -> public void testIndexDeletionWhenNodeRejoins() throws Exception;1543947737;This test ensures that when an index deletion takes place while a node is offline, when that_node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.;public void testIndexDeletionWhenNodeRejoins() throws Exception {_        final String indexName = "test-index-del-on-node-rejoin-idx"__        final int numNodes = 2___        final List<String> nodes__        logger.info("--> starting a cluster with " + numNodes + " nodes")__        nodes = internalCluster().startNodes(numNodes,_            Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build())__        logger.info("--> create an index")__        createIndex(indexName)___        logger.info("--> waiting for green status")__        ensureGreen()__        final String indexUUID = resolveIndex(indexName).getUUID()___        logger.info("--> restart a random date node, deleting the index in between stopping and restarting")__        internalCluster().restartRandomDataNode(new RestartCallback() {_            @Override_            public Settings onNodeStopped(final String nodeName) throws Exception {_                nodes.remove(nodeName)__                logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes)__                assert nodes.size() > 0__                final String otherNode = nodes.get(0)__                logger.info("--> delete index and verify it is deleted")__                final Client client = client(otherNode)__                client.admin().indices().prepareDelete(indexName).execute().actionGet()__                assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists())__                return super.onNodeStopped(nodeName)__            }_        })___        logger.info("--> wait until all nodes are back online")__        client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID)_                                              .waitForNodes(Integer.toString(numNodes))).actionGet()___        logger.info("--> waiting for green status")__        ensureGreen()___        logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node")__        assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists())__        assertBusy(() -> {_            final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class)__            try {_                assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID))__            } catch (IOException e) {_                logger.error("Unable to retrieve available index folders from the node", e)__                fail("Unable to retrieve available index folders from the node")__            }_        })__    };this,test,ensures,that,when,an,index,deletion,takes,place,while,a,node,is,offline,when,that,node,rejoins,the,cluster,it,deletes,the,index,locally,instead,of,importing,it,as,a,dangling,index;public,void,test,index,deletion,when,node,rejoins,throws,exception,final,string,index,name,test,index,del,on,node,rejoin,idx,final,int,num,nodes,2,final,list,string,nodes,logger,info,starting,a,cluster,with,num,nodes,nodes,nodes,internal,cluster,start,nodes,num,nodes,settings,builder,put,index,graveyard,get,key,random,int,between,10,100,build,logger,info,create,an,index,create,index,index,name,logger,info,waiting,for,green,status,ensure,green,final,string,index,uuid,resolve,index,index,name,get,uuid,logger,info,restart,a,random,date,node,deleting,the,index,in,between,stopping,and,restarting,internal,cluster,restart,random,data,node,new,restart,callback,override,public,settings,on,node,stopped,final,string,node,name,throws,exception,nodes,remove,node,name,logger,info,stopped,node,remaining,nodes,node,name,nodes,assert,nodes,size,0,final,string,other,node,nodes,get,0,logger,info,delete,index,and,verify,it,is,deleted,final,client,client,client,other,node,client,admin,indices,prepare,delete,index,name,execute,action,get,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,return,super,on,node,stopped,node,name,logger,info,wait,until,all,nodes,are,back,online,client,admin,cluster,health,requests,cluster,health,request,wait,for,events,priority,languid,wait,for,nodes,integer,to,string,num,nodes,action,get,logger,info,waiting,for,green,status,ensure,green,logger,info,verify,that,the,deleted,index,is,removed,from,the,cluster,and,not,reimported,as,dangling,by,the,restarted,node,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,assert,busy,final,node,environment,node,env,internal,cluster,get,instance,node,environment,class,try,assert,false,index,folder,index,uuid,should,be,deleted,node,env,available,index,folders,contains,index,uuid,catch,ioexception,e,logger,error,unable,to,retrieve,available,index,folders,from,the,node,e,fail,unable,to,retrieve,available,index,folders,from,the,node
GatewayIndexStateIT -> public void testIndexDeletionWhenNodeRejoins() throws Exception;1544081506;This test ensures that when an index deletion takes place while a node is offline, when that_node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.;public void testIndexDeletionWhenNodeRejoins() throws Exception {_        final String indexName = "test-index-del-on-node-rejoin-idx"__        final int numNodes = 2___        final List<String> nodes__        logger.info("--> starting a cluster with " + numNodes + " nodes")__        nodes = internalCluster().startNodes(numNodes,_            Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100))_                    _                    .put(TestZenDiscovery.USE_ZEN2.getKey(), false).build())__        logger.info("--> create an index")__        createIndex(indexName)___        logger.info("--> waiting for green status")__        ensureGreen()__        final String indexUUID = resolveIndex(indexName).getUUID()___        logger.info("--> restart a random date node, deleting the index in between stopping and restarting")__        internalCluster().restartRandomDataNode(new RestartCallback() {_            @Override_            public Settings onNodeStopped(final String nodeName) throws Exception {_                nodes.remove(nodeName)__                logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes)__                assert nodes.size() > 0__                final String otherNode = nodes.get(0)__                logger.info("--> delete index and verify it is deleted")__                final Client client = client(otherNode)__                client.admin().indices().prepareDelete(indexName).execute().actionGet()__                assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists())__                return super.onNodeStopped(nodeName)__            }_        })___        logger.info("--> wait until all nodes are back online")__        client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID)_                                              .waitForNodes(Integer.toString(numNodes))).actionGet()___        logger.info("--> waiting for green status")__        ensureGreen()___        logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node")__        assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists())__        assertBusy(() -> {_            final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class)__            try {_                assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID))__            } catch (IOException e) {_                logger.error("Unable to retrieve available index folders from the node", e)__                fail("Unable to retrieve available index folders from the node")__            }_        })__    };this,test,ensures,that,when,an,index,deletion,takes,place,while,a,node,is,offline,when,that,node,rejoins,the,cluster,it,deletes,the,index,locally,instead,of,importing,it,as,a,dangling,index;public,void,test,index,deletion,when,node,rejoins,throws,exception,final,string,index,name,test,index,del,on,node,rejoin,idx,final,int,num,nodes,2,final,list,string,nodes,logger,info,starting,a,cluster,with,num,nodes,nodes,nodes,internal,cluster,start,nodes,num,nodes,settings,builder,put,index,graveyard,get,key,random,int,between,10,100,put,test,zen,discovery,get,key,false,build,logger,info,create,an,index,create,index,index,name,logger,info,waiting,for,green,status,ensure,green,final,string,index,uuid,resolve,index,index,name,get,uuid,logger,info,restart,a,random,date,node,deleting,the,index,in,between,stopping,and,restarting,internal,cluster,restart,random,data,node,new,restart,callback,override,public,settings,on,node,stopped,final,string,node,name,throws,exception,nodes,remove,node,name,logger,info,stopped,node,remaining,nodes,node,name,nodes,assert,nodes,size,0,final,string,other,node,nodes,get,0,logger,info,delete,index,and,verify,it,is,deleted,final,client,client,client,other,node,client,admin,indices,prepare,delete,index,name,execute,action,get,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,return,super,on,node,stopped,node,name,logger,info,wait,until,all,nodes,are,back,online,client,admin,cluster,health,requests,cluster,health,request,wait,for,events,priority,languid,wait,for,nodes,integer,to,string,num,nodes,action,get,logger,info,waiting,for,green,status,ensure,green,logger,info,verify,that,the,deleted,index,is,removed,from,the,cluster,and,not,reimported,as,dangling,by,the,restarted,node,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,assert,busy,final,node,environment,node,env,internal,cluster,get,instance,node,environment,class,try,assert,false,index,folder,index,uuid,should,be,deleted,node,env,available,index,folders,contains,index,uuid,catch,ioexception,e,logger,error,unable,to,retrieve,available,index,folders,from,the,node,e,fail,unable,to,retrieve,available,index,folders,from,the,node
GatewayIndexStateIT -> public void testIndexDeletionWhenNodeRejoins() throws Exception;1544544737;This test ensures that when an index deletion takes place while a node is offline, when that_node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.;public void testIndexDeletionWhenNodeRejoins() throws Exception {_        final String indexName = "test-index-del-on-node-rejoin-idx"__        final int numNodes = 2___        final List<String> nodes__        logger.info("--> starting a cluster with " + numNodes + " nodes")__        nodes = internalCluster().startNodes(numNodes,_            Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build())__        logger.info("--> create an index")__        createIndex(indexName)___        logger.info("--> waiting for green status")__        ensureGreen()__        final String indexUUID = resolveIndex(indexName).getUUID()___        logger.info("--> restart a random date node, deleting the index in between stopping and restarting")__        internalCluster().restartRandomDataNode(new RestartCallback() {_            @Override_            public Settings onNodeStopped(final String nodeName) throws Exception {_                nodes.remove(nodeName)__                logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes)__                assert nodes.size() > 0__                final String otherNode = nodes.get(0)__                logger.info("--> delete index and verify it is deleted")__                final Client client = client(otherNode)__                client.admin().indices().prepareDelete(indexName).execute().actionGet()__                assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists())__                logger.info("--> index deleted")__                return super.onNodeStopped(nodeName)__            }_        })___        logger.info("--> wait until all nodes are back online")__        client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID)_                                              .waitForNodes(Integer.toString(numNodes))).actionGet()___        logger.info("--> waiting for green status")__        ensureGreen()___        logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node")__        assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists())__        assertBusy(() -> {_            final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class)__            try {_                assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID))__            } catch (IOException e) {_                logger.error("Unable to retrieve available index folders from the node", e)__                fail("Unable to retrieve available index folders from the node")__            }_        })__    };this,test,ensures,that,when,an,index,deletion,takes,place,while,a,node,is,offline,when,that,node,rejoins,the,cluster,it,deletes,the,index,locally,instead,of,importing,it,as,a,dangling,index;public,void,test,index,deletion,when,node,rejoins,throws,exception,final,string,index,name,test,index,del,on,node,rejoin,idx,final,int,num,nodes,2,final,list,string,nodes,logger,info,starting,a,cluster,with,num,nodes,nodes,nodes,internal,cluster,start,nodes,num,nodes,settings,builder,put,index,graveyard,get,key,random,int,between,10,100,build,logger,info,create,an,index,create,index,index,name,logger,info,waiting,for,green,status,ensure,green,final,string,index,uuid,resolve,index,index,name,get,uuid,logger,info,restart,a,random,date,node,deleting,the,index,in,between,stopping,and,restarting,internal,cluster,restart,random,data,node,new,restart,callback,override,public,settings,on,node,stopped,final,string,node,name,throws,exception,nodes,remove,node,name,logger,info,stopped,node,remaining,nodes,node,name,nodes,assert,nodes,size,0,final,string,other,node,nodes,get,0,logger,info,delete,index,and,verify,it,is,deleted,final,client,client,client,other,node,client,admin,indices,prepare,delete,index,name,execute,action,get,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,logger,info,index,deleted,return,super,on,node,stopped,node,name,logger,info,wait,until,all,nodes,are,back,online,client,admin,cluster,health,requests,cluster,health,request,wait,for,events,priority,languid,wait,for,nodes,integer,to,string,num,nodes,action,get,logger,info,waiting,for,green,status,ensure,green,logger,info,verify,that,the,deleted,index,is,removed,from,the,cluster,and,not,reimported,as,dangling,by,the,restarted,node,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,assert,busy,final,node,environment,node,env,internal,cluster,get,instance,node,environment,class,try,assert,false,index,folder,index,uuid,should,be,deleted,node,env,available,index,folders,contains,index,uuid,catch,ioexception,e,logger,error,unable,to,retrieve,available,index,folders,from,the,node,e,fail,unable,to,retrieve,available,index,folders,from,the,node
GatewayIndexStateIT -> public void testIndexDeletionWhenNodeRejoins() throws Exception;1549029235;This test ensures that when an index deletion takes place while a node is offline, when that_node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.;public void testIndexDeletionWhenNodeRejoins() throws Exception {_        final String indexName = "test-index-del-on-node-rejoin-idx"__        final int numNodes = 2___        final List<String> nodes__        logger.info("--> starting a cluster with " + numNodes + " nodes")__        nodes = internalCluster().startNodes(numNodes,_            Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build())__        logger.info("--> create an index")__        createIndex(indexName)___        logger.info("--> waiting for green status")__        ensureGreen()__        final String indexUUID = resolveIndex(indexName).getUUID()___        logger.info("--> restart a random date node, deleting the index in between stopping and restarting")__        internalCluster().restartRandomDataNode(new RestartCallback() {_            @Override_            public Settings onNodeStopped(final String nodeName) throws Exception {_                nodes.remove(nodeName)__                logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes)__                assert nodes.size() > 0__                final String otherNode = nodes.get(0)__                logger.info("--> delete index and verify it is deleted")__                final Client client = client(otherNode)__                client.admin().indices().prepareDelete(indexName).execute().actionGet()__                assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists())__                logger.info("--> index deleted")__                return super.onNodeStopped(nodeName)__            }_        })___        logger.info("--> wait until all nodes are back online")__        client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID)_                                              .waitForNodes(Integer.toString(numNodes))).actionGet()___        logger.info("--> waiting for green status")__        ensureGreen()___        logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node")__        assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists())__        assertBusy(() -> {_            final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class)__            try {_                assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID))__            } catch (IOException e) {_                logger.error("Unable to retrieve available index folders from the node", e)__                fail("Unable to retrieve available index folders from the node")__            }_        })__    };this,test,ensures,that,when,an,index,deletion,takes,place,while,a,node,is,offline,when,that,node,rejoins,the,cluster,it,deletes,the,index,locally,instead,of,importing,it,as,a,dangling,index;public,void,test,index,deletion,when,node,rejoins,throws,exception,final,string,index,name,test,index,del,on,node,rejoin,idx,final,int,num,nodes,2,final,list,string,nodes,logger,info,starting,a,cluster,with,num,nodes,nodes,nodes,internal,cluster,start,nodes,num,nodes,settings,builder,put,index,graveyard,get,key,random,int,between,10,100,build,logger,info,create,an,index,create,index,index,name,logger,info,waiting,for,green,status,ensure,green,final,string,index,uuid,resolve,index,index,name,get,uuid,logger,info,restart,a,random,date,node,deleting,the,index,in,between,stopping,and,restarting,internal,cluster,restart,random,data,node,new,restart,callback,override,public,settings,on,node,stopped,final,string,node,name,throws,exception,nodes,remove,node,name,logger,info,stopped,node,remaining,nodes,node,name,nodes,assert,nodes,size,0,final,string,other,node,nodes,get,0,logger,info,delete,index,and,verify,it,is,deleted,final,client,client,client,other,node,client,admin,indices,prepare,delete,index,name,execute,action,get,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,logger,info,index,deleted,return,super,on,node,stopped,node,name,logger,info,wait,until,all,nodes,are,back,online,client,admin,cluster,health,requests,cluster,health,request,wait,for,events,priority,languid,wait,for,nodes,integer,to,string,num,nodes,action,get,logger,info,waiting,for,green,status,ensure,green,logger,info,verify,that,the,deleted,index,is,removed,from,the,cluster,and,not,reimported,as,dangling,by,the,restarted,node,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,assert,busy,final,node,environment,node,env,internal,cluster,get,instance,node,environment,class,try,assert,false,index,folder,index,uuid,should,be,deleted,node,env,available,index,folders,contains,index,uuid,catch,ioexception,e,logger,error,unable,to,retrieve,available,index,folders,from,the,node,e,fail,unable,to,retrieve,available,index,folders,from,the,node
GatewayIndexStateIT -> public void testIndexDeletionWhenNodeRejoins() throws Exception;1549031403;This test ensures that when an index deletion takes place while a node is offline, when that_node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.;public void testIndexDeletionWhenNodeRejoins() throws Exception {_        final String indexName = "test-index-del-on-node-rejoin-idx"__        final int numNodes = 2___        final List<String> nodes__        logger.info("--> starting a cluster with " + numNodes + " nodes")__        nodes = internalCluster().startNodes(numNodes,_            Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build())__        logger.info("--> create an index")__        createIndex(indexName)___        logger.info("--> waiting for green status")__        ensureGreen()__        final String indexUUID = resolveIndex(indexName).getUUID()___        logger.info("--> restart a random date node, deleting the index in between stopping and restarting")__        internalCluster().restartRandomDataNode(new RestartCallback() {_            @Override_            public Settings onNodeStopped(final String nodeName) throws Exception {_                nodes.remove(nodeName)__                logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes)__                assert nodes.size() > 0__                final String otherNode = nodes.get(0)__                logger.info("--> delete index and verify it is deleted")__                final Client client = client(otherNode)__                client.admin().indices().prepareDelete(indexName).execute().actionGet()__                assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists())__                logger.info("--> index deleted")__                return super.onNodeStopped(nodeName)__            }_        })___        logger.info("--> wait until all nodes are back online")__        client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID)_                                              .waitForNodes(Integer.toString(numNodes))).actionGet()___        logger.info("--> waiting for green status")__        ensureGreen()___        logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node")__        assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists())__        assertBusy(() -> {_            final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class)__            try {_                assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID))__            } catch (IOException e) {_                logger.error("Unable to retrieve available index folders from the node", e)__                fail("Unable to retrieve available index folders from the node")__            }_        })__    };this,test,ensures,that,when,an,index,deletion,takes,place,while,a,node,is,offline,when,that,node,rejoins,the,cluster,it,deletes,the,index,locally,instead,of,importing,it,as,a,dangling,index;public,void,test,index,deletion,when,node,rejoins,throws,exception,final,string,index,name,test,index,del,on,node,rejoin,idx,final,int,num,nodes,2,final,list,string,nodes,logger,info,starting,a,cluster,with,num,nodes,nodes,nodes,internal,cluster,start,nodes,num,nodes,settings,builder,put,index,graveyard,get,key,random,int,between,10,100,build,logger,info,create,an,index,create,index,index,name,logger,info,waiting,for,green,status,ensure,green,final,string,index,uuid,resolve,index,index,name,get,uuid,logger,info,restart,a,random,date,node,deleting,the,index,in,between,stopping,and,restarting,internal,cluster,restart,random,data,node,new,restart,callback,override,public,settings,on,node,stopped,final,string,node,name,throws,exception,nodes,remove,node,name,logger,info,stopped,node,remaining,nodes,node,name,nodes,assert,nodes,size,0,final,string,other,node,nodes,get,0,logger,info,delete,index,and,verify,it,is,deleted,final,client,client,client,other,node,client,admin,indices,prepare,delete,index,name,execute,action,get,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,logger,info,index,deleted,return,super,on,node,stopped,node,name,logger,info,wait,until,all,nodes,are,back,online,client,admin,cluster,health,requests,cluster,health,request,wait,for,events,priority,languid,wait,for,nodes,integer,to,string,num,nodes,action,get,logger,info,waiting,for,green,status,ensure,green,logger,info,verify,that,the,deleted,index,is,removed,from,the,cluster,and,not,reimported,as,dangling,by,the,restarted,node,assert,false,client,admin,indices,prepare,exists,index,name,execute,action,get,is,exists,assert,busy,final,node,environment,node,env,internal,cluster,get,instance,node,environment,class,try,assert,false,index,folder,index,uuid,should,be,deleted,node,env,available,index,folders,contains,index,uuid,catch,ioexception,e,logger,error,unable,to,retrieve,available,index,folders,from,the,node,e,fail,unable,to,retrieve,available,index,folders,from,the,node
