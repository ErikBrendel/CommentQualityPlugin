commented;modifiers;parameterAmount;loc;comment;code
false;public;0;26;;public void testMappingMetaDataParsed() throws Exception {     logger.info("--> starting 1 nodes").     internalCluster().startNode().     logger.info("--> creating test index, with meta routing").     client().admin().indices().prepareCreate("test").addMapping("type1", XContentFactory.jsonBuilder().startObject().startObject("type1").startObject("_routing").field("required", true).endObject().endObject().endObject()).execute().actionGet().     logger.info("--> verify meta _routing required exists").     MappingMetaData mappingMd = client().admin().cluster().prepareState().execute().actionGet().getState().metaData().index("test").getMappings().get("type1").     assertThat(mappingMd.routing().required(), equalTo(true)).     logger.info("--> restarting nodes...").     internalCluster().fullRestart().     logger.info("--> waiting for yellow status").     ensureYellow().     logger.info("--> verify meta _routing required exists").     mappingMd = client().admin().cluster().prepareState().execute().actionGet().getState().metaData().index("test").getMappings().get("type1").     assertThat(mappingMd.routing().required(), equalTo(true)). }
false;public;0;102;;public void testSimpleOpenClose() throws Exception {     logger.info("--> starting 2 nodes").     internalCluster().startNodes(2).     logger.info("--> creating test index").     createIndex("test").     NumShards test = getNumShards("test").     logger.info("--> waiting for green status").     ensureGreen().     ClusterStateResponse stateResponse = client().admin().cluster().prepareState().execute().actionGet().     assertThat(stateResponse.getState().metaData().index("test").getState(), equalTo(IndexMetaData.State.OPEN)).     assertThat(stateResponse.getState().routingTable().index("test").shards().size(), equalTo(test.numPrimaries)).     assertThat(stateResponse.getState().routingTable().index("test").shardsWithState(ShardRoutingState.STARTED).size(), equalTo(test.totalNumShards)).     logger.info("--> indexing a simple document").     client().prepareIndex("test", "type1", "1").setSource("field1", "value1").get().     logger.info("--> closing test index...").     assertAcked(client().admin().indices().prepareClose("test")).     stateResponse = client().admin().cluster().prepareState().execute().actionGet().     assertThat(stateResponse.getState().metaData().index("test").getState(), equalTo(IndexMetaData.State.CLOSE)).     assertThat(stateResponse.getState().routingTable().index("test"), notNullValue()).     logger.info("--> verifying that the state is green").     ensureGreen().     logger.info("--> trying to index into a closed index ...").     try {         client().prepareIndex("test", "type1", "1").setSource("field1", "value1").execute().actionGet().         fail().     } catch (IndexClosedException e) {     // all is well     }     logger.info("--> creating another index (test2) by indexing into it").     client().prepareIndex("test2", "type1", "1").setSource("field1", "value1").execute().actionGet().     logger.info("--> verifying that the state is green").     ensureGreen().     logger.info("--> opening the first index again...").     assertAcked(client().admin().indices().prepareOpen("test")).     logger.info("--> verifying that the state is green").     ensureGreen().     stateResponse = client().admin().cluster().prepareState().execute().actionGet().     assertThat(stateResponse.getState().metaData().index("test").getState(), equalTo(IndexMetaData.State.OPEN)).     assertThat(stateResponse.getState().routingTable().index("test").shards().size(), equalTo(test.numPrimaries)).     assertThat(stateResponse.getState().routingTable().index("test").shardsWithState(ShardRoutingState.STARTED).size(), equalTo(test.totalNumShards)).     logger.info("--> trying to get the indexed document on the first index").     GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet().     assertThat(getResponse.isExists(), equalTo(true)).     logger.info("--> closing test index...").     assertAcked(client().admin().indices().prepareClose("test")).     stateResponse = client().admin().cluster().prepareState().execute().actionGet().     assertThat(stateResponse.getState().metaData().index("test").getState(), equalTo(IndexMetaData.State.CLOSE)).     assertThat(stateResponse.getState().routingTable().index("test"), notNullValue()).     logger.info("--> restarting nodes...").     internalCluster().fullRestart().     logger.info("--> waiting for two nodes and green status").     ensureGreen().     stateResponse = client().admin().cluster().prepareState().execute().actionGet().     assertThat(stateResponse.getState().metaData().index("test").getState(), equalTo(IndexMetaData.State.CLOSE)).     assertThat(stateResponse.getState().routingTable().index("test"), notNullValue()).     logger.info("--> trying to index into a closed index ...").     try {         client().prepareIndex("test", "type1", "1").setSource("field1", "value1").execute().actionGet().         fail().     } catch (IndexClosedException e) {     // all is well     }     logger.info("--> opening index...").     client().admin().indices().prepareOpen("test").execute().actionGet().     logger.info("--> waiting for green status").     ensureGreen().     stateResponse = client().admin().cluster().prepareState().execute().actionGet().     assertThat(stateResponse.getState().metaData().index("test").getState(), equalTo(IndexMetaData.State.OPEN)).     assertThat(stateResponse.getState().routingTable().index("test").shards().size(), equalTo(test.numPrimaries)).     assertThat(stateResponse.getState().routingTable().index("test").shardsWithState(ShardRoutingState.STARTED).size(), equalTo(test.totalNumShards)).     logger.info("--> trying to get the indexed document on the first round (before close and shutdown)").     getResponse = client().prepareGet("test", "type1", "1").execute().actionGet().     assertThat(getResponse.isExists(), equalTo(true)).     logger.info("--> indexing a simple document").     client().prepareIndex("test", "type1", "2").setSource("field1", "value1").execute().actionGet(). }
false;public;0;24;;public void testJustMasterNode() throws Exception {     logger.info("--> cleaning nodes").     logger.info("--> starting 1 master node non data").     internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), false).build()).     logger.info("--> create an index").     client().admin().indices().prepareCreate("test").setWaitForActiveShards(ActiveShardCount.NONE).execute().actionGet().     logger.info("--> closing master node").     internalCluster().closeNonSharedNodes(false).     logger.info("--> starting 1 master node non data again").     internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), false).build()).     logger.info("--> waiting for test index to be created").     ClusterHealthResponse health = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setIndices("test").execute().actionGet().     assertThat(health.isTimedOut(), equalTo(false)).     logger.info("--> verify we have an index").     ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().setIndices("test").execute().actionGet().     assertThat(clusterStateResponse.getState().metaData().hasIndex("test"), equalTo(true)). }
false;public;0;12;;public void testJustMasterNodeAndJustDataNode() throws Exception {     logger.info("--> cleaning nodes").     logger.info("--> starting 1 master node non data").     internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), false).build()).     internalCluster().startNode(Settings.builder().put(Node.NODE_MASTER_SETTING.getKey(), false).build()).     logger.info("--> create an index").     client().admin().indices().prepareCreate("test").execute().actionGet().     client().prepareIndex("test", "type1").setSource("field1", "value1").execute().actionGet(). }
false;public;0;40;;public void testTwoNodesSingleDoc() throws Exception {     logger.info("--> cleaning nodes").     logger.info("--> starting 2 nodes").     internalCluster().startNodes(2).     logger.info("--> indexing a simple document").     client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get().     logger.info("--> waiting for green status").     ClusterHealthResponse health = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().setWaitForNodes("2").execute().actionGet().     assertThat(health.isTimedOut(), equalTo(false)).     logger.info("--> verify 1 doc in the index").     for (int i = 0. i < 10. i++) {         assertHitCount(client().prepareSearch().setQuery(matchAllQuery()).get(), 1L).     }     logger.info("--> closing test index...").     assertAcked(client().admin().indices().prepareClose("test")).     ClusterStateResponse stateResponse = client().admin().cluster().prepareState().execute().actionGet().     assertThat(stateResponse.getState().metaData().index("test").getState(), equalTo(IndexMetaData.State.CLOSE)).     assertThat(stateResponse.getState().routingTable().index("test"), notNullValue()).     logger.info("--> opening the index...").     client().admin().indices().prepareOpen("test").execute().actionGet().     logger.info("--> waiting for green status").     health = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().setWaitForNodes("2").execute().actionGet().     assertThat(health.isTimedOut(), equalTo(false)).     logger.info("--> verify 1 doc in the index").     assertHitCount(client().prepareSearch().setQuery(matchAllQuery()).get(), 1L).     for (int i = 0. i < 10. i++) {         assertHitCount(client().prepareSearch().setQuery(matchAllQuery()).get(), 1L).     } }
false;public;1;13;;@Override public Settings onNodeStopped(final String nodeName) throws Exception {     nodes.remove(nodeName).     logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes).     assert nodes.size() > 0.     final String otherNode = nodes.get(0).     logger.info("--> delete index and verify it is deleted").     final Client client = client(otherNode).     client.admin().indices().prepareDelete(indexName).execute().actionGet().     assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists()).     logger.info("--> index deleted").     return super.onNodeStopped(nodeName). }
true;public;0;51;/**  * This test ensures that when an index deletion takes place while a node is offline, when that  * node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.  */ ;/**  * This test ensures that when an index deletion takes place while a node is offline, when that  * node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.  */ public void testIndexDeletionWhenNodeRejoins() throws Exception {     final String indexName = "test-index-del-on-node-rejoin-idx".     final int numNodes = 2.     final List<String> nodes.     logger.info("--> starting a cluster with " + numNodes + " nodes").     nodes = internalCluster().startNodes(numNodes, Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build()).     logger.info("--> create an index").     createIndex(indexName).     logger.info("--> waiting for green status").     ensureGreen().     final String indexUUID = resolveIndex(indexName).getUUID().     logger.info("--> restart a random date node, deleting the index in between stopping and restarting").     internalCluster().restartRandomDataNode(new RestartCallback() {          @Override         public Settings onNodeStopped(final String nodeName) throws Exception {             nodes.remove(nodeName).             logger.info("--> stopped node[{}], remaining nodes {}", nodeName, nodes).             assert nodes.size() > 0.             final String otherNode = nodes.get(0).             logger.info("--> delete index and verify it is deleted").             final Client client = client(otherNode).             client.admin().indices().prepareDelete(indexName).execute().actionGet().             assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists()).             logger.info("--> index deleted").             return super.onNodeStopped(nodeName).         }     }).     logger.info("--> wait until all nodes are back online").     client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID).waitForNodes(Integer.toString(numNodes))).actionGet().     logger.info("--> waiting for green status").     ensureGreen().     logger.info("--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node").     assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists()).     assertBusy(() -> {         final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class).         try {             assertFalse("index folder " + indexUUID + " should be deleted", nodeEnv.availableIndexFolders().contains(indexUUID)).         } catch (IOException e) {             logger.error("Unable to retrieve available index folders from the node", e).             fail("Unable to retrieve available index folders from the node").         }     }). }
false;public;1;6;;@Override public Settings onNodeStopped(String nodeName) throws Exception {     final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName).     metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta).     return super.onNodeStopped(nodeName). }
true;public;0;59;/**  * This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being  * allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk  * if it is sane and if we can successfully create an IndexService. This also includes plugins etc.  */ ;/**  * This test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being  * allocated in our metadata that we recover. In that case we now have the ability to check the index on local recovery from disk  * if it is sane and if we can successfully create an IndexService. This also includes plugins etc.  */ public void testRecoverBrokenIndexMetadata() throws Exception {     logger.info("--> starting one node").     internalCluster().startNode().     logger.info("--> indexing a simple document").     client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get().     logger.info("--> waiting for green status").     if (usually()) {         ensureYellow().     } else {         internalCluster().startNode().         client().admin().cluster().health(Requests.clusterHealthRequest().waitForGreenStatus().waitForEvents(Priority.LANGUID).waitForNoRelocatingShards(true).waitForNodes("2")).actionGet().     }     ClusterState state = client().admin().cluster().prepareState().get().getState().     final IndexMetaData metaData = state.getMetaData().index("test").     final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(Settings.builder().put(metaData.getSettings()).put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id).put("index.similarity.BM25.type", "classic").put("index.analysis.filter.myCollator.type", "icu_collation")).build().     internalCluster().fullRestart(new RestartCallback() {          @Override         public Settings onNodeStopped(String nodeName) throws Exception {             final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName).             metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta).             return super.onNodeStopped(nodeName).         }     }).     // check that the cluster does not keep reallocating shards     assertBusy(() -> {         final RoutingTable routingTable = client().admin().cluster().prepareState().get().getState().routingTable().         final IndexRoutingTable indexRoutingTable = routingTable.index("test").         assertNotNull(indexRoutingTable).         for (IndexShardRoutingTable shardRoutingTable : indexRoutingTable) {             assertTrue(shardRoutingTable.primaryShard().unassigned()).             assertEquals(UnassignedInfo.AllocationStatus.DECIDERS_NO, shardRoutingTable.primaryShard().unassignedInfo().getLastAllocationStatus()).             assertThat(shardRoutingTable.primaryShard().unassignedInfo().getNumFailedAllocations(), greaterThan(0)).         }     }, 60, TimeUnit.SECONDS).     client().admin().indices().prepareClose("test").get().     state = client().admin().cluster().prepareState().get().getState().     assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState()).     assertEquals("classic", state.getMetaData().index(metaData.getIndex()).getSettings().get("archived.index.similarity.BM25.type")).     // try to open it with the broken setting - fail again!     ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get()).     assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex()).     assertNotNull(ex.getCause()).     assertEquals(IllegalArgumentException.class, ex.getCause().getClass()).     assertEquals(ex.getCause().getMessage(), "Unknown filter type [icu_collation] for [myCollator]"). }
false;public;1;6;;@Override public Settings onNodeStopped(String nodeName) throws Exception {     final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName).     metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta).     return super.onNodeStopped(nodeName). }
true;public;0;64;/**  * This test really tests worst case scenario where we have a missing analyzer setting.  * In that case we now have the ability to check the index on local recovery from disk  * if it is sane and if we can successfully create an IndexService.  * This also includes plugins etc.  */ ;/**  * This test really tests worst case scenario where we have a missing analyzer setting.  * In that case we now have the ability to check the index on local recovery from disk  * if it is sane and if we can successfully create an IndexService.  * This also includes plugins etc.  */ public void testRecoverMissingAnalyzer() throws Exception {     logger.info("--> starting one node").     internalCluster().startNode().     prepareCreate("test").setSettings(Settings.builder().put("index.analysis.analyzer.test.tokenizer", "standard").put("index.number_of_shards", "1")).addMapping("type1", "{\n" + "    \"type1\": {\n" + "      \"properties\": {\n" + "        \"field1\": {\n" + "          \"type\": \"text\",\n" + "          \"analyzer\": \"test\"\n" + "        }\n" + "      }\n" + "    }\n" + "  }}", XContentType.JSON).get().     logger.info("--> indexing a simple document").     client().prepareIndex("test", "type1", "1").setSource("field1", "value one").setRefreshPolicy(IMMEDIATE).get().     logger.info("--> waiting for green status").     if (usually()) {         ensureYellow().     } else {         internalCluster().startNode().         client().admin().cluster().health(Requests.clusterHealthRequest().waitForGreenStatus().waitForEvents(Priority.LANGUID).waitForNoRelocatingShards(true).waitForNodes("2")).actionGet().     }     ClusterState state = client().admin().cluster().prepareState().get().getState().     final IndexMetaData metaData = state.getMetaData().index("test").     final IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings().filter((s) -> "index.analysis.analyzer.test.tokenizer".equals(s) == false)).build().     internalCluster().fullRestart(new RestartCallback() {          @Override         public Settings onNodeStopped(String nodeName) throws Exception {             final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName).             metaStateService.writeIndexAndUpdateManifest("broken metadata", brokenMeta).             return super.onNodeStopped(nodeName).         }     }).     // check that the cluster does not keep reallocating shards     assertBusy(() -> {         final RoutingTable routingTable = client().admin().cluster().prepareState().get().getState().routingTable().         final IndexRoutingTable indexRoutingTable = routingTable.index("test").         assertNotNull(indexRoutingTable).         for (IndexShardRoutingTable shardRoutingTable : indexRoutingTable) {             assertTrue(shardRoutingTable.primaryShard().unassigned()).             assertEquals(UnassignedInfo.AllocationStatus.DECIDERS_NO, shardRoutingTable.primaryShard().unassignedInfo().getLastAllocationStatus()).             assertThat(shardRoutingTable.primaryShard().unassignedInfo().getNumFailedAllocations(), greaterThan(0)).         }     }, 60, TimeUnit.SECONDS).     client().admin().indices().prepareClose("test").get().     // try to open it with the broken setting - fail again!     ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen("test").get()).     assertEquals(ex.getMessage(), "Failed to verify index " + metaData.getIndex()).     assertNotNull(ex.getCause()).     assertEquals(MapperParsingException.class, ex.getCause().getClass()).     assertThat(ex.getCause().getMessage(), containsString("analyzer [test] not found for field [field1]")). }
false;public;1;6;;@Override public Settings onNodeStopped(String nodeName) throws Exception {     final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName).     metaStateService.writeGlobalStateAndUpdateManifest("broken metadata", brokenMeta).     return super.onNodeStopped(nodeName). }
false;public;0;45;;public void testArchiveBrokenClusterSettings() throws Exception {     logger.info("--> starting one node").     internalCluster().startNode().     client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get().     logger.info("--> waiting for green status").     if (usually()) {         ensureYellow().     } else {         internalCluster().startNode().         client().admin().cluster().health(Requests.clusterHealthRequest().waitForGreenStatus().waitForEvents(Priority.LANGUID).waitForNoRelocatingShards(true).waitForNodes("2")).actionGet().     }     ClusterState state = client().admin().cluster().prepareState().get().getState().     final MetaData metaData = state.getMetaData().     final MetaData brokenMeta = MetaData.builder(metaData).persistentSettings(Settings.builder().put(metaData.persistentSettings()).put("this.is.unknown", true).put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), "broken").build()).build().     internalCluster().fullRestart(new RestartCallback() {          @Override         public Settings onNodeStopped(String nodeName) throws Exception {             final MetaStateService metaStateService = internalCluster().getInstance(MetaStateService.class, nodeName).             metaStateService.writeGlobalStateAndUpdateManifest("broken metadata", brokenMeta).             return super.onNodeStopped(nodeName).         }     }).     // wait for state recovery     ensureYellow("test").     state = client().admin().cluster().prepareState().get().getState().     assertEquals("true", state.metaData().persistentSettings().get("archived.this.is.unknown")).     assertEquals("broken", state.metaData().persistentSettings().get("archived." + ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey())).     // delete these settings     client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder().putNull("archived.*")).get().     state = client().admin().cluster().prepareState().get().getState().     assertNull(state.metaData().persistentSettings().get("archived.this.is.unknown")).     assertNull(state.metaData().persistentSettings().get("archived." + ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey())).     assertHitCount(client().prepareSearch().setQuery(matchAllQuery()).get(), 1L). }
