# id;timestamp;commentText;codeText;commentWords;codeWords
SignificantTextAggregatorTests -> public void testSignificanceOnTextArrays() throws IOException;1524684173;Test documents with arrays of text;public void testSignificanceOnTextArrays() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            for (int i = 0_ i < 10_ i++) {_                Document doc = new Document()__                doc.add(new Field("text", "foo", textFieldType))__                String json ="{ \"text\" : [\"foo\",\"foo\"], \"title\" : [\"foo\", \"foo\"]}"__                doc.add(new StoredField("_source", new BytesRef(json)))__                w.addDocument(doc)__            }__            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text")__            sigAgg.sourceFieldNames(Arrays.asList(new String [] {"title", "text"}))__            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)_                                _                searchAndReduce(searcher, new TermQuery(new Term("text", "foo")), sigAgg, textFieldType)__                _                _            }_        }_    };test,documents,with,arrays,of,text;public,void,test,significance,on,text,arrays,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,for,int,i,0,i,10,i,document,doc,new,document,doc,add,new,field,text,foo,text,field,type,string,json,text,foo,foo,title,foo,foo,doc,add,new,stored,field,new,bytes,ref,json,w,add,document,doc,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,sig,agg,source,field,names,arrays,as,list,new,string,title,text,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,search,and,reduce,searcher,new,term,query,new,term,text,foo,sig,agg,text,field,type
SignificantTextAggregatorTests -> public void testSignificanceOnTextArrays() throws IOException;1528762805;Test documents with arrays of text;public void testSignificanceOnTextArrays() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            for (int i = 0_ i < 10_ i++) {_                Document doc = new Document()__                doc.add(new Field("text", "foo", textFieldType))__                String json ="{ \"text\" : [\"foo\",\"foo\"], \"title\" : [\"foo\", \"foo\"]}"__                doc.add(new StoredField("_source", new BytesRef(json)))__                w.addDocument(doc)__            }__            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text")__            sigAgg.sourceFieldNames(Arrays.asList(new String [] {"title", "text"}))__            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)_                                _                searchAndReduce(searcher, new TermQuery(new Term("text", "foo")), sigAgg, textFieldType)__                _                _            }_        }_    };test,documents,with,arrays,of,text;public,void,test,significance,on,text,arrays,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,for,int,i,0,i,10,i,document,doc,new,document,doc,add,new,field,text,foo,text,field,type,string,json,text,foo,foo,title,foo,foo,doc,add,new,stored,field,new,bytes,ref,json,w,add,document,doc,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,sig,agg,source,field,names,arrays,as,list,new,string,title,text,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,search,and,reduce,searcher,new,term,query,new,term,text,foo,sig,agg,text,field,type
SignificantTextAggregatorTests -> public void testSignificanceOnTextArrays() throws IOException;1531937412;Test documents with arrays of text;public void testSignificanceOnTextArrays() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            for (int i = 0_ i < 10_ i++) {_                Document doc = new Document()__                doc.add(new Field("text", "foo", textFieldType))__                String json ="{ \"text\" : [\"foo\",\"foo\"], \"title\" : [\"foo\", \"foo\"]}"__                doc.add(new StoredField("_source", new BytesRef(json)))__                w.addDocument(doc)__            }__            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text")__            sigAgg.sourceFieldNames(Arrays.asList(new String [] {"title", "text"}))__            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)__                searchAndReduce(searcher, new TermQuery(new Term("text", "foo")), sigAgg, textFieldType)__                _                _            }_        }_    };test,documents,with,arrays,of,text;public,void,test,significance,on,text,arrays,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,for,int,i,0,i,10,i,document,doc,new,document,doc,add,new,field,text,foo,text,field,type,string,json,text,foo,foo,title,foo,foo,doc,add,new,stored,field,new,bytes,ref,json,w,add,document,doc,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,sig,agg,source,field,names,arrays,as,list,new,string,title,text,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,search,and,reduce,searcher,new,term,query,new,term,text,foo,sig,agg,text,field,type
SignificantTextAggregatorTests -> public void testSignificanceOnTextArrays() throws IOException;1532028790;Test documents with arrays of text;public void testSignificanceOnTextArrays() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            for (int i = 0_ i < 10_ i++) {_                Document doc = new Document()__                doc.add(new Field("text", "foo", textFieldType))__                String json ="{ \"text\" : [\"foo\",\"foo\"], \"title\" : [\"foo\", \"foo\"]}"__                doc.add(new StoredField("_source", new BytesRef(json)))__                w.addDocument(doc)__            }__            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text")__            sigAgg.sourceFieldNames(Arrays.asList(new String [] {"title", "text"}))__            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)__                searchAndReduce(searcher, new TermQuery(new Term("text", "foo")), sigAgg, textFieldType)__                _                _            }_        }_    };test,documents,with,arrays,of,text;public,void,test,significance,on,text,arrays,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,for,int,i,0,i,10,i,document,doc,new,document,doc,add,new,field,text,foo,text,field,type,string,json,text,foo,foo,title,foo,foo,doc,add,new,stored,field,new,bytes,ref,json,w,add,document,doc,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,sig,agg,source,field,names,arrays,as,list,new,string,title,text,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,search,and,reduce,searcher,new,term,query,new,term,text,foo,sig,agg,text,field,type
SignificantTextAggregatorTests -> public void testSignificanceOnTextArrays() throws IOException;1548178735;Test documents with arrays of text;public void testSignificanceOnTextArrays() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            for (int i = 0_ i < 10_ i++) {_                Document doc = new Document()__                doc.add(new Field("text", "foo", textFieldType))__                String json ="{ \"text\" : [\"foo\",\"foo\"], \"title\" : [\"foo\", \"foo\"]}"__                doc.add(new StoredField("_source", new BytesRef(json)))__                w.addDocument(doc)__            }__            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text")__            sigAgg.sourceFieldNames(Arrays.asList(new String [] {"title", "text"}))__            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)__                searchAndReduce(searcher, new TermQuery(new Term("text", "foo")), sigAgg, textFieldType)__                _                _            }_        }_    };test,documents,with,arrays,of,text;public,void,test,significance,on,text,arrays,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,for,int,i,0,i,10,i,document,doc,new,document,doc,add,new,field,text,foo,text,field,type,string,json,text,foo,foo,title,foo,foo,doc,add,new,stored,field,new,bytes,ref,json,w,add,document,doc,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,sig,agg,source,field,names,arrays,as,list,new,string,title,text,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,search,and,reduce,searcher,new,term,query,new,term,text,foo,sig,agg,text,field,type
SignificantTextAggregatorTests -> @Override     protected Map<String, MappedFieldType> getFieldAliases(MappedFieldType... fieldTypes);1531937412;For each provided field type, we also register an alias with name <field>-alias.;@Override_    protected Map<String, MappedFieldType> getFieldAliases(MappedFieldType... fieldTypes) {_        return Arrays.stream(fieldTypes).collect(Collectors.toMap(_            ft -> ft.name() + "-alias",_            Function.identity()))__    };for,each,provided,field,type,we,also,register,an,alias,with,name,field,alias;override,protected,map,string,mapped,field,type,get,field,aliases,mapped,field,type,field,types,return,arrays,stream,field,types,collect,collectors,to,map,ft,ft,name,alias,function,identity
SignificantTextAggregatorTests -> @Override     protected Map<String, MappedFieldType> getFieldAliases(MappedFieldType... fieldTypes);1532028790;For each provided field type, we also register an alias with name {@code <field>-alias}.;@Override_    protected Map<String, MappedFieldType> getFieldAliases(MappedFieldType... fieldTypes) {_        return Arrays.stream(fieldTypes).collect(Collectors.toMap(_            ft -> ft.name() + "-alias",_            Function.identity()))__    };for,each,provided,field,type,we,also,register,an,alias,with,name,code,field,alias;override,protected,map,string,mapped,field,type,get,field,aliases,mapped,field,type,field,types,return,arrays,stream,field,types,collect,collectors,to,map,ft,ft,name,alias,function,identity
SignificantTextAggregatorTests -> @Override     protected Map<String, MappedFieldType> getFieldAliases(MappedFieldType... fieldTypes);1548178735;For each provided field type, we also register an alias with name {@code <field>-alias}.;@Override_    protected Map<String, MappedFieldType> getFieldAliases(MappedFieldType... fieldTypes) {_        return Arrays.stream(fieldTypes).collect(Collectors.toMap(_            ft -> ft.name() + "-alias",_            Function.identity()))__    };for,each,provided,field,type,we,also,register,an,alias,with,name,code,field,alias;override,protected,map,string,mapped,field,type,get,field,aliases,mapped,field,type,field,types,return,arrays,stream,field,types,collect,collectors,to,map,ft,ft,name,alias,function,identity
SignificantTextAggregatorTests -> public void testSignificance() throws IOException;1524684173;Uses the significant text aggregation to find the keywords in text fields;public void testSignificance() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            for (int i = 0_ i < 10_ i++) {_                Document doc = new Document()__                StringBuilder text = new StringBuilder("common ")__                if (i % 2 == 0) {_                    text.append("odd ")__                } else {                    _                    text.append("even separator" + i + " duplicate duplicate duplicate duplicate duplicate duplicate ")__                }__                doc.add(new Field("text", text.toString(), textFieldType))__                String json ="{ \"text\" : \"" + text.toString() + "\","+_                             " \"json_only_field\" : \"" + text.toString() + "\"" +_                            " }"__                doc.add(new StoredField("_source", new BytesRef(json)))__                w.addDocument(doc)__            }__            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text").filterDuplicateText(true)__            if(randomBoolean()){_                sigAgg.sourceFieldNames(Arrays.asList(new String [] {"json_only_field"}))__            }_            SamplerAggregationBuilder aggBuilder = new SamplerAggregationBuilder("sampler")_                    .subAggregation(sigAgg)__            _            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)__                                _                _                Sampler sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "odd")), aggBuilder, textFieldType)__                SignificantTerms terms = sampler.getAggregations().get("sig_text")__                _                assertNull(terms.getBucketByKey("even"))__                assertNull(terms.getBucketByKey("duplicate"))_                _                assertNull(terms.getBucketByKey("common"))_                _                assertNotNull(terms.getBucketByKey("odd"))___                _                sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "even")), aggBuilder, textFieldType)__                terms = sampler.getAggregations().get("sig_text")__                _                assertNull(terms.getBucketByKey("odd"))__                assertNull(terms.getBucketByKey("duplicate"))_                _                assertNull(terms.getBucketByKey("common"))_    _                assertNull(terms.getBucketByKey("separator2"))__                assertNull(terms.getBucketByKey("separator4"))__                assertNull(terms.getBucketByKey("separator6"))___                assertNotNull(terms.getBucketByKey("even"))__            _            }_        }_    };uses,the,significant,text,aggregation,to,find,the,keywords,in,text,fields;public,void,test,significance,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,for,int,i,0,i,10,i,document,doc,new,document,string,builder,text,new,string,builder,common,if,i,2,0,text,append,odd,else,text,append,even,separator,i,duplicate,duplicate,duplicate,duplicate,duplicate,duplicate,doc,add,new,field,text,text,to,string,text,field,type,string,json,text,text,to,string,text,to,string,doc,add,new,stored,field,new,bytes,ref,json,w,add,document,doc,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,filter,duplicate,text,true,if,random,boolean,sig,agg,source,field,names,arrays,as,list,new,string,sampler,aggregation,builder,agg,builder,new,sampler,aggregation,builder,sampler,sub,aggregation,sig,agg,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,sampler,sampler,search,and,reduce,searcher,new,term,query,new,term,text,odd,agg,builder,text,field,type,significant,terms,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,even,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,not,null,terms,get,bucket,by,key,odd,sampler,search,and,reduce,searcher,new,term,query,new,term,text,even,agg,builder,text,field,type,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,odd,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,null,terms,get,bucket,by,key,separator2,assert,null,terms,get,bucket,by,key,separator4,assert,null,terms,get,bucket,by,key,separator6,assert,not,null,terms,get,bucket,by,key,even
SignificantTextAggregatorTests -> public void testSignificance() throws IOException;1528762805;Uses the significant text aggregation to find the keywords in text fields;public void testSignificance() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            for (int i = 0_ i < 10_ i++) {_                Document doc = new Document()__                StringBuilder text = new StringBuilder("common ")__                if (i % 2 == 0) {_                    text.append("odd ")__                } else {                    _                    text.append("even separator" + i + " duplicate duplicate duplicate duplicate duplicate duplicate ")__                }__                doc.add(new Field("text", text.toString(), textFieldType))__                String json ="{ \"text\" : \"" + text.toString() + "\","+_                             " \"json_only_field\" : \"" + text.toString() + "\"" +_                            " }"__                doc.add(new StoredField("_source", new BytesRef(json)))__                w.addDocument(doc)__            }__            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text").filterDuplicateText(true)__            if(randomBoolean()){_                sigAgg.sourceFieldNames(Arrays.asList(new String [] {"json_only_field"}))__            }_            SamplerAggregationBuilder aggBuilder = new SamplerAggregationBuilder("sampler")_                    .subAggregation(sigAgg)__            _            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)__                                _                _                Sampler sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "odd")), aggBuilder, textFieldType)__                SignificantTerms terms = sampler.getAggregations().get("sig_text")__                _                assertNull(terms.getBucketByKey("even"))__                assertNull(terms.getBucketByKey("duplicate"))_                _                assertNull(terms.getBucketByKey("common"))_                _                assertNotNull(terms.getBucketByKey("odd"))___                _                sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "even")), aggBuilder, textFieldType)__                terms = sampler.getAggregations().get("sig_text")__                _                assertNull(terms.getBucketByKey("odd"))__                assertNull(terms.getBucketByKey("duplicate"))_                _                assertNull(terms.getBucketByKey("common"))_    _                assertNull(terms.getBucketByKey("separator2"))__                assertNull(terms.getBucketByKey("separator4"))__                assertNull(terms.getBucketByKey("separator6"))___                assertNotNull(terms.getBucketByKey("even"))__            _            }_        }_    };uses,the,significant,text,aggregation,to,find,the,keywords,in,text,fields;public,void,test,significance,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,for,int,i,0,i,10,i,document,doc,new,document,string,builder,text,new,string,builder,common,if,i,2,0,text,append,odd,else,text,append,even,separator,i,duplicate,duplicate,duplicate,duplicate,duplicate,duplicate,doc,add,new,field,text,text,to,string,text,field,type,string,json,text,text,to,string,text,to,string,doc,add,new,stored,field,new,bytes,ref,json,w,add,document,doc,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,filter,duplicate,text,true,if,random,boolean,sig,agg,source,field,names,arrays,as,list,new,string,sampler,aggregation,builder,agg,builder,new,sampler,aggregation,builder,sampler,sub,aggregation,sig,agg,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,sampler,sampler,search,and,reduce,searcher,new,term,query,new,term,text,odd,agg,builder,text,field,type,significant,terms,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,even,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,not,null,terms,get,bucket,by,key,odd,sampler,search,and,reduce,searcher,new,term,query,new,term,text,even,agg,builder,text,field,type,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,odd,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,null,terms,get,bucket,by,key,separator2,assert,null,terms,get,bucket,by,key,separator4,assert,null,terms,get,bucket,by,key,separator6,assert,not,null,terms,get,bucket,by,key,even
SignificantTextAggregatorTests -> public void testSignificance() throws IOException;1531937412;Uses the significant text aggregation to find the keywords in text fields;public void testSignificance() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            indexDocuments(w, textFieldType)___            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text").filterDuplicateText(true)__            if(randomBoolean()){_                sigAgg.sourceFieldNames(Arrays.asList(new String [] {"json_only_field"}))__            }_            SamplerAggregationBuilder aggBuilder = new SamplerAggregationBuilder("sampler")_                    .subAggregation(sigAgg)___            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)___                _                Sampler sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "odd")), aggBuilder, textFieldType)__                SignificantTerms terms = sampler.getAggregations().get("sig_text")___                assertNull(terms.getBucketByKey("even"))__                assertNull(terms.getBucketByKey("duplicate"))__                assertNull(terms.getBucketByKey("common"))__                assertNotNull(terms.getBucketByKey("odd"))___                _                sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "even")), aggBuilder, textFieldType)__                terms = sampler.getAggregations().get("sig_text")___                assertNull(terms.getBucketByKey("odd"))__                assertNull(terms.getBucketByKey("duplicate"))__                assertNull(terms.getBucketByKey("common"))__                assertNull(terms.getBucketByKey("separator2"))__                assertNull(terms.getBucketByKey("separator4"))__                assertNull(terms.getBucketByKey("separator6"))___                assertNotNull(terms.getBucketByKey("even"))___            }_        }_    };uses,the,significant,text,aggregation,to,find,the,keywords,in,text,fields;public,void,test,significance,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,index,documents,w,text,field,type,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,filter,duplicate,text,true,if,random,boolean,sig,agg,source,field,names,arrays,as,list,new,string,sampler,aggregation,builder,agg,builder,new,sampler,aggregation,builder,sampler,sub,aggregation,sig,agg,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,sampler,sampler,search,and,reduce,searcher,new,term,query,new,term,text,odd,agg,builder,text,field,type,significant,terms,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,even,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,not,null,terms,get,bucket,by,key,odd,sampler,search,and,reduce,searcher,new,term,query,new,term,text,even,agg,builder,text,field,type,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,odd,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,null,terms,get,bucket,by,key,separator2,assert,null,terms,get,bucket,by,key,separator4,assert,null,terms,get,bucket,by,key,separator6,assert,not,null,terms,get,bucket,by,key,even
SignificantTextAggregatorTests -> public void testSignificance() throws IOException;1532028790;Uses the significant text aggregation to find the keywords in text fields;public void testSignificance() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            indexDocuments(w, textFieldType)___            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text").filterDuplicateText(true)__            if(randomBoolean()){_                sigAgg.sourceFieldNames(Arrays.asList(new String [] {"json_only_field"}))__            }_            SamplerAggregationBuilder aggBuilder = new SamplerAggregationBuilder("sampler")_                    .subAggregation(sigAgg)___            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)___                _                Sampler sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "odd")), aggBuilder, textFieldType)__                SignificantTerms terms = sampler.getAggregations().get("sig_text")___                assertNull(terms.getBucketByKey("even"))__                assertNull(terms.getBucketByKey("duplicate"))__                assertNull(terms.getBucketByKey("common"))__                assertNotNull(terms.getBucketByKey("odd"))___                _                sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "even")), aggBuilder, textFieldType)__                terms = sampler.getAggregations().get("sig_text")___                assertNull(terms.getBucketByKey("odd"))__                assertNull(terms.getBucketByKey("duplicate"))__                assertNull(terms.getBucketByKey("common"))__                assertNull(terms.getBucketByKey("separator2"))__                assertNull(terms.getBucketByKey("separator4"))__                assertNull(terms.getBucketByKey("separator6"))___                assertNotNull(terms.getBucketByKey("even"))___            }_        }_    };uses,the,significant,text,aggregation,to,find,the,keywords,in,text,fields;public,void,test,significance,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,index,documents,w,text,field,type,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,filter,duplicate,text,true,if,random,boolean,sig,agg,source,field,names,arrays,as,list,new,string,sampler,aggregation,builder,agg,builder,new,sampler,aggregation,builder,sampler,sub,aggregation,sig,agg,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,sampler,sampler,search,and,reduce,searcher,new,term,query,new,term,text,odd,agg,builder,text,field,type,significant,terms,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,even,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,not,null,terms,get,bucket,by,key,odd,sampler,search,and,reduce,searcher,new,term,query,new,term,text,even,agg,builder,text,field,type,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,odd,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,null,terms,get,bucket,by,key,separator2,assert,null,terms,get,bucket,by,key,separator4,assert,null,terms,get,bucket,by,key,separator6,assert,not,null,terms,get,bucket,by,key,even
SignificantTextAggregatorTests -> public void testSignificance() throws IOException;1548178735;Uses the significant text aggregation to find the keywords in text fields;public void testSignificance() throws IOException {_        TextFieldType textFieldType = new TextFieldType()__        textFieldType.setName("text")__        textFieldType.setIndexAnalyzer(new NamedAnalyzer("my_analyzer", AnalyzerScope.GLOBAL, new StandardAnalyzer()))___        IndexWriterConfig indexWriterConfig = newIndexWriterConfig()__        indexWriterConfig.setMaxBufferedDocs(100)__        indexWriterConfig.setRAMBufferSizeMB(100)_ _        try (Directory dir = newDirectory()_ IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {_            indexDocuments(w, textFieldType)___            SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder("sig_text", "text").filterDuplicateText(true)__            if(randomBoolean()){_                sigAgg.sourceFieldNames(Arrays.asList(new String [] {"json_only_field"}))__            }_            SamplerAggregationBuilder aggBuilder = new SamplerAggregationBuilder("sampler")_                    .subAggregation(sigAgg)___            try (IndexReader reader = DirectoryReader.open(w)) {_                assertEquals("test expects a single segment", 1, reader.leaves().size())__                IndexSearcher searcher = new IndexSearcher(reader)___                _                InternalSampler sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "odd")), aggBuilder, textFieldType)__                SignificantTerms terms = sampler.getAggregations().get("sig_text")___                assertNull(terms.getBucketByKey("even"))__                assertNull(terms.getBucketByKey("duplicate"))__                assertNull(terms.getBucketByKey("common"))__                assertNotNull(terms.getBucketByKey("odd"))___                _                sampler = searchAndReduce(searcher, new TermQuery(new Term("text", "even")), aggBuilder, textFieldType)__                terms = sampler.getAggregations().get("sig_text")___                assertNull(terms.getBucketByKey("odd"))__                assertNull(terms.getBucketByKey("duplicate"))__                assertNull(terms.getBucketByKey("common"))__                assertNull(terms.getBucketByKey("separator2"))__                assertNull(terms.getBucketByKey("separator4"))__                assertNull(terms.getBucketByKey("separator6"))___                assertNotNull(terms.getBucketByKey("even"))___                assertTrue(AggregationInspectionHelper.hasValue(sampler))__            }_        }_    };uses,the,significant,text,aggregation,to,find,the,keywords,in,text,fields;public,void,test,significance,throws,ioexception,text,field,type,text,field,type,new,text,field,type,text,field,type,set,name,text,text,field,type,set,index,analyzer,new,named,analyzer,analyzer,scope,global,new,standard,analyzer,index,writer,config,index,writer,config,new,index,writer,config,index,writer,config,set,max,buffered,docs,100,index,writer,config,set,rambuffer,size,mb,100,try,directory,dir,new,directory,index,writer,w,new,index,writer,dir,index,writer,config,index,documents,w,text,field,type,significant,text,aggregation,builder,sig,agg,new,significant,text,aggregation,builder,text,filter,duplicate,text,true,if,random,boolean,sig,agg,source,field,names,arrays,as,list,new,string,sampler,aggregation,builder,agg,builder,new,sampler,aggregation,builder,sampler,sub,aggregation,sig,agg,try,index,reader,reader,directory,reader,open,w,assert,equals,test,expects,a,single,segment,1,reader,leaves,size,index,searcher,searcher,new,index,searcher,reader,internal,sampler,sampler,search,and,reduce,searcher,new,term,query,new,term,text,odd,agg,builder,text,field,type,significant,terms,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,even,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,not,null,terms,get,bucket,by,key,odd,sampler,search,and,reduce,searcher,new,term,query,new,term,text,even,agg,builder,text,field,type,terms,sampler,get,aggregations,get,assert,null,terms,get,bucket,by,key,odd,assert,null,terms,get,bucket,by,key,duplicate,assert,null,terms,get,bucket,by,key,common,assert,null,terms,get,bucket,by,key,separator2,assert,null,terms,get,bucket,by,key,separator4,assert,null,terms,get,bucket,by,key,separator6,assert,not,null,terms,get,bucket,by,key,even,assert,true,aggregation,inspection,helper,has,value,sampler
