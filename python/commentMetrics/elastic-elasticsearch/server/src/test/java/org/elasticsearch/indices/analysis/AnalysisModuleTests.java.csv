commented;modifiers;parameterAmount;loc;comment;code
false;public;1;3;;public IndexAnalyzers getIndexAnalyzers(Settings settings) throws IOException {     return getIndexAnalyzers(getNewRegistry(settings), settings). }
false;public;2;4;;public IndexAnalyzers getIndexAnalyzers(AnalysisRegistry registry, Settings settings) throws IOException {     IndexSettings idxSettings = IndexSettingsModule.newIndexSettings("test", settings).     return registry.build(idxSettings). }
false;public;0;4;;@Override public Map<String, AnalysisProvider<TokenFilterFactory>> getTokenFilters() {     return singletonMap("myfilter", MyFilterTokenFilterFactory::new). }
false;public;0;4;;@Override public Map<String, AnalysisProvider<CharFilterFactory>> getCharFilters() {     return AnalysisPlugin.super.getCharFilters(). }
false;public;1;17;;public AnalysisRegistry getNewRegistry(Settings settings) {     try {         return new AnalysisModule(TestEnvironment.newEnvironment(settings), singletonList(new AnalysisPlugin() {              @Override             public Map<String, AnalysisProvider<TokenFilterFactory>> getTokenFilters() {                 return singletonMap("myfilter", MyFilterTokenFilterFactory::new).             }              @Override             public Map<String, AnalysisProvider<CharFilterFactory>> getCharFilters() {                 return AnalysisPlugin.super.getCharFilters().             }         })).getAnalysisRegistry().     } catch (IOException e) {         throw new RuntimeException(e).     } }
false;private;1;7;;private Settings loadFromClasspath(String path) throws IOException {     return Settings.builder().loadFromStream(path, getClass().getResourceAsStream(path), false).put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build(). }
false;public;0;4;;public void testSimpleConfigurationJson() throws IOException {     Settings settings = loadFromClasspath("/org/elasticsearch/index/analysis/test1.json").     testSimpleConfiguration(settings). }
false;public;0;4;;public void testSimpleConfigurationYaml() throws IOException {     Settings settings = loadFromClasspath("/org/elasticsearch/index/analysis/test1.yml").     testSimpleConfiguration(settings). }
false;public;0;12;;public void testAnalyzerAliasNotAllowedPost5x() throws IOException {     Settings settings = Settings.builder().put("index.analysis.analyzer.foobar.type", "standard").put("index.analysis.analyzer.foobar.alias", "foobaz").put(IndexMetaData.SETTING_VERSION_CREATED, VersionUtils.randomVersionBetween(random(), Version.V_6_0_0, null)).put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build().     AnalysisRegistry registry = getNewRegistry(settings).     IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> getIndexAnalyzers(registry, settings)).     assertEquals("setting [index.analysis.analyzer.foobar.alias] is not supported", e.getMessage()). }
false;public;0;27;;public void testVersionedAnalyzers() throws Exception {     String yaml = "/org/elasticsearch/index/analysis/test1.yml".     Settings settings2 = Settings.builder().loadFromStream(yaml, getClass().getResourceAsStream(yaml), false).put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_6_0_0).build().     AnalysisRegistry newRegistry = getNewRegistry(settings2).     IndexAnalyzers indexAnalyzers = getIndexAnalyzers(newRegistry, settings2).     // registry always has the current version     assertThat(newRegistry.getAnalyzer("default"), is(instanceOf(NamedAnalyzer.class))).     NamedAnalyzer defaultNamedAnalyzer = (NamedAnalyzer) newRegistry.getAnalyzer("default").     assertThat(defaultNamedAnalyzer.analyzer(), is(instanceOf(StandardAnalyzer.class))).     assertEquals(Version.CURRENT.luceneVersion, defaultNamedAnalyzer.analyzer().getVersion()).     // analysis service has the expected version     assertThat(indexAnalyzers.get("standard").analyzer(), is(instanceOf(StandardAnalyzer.class))).     assertEquals(Version.V_6_0_0.luceneVersion, indexAnalyzers.get("standard").analyzer().getVersion()).     assertEquals(Version.V_6_0_0.luceneVersion, indexAnalyzers.get("stop").analyzer().getVersion()).     assertThat(indexAnalyzers.get("custom7").analyzer(), is(instanceOf(StandardAnalyzer.class))).     assertEquals(org.apache.lucene.util.Version.fromBits(3, 6, 0), indexAnalyzers.get("custom7").analyzer().getVersion()). }
false;private;1;24;;private void testSimpleConfiguration(Settings settings) throws IOException {     IndexAnalyzers indexAnalyzers = getIndexAnalyzers(settings).     Analyzer analyzer = indexAnalyzers.get("custom1").analyzer().     assertThat(analyzer, instanceOf(CustomAnalyzer.class)).     CustomAnalyzer custom1 = (CustomAnalyzer) analyzer.     assertThat(custom1.tokenizerFactory(), instanceOf(StandardTokenizerFactory.class)).     assertThat(custom1.tokenFilters().length, equalTo(2)).     StopTokenFilterFactory stop1 = (StopTokenFilterFactory) custom1.tokenFilters()[0].     assertThat(stop1.stopWords().size(), equalTo(1)).     // verify position increment gap     analyzer = indexAnalyzers.get("custom6").analyzer().     assertThat(analyzer, instanceOf(CustomAnalyzer.class)).     CustomAnalyzer custom6 = (CustomAnalyzer) analyzer.     assertThat(custom6.getPositionIncrementGap("any_string"), equalTo(256)).     // check custom class name (my)     analyzer = indexAnalyzers.get("custom4").analyzer().     assertThat(analyzer, instanceOf(CustomAnalyzer.class)).     CustomAnalyzer custom4 = (CustomAnalyzer) analyzer.     assertThat(custom4.tokenFilters()[0], instanceOf(MyFilterTokenFilterFactory.class)). }
false;public;0;16;;public void testWordListPath() throws Exception {     Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build().     Environment env = TestEnvironment.newEnvironment(settings).     String[] words = new String[] { "donau", "dampf", "schiff", "spargel", "creme", "suppe" }.     Path wordListFile = generateWordList(words).     settings = Settings.builder().loadFromSource("index: \n  word_list_path: " + wordListFile.toAbsolutePath(), XContentType.YAML).build().     Set<?> wordList = Analysis.getWordSet(env, settings, "index.word_list").     MatcherAssert.assertThat(wordList.size(), equalTo(6)).     // MatcherAssert.assertThat(wordList, hasItems(words)).     Files.delete(wordListFile). }
false;private;1;10;;private Path generateWordList(String[] words) throws Exception {     Path wordListFile = createTempDir().resolve("wordlist.txt").     try (BufferedWriter writer = Files.newBufferedWriter(wordListFile, StandardCharsets.UTF_8)) {         for (String word : words) {             writer.write(word).             writer.write('\n').         }     }     return wordListFile. }
false;public;0;14;;public void testUnderscoreInAnalyzerName() throws IOException {     Settings settings = Settings.builder().put("index.analysis.analyzer._invalid_name.tokenizer", "standard").put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).put(IndexMetaData.SETTING_VERSION_CREATED, "1").build().     try {         getIndexAnalyzers(settings).         fail("This should fail with IllegalArgumentException because the analyzers name starts with _").     } catch (IllegalArgumentException e) {         assertThat(e.getMessage(), either(equalTo("analyzer name must not start with '_'. got \"_invalid_name\"")).or(equalTo("analyzer name must not start with '_'. got \"_invalidName\""))).     } }
false;public;0;10;;public void testStandardFilterBWC() throws IOException {     Version version = VersionUtils.randomVersionBetween(random(), Version.V_7_0_0, Version.CURRENT).     final Settings settings = Settings.builder().put("index.analysis.analyzer.my_standard.tokenizer", "standard").put("index.analysis.analyzer.my_standard.filter", "standard").put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).put(IndexMetaData.SETTING_VERSION_CREATED, version).build().     IndexAnalyzers analyzers = getIndexAnalyzers(settings).     IllegalArgumentException exc = expectThrows(IllegalArgumentException.class, () -> analyzers.get("my_standard").tokenStream("", "")).     assertThat(exc.getMessage(), equalTo("The [standard] token filter has been removed.")). }
false;public;0;11;;@Override public List<PreConfiguredCharFilter> getPreConfiguredCharFilters() {     return Arrays.asList(PreConfiguredCharFilter.singleton("no_version", noVersionSupportsMultiTerm, tokenStream -> new AppendCharFilter(tokenStream, "no_version")), PreConfiguredCharFilter.luceneVersion("lucene_version", luceneVersionSupportsMultiTerm, (tokenStream, luceneVersion) -> new AppendCharFilter(tokenStream, luceneVersion.toString())), PreConfiguredCharFilter.elasticsearchVersion("elasticsearch_version", elasticsearchVersionSupportsMultiTerm, (tokenStream, esVersion) -> new AppendCharFilter(tokenStream, esVersion.toString()))). }
false;public;0;6;;@Override public Map<String, AnalysisProvider<TokenizerFactory>> getTokenizers() {     // Need mock keyword tokenizer here, because alpha / beta versions are broken up by the dash.     return singletonMap("keyword", (indexSettings, environment, name, settings) -> () -> new MockTokenizer(MockTokenizer.KEYWORD, false)). }
true;public;0;47;/**  * Tests that plugins can register pre-configured char filters that vary in behavior based on Elasticsearch version, Lucene version,  * and that do not vary based on version at all.  */ ;/**  * Tests that plugins can register pre-configured char filters that vary in behavior based on Elasticsearch version, Lucene version,  * and that do not vary based on version at all.  */ public void testPluginPreConfiguredCharFilters() throws IOException {     boolean noVersionSupportsMultiTerm = randomBoolean().     boolean luceneVersionSupportsMultiTerm = randomBoolean().     boolean elasticsearchVersionSupportsMultiTerm = randomBoolean().     AnalysisRegistry registry = new AnalysisModule(TestEnvironment.newEnvironment(emptyNodeSettings), singletonList(new AnalysisPlugin() {          @Override         public List<PreConfiguredCharFilter> getPreConfiguredCharFilters() {             return Arrays.asList(PreConfiguredCharFilter.singleton("no_version", noVersionSupportsMultiTerm, tokenStream -> new AppendCharFilter(tokenStream, "no_version")), PreConfiguredCharFilter.luceneVersion("lucene_version", luceneVersionSupportsMultiTerm, (tokenStream, luceneVersion) -> new AppendCharFilter(tokenStream, luceneVersion.toString())), PreConfiguredCharFilter.elasticsearchVersion("elasticsearch_version", elasticsearchVersionSupportsMultiTerm, (tokenStream, esVersion) -> new AppendCharFilter(tokenStream, esVersion.toString()))).         }          @Override         public Map<String, AnalysisProvider<TokenizerFactory>> getTokenizers() {             // Need mock keyword tokenizer here, because alpha / beta versions are broken up by the dash.             return singletonMap("keyword", (indexSettings, environment, name, settings) -> () -> new MockTokenizer(MockTokenizer.KEYWORD, false)).         }     })).getAnalysisRegistry().     Version version = VersionUtils.randomVersion(random()).     IndexAnalyzers analyzers = getIndexAnalyzers(registry, Settings.builder().put("index.analysis.analyzer.no_version.tokenizer", "keyword").put("index.analysis.analyzer.no_version.char_filter", "no_version").put("index.analysis.analyzer.lucene_version.tokenizer", "keyword").put("index.analysis.analyzer.lucene_version.char_filter", "lucene_version").put("index.analysis.analyzer.elasticsearch_version.tokenizer", "keyword").put("index.analysis.analyzer.elasticsearch_version.char_filter", "elasticsearch_version").put(IndexMetaData.SETTING_VERSION_CREATED, version).build()).     assertTokenStreamContents(analyzers.get("no_version").tokenStream("", "test"), new String[] { "testno_version" }).     assertTokenStreamContents(analyzers.get("lucene_version").tokenStream("", "test"), new String[] { "test" + version.luceneVersion }).     assertTokenStreamContents(analyzers.get("elasticsearch_version").tokenStream("", "test"), new String[] { "test" + version }).     assertEquals("test" + (noVersionSupportsMultiTerm ? "no_version" : ""), analyzers.get("no_version").normalize("", "test").utf8ToString()).     assertEquals("test" + (luceneVersionSupportsMultiTerm ? version.luceneVersion.toString() : ""), analyzers.get("lucene_version").normalize("", "test").utf8ToString()).     assertEquals("test" + (elasticsearchVersionSupportsMultiTerm ? version.toString() : ""), analyzers.get("elasticsearch_version").normalize("", "test").utf8ToString()). }
false;public;0;11;;@Override public List<PreConfiguredTokenFilter> getPreConfiguredTokenFilters() {     return Arrays.asList(PreConfiguredTokenFilter.singleton("no_version", noVersionSupportsMultiTerm, tokenStream -> new AppendTokenFilter(tokenStream, "no_version")), PreConfiguredTokenFilter.luceneVersion("lucene_version", luceneVersionSupportsMultiTerm, (tokenStream, luceneVersion) -> new AppendTokenFilter(tokenStream, luceneVersion.toString())), PreConfiguredTokenFilter.elasticsearchVersion("elasticsearch_version", elasticsearchVersionSupportsMultiTerm, (tokenStream, esVersion) -> new AppendTokenFilter(tokenStream, esVersion.toString()))). }
true;public;0;40;/**  * Tests that plugins can register pre-configured token filters that vary in behavior based on Elasticsearch version, Lucene version,  * and that do not vary based on version at all.  */ ;/**  * Tests that plugins can register pre-configured token filters that vary in behavior based on Elasticsearch version, Lucene version,  * and that do not vary based on version at all.  */ public void testPluginPreConfiguredTokenFilters() throws IOException {     boolean noVersionSupportsMultiTerm = randomBoolean().     boolean luceneVersionSupportsMultiTerm = randomBoolean().     boolean elasticsearchVersionSupportsMultiTerm = randomBoolean().     AnalysisRegistry registry = new AnalysisModule(TestEnvironment.newEnvironment(emptyNodeSettings), singletonList(new AnalysisPlugin() {          @Override         public List<PreConfiguredTokenFilter> getPreConfiguredTokenFilters() {             return Arrays.asList(PreConfiguredTokenFilter.singleton("no_version", noVersionSupportsMultiTerm, tokenStream -> new AppendTokenFilter(tokenStream, "no_version")), PreConfiguredTokenFilter.luceneVersion("lucene_version", luceneVersionSupportsMultiTerm, (tokenStream, luceneVersion) -> new AppendTokenFilter(tokenStream, luceneVersion.toString())), PreConfiguredTokenFilter.elasticsearchVersion("elasticsearch_version", elasticsearchVersionSupportsMultiTerm, (tokenStream, esVersion) -> new AppendTokenFilter(tokenStream, esVersion.toString()))).         }     })).getAnalysisRegistry().     Version version = VersionUtils.randomVersion(random()).     IndexAnalyzers analyzers = getIndexAnalyzers(registry, Settings.builder().put("index.analysis.analyzer.no_version.tokenizer", "standard").put("index.analysis.analyzer.no_version.filter", "no_version").put("index.analysis.analyzer.lucene_version.tokenizer", "standard").put("index.analysis.analyzer.lucene_version.filter", "lucene_version").put("index.analysis.analyzer.elasticsearch_version.tokenizer", "standard").put("index.analysis.analyzer.elasticsearch_version.filter", "elasticsearch_version").put(IndexMetaData.SETTING_VERSION_CREATED, version).build()).     assertTokenStreamContents(analyzers.get("no_version").tokenStream("", "test"), new String[] { "testno_version" }).     assertTokenStreamContents(analyzers.get("lucene_version").tokenStream("", "test"), new String[] { "test" + version.luceneVersion }).     assertTokenStreamContents(analyzers.get("elasticsearch_version").tokenStream("", "test"), new String[] { "test" + version }).     assertEquals("test" + (noVersionSupportsMultiTerm ? "no_version" : ""), analyzers.get("no_version").normalize("", "test").utf8ToString()).     assertEquals("test" + (luceneVersionSupportsMultiTerm ? version.luceneVersion.toString() : ""), analyzers.get("lucene_version").normalize("", "test").utf8ToString()).     assertEquals("test" + (elasticsearchVersionSupportsMultiTerm ? version.toString() : ""), analyzers.get("elasticsearch_version").normalize("", "test").utf8ToString()). }
false;public;0;12;;@Override public boolean incrementToken() throws IOException {     if (read) {         return false.     }     clearAttributes().     read = true.     term.resizeBuffer(chars.length).     System.arraycopy(chars, 0, term.buffer(), 0, chars.length).     term.setLength(chars.length).     return true. }
false;public;0;5;;@Override public void reset() throws IOException {     super.reset().     read = false. }
false;public;0;10;;@Override public List<PreConfiguredTokenizer> getPreConfiguredTokenizers() {     return Arrays.asList(PreConfiguredTokenizer.singleton("no_version", () -> new FixedTokenizer("no_version")), PreConfiguredTokenizer.luceneVersion("lucene_version", luceneVersion -> new FixedTokenizer(luceneVersion.toString())), PreConfiguredTokenizer.elasticsearchVersion("elasticsearch_version", esVersion -> new FixedTokenizer(esVersion.toString()))). }
true;public;0;64;/**  * Tests that plugins can register pre-configured token filters that vary in behavior based on Elasticsearch version, Lucene version,  * and that do not vary based on version at all.  */ ;/**  * Tests that plugins can register pre-configured token filters that vary in behavior based on Elasticsearch version, Lucene version,  * and that do not vary based on version at all.  */ public void testPluginPreConfiguredTokenizers() throws IOException {     // Simple tokenizer that always spits out a single token with some preconfigured characters     final class FixedTokenizer extends Tokenizer {          private final CharTermAttribute term = addAttribute(CharTermAttribute.class).          private final char[] chars.          private boolean read = false.          protected FixedTokenizer(String chars) {             this.chars = chars.toCharArray().         }          @Override         public boolean incrementToken() throws IOException {             if (read) {                 return false.             }             clearAttributes().             read = true.             term.resizeBuffer(chars.length).             System.arraycopy(chars, 0, term.buffer(), 0, chars.length).             term.setLength(chars.length).             return true.         }          @Override         public void reset() throws IOException {             super.reset().             read = false.         }     }     AnalysisRegistry registry = new AnalysisModule(TestEnvironment.newEnvironment(emptyNodeSettings), singletonList(new AnalysisPlugin() {          @Override         public List<PreConfiguredTokenizer> getPreConfiguredTokenizers() {             return Arrays.asList(PreConfiguredTokenizer.singleton("no_version", () -> new FixedTokenizer("no_version")), PreConfiguredTokenizer.luceneVersion("lucene_version", luceneVersion -> new FixedTokenizer(luceneVersion.toString())), PreConfiguredTokenizer.elasticsearchVersion("elasticsearch_version", esVersion -> new FixedTokenizer(esVersion.toString()))).         }     })).getAnalysisRegistry().     Version version = VersionUtils.randomVersion(random()).     IndexAnalyzers analyzers = getIndexAnalyzers(registry, Settings.builder().put("index.analysis.analyzer.no_version.tokenizer", "no_version").put("index.analysis.analyzer.lucene_version.tokenizer", "lucene_version").put("index.analysis.analyzer.elasticsearch_version.tokenizer", "elasticsearch_version").put(IndexMetaData.SETTING_VERSION_CREATED, version).build()).     assertTokenStreamContents(analyzers.get("no_version").tokenStream("", "test"), new String[] { "no_version" }).     assertTokenStreamContents(analyzers.get("lucene_version").tokenStream("", "test"), new String[] { version.luceneVersion.toString() }).     assertTokenStreamContents(analyzers.get("elasticsearch_version").tokenStream("", "test"), new String[] { version.toString() }). // These are current broken by https://github.com/elastic/elasticsearch/issues/24752 // assertEquals("test" + (noVersionSupportsMultiTerm ? "no_version" : ""), // analyzers.get("no_version").normalize("", "test").utf8ToString()). // assertEquals("test" + (luceneVersionSupportsMultiTerm ? version.luceneVersion.toString() : ""), // analyzers.get("lucene_version").normalize("", "test").utf8ToString()). // assertEquals("test" + (elasticsearchVersionSupportsMultiTerm ? version.toString() : ""), // analyzers.get("elasticsearch_version").normalize("", "test").utf8ToString()). }
false;public;0;4;;@Override public Map<String, Dictionary> getHunspellDictionaries() {     return singletonMap("foo", dictionary). }
false;public;0;20;;public void testRegisterHunspellDictionary() throws Exception {     Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build().     Environment environment = TestEnvironment.newEnvironment(settings).     InputStream aff = getClass().getResourceAsStream("/indices/analyze/conf_dir/hunspell/en_US/en_US.aff").     InputStream dic = getClass().getResourceAsStream("/indices/analyze/conf_dir/hunspell/en_US/en_US.dic").     Dictionary dictionary.     try (Directory tmp = new SimpleFSDirectory(environment.tmpFile())) {         dictionary = new Dictionary(tmp, "hunspell", aff, dic).     }     AnalysisModule module = new AnalysisModule(environment, singletonList(new AnalysisPlugin() {          @Override         public Map<String, Dictionary> getHunspellDictionaries() {             return singletonMap("foo", dictionary).         }     })).     assertSame(dictionary, module.getHunspellService().getDictionary("foo")). }
false;static;2;7;;static Reader append(Reader input, String appendMe) {     try {         return new StringReader(Streams.copyToString(input) + appendMe).     } catch (IOException e) {         throw new UncheckedIOException(e).     } }
false;protected;1;4;;@Override protected int correct(int currentOff) {     return currentOff. }
false;public;3;4;;@Override public int read(char[] cbuf, int off, int len) throws IOException {     return input.read(cbuf, off, len). }
false;public;0;4;;@Override public String name() {     return suffix. }
false;public;1;4;;@Override public TokenStream create(TokenStream tokenStream) {     return new AppendTokenFilter(tokenStream, suffix). }
false;public,static;1;13;;public static TokenFilterFactory factoryForSuffix(String suffix) {     return new TokenFilterFactory() {          @Override         public String name() {             return suffix.         }          @Override         public TokenStream create(TokenStream tokenStream) {             return new AppendTokenFilter(tokenStream, suffix).         }     }. }
false;public;0;10;;@Override public boolean incrementToken() throws IOException {     if (false == input.incrementToken()) {         return false.     }     term.resizeBuffer(term.length() + appendMe.length).     System.arraycopy(appendMe, 0, term.buffer(), term.length(), appendMe.length).     term.setLength(term.length() + appendMe.length).     return true. }
