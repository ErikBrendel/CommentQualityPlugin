commented;modifiers;parameterAmount;loc;comment;code
false;public;0;19;;public void testSimple() throws IOException {     Query query = new MatchAllDocsQuery().     DateHistogramAggregationBuilder aggBuilder = new DateHistogramAggregationBuilder("histo").     aggBuilder.dateHistogramInterval(DateHistogramInterval.DAY).field(HISTO_FIELD).     aggBuilder.subAggregation(new AvgAggregationBuilder("the_avg").field(VALUE_FIELD)).     aggBuilder.subAggregation(new CumulativeSumPipelineAggregationBuilder("cusum", "the_avg")).     executeTestCase(query, aggBuilder, histogram -> {         assertEquals(10, ((Histogram) histogram).getBuckets().size()).         List<? extends Histogram.Bucket> buckets = ((Histogram) histogram).getBuckets().         double sum = 0.0.         for (Histogram.Bucket bucket : buckets) {             sum += ((InternalAvg) (bucket.getAggregations().get("the_avg"))).value().             assertThat(((InternalSimpleValue) (bucket.getAggregations().get("cusum"))).value(), equalTo(sum)).             assertTrue(AggregationInspectionHelper.hasValue(((InternalAvg) (bucket.getAggregations().get("the_avg"))))).         }     }). }
true;public;0;27;/**  * First value from a derivative is null, so this makes sure the cusum can handle that  */ ;/**  * First value from a derivative is null, so this makes sure the cusum can handle that  */ public void testDerivative() throws IOException {     Query query = new MatchAllDocsQuery().     DateHistogramAggregationBuilder aggBuilder = new DateHistogramAggregationBuilder("histo").     aggBuilder.dateHistogramInterval(DateHistogramInterval.DAY).field(HISTO_FIELD).     aggBuilder.subAggregation(new AvgAggregationBuilder("the_avg").field(VALUE_FIELD)).     aggBuilder.subAggregation(new DerivativePipelineAggregationBuilder("the_deriv", "the_avg")).     aggBuilder.subAggregation(new CumulativeSumPipelineAggregationBuilder("cusum", "the_deriv")).     executeTestCase(query, aggBuilder, histogram -> {         assertEquals(10, ((Histogram) histogram).getBuckets().size()).         List<? extends Histogram.Bucket> buckets = ((Histogram) histogram).getBuckets().         double sum = 0.0.         for (int i = 0. i < buckets.size(). i++) {             if (i == 0) {                 assertThat(((InternalSimpleValue) (buckets.get(i).getAggregations().get("cusum"))).value(), equalTo(0.0)).                 assertTrue(AggregationInspectionHelper.hasValue(((InternalSimpleValue) (buckets.get(i).getAggregations().get("cusum"))))).             } else {                 sum += 1.0.                 assertThat(((InternalSimpleValue) (buckets.get(i).getAggregations().get("cusum"))).value(), equalTo(sum)).                 assertTrue(AggregationInspectionHelper.hasValue(((InternalSimpleValue) (buckets.get(i).getAggregations().get("cusum"))))).             }         }     }). }
false;public;0;18;;public void testCount() throws IOException {     Query query = new MatchAllDocsQuery().     DateHistogramAggregationBuilder aggBuilder = new DateHistogramAggregationBuilder("histo").     aggBuilder.dateHistogramInterval(DateHistogramInterval.DAY).field(HISTO_FIELD).     aggBuilder.subAggregation(new CumulativeSumPipelineAggregationBuilder("cusum", "_count")).     executeTestCase(query, aggBuilder, histogram -> {         assertEquals(10, ((Histogram) histogram).getBuckets().size()).         List<? extends Histogram.Bucket> buckets = ((Histogram) histogram).getBuckets().         double sum = 1.0.         for (Histogram.Bucket bucket : buckets) {             assertThat(((InternalSimpleValue) (bucket.getAggregations().get("cusum"))).value(), equalTo(sum)).             assertTrue(AggregationInspectionHelper.hasValue(((InternalSimpleValue) (bucket.getAggregations().get("cusum"))))).             sum += 1.0.         }     }). }
false;public;0;49;;public void testDocCount() throws IOException {     Query query = new MatchAllDocsQuery().     int numDocs = randomIntBetween(6, 20).     int interval = randomIntBetween(2, 5).     int minRandomValue = 0.     int maxRandomValue = 20.     int numValueBuckets = ((maxRandomValue - minRandomValue) / interval) + 1.     long[] valueCounts = new long[numValueBuckets].     HistogramAggregationBuilder aggBuilder = new HistogramAggregationBuilder("histo").field(VALUE_FIELD).interval(interval).extendedBounds(minRandomValue, maxRandomValue).     aggBuilder.subAggregation(new CumulativeSumPipelineAggregationBuilder("cusum", "_count")).     executeTestCase(query, aggBuilder, histogram -> {         List<? extends Histogram.Bucket> buckets = ((Histogram) histogram).getBuckets().         assertThat(buckets.size(), equalTo(numValueBuckets)).         double sum = 0.         for (int i = 0. i < numValueBuckets. ++i) {             Histogram.Bucket bucket = buckets.get(i).             assertThat(bucket, notNullValue()).             assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) i * interval)).             assertThat(bucket.getDocCount(), equalTo(valueCounts[i])).             sum += bucket.getDocCount().             InternalSimpleValue cumulativeSumValue = bucket.getAggregations().get("cusum").             assertThat(cumulativeSumValue, notNullValue()).             assertThat(cumulativeSumValue.getName(), equalTo("cusum")).             assertThat(cumulativeSumValue.value(), equalTo(sum)).         }     }, indexWriter -> {         Document document = new Document().         for (int i = 0. i < numDocs. i++) {             int fieldValue = randomIntBetween(minRandomValue, maxRandomValue).             document.add(new NumericDocValuesField(VALUE_FIELD, fieldValue)).             final int bucket = (fieldValue / interval).             valueCounts[bucket]++.             indexWriter.addDocument(document).             document.clear().         }     }). }
false;public;0;52;;public void testMetric() throws IOException {     Query query = new MatchAllDocsQuery().     int numDocs = randomIntBetween(6, 20).     int interval = randomIntBetween(2, 5).     int minRandomValue = 0.     int maxRandomValue = 20.     int numValueBuckets = ((maxRandomValue - minRandomValue) / interval) + 1.     long[] valueCounts = new long[numValueBuckets].     HistogramAggregationBuilder aggBuilder = new HistogramAggregationBuilder("histo").field(VALUE_FIELD).interval(interval).extendedBounds(minRandomValue, maxRandomValue).     aggBuilder.subAggregation(new SumAggregationBuilder("sum").field(VALUE_FIELD)).     aggBuilder.subAggregation(new CumulativeSumPipelineAggregationBuilder("cusum", "sum")).     executeTestCase(query, aggBuilder, histogram -> {         List<? extends Histogram.Bucket> buckets = ((Histogram) histogram).getBuckets().         assertThat(buckets.size(), equalTo(numValueBuckets)).         double bucketSum = 0.         for (int i = 0. i < numValueBuckets. ++i) {             Histogram.Bucket bucket = buckets.get(i).             assertThat(bucket, notNullValue()).             assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) i * interval)).             Sum sum = bucket.getAggregations().get("sum").             assertThat(sum, notNullValue()).             bucketSum += sum.value().             InternalSimpleValue sumBucketValue = bucket.getAggregations().get("cusum").             assertThat(sumBucketValue, notNullValue()).             assertThat(sumBucketValue.getName(), equalTo("cusum")).             assertThat(sumBucketValue.value(), equalTo(bucketSum)).         }     }, indexWriter -> {         Document document = new Document().         for (int i = 0. i < numDocs. i++) {             int fieldValue = randomIntBetween(minRandomValue, maxRandomValue).             document.add(new NumericDocValuesField(VALUE_FIELD, fieldValue)).             final int bucket = (fieldValue / interval).             valueCounts[bucket]++.             indexWriter.addDocument(document).             document.clear().         }     }). }
false;public;0;37;;public void testNoBuckets() throws IOException {     int numDocs = randomIntBetween(6, 20).     int interval = randomIntBetween(2, 5).     int minRandomValue = 0.     int maxRandomValue = 20.     int numValueBuckets = ((maxRandomValue - minRandomValue) / interval) + 1.     long[] valueCounts = new long[numValueBuckets].     Query query = new MatchNoDocsQuery().     HistogramAggregationBuilder aggBuilder = new HistogramAggregationBuilder("histo").field(VALUE_FIELD).interval(interval).     aggBuilder.subAggregation(new SumAggregationBuilder("sum").field(VALUE_FIELD)).     aggBuilder.subAggregation(new CumulativeSumPipelineAggregationBuilder("cusum", "sum")).     executeTestCase(query, aggBuilder, histogram -> {         List<? extends Histogram.Bucket> buckets = ((Histogram) histogram).getBuckets().         assertThat(buckets.size(), equalTo(0)).     }, indexWriter -> {         Document document = new Document().         for (int i = 0. i < numDocs. i++) {             int fieldValue = randomIntBetween(minRandomValue, maxRandomValue).             document.add(new NumericDocValuesField(VALUE_FIELD, fieldValue)).             final int bucket = (fieldValue / interval).             valueCounts[bucket]++.             indexWriter.addDocument(document).             document.clear().         }     }). }
true;public;0;7;/**  * The validation should verify the parent aggregation is allowed.  */ ;/**  * The validation should verify the parent aggregation is allowed.  */ public void testValidate() throws IOException {     final Set<PipelineAggregationBuilder> aggBuilders = new HashSet<>().     aggBuilders.add(new CumulativeSumPipelineAggregationBuilder("cusum", "sum")).     final CumulativeSumPipelineAggregationBuilder builder = new CumulativeSumPipelineAggregationBuilder("name", "valid").     builder.validate(PipelineAggregationHelperTests.getRandomSequentiallyOrderedParentAgg(), Collections.emptySet(), aggBuilders). }
true;public;0;11;/**  * The validation should throw an IllegalArgumentException, since parent  * aggregation is not a type of HistogramAggregatorFactory,  * DateHistogramAggregatorFactory or AutoDateHistogramAggregatorFactory.  */ ;/**  * The validation should throw an IllegalArgumentException, since parent  * aggregation is not a type of HistogramAggregatorFactory,  * DateHistogramAggregatorFactory or AutoDateHistogramAggregatorFactory.  */ public void testValidateException() throws IOException {     final Set<PipelineAggregationBuilder> aggBuilders = new HashSet<>().     aggBuilders.add(new CumulativeSumPipelineAggregationBuilder("cusum", "sum")).     TestAggregatorFactory parentFactory = TestAggregatorFactory.createInstance().     final CumulativeSumPipelineAggregationBuilder builder = new CumulativeSumPipelineAggregationBuilder("name", "invalid_agg>metric").     IllegalStateException ex = expectThrows(IllegalStateException.class, () -> builder.validate(parentFactory, Collections.emptySet(), aggBuilders)).     assertEquals("cumulative_sum aggregation [name] must have a histogram, date_histogram or auto_date_histogram as parent", ex.getMessage()). }
false;private;3;18;;private void executeTestCase(Query query, AggregationBuilder aggBuilder, Consumer<InternalAggregation> verify) throws IOException {     executeTestCase(query, aggBuilder, verify, indexWriter -> {         Document document = new Document().         int counter = 0.         for (String date : datasetTimes) {             if (frequently()) {                 indexWriter.commit().             }             long instant = asLong(date).             document.add(new SortedNumericDocValuesField(HISTO_FIELD, instant)).             document.add(new NumericDocValuesField(VALUE_FIELD, datasetValues.get(counter))).             indexWriter.addDocument(document).             document.clear().             counter += 1.         }     }). }
false;private;4;26;;private void executeTestCase(Query query, AggregationBuilder aggBuilder, Consumer<InternalAggregation> verify, CheckedConsumer<RandomIndexWriter, IOException> setup) throws IOException {     try (Directory directory = newDirectory()) {         try (RandomIndexWriter indexWriter = new RandomIndexWriter(random(), directory)) {             setup.accept(indexWriter).         }         try (IndexReader indexReader = DirectoryReader.open(directory)) {             IndexSearcher indexSearcher = newSearcher(indexReader, true, true).             DateFieldMapper.Builder builder = new DateFieldMapper.Builder("_name").             DateFieldMapper.DateFieldType fieldType = builder.fieldType().             fieldType.setHasDocValues(true).             fieldType.setName(HISTO_FIELD).             MappedFieldType valueFieldType = new NumberFieldMapper.NumberFieldType(NumberFieldMapper.NumberType.LONG).             valueFieldType.setHasDocValues(true).             valueFieldType.setName("value_field").             InternalAggregation histogram.             histogram = searchAndReduce(indexSearcher, query, aggBuilder, new MappedFieldType[] { fieldType, valueFieldType }).             verify.accept(histogram).         }     } }
false;private,static;1;3;;private static long asLong(String dateTime) {     return DateFormatters.from(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse(dateTime)).toInstant().toEpochMilli(). }
