commented;modifiers;parameterAmount;loc;comment;code
false;public;0;9;;@Override public boolean incrementToken() throws IOException {     if (incremented) {         return false.     }     term.setLength(0).append(output).     incremented = true.     return true. }
false;protected;1;18;;@Override protected TokenStreamComponents createComponents(String fieldName) {     Tokenizer tokenizer = new Tokenizer() {          boolean incremented = false.          CharTermAttribute term = addAttribute(CharTermAttribute.class).          @Override         public boolean incrementToken() throws IOException {             if (incremented) {                 return false.             }             term.setLength(0).append(output).             incremented = true.             return true.         }     }.     return new TokenStreamComponents(tokenizer). }
false;public;0;4;;@Override public MappedFieldType clone() {     return new FakeFieldType(this). }
false;public;0;4;;@Override public String typeName() {     return "fake". }
false;public;1;8;;@Override public Query existsQuery(QueryShardContext context) {     if (hasDocValues()) {         return new DocValuesFieldExistsQuery(name()).     } else {         return new TermQuery(new Term(FieldNamesFieldMapper.NAME, name())).     } }
false;protected;2;3;;@Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { }
false;protected;0;4;;@Override protected String contentType() {     return null. }
false;public;0;31;;public void testAnalyzers() throws IOException {     FakeFieldType fieldType1 = new FakeFieldType().     fieldType1.setName("field1").     fieldType1.setIndexAnalyzer(new NamedAnalyzer("foo", AnalyzerScope.INDEX, new FakeAnalyzer("index"))).     fieldType1.setSearchAnalyzer(new NamedAnalyzer("bar", AnalyzerScope.INDEX, new FakeAnalyzer("search"))).     fieldType1.setSearchQuoteAnalyzer(new NamedAnalyzer("baz", AnalyzerScope.INDEX, new FakeAnalyzer("search_quote"))).     FieldMapper fieldMapper1 = new FakeFieldMapper("field1", fieldType1).     FakeFieldType fieldType2 = new FakeFieldType().     fieldType2.setName("field2").     FieldMapper fieldMapper2 = new FakeFieldMapper("field2", fieldType2).     Analyzer defaultIndex = new FakeAnalyzer("default_index").     Analyzer defaultSearch = new FakeAnalyzer("default_search").     Analyzer defaultSearchQuote = new FakeAnalyzer("default_search_quote").     DocumentFieldMappers documentFieldMappers = new DocumentFieldMappers(Arrays.asList(fieldMapper1, fieldMapper2), Collections.emptyList(), defaultIndex, defaultSearch, defaultSearchQuote).     assertAnalyzes(documentFieldMappers.indexAnalyzer(), "field1", "index").     assertAnalyzes(documentFieldMappers.searchAnalyzer(), "field1", "search").     assertAnalyzes(documentFieldMappers.searchQuoteAnalyzer(), "field1", "search_quote").     assertAnalyzes(documentFieldMappers.indexAnalyzer(), "field2", "default_index").     assertAnalyzes(documentFieldMappers.searchAnalyzer(), "field2", "default_search").     assertAnalyzes(documentFieldMappers.searchQuoteAnalyzer(), "field2", "default_search_quote"). }
false;private;3;7;;private void assertAnalyzes(Analyzer analyzer, String field, String output) throws IOException {     try (TokenStream tok = analyzer.tokenStream(field, new StringReader(""))) {         CharTermAttribute term = tok.addAttribute(CharTermAttribute.class).         assertTrue(tok.incrementToken()).         assertEquals(output, term.toString()).     } }
