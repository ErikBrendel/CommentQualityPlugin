commented;modifiers;parameterAmount;loc;comment;code
false;public;0;4;;@Before public void resetNodeIndexBeforeEachTest() {     nextNodeIndex.set(0). }
false;public;0;7;;@After public void closeNodeEnvironmentsAfterEachTest() {     for (NodeEnvironment nodeEnvironment : nodeEnvironments) {         nodeEnvironment.close().     }     nodeEnvironments.clear(). }
false;public;0;4;;@Before public void resetPortCounterBeforeEachTest() {     resetPortCounter(). }
true;public;0;17;// check that runRandomly leads to reproducible results ;// check that runRandomly leads to reproducible results public void testRepeatableTests() throws Exception {     final Callable<Long> test = () -> {         resetNodeIndexBeforeEachTest().         final Cluster cluster = new Cluster(randomIntBetween(1, 5)).         cluster.runRandomly().         final long afterRunRandomly = value(cluster.getAnyNode().getLastAppliedClusterState()).         cluster.stabilise().         final long afterStabilisation = value(cluster.getAnyNode().getLastAppliedClusterState()).         return afterRunRandomly ^ afterStabilisation.     }.     final long seed = randomLong().     logger.info("First run with seed [{}]", seed).     final long result1 = RandomizedContext.current().runWithPrivateRandomness(seed, test).     logger.info("Second run with seed [{}]", seed).     final long result2 = RandomizedContext.current().runWithPrivateRandomness(seed, test).     assertEquals(result1, result2). }
false;public;0;18;;public void testCanUpdateClusterStateAfterStabilisation() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     long finalValue = randomLong().     logger.info("--> submitting value [{}] to [{}]", finalValue, leader).     leader.submitValue(finalValue).     cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     for (final ClusterNode clusterNode : cluster.clusterNodes) {         final String nodeId = clusterNode.getId().         final ClusterState appliedState = clusterNode.getLastAppliedClusterState().         assertThat(nodeId + " has the applied value", value(appliedState), is(finalValue)).     } }
false;public;0;8;;public void testDoesNotElectNonMasterNode() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5), false).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     assertTrue(leader.localNode.isMasterNode()). }
false;public;0;11;;public void testNodesJoinAfterStableCluster() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5)).     cluster.runRandomly().     cluster.stabilise().     final long currentTerm = cluster.getAnyLeader().coordinator.getCurrentTerm().     cluster.addNodesAndStabilise(randomIntBetween(1, 2)).     final long newTerm = cluster.getAnyLeader().coordinator.getCurrentTerm().     assertEquals(currentTerm, newTerm). }
false;public;0;28;;public void testExpandsConfigurationWhenGrowingFromOneNodeToThreeButDoesNotShrink() {     final Cluster cluster = new Cluster(1).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     cluster.addNodesAndStabilise(2).     {         assertThat(leader.coordinator.getMode(), is(Mode.LEADER)).         final VotingConfiguration lastCommittedConfiguration = leader.getLastAppliedClusterState().getLastCommittedConfiguration().         assertThat(lastCommittedConfiguration + " should be all nodes", lastCommittedConfiguration.getNodeIds(), equalTo(cluster.clusterNodes.stream().map(ClusterNode::getId).collect(Collectors.toSet()))).     }     final ClusterNode disconnect1 = cluster.getAnyNode().     logger.info("--> disconnecting {}", disconnect1).     disconnect1.disconnect().     cluster.stabilise().     {         final ClusterNode newLeader = cluster.getAnyLeader().         final VotingConfiguration lastCommittedConfiguration = newLeader.getLastAppliedClusterState().getLastCommittedConfiguration().         assertThat(lastCommittedConfiguration + " should be all nodes", lastCommittedConfiguration.getNodeIds(), equalTo(cluster.clusterNodes.stream().map(ClusterNode::getId).collect(Collectors.toSet()))).     } }
false;public;0;79;;public void testExpandsConfigurationWhenGrowingFromThreeToFiveNodesAndShrinksBackToThreeOnFailure() {     final Cluster cluster = new Cluster(3).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     logger.info("setting auto-shrink reconfiguration to true").     leader.submitSetAutoShrinkVotingConfiguration(true).     cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     assertTrue(CLUSTER_AUTO_SHRINK_VOTING_CONFIGURATION.get(leader.getLastAppliedClusterState().metaData().settings())).     cluster.addNodesAndStabilise(2).     {         assertThat(leader.coordinator.getMode(), is(Mode.LEADER)).         final VotingConfiguration lastCommittedConfiguration = leader.getLastAppliedClusterState().getLastCommittedConfiguration().         assertThat(lastCommittedConfiguration + " should be all nodes", lastCommittedConfiguration.getNodeIds(), equalTo(cluster.clusterNodes.stream().map(ClusterNode::getId).collect(Collectors.toSet()))).     }     final ClusterNode disconnect1 = cluster.getAnyNode().     final ClusterNode disconnect2 = cluster.getAnyNodeExcept(disconnect1).     logger.info("--> disconnecting {} and {}", disconnect1, disconnect2).     disconnect1.disconnect().     disconnect2.disconnect().     cluster.stabilise().     {         final ClusterNode newLeader = cluster.getAnyLeader().         final VotingConfiguration lastCommittedConfiguration = newLeader.getLastAppliedClusterState().getLastCommittedConfiguration().         assertThat(lastCommittedConfiguration + " should be 3 nodes", lastCommittedConfiguration.getNodeIds().size(), equalTo(3)).         assertFalse(lastCommittedConfiguration.getNodeIds().contains(disconnect1.getId())).         assertFalse(lastCommittedConfiguration.getNodeIds().contains(disconnect2.getId())).     }     // we still tolerate the loss of one more node here     final ClusterNode disconnect3 = cluster.getAnyNodeExcept(disconnect1, disconnect2).     logger.info("--> disconnecting {}", disconnect3).     disconnect3.disconnect().     cluster.stabilise().     {         final ClusterNode newLeader = cluster.getAnyLeader().         final VotingConfiguration lastCommittedConfiguration = newLeader.getLastAppliedClusterState().getLastCommittedConfiguration().         assertThat(lastCommittedConfiguration + " should be 3 nodes", lastCommittedConfiguration.getNodeIds().size(), equalTo(3)).         assertFalse(lastCommittedConfiguration.getNodeIds().contains(disconnect1.getId())).         assertFalse(lastCommittedConfiguration.getNodeIds().contains(disconnect2.getId())).         assertTrue(lastCommittedConfiguration.getNodeIds().contains(disconnect3.getId())).     }     // however we do not tolerate the loss of yet another one     final ClusterNode disconnect4 = cluster.getAnyNodeExcept(disconnect1, disconnect2, disconnect3).     logger.info("--> disconnecting {}", disconnect4).     disconnect4.disconnect().     cluster.runFor(DEFAULT_STABILISATION_TIME, "allowing time for fault detection").     for (final ClusterNode clusterNode : cluster.clusterNodes) {         assertThat(clusterNode.getId() + " should be a candidate", clusterNode.coordinator.getMode(), equalTo(Mode.CANDIDATE)).     }     // moreover we are still stuck even if two other nodes heal     logger.info("--> healing {} and {}", disconnect1, disconnect2).     disconnect1.heal().     disconnect2.heal().     cluster.runFor(DEFAULT_STABILISATION_TIME, "allowing time for fault detection").     for (final ClusterNode clusterNode : cluster.clusterNodes) {         assertThat(clusterNode.getId() + " should be a candidate", clusterNode.coordinator.getMode(), equalTo(Mode.CANDIDATE)).     }     // we require another node to heal to recover     final ClusterNode toHeal = randomBoolean() ? disconnect3 : disconnect4.     logger.info("--> healing {}", toHeal).     toHeal.heal().     cluster.stabilise(). }
false;public;0;41;;public void testCanShrinkFromFiveNodesToThree() {     final Cluster cluster = new Cluster(5).     cluster.runRandomly().     cluster.stabilise().     {         final ClusterNode leader = cluster.getAnyLeader().         logger.info("setting auto-shrink reconfiguration to false").         leader.submitSetAutoShrinkVotingConfiguration(false).         cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY).         assertFalse(CLUSTER_AUTO_SHRINK_VOTING_CONFIGURATION.get(leader.getLastAppliedClusterState().metaData().settings())).     }     final ClusterNode disconnect1 = cluster.getAnyNode().     final ClusterNode disconnect2 = cluster.getAnyNodeExcept(disconnect1).     logger.info("--> disconnecting {} and {}", disconnect1, disconnect2).     disconnect1.disconnect().     disconnect2.disconnect().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     {         final VotingConfiguration lastCommittedConfiguration = leader.getLastAppliedClusterState().getLastCommittedConfiguration().         assertThat(lastCommittedConfiguration + " should be all nodes", lastCommittedConfiguration.getNodeIds(), equalTo(cluster.clusterNodes.stream().map(ClusterNode::getId).collect(Collectors.toSet()))).     }     logger.info("setting auto-shrink reconfiguration to true").     leader.submitSetAutoShrinkVotingConfiguration(true).     // allow for a reconfiguration     cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY * 2).     assertTrue(CLUSTER_AUTO_SHRINK_VOTING_CONFIGURATION.get(leader.getLastAppliedClusterState().metaData().settings())).     {         final VotingConfiguration lastCommittedConfiguration = leader.getLastAppliedClusterState().getLastCommittedConfiguration().         assertThat(lastCommittedConfiguration + " should be 3 nodes", lastCommittedConfiguration.getNodeIds().size(), equalTo(3)).         assertFalse(lastCommittedConfiguration.getNodeIds().contains(disconnect1.getId())).         assertFalse(lastCommittedConfiguration.getNodeIds().contains(disconnect2.getId())).     } }
false;public;0;23;;public void testDoesNotShrinkConfigurationBelowThreeNodes() {     final Cluster cluster = new Cluster(3).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode disconnect1 = cluster.getAnyNode().     logger.info("--> disconnecting {}", disconnect1).     disconnect1.disconnect().     cluster.stabilise().     final ClusterNode disconnect2 = cluster.getAnyNodeExcept(disconnect1).     logger.info("--> disconnecting {}", disconnect2).     disconnect2.disconnect().     cluster.runFor(DEFAULT_STABILISATION_TIME, "allowing time for fault detection").     for (final ClusterNode clusterNode : cluster.clusterNodes) {         assertThat(clusterNode.getId() + " should be a candidate", clusterNode.coordinator.getMode(), equalTo(Mode.CANDIDATE)).     }     disconnect1.heal().     // would not work if disconnect1 were removed from the configuration     cluster.stabilise(). }
false;public;0;28;;public void testDoesNotShrinkConfigurationBelowFiveNodesIfAutoShrinkDisabled() {     final Cluster cluster = new Cluster(5).     cluster.runRandomly().     cluster.stabilise().     cluster.getAnyLeader().submitSetAutoShrinkVotingConfiguration(false).     cluster.stabilise(DEFAULT_ELECTION_DELAY).     final ClusterNode disconnect1 = cluster.getAnyNode().     final ClusterNode disconnect2 = cluster.getAnyNodeExcept(disconnect1).     logger.info("--> disconnecting {} and {}", disconnect1, disconnect2).     disconnect1.disconnect().     disconnect2.disconnect().     cluster.stabilise().     final ClusterNode disconnect3 = cluster.getAnyNodeExcept(disconnect1, disconnect2).     logger.info("--> disconnecting {}", disconnect3).     disconnect3.disconnect().     cluster.runFor(DEFAULT_STABILISATION_TIME, "allowing time for fault detection").     for (final ClusterNode clusterNode : cluster.clusterNodes) {         assertThat(clusterNode.getId() + " should be a candidate", clusterNode.coordinator.getMode(), equalTo(Mode.CANDIDATE)).     }     disconnect1.heal().     // would not work if disconnect1 were removed from the configuration     cluster.stabilise(). }
false;public;0;21;;public void testLeaderDisconnectionWithDisconnectEventDetectedQuickly() {     final Cluster cluster = new Cluster(randomIntBetween(3, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode originalLeader = cluster.getAnyLeader().     logger.info("--> disconnecting leader {}", originalLeader).     originalLeader.disconnect().     logger.info("--> followers get disconnect event for leader {} ", originalLeader).     cluster.getAllNodesExcept(originalLeader).forEach(cn -> cn.onDisconnectEventFrom(originalLeader)).     // turn leader into candidate, which stabilisation asserts at the end     cluster.getAllNodesExcept(originalLeader).forEach(cn -> originalLeader.onDisconnectEventFrom(cn)).     cluster.stabilise(// disconnect is scheduled     DEFAULT_DELAY_VARIABILITY + // then wait for a new election     DEFAULT_ELECTION_DELAY + // wait for the removal to be committed     DEFAULT_CLUSTER_STATE_UPDATE_DELAY + // then wait for the followup reconfiguration     DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     assertThat(cluster.getAnyLeader().getId(), not(equalTo(originalLeader.getId()))). }
false;public;0;35;;public void testLeaderDisconnectionWithoutDisconnectEventDetectedQuickly() {     final Cluster cluster = new Cluster(randomIntBetween(3, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode originalLeader = cluster.getAnyLeader().     logger.info("--> disconnecting leader {}", originalLeader).     originalLeader.disconnect().     cluster.stabilise(Math.max(// Each follower may have just sent a leader check, which receives no response     defaultMillis(LEADER_CHECK_TIMEOUT_SETTING) + // then wait for the follower to check the leader     defaultMillis(LEADER_CHECK_INTERVAL_SETTING) + // then wait for the exception response     DEFAULT_DELAY_VARIABILITY + // then wait for a new election     DEFAULT_ELECTION_DELAY, // ALSO the leader may have just sent a follower check, which receives no response     defaultMillis(FOLLOWER_CHECK_TIMEOUT_SETTING) + // wait for the leader to check its followers     defaultMillis(FOLLOWER_CHECK_INTERVAL_SETTING) + // then wait for the exception response     DEFAULT_DELAY_VARIABILITY) + // wait for the removal to be committed     DEFAULT_CLUSTER_STATE_UPDATE_DELAY + // then wait for the followup reconfiguration     DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     assertThat(cluster.getAnyLeader().getId(), not(equalTo(originalLeader.getId()))). }
false;public;0;43;;public void testUnresponsiveLeaderDetectedEventually() {     final Cluster cluster = new Cluster(randomIntBetween(3, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode originalLeader = cluster.getAnyLeader().     logger.info("--> blackholing leader {}", originalLeader).     originalLeader.blackhole().     // This stabilisation time bound is undesirably long. TODO try and reduce it.     cluster.stabilise(Math.max(// first wait for all the followers to notice the leader has gone     (defaultMillis(LEADER_CHECK_INTERVAL_SETTING) + defaultMillis(LEADER_CHECK_TIMEOUT_SETTING)) * defaultInt(LEADER_CHECK_RETRY_COUNT_SETTING) + // then wait for a follower to be promoted to leader     DEFAULT_ELECTION_DELAY + // and the first publication times out because of the unresponsive node     defaultMillis(PUBLISH_TIMEOUT_SETTING) + // there might be a term bump causing another election     DEFAULT_ELECTION_DELAY + // then wait for both of:     Math.max(// 1. the term bumping publication to time out     defaultMillis(PUBLISH_TIMEOUT_SETTING), // 2. the new leader to notice that the old leader is unresponsive     (defaultMillis(FOLLOWER_CHECK_INTERVAL_SETTING) + defaultMillis(FOLLOWER_CHECK_TIMEOUT_SETTING)) * defaultInt(FOLLOWER_CHECK_RETRY_COUNT_SETTING)) + // then wait for the new leader to commit a state without the old leader     DEFAULT_CLUSTER_STATE_UPDATE_DELAY + // then wait for the followup reconfiguration     DEFAULT_CLUSTER_STATE_UPDATE_DELAY, // ALSO wait for the leader to notice that its followers are unresponsive     (defaultMillis(FOLLOWER_CHECK_INTERVAL_SETTING) + defaultMillis(FOLLOWER_CHECK_TIMEOUT_SETTING)) * defaultInt(FOLLOWER_CHECK_RETRY_COUNT_SETTING) + // then wait for the leader to try and commit a state removing them, causing it to stand down     DEFAULT_CLUSTER_STATE_UPDATE_DELAY)).     assertThat(cluster.getAnyLeader().getId(), not(equalTo(originalLeader.getId()))). }
false;public;0;18;;public void testFollowerDisconnectionDetectedQuickly() {     final Cluster cluster = new Cluster(randomIntBetween(3, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode follower = cluster.getAnyNodeExcept(leader).     logger.info("--> disconnecting follower {}", follower).     follower.disconnect().     logger.info("--> leader {} and follower {} get disconnect event", leader, follower).     leader.onDisconnectEventFrom(follower).     // to turn follower into candidate, which stabilisation asserts at the end     follower.onDisconnectEventFrom(leader).     cluster.stabilise(// disconnect is scheduled     DEFAULT_DELAY_VARIABILITY + DEFAULT_CLUSTER_STATE_UPDATE_DELAY + // then wait for the followup reconfiguration     DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     assertThat(cluster.getAnyLeader().getId(), equalTo(leader.getId())). }
false;public;0;30;;public void testFollowerDisconnectionWithoutDisconnectEventDetectedQuickly() {     final Cluster cluster = new Cluster(randomIntBetween(3, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode follower = cluster.getAnyNodeExcept(leader).     logger.info("--> disconnecting follower {}", follower).     follower.disconnect().     cluster.stabilise(Math.max(// the leader may have just sent a follower check, which receives no response     defaultMillis(FOLLOWER_CHECK_TIMEOUT_SETTING) + // wait for the leader to check the follower     defaultMillis(FOLLOWER_CHECK_INTERVAL_SETTING) + // then wait for the exception response     DEFAULT_DELAY_VARIABILITY + // then wait for the removal to be committed     DEFAULT_CLUSTER_STATE_UPDATE_DELAY + // then wait for the followup reconfiguration     DEFAULT_CLUSTER_STATE_UPDATE_DELAY, // ALSO the follower may have just sent a leader check, which receives no response     defaultMillis(LEADER_CHECK_TIMEOUT_SETTING) + // then wait for the follower to check the leader     defaultMillis(LEADER_CHECK_INTERVAL_SETTING) + // then wait for the exception response, causing the follower to become a candidate     DEFAULT_DELAY_VARIABILITY)).     assertThat(cluster.getAnyLeader().getId(), equalTo(leader.getId())). }
false;public;0;25;;public void testUnresponsiveFollowerDetectedEventually() {     final Cluster cluster = new Cluster(randomIntBetween(3, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode follower = cluster.getAnyNodeExcept(leader).     logger.info("--> blackholing follower {}", follower).     follower.blackhole().     cluster.stabilise(Math.max(// wait for the leader to notice that the follower is unresponsive     (defaultMillis(FOLLOWER_CHECK_INTERVAL_SETTING) + defaultMillis(FOLLOWER_CHECK_TIMEOUT_SETTING)) * defaultInt(FOLLOWER_CHECK_RETRY_COUNT_SETTING) + // then wait for the leader to commit a state without the follower     DEFAULT_CLUSTER_STATE_UPDATE_DELAY + // then wait for the followup reconfiguration     DEFAULT_CLUSTER_STATE_UPDATE_DELAY, // ALSO wait for the follower to notice the leader is unresponsive     (defaultMillis(LEADER_CHECK_INTERVAL_SETTING) + defaultMillis(LEADER_CHECK_TIMEOUT_SETTING)) * defaultInt(LEADER_CHECK_RETRY_COUNT_SETTING))).     assertThat(cluster.getAnyLeader().getId(), equalTo(leader.getId())). }
false;public;0;13;;public void testAckListenerReceivesAcksFromAllNodes() {     final Cluster cluster = new Cluster(randomIntBetween(3, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     AckCollector ackCollector = leader.submitValue(randomLong()).     cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     for (final ClusterNode clusterNode : cluster.clusterNodes) {         assertTrue("expected ack from " + clusterNode, ackCollector.hasAckedSuccessfully(clusterNode)).     }     assertThat("leader should be last to ack", ackCollector.getSuccessfulAckIndex(leader), equalTo(cluster.clusterNodes.size() - 1)). }
false;public;0;16;;public void testAckListenerReceivesNackFromFollower() {     final Cluster cluster = new Cluster(3).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode follower0 = cluster.getAnyNodeExcept(leader).     final ClusterNode follower1 = cluster.getAnyNodeExcept(leader, follower0).     follower0.setClusterStateApplyResponse(ClusterStateApplyResponse.FAIL).     AckCollector ackCollector = leader.submitValue(randomLong()).     cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     assertTrue("expected ack from " + leader, ackCollector.hasAckedSuccessfully(leader)).     assertTrue("expected nack from " + follower0, ackCollector.hasAckedUnsuccessfully(follower0)).     assertTrue("expected ack from " + follower1, ackCollector.hasAckedSuccessfully(follower1)).     assertThat("leader should be last to ack", ackCollector.getSuccessfulAckIndex(leader), equalTo(1)). }
false;public;0;20;;public void testAckListenerReceivesNackFromLeader() {     final Cluster cluster = new Cluster(3).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode follower0 = cluster.getAnyNodeExcept(leader).     final ClusterNode follower1 = cluster.getAnyNodeExcept(leader, follower0).     final long startingTerm = leader.coordinator.getCurrentTerm().     leader.setClusterStateApplyResponse(ClusterStateApplyResponse.FAIL).     AckCollector ackCollector = leader.submitValue(randomLong()).     cluster.runFor(DEFAULT_CLUSTER_STATE_UPDATE_DELAY, "committing value").     assertTrue(leader.coordinator.getMode() != Coordinator.Mode.LEADER || leader.coordinator.getCurrentTerm() > startingTerm).     leader.setClusterStateApplyResponse(ClusterStateApplyResponse.SUCCEED).     cluster.stabilise().     assertTrue("expected nack from " + leader, ackCollector.hasAckedUnsuccessfully(leader)).     assertTrue("expected ack from " + follower0, ackCollector.hasAckedSuccessfully(follower0)).     assertTrue("expected ack from " + follower1, ackCollector.hasAckedSuccessfully(follower1)).     assertTrue(leader.coordinator.getMode() != Coordinator.Mode.LEADER || leader.coordinator.getCurrentTerm() > startingTerm). }
false;public;0;21;;public void testAckListenerReceivesNoAckFromHangingFollower() {     final Cluster cluster = new Cluster(3).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode follower0 = cluster.getAnyNodeExcept(leader).     final ClusterNode follower1 = cluster.getAnyNodeExcept(leader, follower0).     logger.info("--> blocking cluster state application on {}", follower0).     follower0.setClusterStateApplyResponse(ClusterStateApplyResponse.HANG).     logger.info("--> publishing another value").     AckCollector ackCollector = leader.submitValue(randomLong()).     cluster.runFor(DEFAULT_CLUSTER_STATE_UPDATE_DELAY, "committing value").     assertTrue("expected immediate ack from " + follower1, ackCollector.hasAckedSuccessfully(follower1)).     assertFalse("expected no ack from " + leader, ackCollector.hasAckedSuccessfully(leader)).     cluster.stabilise(defaultMillis(PUBLISH_TIMEOUT_SETTING)).     assertTrue("expected eventual ack from " + leader, ackCollector.hasAckedSuccessfully(leader)).     assertFalse("expected no ack from " + follower0, ackCollector.hasAcked(follower0)). }
false;public;0;22;;public void testAckListenerReceivesNacksIfPublicationTimesOut() {     final Cluster cluster = new Cluster(3).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode follower0 = cluster.getAnyNodeExcept(leader).     final ClusterNode follower1 = cluster.getAnyNodeExcept(leader, follower0).     follower0.blackhole().     follower1.blackhole().     AckCollector ackCollector = leader.submitValue(randomLong()).     cluster.runFor(DEFAULT_CLUSTER_STATE_UPDATE_DELAY, "committing value").     assertFalse("expected no immediate ack from " + leader, ackCollector.hasAcked(leader)).     assertFalse("expected no immediate ack from " + follower0, ackCollector.hasAcked(follower0)).     assertFalse("expected no immediate ack from " + follower1, ackCollector.hasAcked(follower1)).     follower0.heal().     follower1.heal().     cluster.stabilise().     assertTrue("expected eventual nack from " + follower0, ackCollector.hasAckedUnsuccessfully(follower0)).     assertTrue("expected eventual nack from " + follower1, ackCollector.hasAckedUnsuccessfully(follower1)).     assertTrue("expected eventual nack from " + leader, ackCollector.hasAckedUnsuccessfully(leader)). }
false;public;0;22;;public void testAckListenerReceivesNacksIfLeaderStandsDown() {     final Cluster cluster = new Cluster(3).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode follower0 = cluster.getAnyNodeExcept(leader).     final ClusterNode follower1 = cluster.getAnyNodeExcept(leader, follower0).     leader.blackhole().     follower0.onDisconnectEventFrom(leader).     follower1.onDisconnectEventFrom(leader).     // let followers elect a leader among themselves before healing the leader and running the publication     cluster.runFor(// disconnect is scheduled     DEFAULT_DELAY_VARIABILITY + DEFAULT_ELECTION_DELAY, "elect new leader").     // cluster has two nodes in mode LEADER, in different terms ofc, and the one in the lower term wonâ€™t be able to publish anything     leader.heal().     AckCollector ackCollector = leader.submitValue(randomLong()).     // TODO: check if can find a better bound here     cluster.stabilise().     assertTrue("expected nack from " + leader, ackCollector.hasAckedUnsuccessfully(leader)).     assertTrue("expected nack from " + follower0, ackCollector.hasAckedUnsuccessfully(follower0)).     assertTrue("expected nack from " + follower1, ackCollector.hasAckedUnsuccessfully(follower1)). }
false;public;0;16;;public void testAckListenerReceivesNacksFromFollowerInHigherTerm() { // TODO: needs proper term bumping // final Cluster cluster = new Cluster(3). // cluster.runRandomly(). // cluster.stabilise(). // final ClusterNode leader = cluster.getAnyLeader(). // final ClusterNode follower0 = cluster.getAnyNodeExcept(leader). // final ClusterNode follower1 = cluster.getAnyNodeExcept(leader, follower0). //  // follower0.coordinator.joinLeaderInTerm(new StartJoinRequest(follower0.localNode, follower0.coordinator.getCurrentTerm() + 1)). // AckCollector ackCollector = leader.submitValue(randomLong()). // cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY). // assertTrue("expected ack from " + leader, ackCollector.hasAckedSuccessfully(leader)). // assertTrue("expected nack from " + follower0, ackCollector.hasAckedUnsuccessfully(follower0)). // assertTrue("expected ack from " + follower1, ackCollector.hasAckedSuccessfully(follower1)). }
false;public;0;38;;public void testSettingInitialConfigurationTriggersElection() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5)).     cluster.runFor(defaultMillis(DISCOVERY_FIND_PEERS_INTERVAL_SETTING) * 2 + randomLongBetween(0, 60000), "initial discovery phase").     for (final ClusterNode clusterNode : cluster.clusterNodes) {         final String nodeId = clusterNode.getId().         assertThat(nodeId + " is CANDIDATE", clusterNode.coordinator.getMode(), is(CANDIDATE)).         assertThat(nodeId + " is in term 0", clusterNode.coordinator.getCurrentTerm(), is(0L)).         assertThat(nodeId + " last accepted in term 0", clusterNode.coordinator.getLastAcceptedState().term(), is(0L)).         assertThat(nodeId + " last accepted version 0", clusterNode.coordinator.getLastAcceptedState().version(), is(0L)).         assertFalse(nodeId + " has not received an initial configuration", clusterNode.coordinator.isInitialConfigurationSet()).         assertTrue(nodeId + " has an empty last-accepted configuration", clusterNode.coordinator.getLastAcceptedState().getLastAcceptedConfiguration().isEmpty()).         assertTrue(nodeId + " has an empty last-committed configuration", clusterNode.coordinator.getLastAcceptedState().getLastCommittedConfiguration().isEmpty()).         final Set<DiscoveryNode> foundPeers = new HashSet<>().         clusterNode.coordinator.getFoundPeers().forEach(foundPeers::add).         assertTrue(nodeId + " should not have discovered itself", foundPeers.add(clusterNode.getLocalNode())).         assertThat(nodeId + " should have found all peers", foundPeers, hasSize(cluster.size())).     }     final ClusterNode bootstrapNode = cluster.getAnyBootstrappableNode().     bootstrapNode.applyInitialConfiguration().     assertTrue(bootstrapNode.getId() + " has been bootstrapped", bootstrapNode.coordinator.isInitialConfigurationSet()).     cluster.stabilise(// pre-voting round and proceed to an election, so there cannot be any collisions     // TODO this wait is unnecessary, we could trigger the election immediately     defaultMillis(ELECTION_INITIAL_TIMEOUT_SETTING) + // Allow two round-trip for pre-voting and voting     4 * DEFAULT_DELAY_VARIABILITY + // Then a commit of the new leader's first cluster state     DEFAULT_CLUSTER_STATE_UPDATE_DELAY + // Then allow time for all the other nodes to join, each of which might cause a reconfiguration     (cluster.size() - 1) * 2 * DEFAULT_CLUSTER_STATE_UPDATE_DELAY). }
false;public;0;8;;public void testCannotSetInitialConfigurationTwice() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5)).     cluster.runRandomly().     cluster.stabilise().     final Coordinator coordinator = cluster.getAnyNode().coordinator.     assertFalse(coordinator.setInitialConfiguration(coordinator.getLastAcceptedState().getLastCommittedConfiguration())). }
false;public;0;16;;public void testCannotSetInitialConfigurationWithoutQuorum() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5)).     final Coordinator coordinator = cluster.getAnyNode().coordinator.     final VotingConfiguration unknownNodeConfiguration = new VotingConfiguration(Sets.newHashSet(coordinator.getLocalNode().getId(), "unknown-node")).     final String exceptionMessage = expectThrows(CoordinationStateRejectedException.class, () -> coordinator.setInitialConfiguration(unknownNodeConfiguration)).getMessage().     assertThat(exceptionMessage, startsWith("not enough nodes discovered to form a quorum in the initial configuration [knownNodes=[")).     assertThat(exceptionMessage, containsString("unknown-node")).     assertThat(exceptionMessage, containsString(coordinator.getLocalNode().toString())).     // This is VERY BAD: setting a _different_ initial configuration. Yet it works if the first attempt will never be a quorum.     assertTrue(coordinator.setInitialConfiguration(new VotingConfiguration(Collections.singleton(coordinator.getLocalNode().getId())))).     cluster.stabilise(). }
false;public;0;9;;public void testCannotSetInitialConfigurationWithoutLocalNode() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5)).     final Coordinator coordinator = cluster.getAnyNode().coordinator.     final VotingConfiguration unknownNodeConfiguration = new VotingConfiguration(Sets.newHashSet("unknown-node")).     final String exceptionMessage = expectThrows(CoordinationStateRejectedException.class, () -> coordinator.setInitialConfiguration(unknownNodeConfiguration)).getMessage().     assertThat(exceptionMessage, equalTo("local node is not part of initial configuration")). }
false;public;0;36;;public void testDiffBasedPublishing() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final long finalValue = randomLong().     final Map<ClusterNode, PublishClusterStateStats> prePublishStats = cluster.clusterNodes.stream().collect(Collectors.toMap(Function.identity(), cn -> cn.coordinator.stats().getPublishStats())).     logger.info("--> submitting value [{}] to [{}]", finalValue, leader).     leader.submitValue(finalValue).     cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     final Map<ClusterNode, PublishClusterStateStats> postPublishStats = cluster.clusterNodes.stream().collect(Collectors.toMap(Function.identity(), cn -> cn.coordinator.stats().getPublishStats())).     for (ClusterNode cn : cluster.clusterNodes) {         assertThat(value(cn.getLastAppliedClusterState()), is(finalValue)).         if (cn == leader) {             // leader does not update publish stats as it's not using the serialized state             assertEquals(cn.toString(), prePublishStats.get(cn).getFullClusterStateReceivedCount(), postPublishStats.get(cn).getFullClusterStateReceivedCount()).             assertEquals(cn.toString(), prePublishStats.get(cn).getCompatibleClusterStateDiffReceivedCount(), postPublishStats.get(cn).getCompatibleClusterStateDiffReceivedCount()).             assertEquals(cn.toString(), prePublishStats.get(cn).getIncompatibleClusterStateDiffReceivedCount(), postPublishStats.get(cn).getIncompatibleClusterStateDiffReceivedCount()).         } else {             // followers receive a diff             assertEquals(cn.toString(), prePublishStats.get(cn).getFullClusterStateReceivedCount(), postPublishStats.get(cn).getFullClusterStateReceivedCount()).             assertEquals(cn.toString(), prePublishStats.get(cn).getCompatibleClusterStateDiffReceivedCount() + 1, postPublishStats.get(cn).getCompatibleClusterStateDiffReceivedCount()).             assertEquals(cn.toString(), prePublishStats.get(cn).getIncompatibleClusterStateDiffReceivedCount(), postPublishStats.get(cn).getIncompatibleClusterStateDiffReceivedCount()).         }     } }
false;public;0;14;;public void testJoiningNodeReceivesFullState() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5)).     cluster.runRandomly().     cluster.stabilise().     cluster.addNodesAndStabilise(1).     final ClusterNode newNode = cluster.clusterNodes.get(cluster.clusterNodes.size() - 1).     final PublishClusterStateStats newNodePublishStats = newNode.coordinator.stats().getPublishStats().     // initial cluster state send when joining     assertEquals(1L, newNodePublishStats.getFullClusterStateReceivedCount()).     // possible follow-up reconfiguration was published as a diff     assertEquals(cluster.size() % 2, newNodePublishStats.getCompatibleClusterStateDiffReceivedCount()).     assertEquals(0L, newNodePublishStats.getIncompatibleClusterStateDiffReceivedCount()). }
false;public;0;27;;@AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/38867") public void testIncompatibleDiffResendsFullState() {     final Cluster cluster = new Cluster(randomIntBetween(3, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode follower = cluster.getAnyNodeExcept(leader).     logger.info("--> blackholing {}", follower).     follower.blackhole().     final PublishClusterStateStats prePublishStats = follower.coordinator.stats().getPublishStats().     logger.info("--> submitting first value to {}", leader).     leader.submitValue(randomLong()).     cluster.runFor(DEFAULT_CLUSTER_STATE_UPDATE_DELAY + defaultMillis(PUBLISH_TIMEOUT_SETTING), "publish first state").     logger.info("--> healing {}", follower).     follower.heal().     logger.info("--> submitting second value to {}", leader).     leader.submitValue(randomLong()).     cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     final PublishClusterStateStats postPublishStats = follower.coordinator.stats().getPublishStats().     assertEquals(prePublishStats.getFullClusterStateReceivedCount() + 1, postPublishStats.getFullClusterStateReceivedCount()).     assertEquals(prePublishStats.getCompatibleClusterStateDiffReceivedCount(), postPublishStats.getCompatibleClusterStateDiffReceivedCount()).     assertEquals(prePublishStats.getIncompatibleClusterStateDiffReceivedCount() + 1, postPublishStats.getIncompatibleClusterStateDiffReceivedCount()). }
true;public;0;18;/**  * Simulates a situation where a follower becomes disconnected from the leader, but only for such a short time where  * it becomes candidate and puts up a NO_MASTER_BLOCK, but then receives a follower check from the leader. If the leader  * does not notice the node disconnecting, it is important for the node not to be turned back into a follower but try  * and join the leader again.  */ ;/**  * Simulates a situation where a follower becomes disconnected from the leader, but only for such a short time where  * it becomes candidate and puts up a NO_MASTER_BLOCK, but then receives a follower check from the leader. If the leader  * does not notice the node disconnecting, it is important for the node not to be turned back into a follower but try  * and join the leader again.  */ public void testStayCandidateAfterReceivingFollowerCheckFromKnownMaster() {     final Cluster cluster = new Cluster(2, false).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     final ClusterNode nonLeader = cluster.getAnyNodeExcept(leader).     nonLeader.onNode(() -> {         logger.debug("forcing {} to become candidate", nonLeader.getId()).         synchronized (nonLeader.coordinator.mutex) {             nonLeader.coordinator.becomeCandidate("forced").         }         logger.debug("simulate follower check coming through from {} to {}", leader.getId(), nonLeader.getId()).         nonLeader.coordinator.onFollowerCheckRequest(new FollowersChecker.FollowerCheckRequest(leader.coordinator.getCurrentTerm(), leader.getLocalNode())).     }).run().     cluster.stabilise(). }
false;public;0;3;;public void testAppliesNoMasterBlockWritesByDefault() {     testAppliesNoMasterBlock(null, NO_MASTER_BLOCK_WRITES). }
false;public;0;3;;public void testAppliesNoMasterBlockWritesIfConfigured() {     testAppliesNoMasterBlock("write", NO_MASTER_BLOCK_WRITES). }
false;public;0;3;;public void testAppliesNoMasterBlockAllIfConfigured() {     testAppliesNoMasterBlock("all", NO_MASTER_BLOCK_ALL). }
false;private;2;21;;private void testAppliesNoMasterBlock(String noMasterBlockSetting, ClusterBlock expectedBlock) {     final Cluster cluster = new Cluster(3).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode leader = cluster.getAnyLeader().     leader.submitUpdateTask("update NO_MASTER_BLOCK_SETTING", cs -> {         final Builder settingsBuilder = Settings.builder().put(cs.metaData().persistentSettings()).         settingsBuilder.put(NO_MASTER_BLOCK_SETTING.getKey(), noMasterBlockSetting).         return ClusterState.builder(cs).metaData(MetaData.builder(cs.metaData()).persistentSettings(settingsBuilder.build())).build().     }, (source, e) -> {     }).     cluster.runFor(DEFAULT_CLUSTER_STATE_UPDATE_DELAY, "committing setting update").     leader.disconnect().     cluster.runFor(defaultMillis(FOLLOWER_CHECK_TIMEOUT_SETTING) + defaultMillis(FOLLOWER_CHECK_INTERVAL_SETTING) + DEFAULT_CLUSTER_STATE_UPDATE_DELAY, "detecting disconnection").     assertThat(leader.getLastAppliedClusterState().blocks().global(), hasItem(expectedBlock)). // TODO reboot the leader and verify that the same block is applied when it restarts }
false;public;0;22;;public void testNodeCannotJoinIfJoinValidationFailsOnMaster() {     final Cluster cluster = new Cluster(randomIntBetween(1, 3)).     cluster.runRandomly().     cluster.stabilise().     // check that if node join validation fails on master, the nodes can't join     List<ClusterNode> addedNodes = cluster.addNodes(randomIntBetween(1, 2)).     final Set<DiscoveryNode> validatedNodes = new HashSet<>().     cluster.getAnyLeader().extraJoinValidators.add((discoveryNode, clusterState) -> {         validatedNodes.add(discoveryNode).         throw new IllegalArgumentException("join validation failed").     }).     final long previousClusterStateVersion = cluster.getAnyLeader().getLastAppliedClusterState().version().     cluster.runFor(10000, "failing join validation").     assertEquals(validatedNodes, addedNodes.stream().map(ClusterNode::getLocalNode).collect(Collectors.toSet())).     assertTrue(addedNodes.stream().allMatch(ClusterNode::isCandidate)).     final long newClusterStateVersion = cluster.getAnyLeader().getLastAppliedClusterState().version().     assertEquals(previousClusterStateVersion, newClusterStateVersion).     cluster.getAnyLeader().extraJoinValidators.clear().     cluster.stabilise(). }
false;public;0;22;;public void testNodeCannotJoinIfJoinValidationFailsOnJoiningNode() {     final Cluster cluster = new Cluster(randomIntBetween(1, 3)).     cluster.runRandomly().     cluster.stabilise().     // check that if node join validation fails on joining node, the nodes can't join     List<ClusterNode> addedNodes = cluster.addNodes(randomIntBetween(1, 2)).     final Set<DiscoveryNode> validatedNodes = new HashSet<>().     addedNodes.stream().forEach(cn -> cn.extraJoinValidators.add((discoveryNode, clusterState) -> {         validatedNodes.add(discoveryNode).         throw new IllegalArgumentException("join validation failed").     })).     final long previousClusterStateVersion = cluster.getAnyLeader().getLastAppliedClusterState().version().     cluster.runFor(10000, "failing join validation").     assertEquals(validatedNodes, addedNodes.stream().map(ClusterNode::getLocalNode).collect(Collectors.toSet())).     assertTrue(addedNodes.stream().allMatch(ClusterNode::isCandidate)).     final long newClusterStateVersion = cluster.getAnyLeader().getLastAppliedClusterState().version().     assertEquals(previousClusterStateVersion, newClusterStateVersion).     addedNodes.stream().forEach(cn -> cn.extraJoinValidators.clear()).     cluster.stabilise(). }
false;public;0;14;;public void testClusterCannotFormWithFailingJoinValidation() {     final Cluster cluster = new Cluster(randomIntBetween(1, 5)).     // fail join validation on a majority of nodes in the initial configuration     randomValueOtherThanMany(nodes -> cluster.initialConfiguration.hasQuorum(nodes.stream().map(ClusterNode::getLocalNode).map(DiscoveryNode::getId).collect(Collectors.toSet())) == false, () -> randomSubsetOf(cluster.clusterNodes)).forEach(cn -> cn.extraJoinValidators.add((discoveryNode, clusterState) -> {         throw new IllegalArgumentException("join validation failed").     })).     cluster.bootstrapIfNecessary().     cluster.runFor(10000, "failing join validation").     assertTrue(cluster.clusterNodes.stream().allMatch(cn -> cn.getLastAppliedClusterState().version() == 0)). }
false;public;0;39;;public void testCannotJoinClusterWithDifferentUUID() throws IllegalAccessException {     final Cluster cluster1 = new Cluster(randomIntBetween(1, 3)).     cluster1.runRandomly().     cluster1.stabilise().     final Cluster cluster2 = new Cluster(3).     cluster2.runRandomly().     cluster2.stabilise().     final ClusterNode shiftedNode = randomFrom(cluster2.clusterNodes).restartedNode().     final ClusterNode newNode = cluster1.new ClusterNode(nextNodeIndex.getAndIncrement(), shiftedNode.getLocalNode(), n -> shiftedNode.persistedState).     cluster1.clusterNodes.add(newNode).     MockLogAppender mockAppender = new MockLogAppender().     mockAppender.start().     mockAppender.addExpectation(new MockLogAppender.SeenEventExpectation("test1", JoinHelper.class.getCanonicalName(), Level.INFO, "*failed to join*")).     Logger joinLogger = LogManager.getLogger(JoinHelper.class).     Loggers.addAppender(joinLogger, mockAppender).     cluster1.runFor(DEFAULT_STABILISATION_TIME, "failing join validation").     try {         mockAppender.assertAllExpectationsMatched().     } finally {         Loggers.removeAppender(joinLogger, mockAppender).         mockAppender.stop().     }     assertEquals(0, newNode.getLastAppliedClusterState().version()).     final ClusterNode detachedNode = newNode.restartedNode(metaData -> DetachClusterCommand.updateMetaData(metaData), term -> DetachClusterCommand.updateCurrentTerm()).     cluster1.clusterNodes.replaceAll(cn -> cn == newNode ? detachedNode : cn).     cluster1.stabilise(). }
false;public;0;20;;public void testDiscoveryUsesNodesFromLastClusterState() {     final Cluster cluster = new Cluster(randomIntBetween(3, 5)).     cluster.runRandomly().     cluster.stabilise().     final ClusterNode partitionedNode = cluster.getAnyNode().     if (randomBoolean()) {         logger.info("--> blackholing {}", partitionedNode).         partitionedNode.blackhole().     } else {         logger.info("--> disconnecting {}", partitionedNode).         partitionedNode.disconnect().     }     cluster.setEmptySeedHostsList().     cluster.stabilise().     partitionedNode.heal().     cluster.runRandomly(false).     cluster.stabilise(). }
false;private,static;1;3;;private static long defaultMillis(Setting<TimeValue> setting) {     return setting.get(Settings.EMPTY).millis() + Cluster.DEFAULT_DELAY_VARIABILITY. }
false;private,static;1;3;;private static int defaultInt(Setting<Integer> setting) {     return setting.get(Settings.EMPTY). }
false;;1;13;;List<ClusterNode> addNodesAndStabilise(int newNodesCount) {     final List<ClusterNode> addedNodes = addNodes(newNodesCount).     stabilise(// The first pinging discovers the master     defaultMillis(DISCOVERY_FIND_PEERS_INTERVAL_SETTING) + // One message delay to send a join     DEFAULT_DELAY_VARIABILITY + // followup reconfiguration     newNodesCount * 2 * DEFAULT_CLUSTER_STATE_UPDATE_DELAY).     // TODO Investigate whether 4 publications is sufficient due to batching? A bound linear in the number of nodes isn't great.     return addedNodes. }
false;;1;11;;List<ClusterNode> addNodes(int newNodesCount) {     logger.info("--> adding {} nodes", newNodesCount).     final List<ClusterNode> addedNodes = new ArrayList<>().     for (int i = 0. i < newNodesCount. i++) {         final ClusterNode clusterNode = new ClusterNode(nextNodeIndex.getAndIncrement(), true).         addedNodes.add(clusterNode).     }     clusterNodes.addAll(addedNodes).     return addedNodes. }
false;;0;3;;int size() {     return clusterNodes.size(). }
false;;0;3;;void runRandomly() {     runRandomly(true). }
false;public;0;4;;@Override public void run() {     cn.transportService.disconnectFromNode(clusterNode.getLocalNode()). }
false;public;0;4;;@Override public String toString() {     return "disconnect from " + clusterNode.getLocalNode() + " after shutdown". }
false;;1;131;;void runRandomly(boolean allowReboots) {     // TODO supporting (preserving?) existing disruptions needs implementing if needed, for now we just forbid it     assertThat("may reconnect disconnected nodes, probably unexpected", disconnectedNodes, empty()).     assertThat("may reconnect blackholed nodes, probably unexpected", blackholedNodes, empty()).     final List<Runnable> cleanupActions = new ArrayList<>().     cleanupActions.add(disconnectedNodes::clear).     cleanupActions.add(blackholedNodes::clear).     cleanupActions.add(() -> disruptStorage = false).     final int randomSteps = scaledRandomIntBetween(10, 10000).     // for randomized writes and reads     final int keyRange = randomSteps / 50.     logger.info("--> start of safety phase of at least [{}] steps", randomSteps).     deterministicTaskQueue.setExecutionDelayVariabilityMillis(EXTREME_DELAY_VARIABILITY).     disruptStorage = true.     int step = 0.     long finishTime = -1.     while (finishTime == -1 || deterministicTaskQueue.getCurrentTimeMillis() <= finishTime) {         step++.         // for lambdas         final int thisStep = step.         if (randomSteps <= step && finishTime == -1) {             finishTime = deterministicTaskQueue.getLatestDeferredExecutionTime().             deterministicTaskQueue.setExecutionDelayVariabilityMillis(DEFAULT_DELAY_VARIABILITY).             logger.debug("----> [runRandomly {}] reducing delay variability and running until [{}ms]", step, finishTime).         }         try {             if (finishTime == -1 && randomBoolean() && randomBoolean() && randomBoolean()) {                 final ClusterNode clusterNode = getAnyNodePreferringLeaders().                 final int key = randomIntBetween(0, keyRange).                 final int newValue = randomInt().                 clusterNode.onNode(() -> {                     logger.debug("----> [runRandomly {}] proposing new value [{}] to [{}]", thisStep, newValue, clusterNode.getId()).                     clusterNode.submitValue(key, newValue).                 }).run().             } else if (finishTime == -1 && randomBoolean() && randomBoolean() && randomBoolean()) {                 final ClusterNode clusterNode = getAnyNodePreferringLeaders().                 final int key = randomIntBetween(0, keyRange).                 clusterNode.onNode(() -> {                     logger.debug("----> [runRandomly {}] reading value from [{}]", thisStep, clusterNode.getId()).                     clusterNode.readValue(key).                 }).run().             } else if (rarely()) {                 final ClusterNode clusterNode = getAnyNodePreferringLeaders().                 final boolean autoShrinkVotingConfiguration = randomBoolean().                 clusterNode.onNode(() -> {                     logger.debug("----> [runRandomly {}] setting auto-shrink configuration to {} on {}", thisStep, autoShrinkVotingConfiguration, clusterNode.getId()).                     clusterNode.submitSetAutoShrinkVotingConfiguration(autoShrinkVotingConfiguration).                 }).run().             } else if (allowReboots && rarely()) {                 // reboot random node                 final ClusterNode clusterNode = getAnyNode().                 logger.debug("----> [runRandomly {}] rebooting [{}]", thisStep, clusterNode.getId()).                 clusterNode.close().                 clusterNodes.forEach(cn -> deterministicTaskQueue.scheduleNow(cn.onNode(new Runnable() {                      @Override                     public void run() {                         cn.transportService.disconnectFromNode(clusterNode.getLocalNode()).                     }                      @Override                     public String toString() {                         return "disconnect from " + clusterNode.getLocalNode() + " after shutdown".                     }                 }))).                 clusterNodes.replaceAll(cn -> cn == clusterNode ? cn.restartedNode() : cn).             } else if (rarely()) {                 final ClusterNode clusterNode = getAnyNode().                 clusterNode.onNode(() -> {                     logger.debug("----> [runRandomly {}] forcing {} to become candidate", thisStep, clusterNode.getId()).                     synchronized (clusterNode.coordinator.mutex) {                         clusterNode.coordinator.becomeCandidate("runRandomly").                     }                 }).run().             } else if (rarely()) {                 final ClusterNode clusterNode = getAnyNode().                 switch(randomInt(2)) {                     case 0:                         if (clusterNode.heal()) {                             logger.debug("----> [runRandomly {}] healing {}", step, clusterNode.getId()).                         }                         break.                     case 1:                         if (clusterNode.disconnect()) {                             logger.debug("----> [runRandomly {}] disconnecting {}", step, clusterNode.getId()).                         }                         break.                     case 2:                         if (clusterNode.blackhole()) {                             logger.debug("----> [runRandomly {}] blackholing {}", step, clusterNode.getId()).                         }                         break.                 }             } else if (rarely()) {                 final ClusterNode clusterNode = getAnyNode().                 logger.debug("----> [runRandomly {}] applying initial configuration on {}", step, clusterNode.getId()).                 clusterNode.applyInitialConfiguration().             } else {                 if (deterministicTaskQueue.hasDeferredTasks() && randomBoolean()) {                     deterministicTaskQueue.advanceTime().                 } else if (deterministicTaskQueue.hasRunnableTasks()) {                     deterministicTaskQueue.runRandomTask().                 }             }         // TODO other random steps:         // - reboot a node         // - abdicate leadership         } catch (CoordinationStateRejectedException | UncheckedIOException ignored) {         // This is ok: it just means a message couldn't currently be handled.         }         assertConsistentStates().     }     logger.debug("running {} cleanup actions", cleanupActions.size()).     cleanupActions.forEach(Runnable::run).     logger.debug("finished running cleanup actions"). }
false;private;0;6;;private void assertConsistentStates() {     for (final ClusterNode clusterNode : clusterNodes) {         clusterNode.coordinator.invariant().     }     updateCommittedStates(). }
false;private;0;12;;private void updateCommittedStates() {     for (final ClusterNode clusterNode : clusterNodes) {         ClusterState applierState = clusterNode.coordinator.getApplierState().         ClusterState storedState = committedStatesByVersion.get(applierState.getVersion()).         if (storedState == null) {             committedStatesByVersion.put(applierState.getVersion(), applierState).         } else {             assertEquals("expected " + applierState + " but got " + storedState, value(applierState), value(storedState)).         }     } }
false;;0;3;;void stabilise() {     stabilise(DEFAULT_STABILISATION_TIME). }
false;;1;77;;void stabilise(long stabilisationDurationMillis) {     assertThat("stabilisation requires default delay variability (and proper cleanup of raised variability)", deterministicTaskQueue.getExecutionDelayVariabilityMillis(), lessThanOrEqualTo(DEFAULT_DELAY_VARIABILITY)).     assertFalse("stabilisation requires stable storage", disruptStorage).     bootstrapIfNecessary().     runFor(stabilisationDurationMillis, "stabilising").     final ClusterNode leader = getAnyLeader().     final long leaderTerm = leader.coordinator.getCurrentTerm().     final Matcher<Long> isEqualToLeaderVersion = equalTo(leader.coordinator.getLastAcceptedState().getVersion()).     final String leaderId = leader.getId().     assertTrue(leaderId + " has been bootstrapped", leader.coordinator.isInitialConfigurationSet()).     assertTrue(leaderId + " exists in its last-applied state", leader.getLastAppliedClusterState().getNodes().nodeExists(leaderId)).     assertThat(leaderId + " has applied its state ", leader.getLastAppliedClusterState().getVersion(), isEqualToLeaderVersion).     for (final ClusterNode clusterNode : clusterNodes) {         final String nodeId = clusterNode.getId().         assertFalse(nodeId + " should not have an active publication", clusterNode.coordinator.publicationInProgress()).         if (clusterNode == leader) {             continue.         }         if (isConnectedPair(leader, clusterNode)) {             assertThat(nodeId + " is a follower of " + leaderId, clusterNode.coordinator.getMode(), is(FOLLOWER)).             assertThat(nodeId + " has the same term as " + leaderId, clusterNode.coordinator.getCurrentTerm(), is(leaderTerm)).             assertTrue(nodeId + " has voted for " + leaderId, leader.coordinator.hasJoinVoteFrom(clusterNode.getLocalNode())).             assertThat(nodeId + " has the same accepted state as " + leaderId, clusterNode.coordinator.getLastAcceptedState().getVersion(), isEqualToLeaderVersion).             if (clusterNode.getClusterStateApplyResponse() == ClusterStateApplyResponse.SUCCEED) {                 assertThat(nodeId + " has the same applied state as " + leaderId, clusterNode.getLastAppliedClusterState().getVersion(), isEqualToLeaderVersion).                 assertTrue(nodeId + " is in its own latest applied state", clusterNode.getLastAppliedClusterState().getNodes().nodeExists(nodeId)).             }             assertTrue(nodeId + " is in the latest applied state on " + leaderId, leader.getLastAppliedClusterState().getNodes().nodeExists(nodeId)).             assertTrue(nodeId + " has been bootstrapped", clusterNode.coordinator.isInitialConfigurationSet()).             assertThat(nodeId + " has correct master", clusterNode.getLastAppliedClusterState().nodes().getMasterNode(), equalTo(leader.getLocalNode())).             assertThat(nodeId + " has no NO_MASTER_BLOCK", clusterNode.getLastAppliedClusterState().blocks().hasGlobalBlockWithId(NO_MASTER_BLOCK_ID), equalTo(false)).         } else {             assertThat(nodeId + " is not following " + leaderId, clusterNode.coordinator.getMode(), is(CANDIDATE)).             assertThat(nodeId + " has no master", clusterNode.getLastAppliedClusterState().nodes().getMasterNode(), nullValue()).             assertThat(nodeId + " has NO_MASTER_BLOCK", clusterNode.getLastAppliedClusterState().blocks().hasGlobalBlockWithId(NO_MASTER_BLOCK_ID), equalTo(true)).             assertFalse(nodeId + " is not in the applied state on " + leaderId, leader.getLastAppliedClusterState().getNodes().nodeExists(nodeId)).         }     }     final Set<String> connectedNodeIds = clusterNodes.stream().filter(n -> isConnectedPair(leader, n)).map(ClusterNode::getId).collect(Collectors.toSet()).     assertThat(leader.getLastAppliedClusterState().getNodes().getSize(), equalTo(connectedNodeIds.size())).     final ClusterState lastAcceptedState = leader.coordinator.getLastAcceptedState().     final VotingConfiguration lastCommittedConfiguration = lastAcceptedState.getLastCommittedConfiguration().     assertTrue(connectedNodeIds + " should be a quorum of " + lastCommittedConfiguration, lastCommittedConfiguration.hasQuorum(connectedNodeIds)).     assertThat("leader " + leader.getLocalNode() + " should be part of voting configuration " + lastCommittedConfiguration, lastCommittedConfiguration.getNodeIds(), IsCollectionContaining.hasItem(leader.getLocalNode().getId())).     assertThat("no reconfiguration is in progress", lastAcceptedState.getLastCommittedConfiguration(), equalTo(lastAcceptedState.getLastAcceptedConfiguration())).     assertThat("current configuration is already optimal", leader.improveConfiguration(lastAcceptedState), sameInstance(lastAcceptedState)).     logger.info("checking linearizability of history with size {}: {}", history.size(), history).     // See https://github.com/elastic/elasticsearch/issues/39437     // assertTrue("history not linearizable: " + history, linearizabilityChecker.isLinearizable(spec, history, i -> null)).     logger.info("linearizability check completed"). }
false;;0;11;;void bootstrapIfNecessary() {     if (clusterNodes.stream().allMatch(ClusterNode::isNotUsefullyBootstrapped)) {         assertThat("setting initial configuration may fail with disconnected nodes", disconnectedNodes, empty()).         assertThat("setting initial configuration may fail with blackholed nodes", blackholedNodes, empty()).         runFor(defaultMillis(DISCOVERY_FIND_PEERS_INTERVAL_SETTING) * 2, "discovery prior to setting initial configuration").         final ClusterNode bootstrapNode = getAnyBootstrappableNode().         bootstrapNode.applyInitialConfiguration().     } else {         logger.info("setting initial configuration not required").     } }
false;;2;29;;void runFor(long runDurationMillis, String description) {     final long endTime = deterministicTaskQueue.getCurrentTimeMillis() + runDurationMillis.     logger.info("--> runFor({}ms) running until [{}ms]: {}", runDurationMillis, endTime, description).     while (deterministicTaskQueue.getCurrentTimeMillis() < endTime) {         while (deterministicTaskQueue.hasRunnableTasks()) {             try {                 deterministicTaskQueue.runRandomTask().             } catch (CoordinationStateRejectedException e) {                 logger.debug("ignoring benign exception thrown when stabilising", e).             }             for (final ClusterNode clusterNode : clusterNodes) {                 clusterNode.coordinator.invariant().             }             updateCommittedStates().         }         if (deterministicTaskQueue.hasDeferredTasks() == false) {             // A 1-node cluster has no need for fault detection etc so will eventually run out of things to do.             assert clusterNodes.size() == 1 : clusterNodes.size().             break.         }         deterministicTaskQueue.advanceTime().     }     logger.info("--> runFor({}ms) completed run until [{}ms]: {}", runDurationMillis, endTime, description). }
false;private;2;5;;private boolean isConnectedPair(ClusterNode n1, ClusterNode n2) {     return n1 == n2 || (getConnectionStatus(n1.getLocalNode(), n2.getLocalNode()) == ConnectionStatus.CONNECTED && getConnectionStatus(n2.getLocalNode(), n1.getLocalNode()) == ConnectionStatus.CONNECTED). }
false;;0;5;;ClusterNode getAnyLeader() {     List<ClusterNode> allLeaders = clusterNodes.stream().filter(ClusterNode::isLeader).collect(Collectors.toList()).     assertThat("leaders", allLeaders, not(empty())).     return randomFrom(allLeaders). }
false;private;2;14;;private ConnectionStatus getConnectionStatus(DiscoveryNode sender, DiscoveryNode destination) {     ConnectionStatus connectionStatus.     if (blackholedNodes.contains(sender.getId()) || blackholedNodes.contains(destination.getId())) {         connectionStatus = ConnectionStatus.BLACK_HOLE.     } else if (disconnectedNodes.contains(sender.getId()) || disconnectedNodes.contains(destination.getId())) {         connectionStatus = ConnectionStatus.DISCONNECTED.     } else if (nodeExists(sender) && nodeExists(destination)) {         connectionStatus = ConnectionStatus.CONNECTED.     } else {         connectionStatus = usually() ? preferredUnknownNodeConnectionStatus : randomFrom(ConnectionStatus.DISCONNECTED, ConnectionStatus.BLACK_HOLE).     }     return connectionStatus. }
false;;1;3;;boolean nodeExists(DiscoveryNode node) {     return clusterNodes.stream().anyMatch(cn -> cn.getLocalNode().equals(node)). }
false;;0;5;;ClusterNode getAnyBootstrappableNode() {     return randomFrom(clusterNodes.stream().filter(n -> n.getLocalNode().isMasterNode()).filter(n -> initialConfiguration.getNodeIds().contains(n.getLocalNode().getId())).collect(Collectors.toList())). }
false;;0;3;;ClusterNode getAnyNode() {     return getAnyNodeExcept(). }
false;;1;5;;ClusterNode getAnyNodeExcept(ClusterNode... clusterNodes) {     List<ClusterNode> filteredNodes = getAllNodesExcept(clusterNodes).     assert filteredNodes.isEmpty() == false.     return randomFrom(filteredNodes). }
false;;1;6;;List<ClusterNode> getAllNodesExcept(ClusterNode... clusterNodes) {     Set<String> forbiddenIds = Arrays.stream(clusterNodes).map(ClusterNode::getId).collect(Collectors.toSet()).     List<ClusterNode> acceptableNodes = this.clusterNodes.stream().filter(n -> forbiddenIds.contains(n.getId()) == false).collect(Collectors.toList()).     return acceptableNodes. }
false;;0;9;;ClusterNode getAnyNodePreferringLeaders() {     for (int i = 0. i < 3. i++) {         ClusterNode clusterNode = getAnyNode().         if (clusterNode.coordinator.getMode() == LEADER) {             return clusterNode.         }     }     return getAnyNode(). }
false;;0;3;;void setEmptySeedHostsList() {     seedHostsList = emptyList(). }
false;private;1;8;;private void possiblyFail(String description) {     if (disruptStorage && rarely()) {         logger.trace("simulating IO exception [{}]", description).         // This will require node restart and we're not emulating it here.         throw new UncheckedIOException(new IOException("simulated IO exception [" + description + ']')).     } }
false;public;0;4;;@Override public long getCurrentTerm() {     return delegate.getCurrentTerm(). }
false;public;0;4;;@Override public ClusterState getLastAcceptedState() {     return delegate.getLastAcceptedState(). }
false;public;1;5;;@Override public void setCurrentTerm(long currentTerm) {     possiblyFail("before writing term of " + currentTerm).     delegate.setCurrentTerm(currentTerm). }
false;public;1;5;;@Override public void setLastAcceptedState(ClusterState clusterState) {     possiblyFail("before writing last-accepted state of term=" + clusterState.term() + ", version=" + clusterState.version()).     delegate.setLastAcceptedState(clusterState). }
false;protected;1;4;;@Override protected void execute(Runnable runnable) {     deterministicTaskQueue.scheduleNow(onNode(runnable)). }
false;protected;1;4;;@Override protected ConnectionStatus getConnectionStatus(DiscoveryNode destination) {     return Cluster.this.getConnectionStatus(getLocalNode(), destination). }
false;protected;1;5;;@Override protected Optional<DisruptableMockTransport> getDisruptableMockTransport(TransportAddress address) {     return clusterNodes.stream().map(cn -> cn.mockTransport).filter(transport -> transport.getLocalNode().getAddress().equals(address)).findAny(). }
false;public;1;4;;@Override public void connectToNodes(DiscoveryNodes discoveryNodes) { // override this method as it does blocking calls }
false;private;0;53;;private void setUp() {     mockTransport = new DisruptableMockTransport(localNode, logger) {          @Override         protected void execute(Runnable runnable) {             deterministicTaskQueue.scheduleNow(onNode(runnable)).         }          @Override         protected ConnectionStatus getConnectionStatus(DiscoveryNode destination) {             return Cluster.this.getConnectionStatus(getLocalNode(), destination).         }          @Override         protected Optional<DisruptableMockTransport> getDisruptableMockTransport(TransportAddress address) {             return clusterNodes.stream().map(cn -> cn.mockTransport).filter(transport -> transport.getLocalNode().getAddress().equals(address)).findAny().         }     }.     final Settings settings = Settings.builder().putList(ClusterBootstrapService.INITIAL_MASTER_NODES_SETTING.getKey(), ClusterBootstrapService.INITIAL_MASTER_NODES_SETTING.get(Settings.EMPTY)).build().     transportService = mockTransport.createTransportService(settings, deterministicTaskQueue.getThreadPool(this::onNode), NOOP_TRANSPORT_INTERCEPTOR, a -> localNode, null, emptySet()).     masterService = new AckedFakeThreadPoolMasterService(localNode.getId(), "test", runnable -> deterministicTaskQueue.scheduleNow(onNode(runnable))).     final ClusterSettings clusterSettings = new ClusterSettings(settings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS).     clusterApplierService = new DisruptableClusterApplierService(localNode.getId(), settings, clusterSettings, deterministicTaskQueue, this::onNode).     clusterService = new ClusterService(settings, clusterSettings, masterService, clusterApplierService).     clusterService.setNodeConnectionsService(new NodeConnectionsService(clusterService.getSettings(), deterministicTaskQueue.getThreadPool(this::onNode), transportService) {          @Override         public void connectToNodes(DiscoveryNodes discoveryNodes) {         // override this method as it does blocking calls         }     }).     final Collection<BiConsumer<DiscoveryNode, ClusterState>> onJoinValidators = Collections.singletonList((dn, cs) -> extraJoinValidators.forEach(validator -> validator.accept(dn, cs))).     coordinator = new Coordinator("test_node", settings, clusterSettings, transportService, writableRegistry(), ESAllocationTestCase.createAllocationService(Settings.EMPTY), masterService, this::getPersistedState, Cluster.this::provideSeedHosts, clusterApplierService, onJoinValidators, Randomness.get()).     masterService.setClusterStatePublisher(coordinator).     logger.trace("starting up [{}]", localNode).     transportService.start().     transportService.acceptIncomingRequests().     coordinator.start().     clusterService.start().     coordinator.startInitialJoin(). }
false;;0;11;;void close() {     onNode(() -> {         logger.trace("taking down [{}]", localNode).         coordinator.stop().         clusterService.stop().         // transportService.stop(). // does blocking stuff :/         clusterService.close().         coordinator.close().     // transportService.close(). // does blocking stuff :/     }). }
false;;0;3;;ClusterNode restartedNode() {     return restartedNode(Function.identity(), Function.identity()). }
false;;2;9;;ClusterNode restartedNode(Function<MetaData, MetaData> adaptGlobalMetaData, Function<Long, Long> adaptCurrentTerm) {     final TransportAddress address = randomBoolean() ? buildNewFakeTransportAddress() : localNode.getAddress().     final DiscoveryNode newLocalNode = new DiscoveryNode(localNode.getName(), localNode.getId(), // generated deterministically for repeatable tests     UUIDs.randomBase64UUID(random()), address.address().getHostString(), address.getAddress(), address, Collections.emptyMap(), localNode.isMasterNode() ? EnumSet.allOf(Role.class) : emptySet(), Version.CURRENT).     return new ClusterNode(nodeIndex, newLocalNode, node -> new MockPersistedState(newLocalNode, persistedState, adaptGlobalMetaData, adaptCurrentTerm)). }
false;private;0;3;;private PersistedState getPersistedState() {     return persistedState. }
false;;0;3;;String getId() {     return localNode.getId(). }
false;;0;3;;DiscoveryNode getLocalNode() {     return localNode. }
false;;0;3;;boolean isLeader() {     return coordinator.getMode() == LEADER. }
false;;0;3;;boolean isCandidate() {     return coordinator.getMode() == CANDIDATE. }
false;;1;5;;ClusterState improveConfiguration(ClusterState currentState) {     synchronized (coordinator.mutex) {         return coordinator.improveConfiguration(currentState).     } }
false;;1;3;;void setClusterStateApplyResponse(ClusterStateApplyResponse clusterStateApplyResponse) {     clusterApplierService.clusterStateApplyResponse = clusterStateApplyResponse. }
false;;0;3;;ClusterStateApplyResponse getClusterStateApplyResponse() {     return clusterApplierService.clusterStateApplyResponse. }
false;public;0;8;;@Override public void run() {     if (clusterNodes.contains(ClusterNode.this) == false) {         logger.trace("ignoring runnable {} from node {} as node has been removed from cluster", runnable, localNode).         return.     }     wrapped.run(). }
false;public;0;4;;@Override public String toString() {     return wrapped.toString(). }
false;;1;18;;Runnable onNode(Runnable runnable) {     final Runnable wrapped = onNodeLog(localNode, runnable).     return new Runnable() {          @Override         public void run() {             if (clusterNodes.contains(ClusterNode.this) == false) {                 logger.trace("ignoring runnable {} from node {} as node has been removed from cluster", runnable, localNode).                 return.             }             wrapped.run().         }          @Override         public String toString() {             return wrapped.toString().         }     }. }
false;;1;11;;void submitSetAutoShrinkVotingConfiguration(final boolean autoShrinkVotingConfiguration) {     submitUpdateTask("set master nodes failure tolerance [" + autoShrinkVotingConfiguration + "]", cs -> ClusterState.builder(cs).metaData(MetaData.builder(cs.metaData()).persistentSettings(Settings.builder().put(cs.metaData().persistentSettings()).put(CLUSTER_AUTO_SHRINK_VOTING_CONFIGURATION.getKey(), autoShrinkVotingConfiguration).build()).build()).build(), (source, e) -> {     }). }
false;;1;3;;AckCollector submitValue(final long value) {     return submitValue(0, value). }
false;public;3;4;;@Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {     history.respond(eventId, value(oldState, key)). }
false;public;1;6;;@Override public void onNoLongerMaster(String source) {     // in this case, we know for sure that event was not processed by the system and will not change history     // remove event to help avoid bloated history and state space explosion in linearizability checker     history.remove(eventId). }
false;public;2;5;;@Override public void onFailure(String source, Exception e) { // do not remove event from history, the write might still take place // instead, complete history when checking for linearizability }
false;;2;22;;AckCollector submitValue(final int key, final long value) {     final int eventId = history.invoke(new Tuple<>(key, value)).     return submitUpdateTask("new value [" + value + "]", cs -> setValue(cs, key, value), new ClusterStateTaskListener() {          @Override         public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {             history.respond(eventId, value(oldState, key)).         }          @Override         public void onNoLongerMaster(String source) {             // in this case, we know for sure that event was not processed by the system and will not change history             // remove event to help avoid bloated history and state space explosion in linearizability checker             history.remove(eventId).         }          @Override         public void onFailure(String source, Exception e) {         // do not remove event from history, the write might still take place         // instead, complete history when checking for linearizability         }     }). }
false;public;3;4;;@Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {     history.respond(eventId, value(newState, key)). }
false;public;2;6;;@Override public void onFailure(String source, Exception e) {     // reads do not change state     // remove event to help avoid bloated history and state space explosion in linearizability checker     history.remove(eventId). }
false;;1;16;;void readValue(int key) {     final int eventId = history.invoke(new Tuple<>(key, null)).     submitUpdateTask("read value", cs -> ClusterState.builder(cs).build(), new ClusterStateTaskListener() {          @Override         public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {             history.respond(eventId, value(newState, key)).         }          @Override         public void onFailure(String source, Exception e) {             // reads do not change state             // remove event to help avoid bloated history and state space explosion in linearizability checker             history.remove(eventId).         }     }). }
false;public;1;6;;@Override public ClusterState execute(ClusterState currentState) {     assertThat(currentState.term(), greaterThanOrEqualTo(submittedTerm)).     masterService.nextAckCollector = ackCollector.     return clusterStateUpdate.apply(currentState). }
false;public;2;5;;@Override public void onFailure(String source, Exception e) {     logger.debug(() -> new ParameterizedMessage("failed to publish: [{}]", source), e).     taskListener.onFailure(source, e). }
false;public;1;5;;@Override public void onNoLongerMaster(String source) {     logger.trace("no longer master: [{}]", source).     taskListener.onNoLongerMaster(source). }
false;public;3;9;;@Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {     updateCommittedStates().     ClusterState state = committedStatesByVersion.get(newState.version()).     assertNotNull("State not committed : " + newState.toString(), state).     assertStateEquals(state, newState).     logger.trace("successfully published: [{}]", newState).     taskListener.clusterStateProcessed(source, oldState, newState). }
false;;3;40;;AckCollector submitUpdateTask(String source, UnaryOperator<ClusterState> clusterStateUpdate, ClusterStateTaskListener taskListener) {     final AckCollector ackCollector = new AckCollector().     onNode(() -> {         logger.trace("[{}] submitUpdateTask: enqueueing [{}]", localNode.getId(), source).         final long submittedTerm = coordinator.getCurrentTerm().         masterService.submitStateUpdateTask(source, new ClusterStateUpdateTask() {              @Override             public ClusterState execute(ClusterState currentState) {                 assertThat(currentState.term(), greaterThanOrEqualTo(submittedTerm)).                 masterService.nextAckCollector = ackCollector.                 return clusterStateUpdate.apply(currentState).             }              @Override             public void onFailure(String source, Exception e) {                 logger.debug(() -> new ParameterizedMessage("failed to publish: [{}]", source), e).                 taskListener.onFailure(source, e).             }              @Override             public void onNoLongerMaster(String source) {                 logger.trace("no longer master: [{}]", source).                 taskListener.onNoLongerMaster(source).             }              @Override             public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {                 updateCommittedStates().                 ClusterState state = committedStatesByVersion.get(newState.version()).                 assertNotNull("State not committed : " + newState.toString(), state).                 assertStateEquals(state, newState).                 logger.trace("successfully published: [{}]", newState).                 taskListener.clusterStateProcessed(source, oldState, newState).             }         }).     }).run().     return ackCollector. }
false;public;0;4;;@Override public String toString() {     return localNode.toString(). }
false;;0;6;;boolean heal() {     boolean unBlackholed = blackholedNodes.remove(localNode.getId()).     boolean unDisconnected = disconnectedNodes.remove(localNode.getId()).     assert unBlackholed == false || unDisconnected == false.     return unBlackholed || unDisconnected. }
false;;0;6;;boolean disconnect() {     boolean unBlackholed = blackholedNodes.remove(localNode.getId()).     boolean disconnected = disconnectedNodes.add(localNode.getId()).     assert disconnected || unBlackholed == false.     return disconnected. }
false;;0;6;;boolean blackhole() {     boolean unDisconnected = disconnectedNodes.remove(localNode.getId()).     boolean blackholed = blackholedNodes.add(localNode.getId()).     assert blackholed || unDisconnected == false.     return blackholed. }
false;;1;3;;void onDisconnectEventFrom(ClusterNode clusterNode) {     transportService.disconnectFromNode(clusterNode.localNode). }
false;;0;3;;ClusterState getLastAppliedClusterState() {     return clusterApplierService.state(). }
false;;0;23;;void applyInitialConfiguration() {     onNode(() -> {         final Set<String> nodeIdsWithPlaceholders = new HashSet<>(initialConfiguration.getNodeIds()).         Stream.generate(() -> BOOTSTRAP_PLACEHOLDER_PREFIX + UUIDs.randomBase64UUID(random())).limit((Math.max(initialConfiguration.getNodeIds().size(), 2) - 1) / 2).forEach(nodeIdsWithPlaceholders::add).         final Set<String> nodeIds = new HashSet<>(randomSubsetOf(initialConfiguration.getNodeIds().size(), nodeIdsWithPlaceholders)).         // initial configuration should not have a place holder for local node         if (initialConfiguration.getNodeIds().contains(localNode.getId()) && nodeIds.contains(localNode.getId()) == false) {             nodeIds.remove(nodeIds.iterator().next()).             nodeIds.add(localNode.getId()).         }         final VotingConfiguration configurationWithPlaceholders = new VotingConfiguration(nodeIds).         try {             coordinator.setInitialConfiguration(configurationWithPlaceholders).             logger.info("successfully set initial configuration to {}", configurationWithPlaceholders).         } catch (CoordinationStateRejectedException e) {             logger.info(new ParameterizedMessage("failed to set initial configuration to {}", configurationWithPlaceholders), e).         }     }).run(). }
false;private;0;3;;private boolean isNotUsefullyBootstrapped() {     return getLocalNode().isMasterNode() == false || coordinator.isInitialConfigurationSet() == false. }
false;private;1;4;;private List<TransportAddress> provideSeedHosts(HostsResolver ignored) {     return seedHostsList != null ? seedHostsList : clusterNodes.stream().map(ClusterNode::getLocalNode).map(DiscoveryNode::getAddress).collect(Collectors.toList()). }
false;public;0;6;;@Override public void run() {     try (CloseableThreadContext.Instance ignored = CloseableThreadContext.put("nodeId", nodeId)) {         runnable.run().     } }
false;public;0;4;;@Override public String toString() {     return nodeId + ": " + runnable.toString(). }
false;public,static;2;16;;public static Runnable onNodeLog(DiscoveryNode node, Runnable runnable) {     final String nodeId = "{" + node.getId() + "}{" + node.getEphemeralId() + "}".     return new Runnable() {          @Override         public void run() {             try (CloseableThreadContext.Instance ignored = CloseableThreadContext.put("nodeId", nodeId)) {                 runnable.run().             }         }          @Override         public String toString() {             return nodeId + ": " + runnable.toString().         }     }. }
false;public;1;4;;@Override public void onCommit(TimeValue commitTime) { // TODO we only currently care about per-node acks }
false;public;2;9;;@Override public void onNodeAck(DiscoveryNode node, Exception e) {     assertTrue("duplicate ack from " + node, ackedNodes.add(node)).     if (e == null) {         successfulNodes.add(node).     } else {         unsuccessfulNodes.add(node).     } }
false;;1;3;;boolean hasAckedSuccessfully(ClusterNode clusterNode) {     return successfulNodes.contains(clusterNode.localNode). }
false;;1;3;;boolean hasAckedUnsuccessfully(ClusterNode clusterNode) {     return unsuccessfulNodes.contains(clusterNode.localNode). }
false;;1;3;;boolean hasAcked(ClusterNode clusterNode) {     return ackedNodes.contains(clusterNode.localNode). }
false;;1;4;;int getSuccessfulAckIndex(ClusterNode clusterNode) {     assert successfulNodes.contains(clusterNode.localNode) : "get index of " + clusterNode.     return successfulNodes.indexOf(clusterNode.localNode). }
false;public;1;5;;@Override public void onCommit(TimeValue commitTime) {     ackCollector.onCommit(commitTime).     ackListener.onCommit(commitTime). }
false;public;2;5;;@Override public void onNodeAck(DiscoveryNode node, Exception e) {     ackCollector.onNodeAck(node, e).     ackListener.onNodeAck(node, e). }
false;protected;1;18;;@Override protected AckListener wrapAckListener(AckListener ackListener) {     final AckCollector ackCollector = nextAckCollector.     nextAckCollector = new AckCollector().     return new AckListener() {          @Override         public void onCommit(TimeValue commitTime) {             ackCollector.onCommit(commitTime).             ackListener.onCommit(commitTime).         }          @Override         public void onNodeAck(DiscoveryNode node, Exception e) {             ackCollector.onNodeAck(node, e).             ackListener.onNodeAck(node, e).         }     }. }
false;protected;0;4;;@Override protected PrioritizedEsThreadPoolExecutor createThreadPoolExecutor() {     return new MockSinglePrioritizingExecutor(nodeName, deterministicTaskQueue). }
false;public;3;13;;@Override public void onNewClusterState(String source, Supplier<ClusterState> clusterStateSupplier, ClusterApplyListener listener) {     if (clusterStateApplyResponse == ClusterStateApplyResponse.HANG) {         if (randomBoolean()) {             // apply cluster state, but don't notify listener             super.onNewClusterState(source, clusterStateSupplier, (source1, e) -> {             // ignore result             }).         }     } else {         super.onNewClusterState(source, clusterStateSupplier, listener).     } }
false;private,static;2;7;;private static DiscoveryNode createDiscoveryNode(int nodeIndex, boolean masterEligible) {     final TransportAddress address = buildNewFakeTransportAddress().     return new DiscoveryNode("", "node" + nodeIndex, // generated deterministically for repeatable tests     UUIDs.randomBase64UUID(random()), address.address().getHostString(), address.getAddress(), address, Collections.emptyMap(), masterEligible ? EnumSet.allOf(Role.class) : emptySet(), Version.CURRENT). }
false;public;3;10;;public ClusterState setValue(ClusterState clusterState, int key, long value) {     return ClusterState.builder(clusterState).metaData(MetaData.builder(clusterState.metaData()).persistentSettings(Settings.builder().put(clusterState.metaData().persistentSettings()).put("value_" + key, value).build()).build()).build(). }
false;public;1;3;;public long value(ClusterState clusterState) {     return value(clusterState, 0). }
false;public;2;3;;public long value(ClusterState clusterState, int key) {     return clusterState.metaData().persistentSettings().getAsLong("value_" + key, 0L). }
false;public;2;8;;public void assertStateEquals(ClusterState clusterState1, ClusterState clusterState2) {     assertEquals(clusterState1.version(), clusterState2.version()).     assertEquals(clusterState1.term(), clusterState2.term()).     assertEquals(keySet(clusterState1), keySet(clusterState2)).     for (int key : keySet(clusterState1)) {         assertEquals(value(clusterState1, key), value(clusterState2, key)).     } }
false;public;1;4;;public Set<Integer> keySet(ClusterState clusterState) {     return clusterState.metaData().persistentSettings().keySet().stream().filter(s -> s.startsWith("value_")).map(s -> Integer.valueOf(s.substring("value_".length()))).collect(Collectors.toSet()). }
false;public;1;4;;@Override public Object getKey(Object value) {     return ((Tuple) value).v1(). }
false;public;1;4;;@Override public Object getValue(Object value) {     return ((Tuple) value).v2(). }
false;public;0;4;;@Override public Object initialState() {     return 0L. }
false;public;3;17;;@Override public Optional<Object> nextState(Object currentState, Object input, Object output) {     // null input is read, non-null is write     if (input == null) {         // history is completed with null, simulating timeout, which assumes that read went through         if (output == null || currentState.equals(output)) {             return Optional.of(currentState).         }         return Optional.empty().     } else {         if (output == null || currentState.equals(output)) {             // history is completed with null, simulating timeout, which assumes that write went through             return Optional.of(input).         }         return Optional.empty().     } }
false;public;0;8;;public void testRegisterSpecConsistency() {     assertThat(spec.initialState(), equalTo(0L)).     // successful write 42 returns previous value 7     assertThat(spec.nextState(7, 42, 7), equalTo(Optional.of(42))).     // write 42 times out     assertThat(spec.nextState(7, 42, null), equalTo(Optional.of(42))).     // successful read     assertThat(spec.nextState(7, null, 7), equalTo(Optional.of(7))).     // read times out     assertThat(spec.nextState(7, null, null), equalTo(Optional.of(7))).     assertThat(spec.nextState(7, null, 42), equalTo(Optional.empty())). }
