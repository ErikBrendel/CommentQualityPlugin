commented;modifiers;parameterAmount;loc;comment;code
false;protected;0;4;;@Override protected int numberOfReplicas() {     return 2. }
false;public;2;19;;@Override public void doAfterNodes(int numNodes, final Client activeClient) throws Exception {     if (numNodes == 1) {         assertTrue(awaitBusy(() -> {             logger.info("--> running cluster_health (wait for the shards to startup)").             ClusterHealthResponse clusterHealth = activeClient.admin().cluster().health(clusterHealthRequest().waitForYellowStatus().waitForNodes("2").waitForActiveShards(test.numPrimaries * 2)).actionGet().             logger.info("--> done cluster_health, status {}", clusterHealth.getStatus()).             return (!clusterHealth.isTimedOut()) && clusterHealth.getStatus() == ClusterHealthStatus.YELLOW.         }, 30, TimeUnit.SECONDS)).         logger.info("--> one node is closed -- index 1 document into the remaining nodes").         activeClient.prepareIndex("test", "type1", "3").setSource(jsonBuilder().startObject().field("field", "value3").endObject()).get().         assertNoFailures(activeClient.admin().indices().prepareRefresh().get()).         for (int i = 0. i < 10. i++) {             assertHitCount(activeClient.prepareSearch().setSize(0).setQuery(matchAllQuery()).get(), 3L).         }     } }
false;public;0;51;;public void testQuorumRecovery() throws Exception {     logger.info("--> starting 3 nodes").     // we are shutting down nodes - make sure we don't have 2 clusters if we test network     internalCluster().startNodes(3).     createIndex("test").     ensureGreen().     final NumShards test = getNumShards("test").     logger.info("--> indexing...").     client().prepareIndex("test", "type1", "1").setSource(jsonBuilder().startObject().field("field", "value1").endObject()).get().     // We don't check for failures in the flush response: if we do we might get the following:     // FlushNotAllowedEngineException[[test][1] recovery is in progress, flush [COMMIT_TRANSLOG] is not allowed]     flush().     client().prepareIndex("test", "type1", "2").setSource(jsonBuilder().startObject().field("field", "value2").endObject()).get().     refresh().     for (int i = 0. i < 10. i++) {         assertHitCount(client().prepareSearch().setSize(0).setQuery(matchAllQuery()).get(), 2L).     }     logger.info("--> restart all nodes").     internalCluster().fullRestart(new RestartCallback() {          @Override         public void doAfterNodes(int numNodes, final Client activeClient) throws Exception {             if (numNodes == 1) {                 assertTrue(awaitBusy(() -> {                     logger.info("--> running cluster_health (wait for the shards to startup)").                     ClusterHealthResponse clusterHealth = activeClient.admin().cluster().health(clusterHealthRequest().waitForYellowStatus().waitForNodes("2").waitForActiveShards(test.numPrimaries * 2)).actionGet().                     logger.info("--> done cluster_health, status {}", clusterHealth.getStatus()).                     return (!clusterHealth.isTimedOut()) && clusterHealth.getStatus() == ClusterHealthStatus.YELLOW.                 }, 30, TimeUnit.SECONDS)).                 logger.info("--> one node is closed -- index 1 document into the remaining nodes").                 activeClient.prepareIndex("test", "type1", "3").setSource(jsonBuilder().startObject().field("field", "value3").endObject()).get().                 assertNoFailures(activeClient.admin().indices().prepareRefresh().get()).                 for (int i = 0. i < 10. i++) {                     assertHitCount(activeClient.prepareSearch().setSize(0).setQuery(matchAllQuery()).get(), 3L).                 }             }         }     }).     logger.info("--> all nodes are started back, verifying we got the latest version").     logger.info("--> running cluster_health (wait for the shards to startup)").     ensureGreen().     for (int i = 0. i < 10. i++) {         assertHitCount(client().prepareSearch().setSize(0).setQuery(matchAllQuery()).get(), 3L).     } }
