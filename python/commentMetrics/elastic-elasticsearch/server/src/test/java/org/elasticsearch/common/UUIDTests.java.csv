commented;modifiers;parameterAmount;loc;comment;code
false;public;0;3;;public void testRandomUUID() {     verifyUUIDSet(100000, randomUUIDGen). }
false;public;0;3;;public void testTimeUUID() {     verifyUUIDSet(100000, timeUUIDGen). }
false;public;0;3;;public void testThreadedTimeUUID() {     testUUIDThreaded(timeUUIDGen). }
false;public;0;3;;public void testThreadedRandomUUID() {     testUUIDThreaded(randomUUIDGen). }
false;;2;8;;Set<String> verifyUUIDSet(int count, UUIDGenerator uuidSource) {     HashSet<String> uuidSet = new HashSet<>().     for (int i = 0. i < count. ++i) {         uuidSet.add(uuidSource.getBase64UUID()).     }     assertEquals(count, uuidSet.size()).     return uuidSet. }
false;public;0;4;;@Override public void run() {     uuidSet = verifyUUIDSet(count, uuidSource). }
false;public;1;31;;public void testUUIDThreaded(UUIDGenerator uuidSource) {     HashSet<UUIDGenRunner> runners = new HashSet<>().     HashSet<Thread> threads = new HashSet<>().     int count = 20.     int uuids = 10000.     for (int i = 0. i < count. ++i) {         UUIDGenRunner runner = new UUIDGenRunner(uuids, uuidSource).         Thread t = new Thread(runner).         threads.add(t).         runners.add(runner).     }     for (Thread t : threads) {         t.start().     }     boolean retry = false.     do {         for (Thread t : threads) {             try {                 t.join().             } catch (InterruptedException ie) {                 retry = true.             }         }     } while (retry).     HashSet<String> globalSet = new HashSet<>().     for (UUIDGenRunner runner : runners) {         globalSet.addAll(runner.uuidSet).     }     assertEquals(count * uuids, globalSet.size()). }
false;public;0;8;;public void testCompression() throws Exception {     Logger logger = LogManager.getLogger(UUIDTests.class).     // Low number so that the test runs quickly, but the results are more interesting with larger numbers     // of indexed documents     // ~12 in practice     assertThat(testCompression(100000, 10000, 3, logger), Matchers.lessThan(14d)).     // ~13 in practice     assertThat(testCompression(100000, 1000, 3, logger), Matchers.lessThan(15d)).     // ~20 in practice     assertThat(testCompression(100000, 100, 3, logger), Matchers.lessThan(21d)). }
false;protected;0;5;;@Override protected long currentTimeMillis() {     currentTimeMillis += intervalBetweenDocs * 2 * r.nextDouble().     return (long) currentTimeMillis. }
false;protected;0;4;;@Override protected byte[] macAddress() {     return RandomPicks.randomFrom(r, macAddresses). }
false;private,static;4;49;;private static double testCompression(int numDocs, int numDocsPerSecond, int numNodes, Logger logger) throws Exception {     // milliseconds     final double intervalBetweenDocs = 1000. / numDocsPerSecond.     final byte[][] macAddresses = new byte[numNodes][].     Random r = random().     for (int i = 0. i < macAddresses.length. ++i) {         macAddresses[i] = new byte[6].         random().nextBytes(macAddresses[i]).     }     UUIDGenerator generator = new TimeBasedUUIDGenerator() {          double currentTimeMillis = System.currentTimeMillis().          @Override         protected long currentTimeMillis() {             currentTimeMillis += intervalBetweenDocs * 2 * r.nextDouble().             return (long) currentTimeMillis.         }          @Override         protected byte[] macAddress() {             return RandomPicks.randomFrom(r, macAddresses).         }     }.     // Avoid randomization which will slow down things without improving     // the quality of this test     Directory dir = newFSDirectory(createTempDir()).     IndexWriterConfig config = new IndexWriterConfig().setMergeScheduler(// for reproducibility     new SerialMergeScheduler()).     IndexWriter w = new IndexWriter(dir, config).     Document doc = new Document().     StringField id = new StringField("_id", "", Store.NO).     doc.add(id).     long start = System.nanoTime().     for (int i = 0. i < numDocs. ++i) {         id.setStringValue(generator.getBase64UUID()).         w.addDocument(doc).     }     w.forceMerge(1).     long time = (System.nanoTime() - start) / 1000 / 1000.     w.close().     long size = 0.     for (String file : dir.listAll()) {         size += dir.fileLength(file).     }     dir.close().     double bytesPerDoc = (double) size / numDocs.     logger.info(numDocs + " docs indexed at " + numDocsPerSecond + " docs/s required " + new ByteSizeValue(size) + " bytes of disk space, or " + bytesPerDoc + " bytes per document. Took: " + new TimeValue(time) + ".").     return bytesPerDoc. }
