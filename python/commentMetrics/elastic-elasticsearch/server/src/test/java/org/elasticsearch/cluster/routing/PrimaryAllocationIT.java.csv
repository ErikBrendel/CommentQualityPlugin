# id;timestamp;commentText;codeText;commentWords;codeWords
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1524684173;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        IndexShardTestCase.getEngine(oldPrimaryShard).getLocalCheckpointTracker().generateSeqNo()_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,index,shard,test,case,get,engine,old,primary,shard,get,local,checkpoint,tracker,generate,seq,no,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1528706846;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1535723122;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1536611444;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1540847035;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1541618291;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1542740723;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1544081506;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1547134118;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1547196565;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1547245564;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1548326386;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1549388544;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> @TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +         "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")     public void testPrimaryReplicaResyncFailed() throws Exception;1550256017;This test asserts that replicas failed to execute resync operations will be failed but not marked as stale.;@TestLogging("_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE," +_        "org.elasticsearch.indices.recovery:TRACE, org.elasticsearch.cluster.routing.allocation.allocator:TRACE")_    public void testPrimaryReplicaResyncFailed() throws Exception {_        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY)__        final int numberOfReplicas = between(2, 3)__        final String oldPrimary = internalCluster().startDataOnlyNode()__        assertAcked(_            prepareCreate("test", Settings.builder().put(indexSettings())_                .put(SETTING_NUMBER_OF_SHARDS, 1)_                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)))__        final ShardId shardId = new ShardId(clusterService().state().metaData().index("test").getIndex(), 0)__        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas))__        ensureGreen()__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get())__        logger.info("--> Indexing with gap in seqno to ensure that some operations will be replayed in resync")__        long numDocs = scaledRandomIntBetween(5, 50)__        for (int i = 0_ i < numDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId)__        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard))_ _        long moreDocs = scaledRandomIntBetween(1, 10)__        for (int i = 0_ i < moreDocs_ i++) {_            IndexResponse indexResult = index("test", "doc", Long.toString(numDocs + i))__            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1))__        }_        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes))__        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1)__        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect())__        internalCluster().setDisruptionScheme(partition)__        logger.info("--> isolating some replicas during primary-replica resync")__        partition.startDisrupting()__        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary))__        _        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId)__            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName()__            assertThat(newPrimaryNode, not(equalTo(oldPrimary)))__            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2__            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()))__            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {_                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition))__            }_            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__        }, 1, TimeUnit.MINUTES)__        assertAcked(_            client(master).admin().cluster().prepareUpdateSettings()_                .setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "all")).get())__        partition.stopDisrupting()__        partition.ensureHealthy(internalCluster())__        logger.info("--> stop disrupting network and re-enable allocation")__        assertBusy(() -> {_            ClusterState state = client(master).admin().cluster().prepareState().get().getState()__            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas))__            assertThat(state.metaData().index("test").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1))__            for (String node : replicaNodes) {_                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId)__                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs))__            }_        }, 30, TimeUnit.SECONDS)__        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex()__    };this,test,asserts,that,replicas,failed,to,execute,resync,operations,will,be,failed,but,not,marked,as,stale;test,logging,debug,org,elasticsearch,cluster,routing,allocation,trace,org,elasticsearch,cluster,action,shard,trace,org,elasticsearch,indices,recovery,trace,org,elasticsearch,cluster,routing,allocation,allocator,trace,public,void,test,primary,replica,resync,failed,throws,exception,string,master,internal,cluster,start,master,only,node,settings,empty,final,int,number,of,replicas,between,2,3,final,string,old,primary,internal,cluster,start,data,only,node,assert,acked,prepare,create,test,settings,builder,put,index,settings,put,1,put,number,of,replicas,final,shard,id,shard,id,new,shard,id,cluster,service,state,meta,data,index,test,get,index,0,final,set,string,replica,nodes,new,hash,set,internal,cluster,start,data,only,nodes,number,of,replicas,ensure,green,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,none,get,logger,info,indexing,with,gap,in,seqno,to,ensure,that,some,operations,will,be,replayed,in,resync,long,num,docs,scaled,random,int,between,5,50,for,int,i,0,i,num,docs,i,index,response,index,result,index,test,doc,long,to,string,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,index,shard,old,primary,shard,internal,cluster,get,instance,indices,service,class,old,primary,get,shard,or,null,shard,id,engine,test,case,generate,new,seq,no,index,shard,test,case,get,engine,old,primary,shard,long,more,docs,scaled,random,int,between,1,10,for,int,i,0,i,more,docs,i,index,response,index,result,index,test,doc,long,to,string,num,docs,i,assert,that,index,result,get,shard,info,get,successful,equal,to,number,of,replicas,1,final,set,string,replicas,side1,sets,new,hash,set,random,subset,of,between,1,number,of,replicas,1,replica,nodes,final,set,string,replicas,side2,sets,difference,replica,nodes,replicas,side1,network,disruption,partition,new,network,disruption,new,two,partitions,replicas,side1,replicas,side2,new,network,disconnect,internal,cluster,set,disruption,scheme,partition,logger,info,isolating,some,replicas,during,primary,replica,resync,partition,start,disrupting,internal,cluster,stop,random,node,internal,test,cluster,name,filter,old,primary,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,final,index,shard,routing,table,shard,routing,table,state,routing,table,shard,routing,table,shard,id,final,string,new,primary,node,state,get,routing,nodes,node,shard,routing,table,primary,current,node,id,node,get,name,assert,that,new,primary,node,not,equal,to,old,primary,set,string,selected,partition,replicas,side1,contains,new,primary,node,replicas,side1,replicas,side2,assert,that,shard,routing,table,active,shards,has,size,selected,partition,size,for,shard,routing,active,shard,shard,routing,table,active,shards,assert,that,state,get,routing,nodes,node,active,shard,current,node,id,node,get,name,is,in,selected,partition,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,1,time,unit,minutes,assert,acked,client,master,admin,cluster,prepare,update,settings,set,transient,settings,settings,builder,put,cluster,routing,allocation,enable,all,get,partition,stop,disrupting,partition,ensure,healthy,internal,cluster,logger,info,stop,disrupting,network,and,re,enable,allocation,assert,busy,cluster,state,state,client,master,admin,cluster,prepare,state,get,get,state,assert,that,state,routing,table,shard,routing,table,shard,id,active,shards,has,size,number,of,replicas,assert,that,state,meta,data,index,test,in,sync,allocation,ids,shard,id,id,has,size,number,of,replicas,1,for,string,node,replica,nodes,index,shard,shard,internal,cluster,get,instance,indices,service,class,node,get,shard,or,null,shard,id,assert,that,shard,get,local,checkpoint,equal,to,num,docs,more,docs,30,time,unit,seconds,internal,cluster,assert,consistent,history,between,translog,and,lucene,index
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1524684173;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1528706846;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1535723122;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1536611444;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1540847035;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1541618291;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1542740723;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1544081506;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1547134118;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1547196565;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1547245564;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1548326386;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1549388544;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
PrimaryAllocationIT -> public void testForceAllocatePrimaryOnNoDecision() throws Exception;1550256017;This test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,_we will force allocate the primary shard to one of those nodes, even if the allocation deciders all return_a NO decision to allocate.;public void testForceAllocatePrimaryOnNoDecision() throws Exception {_        logger.info("--> starting 1 node")__        final String node = internalCluster().startNode()__        logger.info("--> creating index with 1 primary and 0 replicas")__        final String indexName = "test-idx"__        assertAcked(client().admin().indices()_                        .prepareCreate(indexName)_                        .setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1)_                                         .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0))_                        .get())__        logger.info("--> update the settings to prevent allocation to the data node")__        assertTrue(client().admin().indices().prepareUpdateSettings(indexName)_                       .setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_name", node))_                       .get()_                       .isAcknowledged())__        logger.info("--> full cluster restart")__        internalCluster().fullRestart()__        logger.info("--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter")__        ensureGreen(indexName)__        assertEquals(1, client().admin().cluster().prepareState().get().getState()_                            .routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size())__    };this,test,ensures,that,for,an,unassigned,primary,shard,that,has,a,valid,shard,copy,on,at,least,one,node,we,will,force,allocate,the,primary,shard,to,one,of,those,nodes,even,if,the,allocation,deciders,all,return,a,no,decision,to,allocate;public,void,test,force,allocate,primary,on,no,decision,throws,exception,logger,info,starting,1,node,final,string,node,internal,cluster,start,node,logger,info,creating,index,with,1,primary,and,0,replicas,final,string,index,name,test,idx,assert,acked,client,admin,indices,prepare,create,index,name,set,settings,settings,builder,put,index,meta,data,get,key,1,put,index,meta,data,get,key,0,get,logger,info,update,the,settings,to,prevent,allocation,to,the,data,node,assert,true,client,admin,indices,prepare,update,settings,index,name,set,settings,settings,builder,put,index,meta,data,get,key,node,get,is,acknowledged,logger,info,full,cluster,restart,internal,cluster,full,restart,logger,info,checking,that,the,primary,shard,is,force,allocated,to,the,data,node,despite,being,blocked,by,the,exclude,filter,ensure,green,index,name,assert,equals,1,client,admin,cluster,prepare,state,get,get,state,routing,table,index,index,name,shards,with,state,shard,routing,state,started,size
