commented;modifiers;parameterAmount;loc;comment;code
false;static;0;4;;static ConflictMode randomMode() {     ConflictMode[] values = values().     return values[randomInt(values.length - 1)]. }
true;public;0;157;/**  * Test that we do not loose document whose indexing request was successful, under a randomly selected disruption scheme  * We also collect &amp. report the type of indexing failures that occur.  * <p>  * This test is a superset of tests run in the Jepsen test suite, with the exception of versioned updates  */ ;/**  * Test that we do not loose document whose indexing request was successful, under a randomly selected disruption scheme  * We also collect &amp. report the type of indexing failures that occur.  * <p>  * This test is a superset of tests run in the Jepsen test suite, with the exception of versioned updates  */ @TestLogging("_root:DEBUG,org.elasticsearch.action.bulk:TRACE,org.elasticsearch.action.get:TRACE," + "org.elasticsearch.discovery:TRACE,org.elasticsearch.action.support.replication:TRACE," + "org.elasticsearch.cluster.service:TRACE,org.elasticsearch.indices.recovery:TRACE," + "org.elasticsearch.indices.cluster:TRACE,org.elasticsearch.index.shard:TRACE") public void testAckedIndexing() throws Exception {     final int seconds = !(TEST_NIGHTLY && rarely()) ? 1 : 5.     final String timeout = seconds + "s".     final List<String> nodes = startCluster(rarely() ? 5 : 3).     assertAcked(prepareCreate("test").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1 + randomInt(2)).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomInt(2)))).     ensureGreen().     ServiceDisruptionScheme disruptionScheme = addRandomDisruptionScheme().     logger.info("disruption scheme [{}] added", disruptionScheme).     // id -> node sent.     final ConcurrentHashMap<String, String> ackedDocs = new ConcurrentHashMap<>().     final AtomicBoolean stop = new AtomicBoolean(false).     List<Thread> indexers = new ArrayList<>(nodes.size()).     List<Semaphore> semaphores = new ArrayList<>(nodes.size()).     final AtomicInteger idGenerator = new AtomicInteger(0).     final AtomicReference<CountDownLatch> countDownLatchRef = new AtomicReference<>().     final List<Exception> exceptedExceptions = new CopyOnWriteArrayList<>().     final ConflictMode conflictMode = ConflictMode.randomMode().     logger.info("starting indexers using conflict mode " + conflictMode).     try {         for (final String node : nodes) {             final Semaphore semaphore = new Semaphore(0).             semaphores.add(semaphore).             final Client client = client(node).             final String name = "indexer_" + indexers.size().             final int numPrimaries = getNumShards("test").numPrimaries.             Thread thread = new Thread(() -> {                 while (!stop.get()) {                     String id = null.                     try {                         if (!semaphore.tryAcquire(10, TimeUnit.SECONDS)) {                             continue.                         }                         logger.info("[{}] Acquired semaphore and it has {} permits left", name, semaphore.availablePermits()).                         try {                             id = Integer.toString(idGenerator.incrementAndGet()).                             int shard = Math.floorMod(Murmur3HashFunction.hash(id), numPrimaries).                             logger.trace("[{}] indexing id [{}] through node [{}] targeting shard [{}]", name, id, node, shard).                             IndexRequestBuilder indexRequestBuilder = client.prepareIndex("test", "type", id).setSource("{}", XContentType.JSON).setTimeout(timeout).                             if (conflictMode == ConflictMode.external) {                                 indexRequestBuilder.setVersion(randomIntBetween(1, 10)).setVersionType(VersionType.EXTERNAL).                             } else if (conflictMode == ConflictMode.create) {                                 indexRequestBuilder.setCreate(true).                             }                             IndexResponse response = indexRequestBuilder.get(timeout).                             assertThat(response.getResult(), isOneOf(CREATED, UPDATED)).                             ackedDocs.put(id, node).                             logger.trace("[{}] indexed id [{}] through node [{}], response [{}]", name, id, node, response).                         } catch (ElasticsearchException e) {                             exceptedExceptions.add(e).                             final String docId = id.                             logger.trace(() -> new ParameterizedMessage("[{}] failed id [{}] through node [{}]", name, docId, node), e).                         } finally {                             countDownLatchRef.get().countDown().                             logger.trace("[{}] decreased counter : {}", name, countDownLatchRef.get().getCount()).                         }                     } catch (InterruptedException e) {                     // fine - semaphore interrupt                     } catch (AssertionError | Exception e) {                         logger.info(() -> new ParameterizedMessage("unexpected exception in background thread of [{}]", node), e).                     }                 }             }).             thread.setName(name).             thread.start().             indexers.add(thread).         }         int docsPerIndexer = randomInt(3).         logger.info("indexing {} docs per indexer before partition", docsPerIndexer).         countDownLatchRef.set(new CountDownLatch(docsPerIndexer * indexers.size())).         for (Semaphore semaphore : semaphores) {             semaphore.release(docsPerIndexer).         }         assertTrue(countDownLatchRef.get().await(1, TimeUnit.MINUTES)).         for (int iter = 1 + randomInt(2). iter > 0. iter--) {             logger.info("starting disruptions & indexing (iteration [{}])", iter).             disruptionScheme.startDisrupting().             docsPerIndexer = 1 + randomInt(5).             logger.info("indexing {} docs per indexer during partition", docsPerIndexer).             countDownLatchRef.set(new CountDownLatch(docsPerIndexer * indexers.size())).             Collections.shuffle(semaphores, random()).             for (Semaphore semaphore : semaphores) {                 assertThat(semaphore.availablePermits(), equalTo(0)).                 semaphore.release(docsPerIndexer).             }             logger.info("waiting for indexing requests to complete").             assertTrue(countDownLatchRef.get().await(docsPerIndexer * seconds * 1000 + 2000, TimeUnit.MILLISECONDS)).             logger.info("stopping disruption").             disruptionScheme.stopDisrupting().             for (String node : internalCluster().getNodeNames()) {                 ensureStableCluster(nodes.size(), TimeValue.timeValueMillis(disruptionScheme.expectedTimeToHeal().millis() + DISRUPTION_HEALING_OVERHEAD.millis()), true, node).             }             // is the super-connected node and recovery source and target are on opposite sides of the bridge             if (disruptionScheme instanceof NetworkDisruption && ((NetworkDisruption) disruptionScheme).getDisruptedLinks() instanceof Bridge) {                 assertAcked(client().admin().cluster().prepareReroute().setRetryFailed(true)).             }             ensureGreen("test").             logger.info("validating successful docs").             assertBusy(() -> {                 for (String node : nodes) {                     try {                         logger.debug("validating through node [{}] ([{}] acked docs)", node, ackedDocs.size()).                         for (String id : ackedDocs.keySet()) {                             assertTrue("doc [" + id + "] indexed via node [" + ackedDocs.get(id) + "] not found", client(node).prepareGet("test", "type", id).setPreference("_local").get().isExists()).                         }                     } catch (AssertionError | NoShardAvailableActionException e) {                         throw new AssertionError(e.getMessage() + " (checked via node [" + node + "]", e).                     }                 }             }, 30, TimeUnit.SECONDS).             logger.info("done validating (iteration [{}])", iter).         }     } finally {         logger.info("shutting down indexers").         stop.set(true).         for (Thread indexer : indexers) {             indexer.interrupt().             indexer.join(60000).         }         if (exceptedExceptions.size() > 0) {             StringBuilder sb = new StringBuilder().             for (Exception e : exceptedExceptions) {                 sb.append("\n").append(e.getMessage()).             }             logger.debug("Indexing exceptions during disruption: {}", sb).         }     } }
true;public;0;50;/**  * Test that a document which is indexed on the majority side of a partition, is available from the minority side,  * once the partition is healed  */ ;/**  * Test that a document which is indexed on the majority side of a partition, is available from the minority side,  * once the partition is healed  */ public void testRejoinDocumentExistsInAllShardCopies() throws Exception {     List<String> nodes = startCluster(3).     assertAcked(prepareCreate("test").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 2)).get()).     ensureGreen("test").     nodes = new ArrayList<>(nodes).     Collections.shuffle(nodes, random()).     String isolatedNode = nodes.get(0).     String notIsolatedNode = nodes.get(1).     TwoPartitions partitions = isolateNode(isolatedNode).     NetworkDisruption scheme = addRandomDisruptionType(partitions).     scheme.startDisrupting().     ensureStableCluster(2, notIsolatedNode).     assertFalse(client(notIsolatedNode).admin().cluster().prepareHealth("test").setWaitForYellowStatus().get().isTimedOut()).     IndexResponse indexResponse = internalCluster().client(notIsolatedNode).prepareIndex("test", "type").setSource("field", "value").get().     assertThat(indexResponse.getVersion(), equalTo(1L)).     logger.info("Verifying if document exists via node[{}]", notIsolatedNode).     GetResponse getResponse = internalCluster().client(notIsolatedNode).prepareGet("test", "type", indexResponse.getId()).setPreference("_local").get().     assertThat(getResponse.isExists(), is(true)).     assertThat(getResponse.getVersion(), equalTo(1L)).     assertThat(getResponse.getId(), equalTo(indexResponse.getId())).     scheme.stopDisrupting().     ensureStableCluster(3).     ensureGreen("test").     for (String node : nodes) {         logger.info("Verifying if document exists after isolating node[{}] via node[{}]", isolatedNode, node).         getResponse = internalCluster().client(node).prepareGet("test", "type", indexResponse.getId()).setPreference("_local").get().         assertThat(getResponse.isExists(), is(true)).         assertThat(getResponse.getVersion(), equalTo(1L)).         assertThat(getResponse.getId(), equalTo(indexResponse.getId())).     } }
false;public;1;5;;@Override public void onResponse(final Void aVoid) {     success.set(true).     latch.countDown(). }
false;public;1;6;;@Override public void onFailure(Exception e) {     success.set(false).     latch.countDown().     assert false. }
true;public;0;68;// simulate handling of sending shard failure during an isolation ;// simulate handling of sending shard failure during an isolation public void testSendingShardFailure() throws Exception {     List<String> nodes = startCluster(3).     String masterNode = internalCluster().getMasterName().     List<String> nonMasterNodes = nodes.stream().filter(node -> !node.equals(masterNode)).collect(Collectors.toList()).     String nonMasterNode = randomFrom(nonMasterNodes).     assertAcked(prepareCreate("test").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 3).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 2))).     ensureGreen().     String nonMasterNodeId = internalCluster().clusterService(nonMasterNode).localNode().getId().     // fail a random shard     ShardRouting failedShard = randomFrom(clusterService().state().getRoutingNodes().node(nonMasterNodeId).shardsWithState(ShardRoutingState.STARTED)).     ShardStateAction service = internalCluster().getInstance(ShardStateAction.class, nonMasterNode).     CountDownLatch latch = new CountDownLatch(1).     AtomicBoolean success = new AtomicBoolean().     String isolatedNode = randomBoolean() ? masterNode : nonMasterNode.     TwoPartitions partitions = isolateNode(isolatedNode).     // we cannot use the NetworkUnresponsive disruption type here as it will swallow the "shard failed" request, calling neither     // onSuccess nor onFailure on the provided listener.     NetworkLinkDisruptionType disruptionType = new NetworkDisconnect().     NetworkDisruption networkDisruption = new NetworkDisruption(partitions, disruptionType).     setDisruptionScheme(networkDisruption).     networkDisruption.startDisrupting().     service.localShardFailed(failedShard, "simulated", new CorruptIndexException("simulated", (String) null), new ActionListener<Void>() {          @Override         public void onResponse(final Void aVoid) {             success.set(true).             latch.countDown().         }          @Override         public void onFailure(Exception e) {             success.set(false).             latch.countDown().             assert false.         }     }).     if (isolatedNode.equals(nonMasterNode)) {         assertNoMaster(nonMasterNode).     } else {         ensureStableCluster(2, nonMasterNode).     }     // heal the partition     networkDisruption.removeAndEnsureHealthy(internalCluster()).     // the cluster should stabilize     ensureStableCluster(3).     latch.await().     // the listener should be notified     assertTrue(success.get()).     // the failed shard should be gone     List<ShardRouting> shards = clusterService().state().getRoutingTable().allShards("test").     for (ShardRouting shard : shards) {         assertThat(shard.allocationId(), not(equalTo(failedShard.allocationId()))).     } }
false;public;1;4;;@Override public boolean clearData(String nodeName) {     return true. }
false;public;1;4;;@Override public Settings onNodeStopped(String nodeName) {     return Settings.builder().put(ClusterBootstrapService.INITIAL_MASTER_NODES_SETTING.getKey(), nodeName).build(). }
false;public;0;4;;@Override public boolean validateClusterForming() {     return false. }
false;public;0;26;;public void testCannotJoinIfMasterLostDataFolder() throws Exception {     String masterNode = internalCluster().startMasterOnlyNode().     String dataNode = internalCluster().startDataOnlyNode().     internalCluster().restartNode(masterNode, new InternalTestCluster.RestartCallback() {          @Override         public boolean clearData(String nodeName) {             return true.         }          @Override         public Settings onNodeStopped(String nodeName) {             return Settings.builder().put(ClusterBootstrapService.INITIAL_MASTER_NODES_SETTING.getKey(), nodeName).build().         }          @Override         public boolean validateClusterForming() {             return false.         }     }).     assertFalse(internalCluster().client(masterNode).admin().cluster().prepareHealth().get().isTimedOut()).     assertTrue(internalCluster().client(masterNode).admin().cluster().prepareHealth().setWaitForNodes("2").setTimeout("2s").get().isTimedOut()).     // otherwise we will fail during clean-up     internalCluster().stopRandomNode(InternalTestCluster.nameFilter(dataNode)). }
true;public;0;26;/**  * Tests that indices are properly deleted even if there is a master transition in between.  * Test for https://github.com/elastic/elasticsearch/issues/11665  */ ;/**  * Tests that indices are properly deleted even if there is a master transition in between.  * Test for https://github.com/elastic/elasticsearch/issues/11665  */ public void testIndicesDeleted() throws Exception {     final String idxName = "test".     final List<String> allMasterEligibleNodes = internalCluster().startMasterOnlyNodes(2).     final String dataNode = internalCluster().startDataOnlyNode().     ensureStableCluster(3).     assertAcked(prepareCreate("test")).     final String masterNode1 = internalCluster().getMasterName().     NetworkDisruption networkDisruption = new NetworkDisruption(new TwoPartitions(masterNode1, dataNode), new NetworkDisruption.NetworkUnresponsive()).     internalCluster().setDisruptionScheme(networkDisruption).     networkDisruption.startDisrupting().     // We know this will time out due to the partition, we check manually below to not proceed until     // the delete has been applied to the master node and the master eligible node.     internalCluster().client(masterNode1).admin().indices().prepareDelete(idxName).setTimeout("0s").get().     // Don't restart the master node until we know the index deletion has taken effect on master and the master eligible node.     assertBusy(() -> {         for (String masterNode : allMasterEligibleNodes) {             final ClusterState masterState = internalCluster().clusterService(masterNode).state().             assertTrue("index not deleted on " + masterNode, masterState.metaData().hasIndex(idxName) == false).         }     }).     internalCluster().restartNode(masterNode1, InternalTestCluster.EMPTY_CALLBACK).     ensureYellow().     assertFalse(client().admin().indices().prepareExists(idxName).get().isExists()). }
