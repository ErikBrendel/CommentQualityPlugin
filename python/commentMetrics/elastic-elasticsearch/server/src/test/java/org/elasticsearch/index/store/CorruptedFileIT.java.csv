commented;modifiers;parameterAmount;loc;comment;code
false;protected;1;11;;@Override protected Settings nodeSettings(int nodeOrdinal) {     return Settings.builder().put(super.nodeSettings(nodeOrdinal)).put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_INCOMING_RECOVERIES_SETTING.getKey(), 5).put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_OUTGOING_RECOVERIES_SETTING.getKey(), 5).build(). }
false;protected;0;8;;@Override protected Collection<Class<? extends Plugin>> nodePlugins() {     return Arrays.asList(MockTransportService.TestPlugin.class, MockIndexEventListener.TestPlugin.class, MockFSIndexStore.TestPlugin.class, InternalSettingsPlugin.class). }
false;public;3;25;;@Override public void afterIndexShardClosed(ShardId sid, @Nullable IndexShard indexShard, Settings indexSettings) {     if (indexShard != null) {         Store store = indexShard.store().         store.incRef().         try {             if (!Lucene.indexExists(store.directory()) && indexShard.state() == IndexShardState.STARTED) {                 return.             }             BytesStreamOutput os = new BytesStreamOutput().             PrintStream out = new PrintStream(os, false, StandardCharsets.UTF_8.name()).             CheckIndex.Status status = store.checkIndex(out).             out.flush().             if (!status.clean) {                 logger.warn("check index [failure]\n{}", os.bytes().utf8ToString()).                 throw new IOException("index check failure").             }         } catch (Exception e) {             exception.add(e).         } finally {             store.decRef().             latch.countDown().         }     } }
true;public;0;107;/**  * Tests that we can actually recover from a corruption on the primary given that we have replica shards around.  */ ;/**  * Tests that we can actually recover from a corruption on the primary given that we have replica shards around.  */ public void testCorruptFileAndRecover() throws ExecutionException, InterruptedException, IOException {     int numDocs = scaledRandomIntBetween(100, 1000).     // have enough space for 3 copies     internalCluster().ensureAtLeastNumDataNodes(3).     if (cluster().numDataNodes() == 3) {         logger.info("--> cluster has [3] data nodes, corrupted primary will be overwritten").     }     assertThat(cluster().numDataNodes(), greaterThanOrEqualTo(3)).     assertAcked(prepareCreate("test").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, "1").put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "1").put(MergePolicyConfig.INDEX_MERGE_ENABLED, false).put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), // no checkindex - we corrupt shards on purpose     false).put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)))).     ensureGreen().     disableAllocation("test").     IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs].     for (int i = 0. i < builders.length. i++) {         builders[i] = client().prepareIndex("test", "type").setSource("field", "value").     }     indexRandom(true, builders).     ensureGreen().     assertAllSuccessful(client().admin().indices().prepareFlush().setForce(true).execute().actionGet()).     // we have to flush at least once here since we don't corrupt the translog     SearchResponse countResponse = client().prepareSearch().setSize(0).get().     assertHitCount(countResponse, numDocs).     final int numShards = numShards("test").     ShardRouting corruptedShardRouting = corruptRandomPrimaryFile().     logger.info("--> {} corrupted", corruptedShardRouting).     enableAllocation("test").     /*          * we corrupted the primary shard - now lets make sure we never recover from it successfully          */     Settings build = Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "2").build().     client().admin().indices().prepareUpdateSettings("test").setSettings(build).get().     ClusterHealthResponse health = client().admin().cluster().health(Requests.clusterHealthRequest("test").waitForGreenStatus().timeout(// sometimes due to cluster rebalacing and random settings default timeout is just not enough.     "5m").waitForNoRelocatingShards(true)).actionGet().     if (health.isTimedOut()) {         logger.info("cluster state:\n{}\n{}", client().admin().cluster().prepareState().get().getState(), client().admin().cluster().preparePendingClusterTasks().get()).         assertThat("timed out waiting for green state", health.isTimedOut(), equalTo(false)).     }     assertThat(health.getStatus(), equalTo(ClusterHealthStatus.GREEN)).     final int numIterations = scaledRandomIntBetween(5, 20).     for (int i = 0. i < numIterations. i++) {         SearchResponse response = client().prepareSearch().setSize(numDocs).get().         assertHitCount(response, numDocs).     }     /*          * now hook into the IndicesService and register a close listener to          * run the checkindex. if the corruption is still there we will catch it.          */     // primary + 2 replicas     final CountDownLatch latch = new CountDownLatch(numShards * 3).     final CopyOnWriteArrayList<Exception> exception = new CopyOnWriteArrayList<>().     final IndexEventListener listener = new IndexEventListener() {          @Override         public void afterIndexShardClosed(ShardId sid, @Nullable IndexShard indexShard, Settings indexSettings) {             if (indexShard != null) {                 Store store = indexShard.store().                 store.incRef().                 try {                     if (!Lucene.indexExists(store.directory()) && indexShard.state() == IndexShardState.STARTED) {                         return.                     }                     BytesStreamOutput os = new BytesStreamOutput().                     PrintStream out = new PrintStream(os, false, StandardCharsets.UTF_8.name()).                     CheckIndex.Status status = store.checkIndex(out).                     out.flush().                     if (!status.clean) {                         logger.warn("check index [failure]\n{}", os.bytes().utf8ToString()).                         throw new IOException("index check failure").                     }                 } catch (Exception e) {                     exception.add(e).                 } finally {                     store.decRef().                     latch.countDown().                 }             }         }     }.     for (MockIndexEventListener.TestEventListener eventListener : internalCluster().getDataNodeInstances(MockIndexEventListener.TestEventListener.class)) {         eventListener.setNewDelegate(listener).     }     try {         client().admin().indices().prepareDelete("test").get().         latch.await().         assertThat(exception, empty()).     } finally {         for (MockIndexEventListener.TestEventListener eventListener : internalCluster().getDataNodeInstances(MockIndexEventListener.TestEventListener.class)) {             eventListener.setNewDelegate(null).         }     } }
true;public;0;67;/**  * Tests corruption that happens on a single shard when no replicas are present. We make sure that the primary stays unassigned  * and all other replicas for the healthy shards happens  */ ;/**  * Tests corruption that happens on a single shard when no replicas are present. We make sure that the primary stays unassigned  * and all other replicas for the healthy shards happens  */ public void testCorruptPrimaryNoReplica() throws ExecutionException, InterruptedException, IOException {     int numDocs = scaledRandomIntBetween(100, 1000).     internalCluster().ensureAtLeastNumDataNodes(2).     assertAcked(prepareCreate("test").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "0").put(MergePolicyConfig.INDEX_MERGE_ENABLED, false).put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), // no checkindex - we corrupt shards on purpose     false).put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)))).     ensureGreen().     IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs].     for (int i = 0. i < builders.length. i++) {         builders[i] = client().prepareIndex("test", "type").setSource("field", "value").     }     indexRandom(true, builders).     ensureGreen().     assertAllSuccessful(client().admin().indices().prepareFlush().setForce(true).execute().actionGet()).     // we have to flush at least once here since we don't corrupt the translog     SearchResponse countResponse = client().prepareSearch().setSize(0).get().     assertHitCount(countResponse, numDocs).     ShardRouting shardRouting = corruptRandomPrimaryFile().     /*          * we corrupted the primary shard - now lets make sure we never recover from it successfully          */     Settings build = Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "1").build().     client().admin().indices().prepareUpdateSettings("test").setSettings(build).get().     client().admin().cluster().prepareReroute().get().     boolean didClusterTurnRed = awaitBusy(() -> {         ClusterHealthStatus test = client().admin().cluster().health(Requests.clusterHealthRequest("test")).actionGet().getStatus().         return test == ClusterHealthStatus.RED.     }, 5, // sometimes on slow nodes the replication / recovery is just dead slow     TimeUnit.MINUTES).     final ClusterHealthResponse response = client().admin().cluster().health(Requests.clusterHealthRequest("test")).get().     if (response.getStatus() != ClusterHealthStatus.RED) {         logger.info("Cluster turned red in busy loop: {}", didClusterTurnRed).         logger.info("cluster state:\n{}\n{}", client().admin().cluster().prepareState().get().getState(), client().admin().cluster().preparePendingClusterTasks().get()).     }     assertThat(response.getStatus(), is(ClusterHealthStatus.RED)).     ClusterState state = client().admin().cluster().prepareState().get().getState().     GroupShardsIterator<ShardIterator> shardIterators = state.getRoutingTable().activePrimaryShardsGrouped(new String[] { "test" }, false).     for (ShardIterator iterator : shardIterators) {         ShardRouting routing.         while ((routing = iterator.nextOrNull()) != null) {             if (routing.getId() == shardRouting.getId()) {                 assertThat(routing.state(), equalTo(ShardRoutingState.UNASSIGNED)).             } else {                 assertThat(routing.state(), anyOf(equalTo(ShardRoutingState.RELOCATING), equalTo(ShardRoutingState.STARTED))).             }         }     }     final List<Path> files = listShardFiles(shardRouting).     Path corruptedFile = null.     for (Path file : files) {         if (file.getFileName().toString().startsWith("corrupted_")) {             corruptedFile = file.             break.         }     }     assertThat(corruptedFile, notNullValue()). }
true;public;0;51;/**  * This test triggers a corrupt index exception during finalization size if an empty commit point is transferred  * during recovery we don't know the version of the segments_N file because it has no segments we can take it from.  * This simulates recoveries from old indices or even without checksums and makes sure if we fail during finalization  * we also check if the primary is ok. Without the relevant checks this test fails with a RED cluster  */ ;/**  * This test triggers a corrupt index exception during finalization size if an empty commit point is transferred  * during recovery we don't know the version of the segments_N file because it has no segments we can take it from.  * This simulates recoveries from old indices or even without checksums and makes sure if we fail during finalization  * we also check if the primary is ok. Without the relevant checks this test fails with a RED cluster  */ public void testCorruptionOnNetworkLayerFinalizingRecovery() throws ExecutionException, InterruptedException, IOException {     internalCluster().ensureAtLeastNumDataNodes(2).     NodesStatsResponse nodeStats = client().admin().cluster().prepareNodesStats().get().     List<NodeStats> dataNodeStats = new ArrayList<>().     for (NodeStats stat : nodeStats.getNodes()) {         if (stat.getNode().isDataNode()) {             dataNodeStats.add(stat).         }     }     assertThat(dataNodeStats.size(), greaterThanOrEqualTo(2)).     Collections.shuffle(dataNodeStats, random()).     NodeStats primariesNode = dataNodeStats.get(0).     NodeStats unluckyNode = dataNodeStats.get(1).     assertAcked(prepareCreate("test").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "0").put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put("index.routing.allocation.include._name", primariesNode.getNode().getName()).put(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), EnableAllocationDecider.Rebalance.NONE).put("index.allocation.max_retries", // keep on retrying     Integer.MAX_VALUE))).     // allocated with empty commit     ensureGreen().     final AtomicBoolean corrupt = new AtomicBoolean(true).     final CountDownLatch hasCorrupted = new CountDownLatch(1).     for (NodeStats dataNode : dataNodeStats) {         MockTransportService mockTransportService = ((MockTransportService) internalCluster().getInstance(TransportService.class, dataNode.getNode().getName())).         mockTransportService.addSendBehavior(internalCluster().getInstance(TransportService.class, unluckyNode.getNode().getName()), (connection, requestId, action, request, options) -> {             if (corrupt.get() && action.equals(PeerRecoveryTargetService.Actions.FILE_CHUNK)) {                 RecoveryFileChunkRequest req = (RecoveryFileChunkRequest) request.                 byte[] array = BytesRef.deepCopyOf(req.content().toBytesRef()).bytes.                 int i = randomIntBetween(0, req.content().length() - 1).                 // flip one byte in the content                 array[i] = (byte) ~array[i].                 hasCorrupted.countDown().             }             connection.sendRequest(requestId, action, request, options).         }).     }     Settings build = Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "1").put("index.routing.allocation.include._name", primariesNode.getNode().getName() + "," + unluckyNode.getNode().getName()).build().     client().admin().indices().prepareUpdateSettings("test").setSettings(build).get().     client().admin().cluster().prepareReroute().get().     hasCorrupted.await().     corrupt.set(false).     ensureGreen(). }
true;public;0;95;/**  * Tests corruption that happens on the network layer and that the primary does not get affected by corruption that happens on the way  * to the replica. The file on disk stays uncorrupted  */ ;/**  * Tests corruption that happens on the network layer and that the primary does not get affected by corruption that happens on the way  * to the replica. The file on disk stays uncorrupted  */ public void testCorruptionOnNetworkLayer() throws ExecutionException, InterruptedException {     int numDocs = scaledRandomIntBetween(100, 1000).     internalCluster().ensureAtLeastNumDataNodes(2).     if (cluster().numDataNodes() < 3) {         internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), true).put(Node.NODE_MASTER_SETTING.getKey(), false)).     }     NodesStatsResponse nodeStats = client().admin().cluster().prepareNodesStats().get().     List<NodeStats> dataNodeStats = new ArrayList<>().     for (NodeStats stat : nodeStats.getNodes()) {         if (stat.getNode().isDataNode()) {             dataNodeStats.add(stat).         }     }     assertThat(dataNodeStats.size(), greaterThanOrEqualTo(2)).     Collections.shuffle(dataNodeStats, random()).     NodeStats primariesNode = dataNodeStats.get(0).     NodeStats unluckyNode = dataNodeStats.get(1).     assertAcked(prepareCreate("test").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "0").put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, // don't go crazy here it must recovery fast     between(1, 4)).put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), false).put("index.routing.allocation.include._name", primariesNode.getNode().getName()).put(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), EnableAllocationDecider.Rebalance.NONE))).     ensureGreen().     IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs].     for (int i = 0. i < builders.length. i++) {         builders[i] = client().prepareIndex("test", "type").setSource("field", "value").     }     indexRandom(true, builders).     ensureGreen().     assertAllSuccessful(client().admin().indices().prepareFlush().setForce(true).execute().actionGet()).     // we have to flush at least once here since we don't corrupt the translog     SearchResponse countResponse = client().prepareSearch().setSize(0).get().     assertHitCount(countResponse, numDocs).     final boolean truncate = randomBoolean().     for (NodeStats dataNode : dataNodeStats) {         MockTransportService mockTransportService = ((MockTransportService) internalCluster().getInstance(TransportService.class, dataNode.getNode().getName())).         mockTransportService.addSendBehavior(internalCluster().getInstance(TransportService.class, unluckyNode.getNode().getName()), (connection, requestId, action, request, options) -> {             if (action.equals(PeerRecoveryTargetService.Actions.FILE_CHUNK)) {                 RecoveryFileChunkRequest req = (RecoveryFileChunkRequest) request.                 if (truncate && req.length() > 1) {                     BytesRef bytesRef = req.content().toBytesRef().                     BytesArray array = new BytesArray(bytesRef.bytes, bytesRef.offset, (int) req.length() - 1).                     request = new RecoveryFileChunkRequest(req.recoveryId(), req.shardId(), req.metadata(), req.position(), array, req.lastChunk(), req.totalTranslogOps(), req.sourceThrottleTimeInNanos()).                 } else {                     assert req.content().toBytesRef().bytes == req.content().toBytesRef().bytes : "no internal reference!!".                     final byte[] array = req.content().toBytesRef().bytes.                     int i = randomIntBetween(0, req.content().length() - 1).                     // flip one byte in the content                     array[i] = (byte) ~array[i].                 }             }             connection.sendRequest(requestId, action, request, options).         }).     }     Settings build = Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "1").put("index.routing.allocation.include._name", "*").build().     client().admin().indices().prepareUpdateSettings("test").setSettings(build).get().     client().admin().cluster().prepareReroute().get().     ClusterHealthResponse actionGet = client().admin().cluster().health(Requests.clusterHealthRequest("test").waitForGreenStatus()).actionGet().     if (actionGet.isTimedOut()) {         logger.info("ensureGreen timed out, cluster state:\n{}\n{}", client().admin().cluster().prepareState().get().getState(), client().admin().cluster().preparePendingClusterTasks().get()).         assertThat("timed out waiting for green state", actionGet.isTimedOut(), equalTo(false)).     }     // we are green so primaries got not corrupted.     // ensure that no shard is actually allocated on the unlucky node     ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().get().     for (IndexShardRoutingTable table : clusterStateResponse.getState().getRoutingTable().index("test")) {         for (ShardRouting routing : table) {             if (unluckyNode.getNode().getId().equals(routing.currentNodeId())) {                 assertThat(routing.state(), not(equalTo(ShardRoutingState.STARTED))).                 assertThat(routing.state(), not(equalTo(ShardRoutingState.RELOCATING))).             }         }     }     final int numIterations = scaledRandomIntBetween(5, 20).     for (int i = 0. i < numIterations. i++) {         SearchResponse response = client().prepareSearch().setSize(numDocs).get().         assertHitCount(response, numDocs).     } }
true;public;0;56;/**  * Tests that restoring of a corrupted shard fails and we get a partial snapshot.  * TODO once checksum verification on snapshotting is implemented this test needs to be fixed or split into several  * parts... We should also corrupt files on the actual snapshot and check that we don't restore the corrupted shard.  */ ;/**  * Tests that restoring of a corrupted shard fails and we get a partial snapshot.  * TODO once checksum verification on snapshotting is implemented this test needs to be fixed or split into several  * parts... We should also corrupt files on the actual snapshot and check that we don't restore the corrupted shard.  */ public void testCorruptFileThenSnapshotAndRestore() throws ExecutionException, InterruptedException, IOException {     int numDocs = scaledRandomIntBetween(100, 1000).     internalCluster().ensureAtLeastNumDataNodes(2).     assertAcked(prepareCreate("test").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, // no replicas for this test     "0").put(MergePolicyConfig.INDEX_MERGE_ENABLED, false).put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), false).put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)))).     ensureGreen().     IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs].     for (int i = 0. i < builders.length. i++) {         builders[i] = client().prepareIndex("test", "type").setSource("field", "value").     }     indexRandom(true, builders).     ensureGreen().     assertAllSuccessful(client().admin().indices().prepareFlush().setForce(true).execute().actionGet()).     // we have to flush at least once here since we don't corrupt the translog     SearchResponse countResponse = client().prepareSearch().setSize(0).get().     assertHitCount(countResponse, numDocs).     ShardRouting shardRouting = corruptRandomPrimaryFile(false).     logger.info("--> shard {} has a corrupted file", shardRouting).     // we don't corrupt segments.gen since S/R doesn't snapshot this file     // the other problem here why we can't corrupt segments.X files is that the snapshot flushes again before     // it snapshots and that will write a new segments.X+1 file     logger.info("-->  creating repository").     assertAcked(client().admin().cluster().preparePutRepository("test-repo").setType("fs").setSettings(Settings.builder().put("location", randomRepoPath().toAbsolutePath()).put("compress", randomBoolean()).put("chunk_size", randomIntBetween(100, 1000), ByteSizeUnit.BYTES))).     logger.info("--> snapshot").     final CreateSnapshotResponse createSnapshotResponse = client().admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test").get().     final SnapshotState snapshotState = createSnapshotResponse.getSnapshotInfo().state().     logger.info("--> snapshot terminated with state " + snapshotState).     final List<Path> files = listShardFiles(shardRouting).     Path corruptedFile = null.     for (Path file : files) {         if (file.getFileName().toString().startsWith("corrupted_")) {             corruptedFile = file.             break.         }     }     assertThat(createSnapshotResponse.getSnapshotInfo().state(), equalTo(SnapshotState.PARTIAL)).     assertThat(corruptedFile, notNullValue()). }
true;public;0;60;/**  * This test verifies that if we corrupt a replica, we can still get to green, even though  * listing its store fails. Note, we need to make sure that replicas are allocated on all data  * nodes, so that replica won't be sneaky and allocated on a node that doesn't have a corrupted  * replica.  */ ;/**  * This test verifies that if we corrupt a replica, we can still get to green, even though  * listing its store fails. Note, we need to make sure that replicas are allocated on all data  * nodes, so that replica won't be sneaky and allocated on a node that doesn't have a corrupted  * replica.  */ public void testReplicaCorruption() throws Exception {     int numDocs = scaledRandomIntBetween(100, 1000).     internalCluster().ensureAtLeastNumDataNodes(2).     assertAcked(prepareCreate("test").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, cluster().numDataNodes() - 1).put(MergePolicyConfig.INDEX_MERGE_ENABLED, false).put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), // no checkindex - we corrupt shards on purpose     false).put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), // no translog based flush - it might change the .liv / segments.N files     new ByteSizeValue(1, ByteSizeUnit.PB)))).     ensureGreen().     IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs].     for (int i = 0. i < builders.length. i++) {         builders[i] = client().prepareIndex("test", "type").setSource("field", "value").     }     indexRandom(true, builders).     ensureGreen().     assertAllSuccessful(client().admin().indices().prepareFlush().setForce(true).execute().actionGet()).     // we have to flush at least once here since we don't corrupt the translog     SearchResponse countResponse = client().prepareSearch().setSize(0).get().     assertHitCount(countResponse, numDocs).     // disable allocations of replicas post restart (the restart will change replicas to primaries, so we have     // to capture replicas post restart)     assertAcked(client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "primaries"))).     internalCluster().fullRestart().     ensureYellow().     final Index index = resolveIndex("test").     final IndicesShardStoresResponse stores = client().admin().indices().prepareShardStores(index.getName()).get().     for (IntObjectCursor<List<IndicesShardStoresResponse.StoreStatus>> shards : stores.getStoreStatuses().get(index.getName())) {         for (IndicesShardStoresResponse.StoreStatus store : shards.value) {             final ShardId shardId = new ShardId(index, shards.key).             if (store.getAllocationStatus().equals(IndicesShardStoresResponse.StoreStatus.AllocationStatus.UNUSED)) {                 for (Path path : findFilesToCorruptOnNode(store.getNode().getName(), shardId)) {                     try (OutputStream os = Files.newOutputStream(path)) {                         os.write(0).                     }                     logger.info("corrupting file {} on node {}", path, store.getNode().getName()).                 }             }         }     }     // enable allocation     assertAcked(client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder().putNull(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey()))).     ensureGreen(). }
false;private;1;5;;private int numShards(String... index) {     ClusterState state = client().admin().cluster().prepareState().get().getState().     GroupShardsIterator shardIterators = state.getRoutingTable().activePrimaryShardsGrouped(index, false).     return shardIterators.size(). }
false;private;2;16;;private List<Path> findFilesToCorruptOnNode(final String nodeName, final ShardId shardId) throws IOException {     List<Path> files = new ArrayList<>().     for (Path path : internalCluster().getInstance(NodeEnvironment.class, nodeName).availableShardPaths(shardId)) {         path = path.resolve("index").         if (Files.exists(path)) {             // multi data path might only have one path in use             try (DirectoryStream<Path> stream = Files.newDirectoryStream(path)) {                 for (Path item : stream) {                     if (item.getFileName().toString().startsWith("segments_")) {                         files.add(item).                     }                 }             }         }     }     return files. }
false;private;0;3;;private ShardRouting corruptRandomPrimaryFile() throws IOException {     return corruptRandomPrimaryFile(true). }
false;private;1;41;;private ShardRouting corruptRandomPrimaryFile(final boolean includePerCommitFiles) throws IOException {     ClusterState state = client().admin().cluster().prepareState().get().getState().     Index test = state.metaData().index("test").getIndex().     GroupShardsIterator shardIterators = state.getRoutingTable().activePrimaryShardsGrouped(new String[] { "test" }, false).     List<ShardIterator> iterators = iterableAsArrayList(shardIterators).     ShardIterator shardIterator = RandomPicks.randomFrom(random(), iterators).     ShardRouting shardRouting = shardIterator.nextOrNull().     assertNotNull(shardRouting).     assertTrue(shardRouting.primary()).     assertTrue(shardRouting.assignedToNode()).     String nodeId = shardRouting.currentNodeId().     NodesStatsResponse nodeStatses = client().admin().cluster().prepareNodesStats(nodeId).setFs(true).get().     // treeset makes sure iteration order is deterministic     Set<Path> files = new TreeSet<>().     for (FsInfo.Path info : nodeStatses.getNodes().get(0).getFs()) {         String path = info.getPath().         Path file = PathUtils.get(path).resolve("indices").resolve(test.getUUID()).resolve(Integer.toString(shardRouting.getId())).resolve("index").         if (Files.exists(file)) {             // multi data path might only have one path in use             try (Directory dir = FSDirectory.open(file)) {                 SegmentInfos segmentCommitInfos = Lucene.readSegmentInfos(dir).                 if (includePerCommitFiles) {                     files.add(file.resolve(segmentCommitInfos.getSegmentsFileName())).                 }                 for (SegmentCommitInfo commitInfo : segmentCommitInfos) {                     if (commitInfo.getDelCount() + commitInfo.getSoftDelCount() == commitInfo.info.maxDoc()) {                         // don't corrupt fully deleted segments - they might be removed on snapshot                         continue.                     }                     for (String commitFile : commitInfo.files()) {                         if (includePerCommitFiles || isPerSegmentFile(commitFile)) {                             files.add(file.resolve(commitFile)).                         }                     }                 }             }         }     }     CorruptionUtils.corruptFile(random(), files.toArray(new Path[0])).     return shardRouting. }
false;private,static;1;4;;private static boolean isPerCommitFile(String fileName) {     // .liv and segments_N are per commit files and might change after corruption     return fileName.startsWith("segments") || fileName.endsWith(".liv"). }
false;private,static;1;3;;private static boolean isPerSegmentFile(String fileName) {     return isPerCommitFile(fileName) == false. }
false;public;1;19;;public List<Path> listShardFiles(ShardRouting routing) throws IOException {     NodesStatsResponse nodeStatses = client().admin().cluster().prepareNodesStats(routing.currentNodeId()).setFs(true).get().     ClusterState state = client().admin().cluster().prepareState().get().getState().     final Index test = state.metaData().index("test").getIndex().     assertThat(routing.toString(), nodeStatses.getNodes().size(), equalTo(1)).     List<Path> files = new ArrayList<>().     for (FsInfo.Path info : nodeStatses.getNodes().get(0).getFs()) {         String path = info.getPath().         Path file = PathUtils.get(path).resolve("indices/" + test.getUUID() + "/" + Integer.toString(routing.getId()) + "/index").         if (Files.exists(file)) {             // multi data path might only have one path in use             try (DirectoryStream<Path> stream = Files.newDirectoryStream(file)) {                 for (Path item : stream) {                     files.add(item).                 }             }         }     }     return files. }
