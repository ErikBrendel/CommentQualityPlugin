commented;modifiers;parameterAmount;loc;comment;code
false;protected;0;6;;@Override protected Collection<Class<? extends Plugin>> nodePlugins() {     final HashSet<Class<? extends Plugin>> classes = new HashSet<>(super.nodePlugins()).     classes.add(MockTransportService.TestPlugin.class).     return classes. }
false;public;0;131;;public void testTwoNodesNoMasterBlock() throws Exception {     internalCluster().setBootstrapMasterNodeIndex(1).     Settings settings = Settings.builder().put("discovery.initial_state_timeout", "500ms").build().     logger.info("--> start first node").     String node1Name = internalCluster().startNode(settings).     logger.info("--> should be blocked, no master...").     ClusterState state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().     assertThat(state.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(true)).     // verify that we still see the local node in the cluster state     assertThat(state.nodes().getSize(), equalTo(1)).     logger.info("--> start second node, cluster should be formed").     String node2Name = internalCluster().startNode(settings).     ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes("2").execute().actionGet().     assertThat(clusterHealthResponse.isTimedOut(), equalTo(false)).     state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().     assertThat(state.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(false)).     state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().     assertThat(state.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(false)).     state = client().admin().cluster().prepareState().execute().actionGet().getState().     assertThat(state.nodes().getSize(), equalTo(2)).     assertThat(state.metaData().indices().containsKey("test"), equalTo(false)).     createIndex("test").     NumShards numShards = getNumShards("test").     logger.info("--> indexing some data").     for (int i = 0. i < 100. i++) {         client().prepareIndex("test", "type1", Integer.toString(i)).setSource("field", "value").execute().actionGet().     }     // make sure that all shards recovered before trying to flush     assertThat(client().admin().cluster().prepareHealth("test").setWaitForActiveShards(numShards.totalNumShards).execute().actionGet().getActiveShards(), equalTo(numShards.totalNumShards)).     // flush for simpler debugging     flushAndRefresh().     logger.info("--> verify we get the data back").     for (int i = 0. i < 10. i++) {         assertThat(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet().getHits().getTotalHits().value, equalTo(100L)).     }     String masterNode = internalCluster().getMasterName().     String otherNode = node1Name.equals(masterNode) ? node2Name : node1Name.     logger.info("--> add voting config exclusion for non-master node, to be sure it's not elected").     client().execute(AddVotingConfigExclusionsAction.INSTANCE, new AddVotingConfigExclusionsRequest(new String[] { otherNode })).get().     logger.info("--> stop master node, no master block should appear").     internalCluster().stopRandomNode(InternalTestCluster.nameFilter(masterNode)).     awaitBusy(() -> {         ClusterState clusterState = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().         return clusterState.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID).     }).     state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().     assertThat(state.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(true)).     // verify that both nodes are still in the cluster state but there is no master     assertThat(state.nodes().getSize(), equalTo(2)).     assertThat(state.nodes().getMasterNode(), equalTo(null)).     logger.info("--> starting the previous master node again...").     node2Name = internalCluster().startNode(settings).     clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().setWaitForNodes("2").execute().actionGet().     assertThat(clusterHealthResponse.isTimedOut(), equalTo(false)).     state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().     assertThat(state.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(false)).     state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().     assertThat(state.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(false)).     state = client().admin().cluster().prepareState().execute().actionGet().getState().     assertThat(state.nodes().getSize(), equalTo(2)).     assertThat(state.metaData().indices().containsKey("test"), equalTo(true)).     ensureGreen().     logger.info("--> verify we get the data back after cluster reform").     for (int i = 0. i < 10. i++) {         assertHitCount(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet(), 100).     }     logger.info("--> clearing voting config exclusions").     ClearVotingConfigExclusionsRequest clearRequest = new ClearVotingConfigExclusionsRequest().     clearRequest.setWaitForRemoval(false).     client().execute(ClearVotingConfigExclusionsAction.INSTANCE, clearRequest).get().     masterNode = internalCluster().getMasterName().     otherNode = node1Name.equals(masterNode) ? node2Name : node1Name.     logger.info("--> add voting config exclusion for master node, to be sure it's not elected").     client().execute(AddVotingConfigExclusionsAction.INSTANCE, new AddVotingConfigExclusionsRequest(new String[] { masterNode })).get().     logger.info("--> stop non-master node, no master block should appear").     internalCluster().stopRandomNode(InternalTestCluster.nameFilter(otherNode)).     assertBusy(() -> {         ClusterState state1 = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().         assertThat(state1.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(true)).     }).     logger.info("--> starting the previous master node again...").     internalCluster().startNode(settings).     ensureGreen().     clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes("2").setWaitForGreenStatus().execute().actionGet().     assertThat(clusterHealthResponse.isTimedOut(), equalTo(false)).     state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().     assertThat(state.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(false)).     state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().     assertThat(state.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(false)).     state = client().admin().cluster().prepareState().execute().actionGet().getState().     assertThat(state.nodes().getSize(), equalTo(2)).     assertThat(state.metaData().indices().containsKey("test"), equalTo(true)).     logger.info("Running Cluster Health").     ensureGreen().     logger.info("--> verify we the data back").     for (int i = 0. i < 10. i++) {         assertHitCount(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet(), 100).     } }
false;public;0;74;;@AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/39172") public void testThreeNodesNoMasterBlock() throws Exception {     internalCluster().setBootstrapMasterNodeIndex(2).     Settings settings = Settings.builder().put("discovery.initial_state_timeout", "500ms").build().     logger.info("--> start first 2 nodes").     internalCluster().startNodes(2, settings).     ClusterState state.     assertBusy(() -> {         for (Client client : clients()) {             ClusterState state1 = client.admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().             assertThat(state1.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(true)).         }     }).     logger.info("--> start one more node").     internalCluster().startNode(settings).     ensureGreen().     ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes("3").execute().actionGet().     assertThat(clusterHealthResponse.isTimedOut(), equalTo(false)).     state = client().admin().cluster().prepareState().execute().actionGet().getState().     assertThat(state.nodes().getSize(), equalTo(3)).     createIndex("test").     NumShards numShards = getNumShards("test").     logger.info("--> indexing some data").     for (int i = 0. i < 100. i++) {         client().prepareIndex("test", "type1", Integer.toString(i)).setSource("field", "value").execute().actionGet().     }     ensureGreen().     // make sure that all shards recovered before trying to flush     assertThat(client().admin().cluster().prepareHealth("test").setWaitForActiveShards(numShards.totalNumShards).execute().actionGet().isTimedOut(), equalTo(false)).     // flush for simpler debugging     client().admin().indices().prepareFlush().execute().actionGet().     refresh().     logger.info("--> verify we get the data back").     for (int i = 0. i < 10. i++) {         assertHitCount(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet(), 100).     }     internalCluster().stopRandomNonMasterNode().     internalCluster().stopRandomNonMasterNode().     logger.info("--> verify that there is no master anymore on remaining node").     // spin here to wait till the state is set     assertBusy(() -> {         ClusterState st = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState().         assertThat(st.blocks().hasGlobalBlockWithId(NoMasterBlockService.NO_MASTER_BLOCK_ID), equalTo(true)).     }).     logger.info("--> start back the 2 nodes ").     internalCluster().startNodes(2, settings).     internalCluster().validateClusterFormed().     ensureGreen().     state = client().admin().cluster().prepareState().execute().actionGet().getState().     assertThat(state.nodes().getSize(), equalTo(3)).     logger.info("--> verify we the data back").     for (int i = 0. i < 10. i++) {         assertHitCount(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet(), 100).     } }
false;public;3;4;;@Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {     latch.countDown(). }
false;public;1;9;;@Override public ClusterState execute(ClusterState currentState) throws Exception {     logger.debug("--> starting the disruption, preventing cluster state publishing").     partition.startDisrupting().     MetaData.Builder metaData = MetaData.builder(currentState.metaData()).persistentSettings(Settings.builder().put(currentState.metaData().persistentSettings()).put("_SHOULD_NOT_BE_THERE_", true).build()).     return ClusterState.builder(currentState).metaData(metaData).build(). }
false;public;2;5;;@Override public void onFailure(String source, Exception e) {     failure.set(e).     latch.countDown(). }
false;public;0;76;;public void testCannotCommitStateThreeNodes() throws Exception {     internalCluster().setBootstrapMasterNodeIndex(2).     Settings settings = Settings.builder().put("discovery.initial_state_timeout", "500ms").build().     internalCluster().startNodes(3, settings).     ensureStableCluster(3).     final String master = internalCluster().getMasterName().     Set<String> otherNodes = new HashSet<>(Arrays.asList(internalCluster().getNodeNames())).     otherNodes.remove(master).     NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(Collections.singleton(master), otherNodes), new NetworkDisruption.NetworkDisconnect()).     internalCluster().setDisruptionScheme(partition).     final CountDownLatch latch = new CountDownLatch(1).     final AtomicReference<Exception> failure = new AtomicReference<>().     logger.debug("--> submitting for cluster state to be rejected").     final ClusterService masterClusterService = internalCluster().clusterService(master).     masterClusterService.submitStateUpdateTask("test", new ClusterStateUpdateTask() {          @Override         public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {             latch.countDown().         }          @Override         public ClusterState execute(ClusterState currentState) throws Exception {             logger.debug("--> starting the disruption, preventing cluster state publishing").             partition.startDisrupting().             MetaData.Builder metaData = MetaData.builder(currentState.metaData()).persistentSettings(Settings.builder().put(currentState.metaData().persistentSettings()).put("_SHOULD_NOT_BE_THERE_", true).build()).             return ClusterState.builder(currentState).metaData(metaData).build().         }          @Override         public void onFailure(String source, Exception e) {             failure.set(e).             latch.countDown().         }     }).     logger.debug("--> waiting for cluster state to be processed/rejected").     latch.await().     assertThat(failure.get(), instanceOf(FailedToCommitClusterStateException.class)).     logger.debug("--> check that there is no master in minor partition").     assertBusy(() -> assertThat(masterClusterService.state().nodes().getMasterNode(), nullValue())).     // let major partition to elect new master, to ensure that old master is not elected once partition is restored,     // otherwise persistent setting (which is a part of accepted state on old master) will be propagated to other nodes     logger.debug("--> wait for master to be elected in major partition").     assertBusy(() -> {         DiscoveryNode masterNode = internalCluster().client(randomFrom(otherNodes)).admin().cluster().prepareState().execute().actionGet().getState().nodes().getMasterNode().         assertThat(masterNode, notNullValue()).         assertThat(masterNode.getName(), not(equalTo(master))).     }).     partition.stopDisrupting().     logger.debug("--> waiting for cluster to heal").     assertNoTimeout(client().admin().cluster().prepareHealth().setWaitForNodes("3").setWaitForEvents(Priority.LANGUID)).     for (String node : internalCluster().getNodeNames()) {         Settings nodeSetting = internalCluster().clusterService(node).state().metaData().settings().         assertThat(node + " processed the cluster state despite of a min master node violation", nodeSetting.get("_SHOULD_NOT_BE_THERE_"), nullValue()).     } }
