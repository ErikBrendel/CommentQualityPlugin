commented;modifiers;parameterAmount;loc;comment;code
false;public;0;5;;@Override public void setUp() throws Exception {     super.setUp().     format = randomNumericDocValueFormat(). }
false;protected;11;22;;@Override protected InternalSignificantTerms createTestInstance(String name, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData, InternalAggregations aggs, int requiredSize, int numBuckets, long subsetSize, int[] subsetDfs, long supersetSize, int[] supersetDfs, SignificanceHeuristic significanceHeuristic) {     List<SignificantLongTerms.Bucket> buckets = new ArrayList<>(numBuckets).     Set<Long> terms = new HashSet<>().     for (int i = 0. i < numBuckets. ++i) {         long term = randomValueOtherThanMany(l -> terms.add(l) == false, random()::nextLong).         SignificantLongTerms.Bucket bucket = new SignificantLongTerms.Bucket(subsetDfs[i], subsetSize, supersetDfs[i], supersetSize, term, aggs, format).         bucket.updateScore(significanceHeuristic).         buckets.add(bucket).     }     return new SignificantLongTerms(name, requiredSize, 1L, pipelineAggregators, metaData, format, subsetSize, supersetSize, significanceHeuristic, buckets). }
false;protected;0;4;;@Override protected Writeable.Reader<InternalSignificantTerms<?, ?>> instanceReader() {     return SignificantLongTerms::new. }
false;protected;0;4;;@Override protected Class<? extends ParsedMultiBucketAggregation> implementationClass() {     return ParsedSignificantLongTerms.class. }
false;protected;1;78;;@Override protected InternalSignificantTerms<?, ?> mutateInstance(InternalSignificantTerms<?, ?> instance) {     if (instance instanceof SignificantLongTerms) {         SignificantLongTerms longTerms = (SignificantLongTerms) instance.         String name = longTerms.getName().         int requiredSize = longTerms.requiredSize.         long minDocCount = longTerms.minDocCount.         DocValueFormat format = longTerms.format.         long subsetSize = longTerms.getSubsetSize().         long supersetSize = longTerms.getSupersetSize().         List<SignificantLongTerms.Bucket> buckets = longTerms.getBuckets().         SignificanceHeuristic significanceHeuristic = longTerms.significanceHeuristic.         List<PipelineAggregator> pipelineAggregators = longTerms.pipelineAggregators().         Map<String, Object> metaData = longTerms.getMetaData().         switch(between(0, 5)) {             case 0:                 name += randomAlphaOfLength(5).                 break.             case 1:                 requiredSize += between(1, 100).                 break.             case 2:                 minDocCount += between(1, 100).                 break.             case 3:                 subsetSize += between(1, 100).                 break.             case 4:                 supersetSize += between(1, 100).                 break.             case 5:                 buckets = new ArrayList<>(buckets).                 buckets.add(new SignificantLongTerms.Bucket(randomLong(), randomNonNegativeLong(), randomNonNegativeLong(), randomNonNegativeLong(), randomNonNegativeLong(), InternalAggregations.EMPTY, format)).                 break.             case 8:                 if (metaData == null) {                     metaData = new HashMap<>(1).                 } else {                     metaData = new HashMap<>(instance.getMetaData()).                 }                 metaData.put(randomAlphaOfLength(15), randomInt()).                 break.             default:                 throw new AssertionError("Illegal randomisation branch").         }         return new SignificantLongTerms(name, requiredSize, minDocCount, pipelineAggregators, metaData, format, subsetSize, supersetSize, significanceHeuristic, buckets).     } else {         String name = instance.getName().         int requiredSize = instance.requiredSize.         long minDocCount = instance.minDocCount.         List<PipelineAggregator> pipelineAggregators = instance.pipelineAggregators().         Map<String, Object> metaData = instance.getMetaData().         switch(between(0, 3)) {             case 0:                 name += randomAlphaOfLength(5).                 break.             case 1:                 requiredSize += between(1, 100).                 break.             case 2:                 minDocCount += between(1, 100).                 break.             case 3:                 if (metaData == null) {                     metaData = new HashMap<>(1).                 } else {                     metaData = new HashMap<>(instance.getMetaData()).                 }                 metaData.put(randomAlphaOfLength(15), randomInt()).                 break.             default:                 throw new AssertionError("Illegal randomisation branch").         }         return new UnmappedSignificantTerms(name, requiredSize, minDocCount, pipelineAggregators, metaData).     } }
