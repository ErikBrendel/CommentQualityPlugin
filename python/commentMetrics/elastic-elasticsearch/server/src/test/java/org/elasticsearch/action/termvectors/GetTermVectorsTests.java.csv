commented;modifiers;parameterAmount;loc;comment;code
false;protected;0;4;;@Override protected Collection<Class<? extends Plugin>> getPlugins() {     return Collections.singleton(MockPayloadAnalyzerPlugin.class). }
false;public;0;4;;@Override public String name() {     return "mock_payload_filter". }
false;public;1;13;;@Override public TokenStream create(TokenStream tokenStream) {     String delimiter = settings.get("delimiter").     PayloadEncoder encoder = null.     if (settings.get("encoding").equals("float")) {         encoder = new FloatEncoder().     } else if (settings.get("encoding").equals("int")) {         encoder = new IntegerEncoder().     } else if (settings.get("encoding").equals("identity")) {         encoder = new IdentityEncoder().     }     return new MockPayloadTokenFilter(tokenStream, delimiter.charAt(0), encoder). }
false;public;0;25;;@Override public Map<String, AnalysisModule.AnalysisProvider<TokenFilterFactory>> getTokenFilters() {     return Collections.singletonMap("mock_payload_filter", (indexSettings, environment, name, settings) -> {         return new TokenFilterFactory() {              @Override             public String name() {                 return "mock_payload_filter".             }              @Override             public TokenStream create(TokenStream tokenStream) {                 String delimiter = settings.get("delimiter").                 PayloadEncoder encoder = null.                 if (settings.get("encoding").equals("float")) {                     encoder = new FloatEncoder().                 } else if (settings.get("encoding").equals("int")) {                     encoder = new IntegerEncoder().                 } else if (settings.get("encoding").equals("identity")) {                     encoder = new IdentityEncoder().                 }                 return new MockPayloadTokenFilter(tokenStream, delimiter.charAt(0), encoder).             }         }.     }). }
false;public;0;5;;@Override public List<PreConfiguredTokenizer> getPreConfiguredTokenizers() {     return Collections.singletonList(PreConfiguredTokenizer.singleton("mock-whitespace", () -> new MockTokenizer(MockTokenizer.WHITESPACE, false))). }
false;public;0;19;;@Override public boolean incrementToken() throws IOException {     if (input.incrementToken()) {         final char[] buffer = termAtt.buffer().         final int length = termAtt.length().         for (int i = 0. i < length. i++) {             if (buffer[i] == delimiter) {                 payAtt.setPayload(encoder.encode(buffer, i + 1, (length - (i + 1)))).                 // simply set a new length                 termAtt.setLength(i).                 return true.             }         }         // we have not seen the delimiter         payAtt.setPayload(null).         return true.     } else {         return false.     } }
false;public;0;61;;public void testRandomPayloadWithDelimitedPayloadTokenFilter() throws IOException {     // create the test document     int encoding = randomIntBetween(0, 2).     String encodingString = "".     if (encoding == 0) {         encodingString = "float".     }     if (encoding == 1) {         encodingString = "int".     }     if (encoding == 2) {         encodingString = "identity".     }     String[] tokens = crateRandomTokens().     Map<String, List<BytesRef>> payloads = createPayloads(tokens, encoding).     String delimiter = createRandomDelimiter(tokens).     String queryString = createString(tokens, payloads, encoding, delimiter.charAt(0)).     // create the mapping     XContentBuilder mapping = jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("field").field("type", "text").field("term_vector", "with_positions_offsets_payloads").field("analyzer", "payload_test").endObject().endObject().endObject().endObject().     Settings setting = Settings.builder().put("index.analysis.analyzer.payload_test.tokenizer", "mock-whitespace").putList("index.analysis.analyzer.payload_test.filter", "my_delimited_payload").put("index.analysis.filter.my_delimited_payload.delimiter", delimiter).put("index.analysis.filter.my_delimited_payload.encoding", encodingString).put("index.analysis.filter.my_delimited_payload.type", "mock_payload_filter").build().     createIndex("test", setting, "type1", mapping).     client().prepareIndex("test", "type1", Integer.toString(1)).setSource(jsonBuilder().startObject().field("field", queryString).endObject()).execute().actionGet().     client().admin().indices().prepareRefresh().get().     TermVectorsRequestBuilder resp = client().prepareTermVectors("test", "type1", Integer.toString(1)).setPayloads(true).setOffsets(true).setPositions(true).setSelectedFields().     TermVectorsResponse response = resp.execute().actionGet().     assertThat("doc id 1 doesn't exists but should", response.isExists(), equalTo(true)).     Fields fields = response.getFields().     assertThat(fields.size(), equalTo(1)).     Terms terms = fields.terms("field").     TermsEnum iterator = terms.iterator().     while (iterator.next() != null) {         String term = iterator.term().utf8ToString().         PostingsEnum docsAndPositions = iterator.postings(null, PostingsEnum.ALL).         assertThat(docsAndPositions.nextDoc(), equalTo(0)).         List<BytesRef> curPayloads = payloads.get(term).         assertThat(term, curPayloads, notNullValue()).         assertNotNull(docsAndPositions).         for (int k = 0. k < docsAndPositions.freq(). k++) {             docsAndPositions.nextPosition().             if (docsAndPositions.getPayload() != null) {                 String infoString = "\nterm: " + term + " has payload \n" + docsAndPositions.getPayload().toString() + "\n but should have payload \n" + curPayloads.get(k).toString().                 assertThat(infoString, docsAndPositions.getPayload(), equalTo(curPayloads.get(k))).             } else {                 String infoString = "\nterm: " + term + " has no payload but should have payload \n" + curPayloads.get(k).toString().                 assertThat(infoString, curPayloads.get(k).length, equalTo(0)).             }         }     }     assertThat(iterator.next(), nullValue()). }
false;private;4;35;;private String createString(String[] tokens, Map<String, List<BytesRef>> payloads, int encoding, char delimiter) {     String resultString = "".     Map<String, Integer> payloadCounter = new HashMap<>().     for (String token : tokens) {         if (!payloadCounter.containsKey(token)) {             payloadCounter.putIfAbsent(token, 0).         } else {             payloadCounter.put(token, payloadCounter.get(token) + 1).         }         resultString = resultString + token.         BytesRef payload = payloads.get(token).get(payloadCounter.get(token)).         if (payload.length > 0) {             resultString = resultString + delimiter.             switch(encoding) {                 case 0:                     {                         resultString = resultString + Float.toString(PayloadHelper.decodeFloat(payload.bytes, payload.offset)).                         break.                     }                 case 1:                     {                         resultString = resultString + Integer.toString(PayloadHelper.decodeInt(payload.bytes, payload.offset)).                         break.                     }                 case 2:                     {                         resultString = resultString + payload.utf8ToString().                         break.                     }                 default:                     {                         throw new ElasticsearchException("unsupported encoding type").                     }             }         }         resultString = resultString + " ".     }     return resultString. }
false;private;0;9;;private String[] crateRandomTokens() {     String[] tokens = { "the", "quick", "brown", "fox" }.     int numTokensWithDuplicates = randomIntBetween(3, 15).     String[] finalTokens = new String[numTokensWithDuplicates].     for (int i = 0. i < numTokensWithDuplicates. i++) {         finalTokens[i] = tokens[randomIntBetween(0, tokens.length - 1)].     }     return finalTokens. }
false;private;1;17;;private String createRandomDelimiter(String[] tokens) {     String delimiter = "".     boolean isTokenOrWhitespace = true.     while (isTokenOrWhitespace) {         isTokenOrWhitespace = false.         delimiter = randomUnicodeOfLength(1).         for (String token : tokens) {             if (token.contains(delimiter)) {                 isTokenOrWhitespace = true.             }         }         if (Character.isWhitespace(delimiter.charAt(0))) {             isTokenOrWhitespace = true.         }     }     return delimiter. }
false;private;2;36;;private Map<String, List<BytesRef>> createPayloads(String[] tokens, int encoding) {     Map<String, List<BytesRef>> payloads = new HashMap<>().     for (String token : tokens) {         payloads.computeIfAbsent(token, k -> new ArrayList<>()).         boolean createPayload = randomBoolean().         if (createPayload) {             switch(encoding) {                 case 0:                     {                         float theFloat = randomFloat().                         payloads.get(token).add(new BytesRef(PayloadHelper.encodeFloat(theFloat))).                         break.                     }                 case 1:                     {                         payloads.get(token).add(new BytesRef(PayloadHelper.encodeInt(randomInt()))).                         break.                     }                 case 2:                     {                         String payload = randomUnicodeOfLengthBetween(50, 100).                         for (int c = 0. c < payload.length(). c++) {                             if (Character.isWhitespace(payload.charAt(c))) {                                 payload = payload.replace(payload.charAt(c), 'w').                             }                         }                         payloads.get(token).add(new BytesRef(payload)).                         break.                     }                 default:                     {                         throw new ElasticsearchException("unsupported encoding type").                     }             }         } else {             payloads.get(token).add(new BytesRef()).         }     }     return payloads. }
