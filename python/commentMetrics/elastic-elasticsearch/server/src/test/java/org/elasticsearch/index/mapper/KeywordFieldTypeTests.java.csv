commented;modifiers;parameterAmount;loc;comment;code
false;public;1;4;;@Override public void modify(MappedFieldType type) {     ((KeywordFieldType) type).setNormalizer(Lucene.KEYWORD_ANALYZER). }
false;public;1;5;;@Override public void modify(MappedFieldType type) {     KeywordFieldType keywordType = (KeywordFieldType) type.     keywordType.setSplitQueriesOnWhitespace(!keywordType.splitQueriesOnWhitespace()). }
false;public;0;16;;@Before public void setupProperties() {     addModifier(new Modifier("normalizer", false) {          @Override         public void modify(MappedFieldType type) {             ((KeywordFieldType) type).setNormalizer(Lucene.KEYWORD_ANALYZER).         }     }).     addModifier(new Modifier("split_queries_on_whitespace", true) {          @Override         public void modify(MappedFieldType type) {             KeywordFieldType keywordType = (KeywordFieldType) type.             keywordType.setSplitQueriesOnWhitespace(!keywordType.splitQueriesOnWhitespace()).         }     }). }
false;protected;0;4;;@Override protected MappedFieldType createDefaultFieldType() {     return new KeywordFieldMapper.KeywordFieldType(). }
false;public;0;8;;public void testIsFieldWithinQuery() throws IOException {     KeywordFieldType ft = new KeywordFieldType().     // current impl ignores args and shourd always return INTERSECTS     assertEquals(Relation.INTERSECTS, ft.isFieldWithinQuery(null, RandomStrings.randomAsciiOfLengthBetween(random(), 0, 5), RandomStrings.randomAsciiOfLengthBetween(random(), 0, 5), randomBoolean(), randomBoolean(), null, null, null)). }
false;public;0;11;;public void testTermQuery() {     MappedFieldType ft = createDefaultFieldType().     ft.setName("field").     ft.setIndexOptions(IndexOptions.DOCS).     assertEquals(new TermQuery(new Term("field", "foo")), ft.termQuery("foo", null)).     ft.setIndexOptions(IndexOptions.NONE).     IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> ft.termQuery("bar", null)).     assertEquals("Cannot search on field [field] since it is not indexed.", e.getMessage()). }
false;protected;1;6;;@Override protected TokenStreamComponents createComponents(String fieldName) {     Tokenizer in = new WhitespaceTokenizer().     TokenFilter out = new LowerCaseFilter(in).     return new TokenStreamComponents(in, out). }
false;protected;2;4;;@Override protected TokenStream normalize(String fieldName, TokenStream in) {     return new LowerCaseFilter(in). }
false;public;0;24;;public void testTermQueryWithNormalizer() {     MappedFieldType ft = createDefaultFieldType().     ft.setName("field").     ft.setIndexOptions(IndexOptions.DOCS).     Analyzer normalizer = new Analyzer() {          @Override         protected TokenStreamComponents createComponents(String fieldName) {             Tokenizer in = new WhitespaceTokenizer().             TokenFilter out = new LowerCaseFilter(in).             return new TokenStreamComponents(in, out).         }          @Override         protected TokenStream normalize(String fieldName, TokenStream in) {             return new LowerCaseFilter(in).         }     }.     ft.setSearchAnalyzer(new NamedAnalyzer("my_normalizer", AnalyzerScope.INDEX, normalizer)).     assertEquals(new TermQuery(new Term("field", "foo bar")), ft.termQuery("fOo BaR", null)).     ft.setIndexOptions(IndexOptions.NONE).     IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> ft.termQuery("bar", null)).     assertEquals("Cannot search on field [field] since it is not indexed.", e.getMessage()). }
false;public;0;15;;public void testTermsQuery() {     MappedFieldType ft = createDefaultFieldType().     ft.setName("field").     ft.setIndexOptions(IndexOptions.DOCS).     List<BytesRef> terms = new ArrayList<>().     terms.add(new BytesRef("foo")).     terms.add(new BytesRef("bar")).     assertEquals(new TermInSetQuery("field", terms), ft.termsQuery(Arrays.asList("foo", "bar"), null)).     ft.setIndexOptions(IndexOptions.NONE).     IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> ft.termsQuery(Arrays.asList("foo", "bar"), null)).     assertEquals("Cannot search on field [field] since it is not indexed.", e.getMessage()). }
false;public;0;16;;public void testExistsQuery() {     MappedFieldType ft = createDefaultFieldType().     ft.setName("field").     ft.setHasDocValues(true).     ft.setOmitNorms(true).     assertEquals(new DocValuesFieldExistsQuery("field"), ft.existsQuery(null)).     ft.setHasDocValues(false).     ft.setOmitNorms(false).     assertEquals(new NormsFieldExistsQuery("field"), ft.existsQuery(null)).     ft.setHasDocValues(false).     ft.setOmitNorms(true).     assertEquals(new TermQuery(new Term(FieldNamesFieldMapper.NAME, "field")), ft.existsQuery(null)). }
false;public;0;12;;public void testRegexpQuery() {     MappedFieldType ft = createDefaultFieldType().     ft.setName("field").     ft.setIndexOptions(IndexOptions.DOCS).     assertEquals(new RegexpQuery(new Term("field", "foo.*")), ft.regexpQuery("foo.*", 0, 10, null, null)).     ft.setIndexOptions(IndexOptions.NONE).     IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> ft.regexpQuery("foo.*", 0, 10, null, null)).     assertEquals("Cannot search on field [field] since it is not indexed.", e.getMessage()). }
false;public;0;12;;public void testFuzzyQuery() {     MappedFieldType ft = createDefaultFieldType().     ft.setName("field").     ft.setIndexOptions(IndexOptions.DOCS).     assertEquals(new FuzzyQuery(new Term("field", "foo"), 2, 1, 50, true), ft.fuzzyQuery("foo", Fuzziness.fromEdits(2), 1, 50, true)).     ft.setIndexOptions(IndexOptions.NONE).     IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> ft.fuzzyQuery("foo", Fuzziness.fromEdits(2), 1, 50, true)).     assertEquals("Cannot search on field [field] since it is not indexed.", e.getMessage()). }
false;public;0;8;;public void testNormalizeQueries() {     MappedFieldType ft = createDefaultFieldType().     ft.setName("field").     ft.setSearchAnalyzer(Lucene.KEYWORD_ANALYZER).     assertEquals(new TermQuery(new Term("field", new BytesRef("FOO"))), ft.termQuery("FOO", null)).     ft.setSearchAnalyzer(Lucene.STANDARD_ANALYZER).     assertEquals(new TermQuery(new Term("field", new BytesRef("foo"))), ft.termQuery("FOO", null)). }
