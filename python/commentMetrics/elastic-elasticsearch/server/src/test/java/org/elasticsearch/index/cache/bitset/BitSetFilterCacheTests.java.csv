commented;modifiers;parameterAmount;loc;comment;code
false;private,static;2;10;;private static int matchCount(BitSetProducer producer, IndexReader reader) throws IOException {     int count = 0.     for (LeafReaderContext ctx : reader.leaves()) {         final BitSet bitSet = producer.getBitSet(ctx).         if (bitSet != null) {             count += bitSet.cardinality().         }     }     return count. }
false;public;2;4;;@Override public void onCache(ShardId shardId, Accountable accountable) { }
false;public;2;4;;@Override public void onRemoval(ShardId shardId, Accountable accountable) { }
false;public;0;59;;public void testInvalidateEntries() throws Exception {     IndexWriter writer = new IndexWriter(new RAMDirectory(), new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy())).     Document document = new Document().     document.add(new StringField("field", "value", Field.Store.NO)).     writer.addDocument(document).     writer.commit().     document = new Document().     document.add(new StringField("field", "value", Field.Store.NO)).     writer.addDocument(document).     writer.commit().     document = new Document().     document.add(new StringField("field", "value", Field.Store.NO)).     writer.addDocument(document).     writer.commit().     DirectoryReader reader = DirectoryReader.open(writer).     reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId("test", "_na_", 0)).     BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, new BitsetFilterCache.Listener() {          @Override         public void onCache(ShardId shardId, Accountable accountable) {         }          @Override         public void onRemoval(ShardId shardId, Accountable accountable) {         }     }).     BitSetProducer filter = cache.getBitSetProducer(new TermQuery(new Term("field", "value"))).     assertThat(matchCount(filter, reader), equalTo(3)).     // now cached     assertThat(matchCount(filter, reader), equalTo(3)).     // There are 3 segments     assertThat(cache.getLoadedFilters().weight(), equalTo(3L)).     writer.forceMerge(1).     reader.close().     reader = DirectoryReader.open(writer).     reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId("test", "_na_", 0)).     assertThat(matchCount(filter, reader), equalTo(3)).     // now cached     assertThat(matchCount(filter, reader), equalTo(3)).     // Only one segment now, so the size must be 1     assertThat(cache.getLoadedFilters().weight(), equalTo(1L)).     reader.close().     writer.close().     // There is no reference from readers and writer to any segment in the test index, so the size in the fbs cache must be 0     assertThat(cache.getLoadedFilters().weight(), equalTo(0L)). }
false;public;2;12;;@Override public void onCache(ShardId shardId, Accountable accountable) {     onCacheCalls.incrementAndGet().     stats.addAndGet(accountable.ramBytesUsed()).     if (writerReader != reader) {         assertNotNull(shardId).         assertEquals("test", shardId.getIndexName()).         assertEquals(0, shardId.id()).     } else {         assertNull(shardId).     } }
false;public;2;12;;@Override public void onRemoval(ShardId shardId, Accountable accountable) {     onRemoveCalls.incrementAndGet().     stats.addAndGet(-accountable.ramBytesUsed()).     if (writerReader != reader) {         assertNotNull(shardId).         assertEquals("test", shardId.getIndexName()).         assertEquals(0, shardId.id()).     } else {         assertNull(shardId).     } }
false;public;0;52;;public void testListener() throws IOException {     IndexWriter writer = new IndexWriter(new RAMDirectory(), new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy())).     Document document = new Document().     document.add(new StringField("field", "value", Field.Store.NO)).     writer.addDocument(document).     writer.commit().     final DirectoryReader writerReader = DirectoryReader.open(writer).     final IndexReader reader = ElasticsearchDirectoryReader.wrap(writerReader, new ShardId("test", "_na_", 0)).     final AtomicLong stats = new AtomicLong().     final AtomicInteger onCacheCalls = new AtomicInteger().     final AtomicInteger onRemoveCalls = new AtomicInteger().     final BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, new BitsetFilterCache.Listener() {          @Override         public void onCache(ShardId shardId, Accountable accountable) {             onCacheCalls.incrementAndGet().             stats.addAndGet(accountable.ramBytesUsed()).             if (writerReader != reader) {                 assertNotNull(shardId).                 assertEquals("test", shardId.getIndexName()).                 assertEquals(0, shardId.id()).             } else {                 assertNull(shardId).             }         }          @Override         public void onRemoval(ShardId shardId, Accountable accountable) {             onRemoveCalls.incrementAndGet().             stats.addAndGet(-accountable.ramBytesUsed()).             if (writerReader != reader) {                 assertNotNull(shardId).                 assertEquals("test", shardId.getIndexName()).                 assertEquals(0, shardId.id()).             } else {                 assertNull(shardId).             }         }     }).     BitSetProducer filter = cache.getBitSetProducer(new TermQuery(new Term("field", "value"))).     assertThat(matchCount(filter, reader), equalTo(1)).     assertTrue(stats.get() > 0).     assertEquals(1, onCacheCalls.get()).     assertEquals(0, onRemoveCalls.get()).     IOUtils.close(reader, writer).     assertEquals(1, onRemoveCalls.get()).     assertEquals(0, stats.get()). }
false;public;0;9;;public void testSetNullListener() {     try {         new BitsetFilterCache(INDEX_SETTINGS, null).         fail("listener can't be null").     } catch (IllegalArgumentException ex) {         assertEquals("listener must not be null", ex.getMessage()).     // all is well     } }
false;public;2;4;;@Override public void onCache(ShardId shardId, Accountable accountable) { }
false;public;2;4;;@Override public void onRemoval(ShardId shardId, Accountable accountable) { }
false;public;0;34;;public void testRejectOtherIndex() throws IOException {     BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, new BitsetFilterCache.Listener() {          @Override         public void onCache(ShardId shardId, Accountable accountable) {         }          @Override         public void onRemoval(ShardId shardId, Accountable accountable) {         }     }).     Directory dir = newDirectory().     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig()).     writer.addDocument(new Document()).     DirectoryReader reader = DirectoryReader.open(writer).     writer.close().     reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId("test2", "_na_", 0)).     BitSetProducer producer = cache.getBitSetProducer(new MatchAllDocsQuery()).     try {         producer.getBitSet(reader.leaves().get(0)).         fail().     } catch (IllegalStateException expected) {         assertEquals("Trying to load bit set for index [test2] with cache of index [test]", expected.getMessage()).     } finally {         IOUtils.close(reader, dir).     } }
