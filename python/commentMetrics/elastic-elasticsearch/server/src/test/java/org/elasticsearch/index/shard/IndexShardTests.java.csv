# id;timestamp;commentText;codeText;commentWords;codeWords
IndexShardTests -> public void testSnapshotStore() throws IOException;1524684173;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "test", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,test,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1524780945;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "test", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,test,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1525162917;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1527583663;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1528706846;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1529916081;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1531910483;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1532434432;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1532947734;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1533295538;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1533738061;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1534338685;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1534539448;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1535723122;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1535965276;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1536137328;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1536314350;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1536611444;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1536828374;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1537371806;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1537806831;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1538067637;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1541176068;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1541431100;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1542697754;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1542961598;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1542979766;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1543221203;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1543236252;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1543832502;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1543930771;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1543947737;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1544081506;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1544322464;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1544783963;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1544893197;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1545678410;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1547508054;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1547520882;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1547576245;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1547625930;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1548124510;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1548871018;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1548872440;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1549395161;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1549406621;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testSnapshotStore() throws IOException;1550668107;test one can snapshot the store at various lifecycle stages;public void testSnapshotStore() throws IOException {_        final IndexShard shard = newStartedShard(true)__        indexDoc(shard, "_doc", "0")__        flushShard(shard)___        final IndexShard newShard = reinitShard(shard)__        DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)___        Store.MetadataSnapshot snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(), localNode, null))___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        assertTrue(newShard.recoverFromStore())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted())___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        newShard.close("test", false)___        snapshot = newShard.snapshotStoreMetadata()__        assertThat(snapshot.getSegmentsFile().name(), equalTo("segments_3"))___        closeShards(newShard)__    };test,one,can,snapshot,the,store,at,various,lifecycle,stages;public,void,test,snapshot,store,throws,ioexception,final,index,shard,shard,new,started,shard,true,index,doc,shard,0,flush,shard,shard,final,index,shard,new,shard,reinit,shard,shard,discovery,node,local,node,new,discovery,node,foo,build,new,fake,transport,address,empty,map,empty,set,version,current,store,metadata,snapshot,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,local,node,null,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,assert,true,new,shard,recover,from,store,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,index,shard,test,case,update,routing,entry,new,shard,new,shard,routing,entry,move,to,started,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,new,shard,close,test,false,snapshot,new,shard,snapshot,store,metadata,assert,that,snapshot,get,segments,file,name,equal,to,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1524684173;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "doc", "0", "{\"foo\" : \"bar\"}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "doc", "1", "{\"foo\" : \"bar\"}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,doc,0,foo,bar,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,doc,1,foo,bar,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1524780945;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "doc", "0", "{\"foo\" : \"bar\"}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "doc", "1", "{\"foo\" : \"bar\"}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,doc,0,foo,bar,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,doc,1,foo,bar,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1525162917;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1527583663;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1528706846;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1529916081;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1531910483;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1532434432;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1532947734;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1533295538;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1533738061;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1534338685;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1534539448;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1535723122;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1535965276;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1536137328;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1536314350;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1536611444;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1536828374;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1537371806;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1537806831;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1538067637;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1541176068;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1541431100;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1542697754;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1542961598;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1542979766;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1543221203;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1543236252;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1543832502;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1543930771;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1543947737;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1544081506;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1544322464;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1544783963;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1544893197;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1545678410;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1547508054;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1547520882;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1547576245;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1547625930;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1548124510;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1548871018;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1548872440;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1549395161;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1549406621;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotConcurrently() throws IOException, InterruptedException;1550668107;here we are simulating the scenario that happens when we do async shard fetching from GatewaySerivce while we are finishing_a recovery and concurrently clean files. This should always be possible without any exception. Yet there was a bug where IndexShard_acquired the index writer lock before it called into the store that has it's own locking for metadata reads;public void testReadSnapshotConcurrently() throws IOException, InterruptedException {_        IndexShard indexShard = newStartedShard()__        indexDoc(indexShard, "_doc", "0", "{}")__        if (randomBoolean()) {_            indexShard.refresh("test")__        }_        indexDoc(indexShard, "_doc", "1", "{}")__        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final IndexShard newShard = reinitShard(indexShard)__        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " +storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        expectThrows(AlreadyClosedException.class, () -> newShard.getEngine())_ _        Thread thread = new Thread(() -> {_            latch.countDown()__            while(stop.get() == false){_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).different.size())__                    assertEquals(0, storeFileMetaDatas.recoveryDiff(readMeta).missing.size())__                    assertEquals(storeFileMetaDatas.size(), storeFileMetaDatas.recoveryDiff(readMeta).identical.size())__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        thread.start()__        latch.await()___        int iters = iterations(10, 100)__        for (int i = 0_ i < iters_ i++) {_            newShard.store().cleanupAndVerify("test", storeFileMetaDatas)__        }_        assertTrue(stop.compareAndSet(false, true))__        thread.join()__        closeShards(newShard)__    };here,we,are,simulating,the,scenario,that,happens,when,we,do,async,shard,fetching,from,gateway,serivce,while,we,are,finishing,a,recovery,and,concurrently,clean,files,this,should,always,be,possible,without,any,exception,yet,there,was,a,bug,where,index,shard,acquired,the,index,writer,lock,before,it,called,into,the,store,that,has,it,s,own,locking,for,metadata,reads;public,void,test,read,snapshot,concurrently,throws,ioexception,interrupted,exception,index,shard,index,shard,new,started,shard,index,doc,index,shard,0,if,random,boolean,index,shard,refresh,test,index,doc,index,shard,1,index,shard,flush,new,flush,request,close,shards,index,shard,final,index,shard,new,shard,reinit,shard,index,shard,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,expect,throws,already,closed,exception,class,new,shard,get,engine,thread,thread,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,different,size,assert,equals,0,store,file,meta,datas,recovery,diff,read,meta,missing,size,assert,equals,store,file,meta,datas,size,store,file,meta,datas,recovery,diff,read,meta,identical,size,catch,ioexception,e,throw,new,assertion,error,e,thread,start,latch,await,int,iters,iterations,10,100,for,int,i,0,i,iters,i,new,shard,store,cleanup,and,verify,test,store,file,meta,datas,assert,true,stop,compare,and,set,false,true,thread,join,close,shards,new,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException;1524684173;This test makes sure that people can use the shard routing entry to check whether a shard was already promoted to_a primary. Concretely this means, that when we publish the routing entry via {@link IndexShard#routingEntry()} the following_should have happened_1) Internal state (ala ReplicationTracker) have been updated_2) Primary term is set to the new term;public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException {_        final IndexShard indexShard = newStartedShard(false)__        final long promotedTerm = indexShard.getPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPrimaryTerm(), equalTo(promotedTerm))__                    assertThat(indexShard.getReplicationGroup(), notNullValue())__                }_            }_        })__        thread.start()___        final ShardRouting replicaRouting = indexShard.routingEntry()__        final ShardRouting primaryRouting = newShardRouting(replicaRouting.shardId(), replicaRouting.currentNodeId(), null, true,_            ShardRoutingState.STARTED, replicaRouting.allocationId())____        final Set<String> inSyncAllocationIds = Collections.singleton(primaryRouting.allocationId().getId())__        final IndexShardRoutingTable routingTable =_            new IndexShardRoutingTable.Builder(primaryRouting.shardId()).addShard(primaryRouting).build()__        barrier.await()__        _        indexShard.updateShardState(primaryRouting, promotedTerm, (shard, listener) -> {}, 0L, inSyncAllocationIds, routingTable,_            Collections.emptySet())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,to,check,whether,a,shard,was,already,promoted,to,a,primary,concretely,this,means,that,when,we,publish,the,routing,entry,via,link,index,shard,routing,entry,the,following,should,have,happened,1,internal,state,ala,replication,tracker,have,been,updated,2,primary,term,is,set,to,the,new,term;public,void,test,publishing,order,on,promotion,throws,ioexception,broken,barrier,exception,interrupted,exception,final,index,shard,index,shard,new,started,shard,false,final,long,promoted,term,index,shard,get,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,primary,term,equal,to,promoted,term,assert,that,index,shard,get,replication,group,not,null,value,thread,start,final,shard,routing,replica,routing,index,shard,routing,entry,final,shard,routing,primary,routing,new,shard,routing,replica,routing,shard,id,replica,routing,current,node,id,null,true,shard,routing,state,started,replica,routing,allocation,id,final,set,string,in,sync,allocation,ids,collections,singleton,primary,routing,allocation,id,get,id,final,index,shard,routing,table,routing,table,new,index,shard,routing,table,builder,primary,routing,shard,id,add,shard,primary,routing,build,barrier,await,index,shard,update,shard,state,primary,routing,promoted,term,shard,listener,0l,in,sync,allocation,ids,routing,table,collections,empty,set,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException;1524780945;This test makes sure that people can use the shard routing entry to check whether a shard was already promoted to_a primary. Concretely this means, that when we publish the routing entry via {@link IndexShard#routingEntry()} the following_should have happened_1) Internal state (ala ReplicationTracker) have been updated_2) Primary term is set to the new term;public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException {_        final IndexShard indexShard = newStartedShard(false)__        final long promotedTerm = indexShard.getPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPrimaryTerm(), equalTo(promotedTerm))__                    assertThat(indexShard.getReplicationGroup(), notNullValue())__                }_            }_        })__        thread.start()___        final ShardRouting replicaRouting = indexShard.routingEntry()__        final ShardRouting primaryRouting = newShardRouting(replicaRouting.shardId(), replicaRouting.currentNodeId(), null, true,_            ShardRoutingState.STARTED, replicaRouting.allocationId())____        final Set<String> inSyncAllocationIds = Collections.singleton(primaryRouting.allocationId().getId())__        final IndexShardRoutingTable routingTable =_            new IndexShardRoutingTable.Builder(primaryRouting.shardId()).addShard(primaryRouting).build()__        barrier.await()__        _        indexShard.updateShardState(primaryRouting, promotedTerm, (shard, listener) -> {}, 0L, inSyncAllocationIds, routingTable,_            Collections.emptySet())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,to,check,whether,a,shard,was,already,promoted,to,a,primary,concretely,this,means,that,when,we,publish,the,routing,entry,via,link,index,shard,routing,entry,the,following,should,have,happened,1,internal,state,ala,replication,tracker,have,been,updated,2,primary,term,is,set,to,the,new,term;public,void,test,publishing,order,on,promotion,throws,ioexception,broken,barrier,exception,interrupted,exception,final,index,shard,index,shard,new,started,shard,false,final,long,promoted,term,index,shard,get,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,primary,term,equal,to,promoted,term,assert,that,index,shard,get,replication,group,not,null,value,thread,start,final,shard,routing,replica,routing,index,shard,routing,entry,final,shard,routing,primary,routing,new,shard,routing,replica,routing,shard,id,replica,routing,current,node,id,null,true,shard,routing,state,started,replica,routing,allocation,id,final,set,string,in,sync,allocation,ids,collections,singleton,primary,routing,allocation,id,get,id,final,index,shard,routing,table,routing,table,new,index,shard,routing,table,builder,primary,routing,shard,id,add,shard,primary,routing,build,barrier,await,index,shard,update,shard,state,primary,routing,promoted,term,shard,listener,0l,in,sync,allocation,ids,routing,table,collections,empty,set,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException;1525162917;This test makes sure that people can use the shard routing entry to check whether a shard was already promoted to_a primary. Concretely this means, that when we publish the routing entry via {@link IndexShard#routingEntry()} the following_should have happened_1) Internal state (ala ReplicationTracker) have been updated_2) Primary term is set to the new term;public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException {_        final IndexShard indexShard = newStartedShard(false)__        final long promotedTerm = indexShard.getPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPrimaryTerm(), equalTo(promotedTerm))__                    assertThat(indexShard.getReplicationGroup(), notNullValue())__                }_            }_        })__        thread.start()___        final ShardRouting replicaRouting = indexShard.routingEntry()__        final ShardRouting primaryRouting = newShardRouting(replicaRouting.shardId(), replicaRouting.currentNodeId(), null, true,_            ShardRoutingState.STARTED, replicaRouting.allocationId())____        final Set<String> inSyncAllocationIds = Collections.singleton(primaryRouting.allocationId().getId())__        final IndexShardRoutingTable routingTable =_            new IndexShardRoutingTable.Builder(primaryRouting.shardId()).addShard(primaryRouting).build()__        barrier.await()__        _        indexShard.updateShardState(primaryRouting, promotedTerm, (shard, listener) -> {}, 0L, inSyncAllocationIds, routingTable,_            Collections.emptySet())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,to,check,whether,a,shard,was,already,promoted,to,a,primary,concretely,this,means,that,when,we,publish,the,routing,entry,via,link,index,shard,routing,entry,the,following,should,have,happened,1,internal,state,ala,replication,tracker,have,been,updated,2,primary,term,is,set,to,the,new,term;public,void,test,publishing,order,on,promotion,throws,ioexception,broken,barrier,exception,interrupted,exception,final,index,shard,index,shard,new,started,shard,false,final,long,promoted,term,index,shard,get,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,primary,term,equal,to,promoted,term,assert,that,index,shard,get,replication,group,not,null,value,thread,start,final,shard,routing,replica,routing,index,shard,routing,entry,final,shard,routing,primary,routing,new,shard,routing,replica,routing,shard,id,replica,routing,current,node,id,null,true,shard,routing,state,started,replica,routing,allocation,id,final,set,string,in,sync,allocation,ids,collections,singleton,primary,routing,allocation,id,get,id,final,index,shard,routing,table,routing,table,new,index,shard,routing,table,builder,primary,routing,shard,id,add,shard,primary,routing,build,barrier,await,index,shard,update,shard,state,primary,routing,promoted,term,shard,listener,0l,in,sync,allocation,ids,routing,table,collections,empty,set,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException;1527583663;This test makes sure that people can use the shard routing entry to check whether a shard was already promoted to_a primary. Concretely this means, that when we publish the routing entry via {@link IndexShard#routingEntry()} the following_should have happened_1) Internal state (ala ReplicationTracker) have been updated_2) Primary term is set to the new term;public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException {_        final IndexShard indexShard = newStartedShard(false)__        final long promotedTerm = indexShard.getPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPrimaryTerm(), equalTo(promotedTerm))__                    assertThat(indexShard.getReplicationGroup(), notNullValue())__                }_            }_        })__        thread.start()___        final ShardRouting replicaRouting = indexShard.routingEntry()__        final ShardRouting primaryRouting = newShardRouting(replicaRouting.shardId(), replicaRouting.currentNodeId(), null, true,_            ShardRoutingState.STARTED, replicaRouting.allocationId())____        final Set<String> inSyncAllocationIds = Collections.singleton(primaryRouting.allocationId().getId())__        final IndexShardRoutingTable routingTable =_            new IndexShardRoutingTable.Builder(primaryRouting.shardId()).addShard(primaryRouting).build()__        barrier.await()__        _        indexShard.updateShardState(primaryRouting, promotedTerm, (shard, listener) -> {}, 0L, inSyncAllocationIds, routingTable,_            Collections.emptySet())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,to,check,whether,a,shard,was,already,promoted,to,a,primary,concretely,this,means,that,when,we,publish,the,routing,entry,via,link,index,shard,routing,entry,the,following,should,have,happened,1,internal,state,ala,replication,tracker,have,been,updated,2,primary,term,is,set,to,the,new,term;public,void,test,publishing,order,on,promotion,throws,ioexception,broken,barrier,exception,interrupted,exception,final,index,shard,index,shard,new,started,shard,false,final,long,promoted,term,index,shard,get,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,primary,term,equal,to,promoted,term,assert,that,index,shard,get,replication,group,not,null,value,thread,start,final,shard,routing,replica,routing,index,shard,routing,entry,final,shard,routing,primary,routing,new,shard,routing,replica,routing,shard,id,replica,routing,current,node,id,null,true,shard,routing,state,started,replica,routing,allocation,id,final,set,string,in,sync,allocation,ids,collections,singleton,primary,routing,allocation,id,get,id,final,index,shard,routing,table,routing,table,new,index,shard,routing,table,builder,primary,routing,shard,id,add,shard,primary,routing,build,barrier,await,index,shard,update,shard,state,primary,routing,promoted,term,shard,listener,0l,in,sync,allocation,ids,routing,table,collections,empty,set,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException;1528706846;This test makes sure that people can use the shard routing entry to check whether a shard was already promoted to_a primary. Concretely this means, that when we publish the routing entry via {@link IndexShard#routingEntry()} the following_should have happened_1) Internal state (ala ReplicationTracker) have been updated_2) Primary term is set to the new term;public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException {_        final IndexShard indexShard = newStartedShard(false)__        final long promotedTerm = indexShard.getPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPrimaryTerm(), equalTo(promotedTerm))__                    assertThat(indexShard.getReplicationGroup(), notNullValue())__                }_            }_        })__        thread.start()___        final ShardRouting replicaRouting = indexShard.routingEntry()__        final ShardRouting primaryRouting = newShardRouting(replicaRouting.shardId(), replicaRouting.currentNodeId(), null, true,_            ShardRoutingState.STARTED, replicaRouting.allocationId())____        final Set<String> inSyncAllocationIds = Collections.singleton(primaryRouting.allocationId().getId())__        final IndexShardRoutingTable routingTable =_            new IndexShardRoutingTable.Builder(primaryRouting.shardId()).addShard(primaryRouting).build()__        barrier.await()__        _        indexShard.updateShardState(primaryRouting, promotedTerm, (shard, listener) -> {}, 0L, inSyncAllocationIds, routingTable,_            Collections.emptySet())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,to,check,whether,a,shard,was,already,promoted,to,a,primary,concretely,this,means,that,when,we,publish,the,routing,entry,via,link,index,shard,routing,entry,the,following,should,have,happened,1,internal,state,ala,replication,tracker,have,been,updated,2,primary,term,is,set,to,the,new,term;public,void,test,publishing,order,on,promotion,throws,ioexception,broken,barrier,exception,interrupted,exception,final,index,shard,index,shard,new,started,shard,false,final,long,promoted,term,index,shard,get,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,primary,term,equal,to,promoted,term,assert,that,index,shard,get,replication,group,not,null,value,thread,start,final,shard,routing,replica,routing,index,shard,routing,entry,final,shard,routing,primary,routing,new,shard,routing,replica,routing,shard,id,replica,routing,current,node,id,null,true,shard,routing,state,started,replica,routing,allocation,id,final,set,string,in,sync,allocation,ids,collections,singleton,primary,routing,allocation,id,get,id,final,index,shard,routing,table,routing,table,new,index,shard,routing,table,builder,primary,routing,shard,id,add,shard,primary,routing,build,barrier,await,index,shard,update,shard,state,primary,routing,promoted,term,shard,listener,0l,in,sync,allocation,ids,routing,table,collections,empty,set,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException;1529916081;This test makes sure that people can use the shard routing entry to check whether a shard was already promoted to_a primary. Concretely this means, that when we publish the routing entry via {@link IndexShard#routingEntry()} the following_should have happened_1) Internal state (ala ReplicationTracker) have been updated_2) Primary term is set to the new term;public void testPublishingOrderOnPromotion() throws IOException, BrokenBarrierException, InterruptedException {_        final IndexShard indexShard = newStartedShard(false)__        final long promotedTerm = indexShard.getPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPrimaryTerm(), equalTo(promotedTerm))__                    assertThat(indexShard.getReplicationGroup(), notNullValue())__                }_            }_        })__        thread.start()___        final ShardRouting replicaRouting = indexShard.routingEntry()__        final ShardRouting primaryRouting = newShardRouting(replicaRouting.shardId(), replicaRouting.currentNodeId(), null, true,_            ShardRoutingState.STARTED, replicaRouting.allocationId())____        final Set<String> inSyncAllocationIds = Collections.singleton(primaryRouting.allocationId().getId())__        final IndexShardRoutingTable routingTable =_            new IndexShardRoutingTable.Builder(primaryRouting.shardId()).addShard(primaryRouting).build()__        barrier.await()__        _        indexShard.updateShardState(primaryRouting, promotedTerm, (shard, listener) -> {}, 0L, inSyncAllocationIds, routingTable,_            Collections.emptySet())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,to,check,whether,a,shard,was,already,promoted,to,a,primary,concretely,this,means,that,when,we,publish,the,routing,entry,via,link,index,shard,routing,entry,the,following,should,have,happened,1,internal,state,ala,replication,tracker,have,been,updated,2,primary,term,is,set,to,the,new,term;public,void,test,publishing,order,on,promotion,throws,ioexception,broken,barrier,exception,interrupted,exception,final,index,shard,index,shard,new,started,shard,false,final,long,promoted,term,index,shard,get,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,primary,term,equal,to,promoted,term,assert,that,index,shard,get,replication,group,not,null,value,thread,start,final,shard,routing,replica,routing,index,shard,routing,entry,final,shard,routing,primary,routing,new,shard,routing,replica,routing,shard,id,replica,routing,current,node,id,null,true,shard,routing,state,started,replica,routing,allocation,id,final,set,string,in,sync,allocation,ids,collections,singleton,primary,routing,allocation,id,get,id,final,index,shard,routing,table,routing,table,new,index,shard,routing,table,builder,primary,routing,shard,id,add,shard,primary,routing,build,barrier,await,index,shard,update,shard,state,primary,routing,promoted,term,shard,listener,0l,in,sync,allocation,ids,routing,table,collections,empty,set,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1524684173;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "doc", Long.toString(i), "{\"foo\" : \"bar\"}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_            null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,doc,long,to,string,i,foo,bar,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1524780945;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "doc", Long.toString(i), "{\"foo\" : \"bar\"}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_            null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,doc,long,to,string,i,foo,bar,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1525162917;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_            null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1527583663;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_            null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1528706846;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_            null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1529916081;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_            null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1531910483;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_            null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1532434432;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1532947734;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1533295538;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1533738061;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1534338685;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1534539448;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1535723122;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum", "fix")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,fix,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1535965276;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1536137328;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1536314350;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.StoreRecoverySource.EXISTING_STORE_INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,store,recovery,source,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1536611444;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1536828374;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1537371806;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1537806831;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1538067637;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1541176068;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1541431100;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1542697754;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1542961598;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1542979766;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1543221203;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1543236252;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1543832502;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1543930771;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1543947737;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1544081506;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1544322464;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1544783963;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1544893197;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1545678410;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1547508054;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1547520882;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1547576245;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1547625930;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1548124510;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1548871018;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1548872440;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1549395161;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1549406621;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData,_                null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> public void testReadSnapshotAndCheckIndexConcurrently() throws Exception;1550668107;Simulates a scenario that happens when we are async fetching snapshot metadata from GatewayService_and checking index concurrently. This should always be possible without any exception.;public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {_        final boolean isPrimary = randomBoolean()__        IndexShard indexShard = newStartedShard(isPrimary)__        final long numDocs = between(10, 100)__        for (long i = 0_ i < numDocs_ i++) {_            indexDoc(indexShard, "_doc", Long.toString(i), "{}")__            if (randomBoolean()) {_                indexShard.refresh("test")__            }_        }_        indexShard.flush(new FlushRequest())__        closeShards(indexShard)___        final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(),_            isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE_        )__        final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData())_            .settings(Settings.builder()_                .put(indexShard.indexSettings.getSettings())_                .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom("false", "true", "checksum")))_            .build()__        final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData, null, null, indexShard.engineFactory,_                indexShard.getGlobalCheckpointSyncer(), indexShard.getRetentionLeaseSyncer(), EMPTY_EVENT_LISTENER)___        Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata()__        assertTrue("at least 2 files, commit and data: " + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1)__        AtomicBoolean stop = new AtomicBoolean(false)__        CountDownLatch latch = new CountDownLatch(1)__        Thread snapshotter = new Thread(() -> {_            latch.countDown()__            while (stop.get() == false) {_                try {_                    Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata()__                    assertThat(readMeta.getNumDocs(), equalTo(numDocs))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0))__                    assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()))__                } catch (IOException e) {_                    throw new AssertionError(e)__                }_            }_        })__        snapshotter.start()___        if (isPrimary) {_            newShard.markAsRecovering("store", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null))__        } else {_            newShard.markAsRecovering("peer", new RecoveryState(newShard.routingEntry(),_                getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())))__        }_        int iters = iterations(10, 100)__        latch.await()__        for (int i = 0_ i < iters_ i++) {_            newShard.checkIndex()__        }_        assertTrue(stop.compareAndSet(false, true))__        snapshotter.join()__        closeShards(newShard)__    };simulates,a,scenario,that,happens,when,we,are,async,fetching,snapshot,metadata,from,gateway,service,and,checking,index,concurrently,this,should,always,be,possible,without,any,exception;public,void,test,read,snapshot,and,check,index,concurrently,throws,exception,final,boolean,is,primary,random,boolean,index,shard,index,shard,new,started,shard,is,primary,final,long,num,docs,between,10,100,for,long,i,0,i,num,docs,i,index,doc,index,shard,long,to,string,i,if,random,boolean,index,shard,refresh,test,index,shard,flush,new,flush,request,close,shards,index,shard,final,shard,routing,shard,routing,shard,routing,helper,init,with,same,id,index,shard,routing,entry,is,primary,recovery,source,existing,store,recovery,source,instance,recovery,source,peer,recovery,source,instance,final,index,meta,data,index,meta,data,index,meta,data,builder,index,shard,index,settings,get,index,meta,data,settings,settings,builder,put,index,shard,index,settings,get,settings,put,index,settings,get,key,random,from,false,true,checksum,build,final,index,shard,new,shard,new,shard,shard,routing,index,shard,shard,path,index,meta,data,null,null,index,shard,engine,factory,index,shard,get,global,checkpoint,syncer,index,shard,get,retention,lease,syncer,store,metadata,snapshot,store,file,meta,datas,new,shard,snapshot,store,metadata,assert,true,at,least,2,files,commit,and,data,store,file,meta,datas,to,string,store,file,meta,datas,size,1,atomic,boolean,stop,new,atomic,boolean,false,count,down,latch,latch,new,count,down,latch,1,thread,snapshotter,new,thread,latch,count,down,while,stop,get,false,try,store,metadata,snapshot,read,meta,new,shard,snapshot,store,metadata,assert,that,read,meta,get,num,docs,equal,to,num,docs,assert,that,store,file,meta,datas,recovery,diff,read,meta,different,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,missing,size,equal,to,0,assert,that,store,file,meta,datas,recovery,diff,read,meta,identical,size,equal,to,store,file,meta,datas,size,catch,ioexception,e,throw,new,assertion,error,e,snapshotter,start,if,is,primary,new,shard,mark,as,recovering,store,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,null,else,new,shard,mark,as,recovering,peer,new,recovery,state,new,shard,routing,entry,get,fake,disco,node,new,shard,routing,entry,current,node,id,get,fake,disco,node,new,shard,routing,entry,current,node,id,int,iters,iterations,10,100,latch,await,for,int,i,0,i,iters,i,new,shard,check,index,assert,true,stop,compare,and,set,false,true,snapshotter,join,close,shards,new,shard
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1542961598;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1542979766;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1543221203;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1543236252;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1543832502;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1543930771;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1543947737;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1544081506;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1544322464;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1544783963;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1544893197;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1545678410;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1547508054;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1547520882;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1547576245;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1547625930;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1548124510;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1548871018;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1548872440;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1549395161;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1549406621;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,                                                          final long opPrimaryTerm,                                                          final long globalCheckpoint,                                                          final long maxSeqNoOfUpdatesOrDeletes,                                                          final ActionListener<Releasable> listener,                                                          final String info);1550668107;Randomizes the usage of {@link IndexShard#acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)} and_{@link IndexShard#acquireAllReplicaOperationsPermits(long, long, long, ActionListener, TimeValue)} in order to acquire a permit.;private void randomReplicaOperationPermitAcquisition(final IndexShard indexShard,_                                                         final long opPrimaryTerm,_                                                         final long globalCheckpoint,_                                                         final long maxSeqNoOfUpdatesOrDeletes,_                                                         final ActionListener<Releasable> listener,_                                                         final String info) {_        if (randomBoolean()) {_            final String executor = ThreadPool.Names.WRITE__            indexShard.acquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, executor, info)__        } else {_            final TimeValue timeout = TimeValue.timeValueSeconds(30L)__            indexShard.acquireAllReplicaOperationsPermits(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener, timeout)__        }_    };randomizes,the,usage,of,link,index,shard,acquire,replica,operation,permit,long,long,long,action,listener,string,object,and,link,index,shard,acquire,all,replica,operations,permits,long,long,long,action,listener,time,value,in,order,to,acquire,a,permit;private,void,random,replica,operation,permit,acquisition,final,index,shard,index,shard,final,long,op,primary,term,final,long,global,checkpoint,final,long,max,seq,no,of,updates,or,deletes,final,action,listener,releasable,listener,final,string,info,if,random,boolean,final,string,executor,thread,pool,names,write,index,shard,acquire,replica,operation,permit,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,executor,info,else,final,time,value,timeout,time,value,time,value,seconds,30l,index,shard,acquire,all,replica,operations,permits,op,primary,term,global,checkpoint,max,seq,no,of,updates,or,deletes,listener,timeout
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1524684173;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "test", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i,_                        1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse,_                        getMappingUpdater(indexShard, sourceToParse.type()))__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,test,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,version,type,external,index,request,false,source,to,parse,get,mapping,updater,index,shard,source,to,parse,type,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1524780945;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "test", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i,_                        1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse,_                        getMappingUpdater(indexShard, sourceToParse.type()))__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,test,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,version,type,external,index,request,false,source,to,parse,get,mapping,updater,index,shard,source,to,parse,type,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1525162917;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i,_                    1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,version,type,external,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1527583663;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i,_                    1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,version,type,external,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1528706846;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i,_                    1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,version,type,external,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1529916081;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i,_                    1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,version,type,external,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1531910483;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1532434432;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1532947734;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1533295538;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1533738061;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1534338685;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1534539448;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1535723122;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1535965276;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1536137328;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1536314350;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1536611444;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1536828374;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1537371806;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1537806831;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max, gap)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max,gap
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1538067637;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1541176068;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1541431100;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1542697754;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1542961598;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1542979766;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1543221203;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1543236252;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1543832502;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1543930771;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1543947737;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1544081506;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1544322464;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1544783963;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1544893197;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,source,to,parse,source,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1545678410;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1547508054;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1547520882;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1547576245;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1547625930;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1548124510;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1548871018;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1548872440;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1549395161;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1549406621;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> private Result indexOnReplicaWithGaps(             final IndexShard indexShard,             final int operations,             final int offset) throws IOException;1550668107;Index on the specified shard while introducing sequence number gaps.__@param indexShard the shard_@param operations the number of operations_@param offset     the starting sequence number_@return a pair of the maximum sequence number and whether or not a gap was introduced_@throws IOException if an I/O exception occurs while indexing on the shard;private Result indexOnReplicaWithGaps(_            final IndexShard indexShard,_            final int operations,_            final int offset) throws IOException {_        int localCheckpoint = offset__        int max = offset__        boolean gap = false__        for (int i = offset + 1_ i < operations_ i++) {_            if (!rarely() || i == operations - 1) { _                final String id = Integer.toString(i)__                SourceToParse sourceToParse = new SourceToParse(indexShard.shardId().getIndexName(), "_doc", id,_                        new BytesArray("{}"), XContentType.JSON)__                indexShard.applyIndexOperationOnReplica(i, 1,_                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse)__                if (!gap && i == localCheckpoint + 1) {_                    localCheckpoint++__                }_                max = i__            } else {_                gap = true__            }_            if (rarely()) {_                indexShard.flush(new FlushRequest())__            }_        }_        assert localCheckpoint == indexShard.getLocalCheckpoint()__        assert !gap || (localCheckpoint != max)__        return new Result(localCheckpoint, max)__    };index,on,the,specified,shard,while,introducing,sequence,number,gaps,param,index,shard,the,shard,param,operations,the,number,of,operations,param,offset,the,starting,sequence,number,return,a,pair,of,the,maximum,sequence,number,and,whether,or,not,a,gap,was,introduced,throws,ioexception,if,an,i,o,exception,occurs,while,indexing,on,the,shard;private,result,index,on,replica,with,gaps,final,index,shard,index,shard,final,int,operations,final,int,offset,throws,ioexception,int,local,checkpoint,offset,int,max,offset,boolean,gap,false,for,int,i,offset,1,i,operations,i,if,rarely,i,operations,1,final,string,id,integer,to,string,i,source,to,parse,source,to,parse,new,source,to,parse,index,shard,shard,id,get,index,name,id,new,bytes,array,xcontent,type,json,index,shard,apply,index,operation,on,replica,i,1,index,request,false,source,to,parse,if,gap,i,local,checkpoint,1,local,checkpoint,max,i,else,gap,true,if,rarely,index,shard,flush,new,flush,request,assert,local,checkpoint,index,shard,get,local,checkpoint,assert,gap,local,checkpoint,max,return,new,result,local,checkpoint,max
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1531910483;This test makes sure that people can use the shard routing entry to check whether a shard was already promoted to_a primary. Concretely this means, that when we publish the routing entry via {@link IndexShard#routingEntry()} the following_should have happened_1) Internal state (ala ReplicationTracker) have been updated_2) Primary term is set to the new term;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPrimaryTerm(), equalTo(promotedTerm))__                    assertThat(indexShard.getReplicationGroup(), notNullValue())__                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,to,check,whether,a,shard,was,already,promoted,to,a,primary,concretely,this,means,that,when,we,publish,the,routing,entry,via,link,index,shard,routing,entry,the,following,should,have,happened,1,internal,state,ala,replication,tracker,have,been,updated,2,primary,term,is,set,to,the,new,term;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,primary,term,equal,to,promoted,term,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1532434432;This test makes sure that people can use the shard routing entry to check whether a shard was already promoted to_a primary. Concretely this means, that when we publish the routing entry via {@link IndexShard#routingEntry()} the following_should have happened_1) Internal state (ala ReplicationTracker) have been updated_2) Primary term is set to the new term;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPrimaryTerm(), equalTo(promotedTerm))__                    assertThat(indexShard.getReplicationGroup(), notNullValue())__                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,to,check,whether,a,shard,was,already,promoted,to,a,primary,concretely,this,means,that,when,we,publish,the,routing,entry,via,link,index,shard,routing,entry,the,following,should,have,happened,1,internal,state,ala,replication,tracker,have,been,updated,2,primary,term,is,set,to,the,new,term;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,primary,term,equal,to,promoted,term,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1532947734;This test makes sure that people can use the shard routing entry to check whether a shard was already promoted to_a primary. Concretely this means, that when we publish the routing entry via {@link IndexShard#routingEntry()} the following_should have happened_1) Internal state (ala ReplicationTracker) have been updated_2) Primary term is set to the new term;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPrimaryTerm(), equalTo(promotedTerm))__                    assertThat(indexShard.getReplicationGroup(), notNullValue())__                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,to,check,whether,a,shard,was,already,promoted,to,a,primary,concretely,this,means,that,when,we,publish,the,routing,entry,via,link,index,shard,routing,entry,the,following,should,have,happened,1,internal,state,ala,replication,tracker,have,been,updated,2,primary,term,is,set,to,the,new,term;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,primary,term,equal,to,promoted,term,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1533295538;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1533738061;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1534338685;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1534539448;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1535723122;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1535965276;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1536137328;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1536314350;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1536611444;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1536828374;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1537371806;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1537806831;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1538067637;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1541176068;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1541431100;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1542697754;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1542961598;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1542979766;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1543221203;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1543236252;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1543832502;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1543930771;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1543947737;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1544081506;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1544322464;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1544783963;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1544893197;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1545678410;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1547508054;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1547520882;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1547576245;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1547625930;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1548124510;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1548871018;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1548872440;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1549395161;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1549406621;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
IndexShardTests -> public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException;1550668107;This test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already_promoted to a primary.;public void testPublishingOrderOnPromotion() throws IOException, InterruptedException, BrokenBarrierException {_        final IndexShard indexShard = newShard(false)__        recoveryEmptyReplica(indexShard, randomBoolean())__        final long promotedTerm = indexShard.getPendingPrimaryTerm() + 1__        final CyclicBarrier barrier = new CyclicBarrier(2)__        final AtomicBoolean stop = new AtomicBoolean()__        final Thread thread = new Thread(() -> {_            try {_                barrier.await()__            } catch (final BrokenBarrierException | InterruptedException e) {_                throw new RuntimeException(e)__            }_            while(stop.get() == false) {_                if (indexShard.routingEntry().primary()) {_                    assertThat(indexShard.getPendingPrimaryTerm(), equalTo(promotedTerm))__                    final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>()__                    indexShard.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, "bla")__                    try (Releasable ignored = permitAcquiredFuture.actionGet()) {_                        assertThat(indexShard.getReplicationGroup(), notNullValue())__                    }_                }_            }_        })__        thread.start()___        barrier.await()__        final ShardRouting replicaRouting = indexShard.routingEntry()__        promoteReplica(indexShard, Collections.singleton(replicaRouting.allocationId().getId()),_            new IndexShardRoutingTable.Builder(replicaRouting.shardId()).addShard(replicaRouting).build())___        stop.set(true)__        thread.join()__        closeShards(indexShard)__    };this,test,makes,sure,that,people,can,use,the,shard,routing,entry,take,an,operation,permit,to,check,whether,a,shard,was,already,promoted,to,a,primary;public,void,test,publishing,order,on,promotion,throws,ioexception,interrupted,exception,broken,barrier,exception,final,index,shard,index,shard,new,shard,false,recovery,empty,replica,index,shard,random,boolean,final,long,promoted,term,index,shard,get,pending,primary,term,1,final,cyclic,barrier,barrier,new,cyclic,barrier,2,final,atomic,boolean,stop,new,atomic,boolean,final,thread,thread,new,thread,try,barrier,await,catch,final,broken,barrier,exception,interrupted,exception,e,throw,new,runtime,exception,e,while,stop,get,false,if,index,shard,routing,entry,primary,assert,that,index,shard,get,pending,primary,term,equal,to,promoted,term,final,plain,action,future,releasable,permit,acquired,future,new,plain,action,future,index,shard,acquire,primary,operation,permit,permit,acquired,future,thread,pool,names,same,bla,try,releasable,ignored,permit,acquired,future,action,get,assert,that,index,shard,get,replication,group,not,null,value,thread,start,barrier,await,final,shard,routing,replica,routing,index,shard,routing,entry,promote,replica,index,shard,collections,singleton,replica,routing,allocation,id,get,id,new,index,shard,routing,table,builder,replica,routing,shard,id,add,shard,replica,routing,build,stop,set,true,thread,join,close,shards,index,shard
