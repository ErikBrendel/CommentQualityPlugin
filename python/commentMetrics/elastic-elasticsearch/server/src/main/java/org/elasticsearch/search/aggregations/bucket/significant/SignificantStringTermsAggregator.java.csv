commented;modifiers;parameterAmount;loc;comment;code
false;public;2;5;;@Override public void collect(int doc, long bucket) throws IOException {     super.collect(doc, bucket).     numCollectedDocs++. }
false;public;2;11;;@Override public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, final LeafBucketCollector sub) throws IOException {     return new LeafBucketCollectorBase(super.getLeafCollector(ctx, sub), null) {          @Override         public void collect(int doc, long bucket) throws IOException {             super.collect(doc, bucket).             numCollectedDocs++.         }     }. }
false;public;1;51;;@Override public SignificantStringTerms buildAggregation(long owningBucketOrdinal) throws IOException {     assert owningBucketOrdinal == 0.     final int size = (int) Math.min(bucketOrds.size(), bucketCountThresholds.getShardSize()).     long supersetSize = termsAggFactory.getSupersetNumDocs().     long subsetSize = numCollectedDocs.     BucketSignificancePriorityQueue<SignificantStringTerms.Bucket> ordered = new BucketSignificancePriorityQueue<>(size).     SignificantStringTerms.Bucket spare = null.     for (int i = 0. i < bucketOrds.size(). i++) {         final int docCount = bucketDocCount(i).         if (docCount < bucketCountThresholds.getShardMinDocCount()) {             continue.         }         if (spare == null) {             spare = new SignificantStringTerms.Bucket(new BytesRef(), 0, 0, 0, 0, null, format).         }         bucketOrds.get(i, spare.termBytes).         spare.subsetDf = docCount.         spare.subsetSize = subsetSize.         spare.supersetDf = termsAggFactory.getBackgroundFrequency(spare.termBytes).         spare.supersetSize = supersetSize.         // During shard-local down-selection we use subset/superset stats         // that are for this shard only         // Back at the central reducer these properties will be updated with         // global stats         spare.updateScore(significanceHeuristic).         spare.bucketOrd = i.         spare = ordered.insertWithOverflow(spare).         if (spare == null) {             consumeBucketsAndMaybeBreak(1).         }     }     final SignificantStringTerms.Bucket[] list = new SignificantStringTerms.Bucket[ordered.size()].     for (int i = ordered.size() - 1. i >= 0. i--) {         final SignificantStringTerms.Bucket bucket = ordered.pop().         // the terms are owned by the BytesRefHash, we need to pull a copy since the BytesRef hash data may be recycled at some point         bucket.termBytes = BytesRef.deepCopyOf(bucket.termBytes).         bucket.aggregations = bucketAggregations(bucket.bucketOrd).         list[i] = bucket.     }     return new SignificantStringTerms(name, bucketCountThresholds.getRequiredSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators(), metaData(), format, subsetSize, supersetSize, significanceHeuristic, Arrays.asList(list)). }
false;public;0;9;;@Override public SignificantStringTerms buildEmptyAggregation() {     // We need to account for the significance of a miss in our global stats - provide corpus size as context     ContextIndexSearcher searcher = context.searcher().     IndexReader topReader = searcher.getIndexReader().     int supersetSize = topReader.numDocs().     return new SignificantStringTerms(name, bucketCountThresholds.getRequiredSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators(), metaData(), format, 0, supersetSize, significanceHeuristic, emptyList()). }
false;public;0;4;;@Override public void doClose() {     Releasables.close(bucketOrds, termsAggFactory). }
