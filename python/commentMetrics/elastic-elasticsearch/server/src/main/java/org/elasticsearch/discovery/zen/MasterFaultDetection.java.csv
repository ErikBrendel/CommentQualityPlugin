commented;modifiers;parameterAmount;loc;comment;code
true;;3;1;/**  * called when pinging the master failed, like a timeout, transport disconnects etc  */ ;/**  * called when pinging the master failed, like a timeout, transport disconnects etc  */ void onMasterFailure(DiscoveryNode masterNode, Throwable cause, String reason).
false;public;0;3;;public DiscoveryNode masterNode() {     return this.masterNode. }
false;public;1;3;;public void addListener(Listener listener) {     listeners.add(listener). }
false;public;1;3;;public void removeListener(Listener listener) {     listeners.remove(listener). }
false;public;2;9;;public void restart(DiscoveryNode masterNode, String reason) {     synchronized (masterNodeMutex) {         if (logger.isDebugEnabled()) {             logger.debug("[master] restarting fault detection against master [{}], reason [{}]", masterNode, reason).         }         innerStop().         innerStart(masterNode).     } }
false;private;1;12;;private void innerStart(final DiscoveryNode masterNode) {     this.masterNode = masterNode.     this.retryCount = 0.     this.notifiedMasterFailure.set(false).     if (masterPinger != null) {         masterPinger.stop().     }     this.masterPinger = new MasterPinger().     // we start pinging slightly later to allow the chosen master to complete it's own master election     threadPool.schedule(masterPinger, pingInterval, ThreadPool.Names.SAME). }
false;public;1;10;;public void stop(String reason) {     synchronized (masterNodeMutex) {         if (masterNode != null) {             if (logger.isDebugEnabled()) {                 logger.debug("[master] stopping fault detection against master [{}], reason [{}]", masterNode, reason).             }         }         innerStop().     } }
false;private;0;9;;private void innerStop() {     // also will stop the next ping schedule     this.retryCount = 0.     if (masterPinger != null) {         masterPinger.stop().         masterPinger = null.     }     this.masterNode = null. }
false;public;0;6;;@Override public void close() {     super.close().     stop("closing").     this.listeners.clear(). }
false;protected;1;26;;@Override protected void handleTransportDisconnect(DiscoveryNode node) {     synchronized (masterNodeMutex) {         if (!node.equals(this.masterNode)) {             return.         }         if (connectOnNetworkDisconnect) {             try {                 transportService.connectToNode(node).                 // if all is well, make sure we restart the pinger                 if (masterPinger != null) {                     masterPinger.stop().                 }                 this.masterPinger = new MasterPinger().                 // we use schedule with a 0 time value to run the pinger on the pool as it will run on later                 threadPool.schedule(masterPinger, TimeValue.timeValueMillis(0), ThreadPool.Names.SAME).             } catch (Exception e) {                 logger.trace("[master] [{}] transport disconnected (with verified connect)", masterNode).                 notifyMasterFailure(masterNode, null, "transport disconnected (with verified connect)").             }         } else {             logger.trace("[master] [{}] transport disconnected", node).             notifyMasterFailure(node, null, "transport disconnected").         }     } }
false;private;3;14;;private void notifyMasterFailure(final DiscoveryNode masterNode, final Throwable cause, final String reason) {     if (notifiedMasterFailure.compareAndSet(false, true)) {         try {             threadPool.generic().execute(() -> {                 for (Listener listener : listeners) {                     listener.onMasterFailure(masterNode, cause, reason).                 }             }).         } catch (EsRejectedExecutionException e) {             logger.error("master failure notification was rejected, it's highly likely the node is shutting down", e).         }         stop("master failure, " + reason).     } }
false;public;0;3;;public void stop() {     this.running = false. }
false;public;1;4;;@Override public MasterPingResponseResponse read(StreamInput in) throws IOException {     return new MasterPingResponseResponse(in). }
false;public;1;13;;@Override public void handleResponse(MasterPingResponseResponse response) {     if (!running) {         return.     }     // reset the counter, we got a good result     MasterFaultDetection.this.retryCount = 0.     // check if the master node did not get switched on us..., if it did, we simply return with no reschedule     if (masterToPing.equals(MasterFaultDetection.this.masterNode())) {         // we don't stop on disconnection from master, we keep pinging it         threadPool.schedule(MasterPinger.this, pingInterval, ThreadPool.Names.SAME).     } }
false;public;1;43;;@Override public void handleException(TransportException exp) {     if (!running) {         return.     }     synchronized (masterNodeMutex) {         // check if the master node did not get switched on us...         if (masterToPing.equals(MasterFaultDetection.this.masterNode())) {             if (exp instanceof ConnectTransportException || exp.getCause() instanceof ConnectTransportException) {                 handleTransportDisconnect(masterToPing).                 return.             } else if (exp.getCause() instanceof NotMasterException) {                 logger.debug("[master] pinging a master {} that is no longer a master", masterNode).                 notifyMasterFailure(masterToPing, exp, "no longer master").                 return.             } else if (exp.getCause() instanceof ThisIsNotTheMasterYouAreLookingForException) {                 logger.debug("[master] pinging a master {} that is not the master", masterNode).                 notifyMasterFailure(masterToPing, exp, "not master").                 return.             } else if (exp.getCause() instanceof NodeDoesNotExistOnMasterException) {                 logger.debug("[master] pinging a master {} but we do not exists on it, act as if its master failure", masterNode).                 notifyMasterFailure(masterToPing, exp, "do not exists on master, act as master failure").                 return.             }             int retryCount = ++MasterFaultDetection.this.retryCount.             logger.trace(() -> new ParameterizedMessage("[master] failed to ping [{}], retry [{}] out of [{}]", masterNode, retryCount, pingRetryCount), exp).             if (retryCount >= pingRetryCount) {                 logger.debug("[master] failed to ping [{}], tried [{}] times, each with maximum [{}] timeout", masterNode, pingRetryCount, pingRetryTimeout).                 // not good, failure                 notifyMasterFailure(masterToPing, null, "failed to ping, tried [" + pingRetryCount + "] times, each with  maximum [" + pingRetryTimeout + "] timeout").             } else {                 // resend the request, not reschedule, rely on send timeout                 transportService.sendRequest(masterToPing, MASTER_PING_ACTION_NAME, request, options, this).             }         }     } }
false;public;0;4;;@Override public String executor() {     return ThreadPool.Names.SAME. }
false;public;0;89;;@Override public void run() {     if (!running) {         // return and don't spawn...         return.     }     final DiscoveryNode masterToPing = masterNode.     if (masterToPing == null) {         // master is null, should not happen, but we are still running, so reschedule         threadPool.schedule(MasterPinger.this, pingInterval, ThreadPool.Names.SAME).         return.     }     final MasterPingRequest request = new MasterPingRequest(clusterStateSupplier.get().nodes().getLocalNode(), masterToPing, clusterName).     final TransportRequestOptions options = TransportRequestOptions.builder().withType(TransportRequestOptions.Type.PING).withTimeout(pingRetryTimeout).build().     transportService.sendRequest(masterToPing, MASTER_PING_ACTION_NAME, request, options, new TransportResponseHandler<MasterPingResponseResponse>() {          @Override         public MasterPingResponseResponse read(StreamInput in) throws IOException {             return new MasterPingResponseResponse(in).         }          @Override         public void handleResponse(MasterPingResponseResponse response) {             if (!running) {                 return.             }             // reset the counter, we got a good result             MasterFaultDetection.this.retryCount = 0.             // check if the master node did not get switched on us..., if it did, we simply return with no reschedule             if (masterToPing.equals(MasterFaultDetection.this.masterNode())) {                 // we don't stop on disconnection from master, we keep pinging it                 threadPool.schedule(MasterPinger.this, pingInterval, ThreadPool.Names.SAME).             }         }          @Override         public void handleException(TransportException exp) {             if (!running) {                 return.             }             synchronized (masterNodeMutex) {                 // check if the master node did not get switched on us...                 if (masterToPing.equals(MasterFaultDetection.this.masterNode())) {                     if (exp instanceof ConnectTransportException || exp.getCause() instanceof ConnectTransportException) {                         handleTransportDisconnect(masterToPing).                         return.                     } else if (exp.getCause() instanceof NotMasterException) {                         logger.debug("[master] pinging a master {} that is no longer a master", masterNode).                         notifyMasterFailure(masterToPing, exp, "no longer master").                         return.                     } else if (exp.getCause() instanceof ThisIsNotTheMasterYouAreLookingForException) {                         logger.debug("[master] pinging a master {} that is not the master", masterNode).                         notifyMasterFailure(masterToPing, exp, "not master").                         return.                     } else if (exp.getCause() instanceof NodeDoesNotExistOnMasterException) {                         logger.debug("[master] pinging a master {} but we do not exists on it, act as if its master failure", masterNode).                         notifyMasterFailure(masterToPing, exp, "do not exists on master, act as master failure").                         return.                     }                     int retryCount = ++MasterFaultDetection.this.retryCount.                     logger.trace(() -> new ParameterizedMessage("[master] failed to ping [{}], retry [{}] out of [{}]", masterNode, retryCount, pingRetryCount), exp).                     if (retryCount >= pingRetryCount) {                         logger.debug("[master] failed to ping [{}], tried [{}] times, each with maximum [{}] timeout", masterNode, pingRetryCount, pingRetryTimeout).                         // not good, failure                         notifyMasterFailure(masterToPing, null, "failed to ping, tried [" + pingRetryCount + "] times, each with  maximum [" + pingRetryTimeout + "] timeout").                     } else {                         // resend the request, not reschedule, rely on send timeout                         transportService.sendRequest(masterToPing, MASTER_PING_ACTION_NAME, request, options, this).                     }                 }             }         }          @Override         public String executor() {             return ThreadPool.Names.SAME.         }     }). }
false;public;0;4;;@Override public Throwable fillInStackTrace() {     return null. }
false;public;0;4;;@Override public Throwable fillInStackTrace() {     return null. }
false;public;1;9;;@Override public ClusterState execute(ClusterState currentState) throws Exception {     // if we are no longer master, fail...     DiscoveryNodes nodes = currentState.nodes().     if (!nodes.nodeExists(request.sourceNode)) {         throw new NodeDoesNotExistOnMasterException().     }     return currentState. }
false;public;1;4;;@Override public void onNoLongerMaster(String source) {     onFailure(source, new NotMasterException("local node is not master")). }
false;public;2;12;;@Override public void onFailure(String source, @Nullable Exception e) {     if (e == null) {         e = new ElasticsearchException("unknown error while processing ping").     }     try {         channel.sendResponse(e).     } catch (IOException inner) {         inner.addSuppressed(e).         logger.warn("error while sending ping response", inner).     } }
false;public;3;8;;@Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {     try {         channel.sendResponse(new MasterPingResponseResponse()).     } catch (IOException e) {         logger.warn("error while sending ping response", e).     } }
false;public;3;71;;@Override public void messageReceived(final MasterPingRequest request, final TransportChannel channel, Task task) throws Exception {     final DiscoveryNodes nodes = clusterStateSupplier.get().nodes().     // this can happen if the master got "kill -9" and then another node started using the same port     if (!request.masterNode.equals(nodes.getLocalNode())) {         throw new ThisIsNotTheMasterYouAreLookingForException().     }     // ping from nodes of version < 1.4.0 will have the clustername set to null     if (request.clusterName != null && !request.clusterName.equals(clusterName)) {         logger.trace("master fault detection ping request is targeted for a different [{}] cluster then us [{}]", request.clusterName, clusterName).         throw new ThisIsNotTheMasterYouAreLookingForException("master fault detection ping request is targeted for a different [" + request.clusterName + "] cluster then us [" + clusterName + "]").     }     if (!nodes.isLocalNodeElectedMaster() || !nodes.nodeExists(request.sourceNode)) {         logger.trace("checking ping from {} under a cluster state thread", request.sourceNode).         masterService.submitStateUpdateTask("master ping (from: " + request.sourceNode + ")", new ClusterStateUpdateTask() {              @Override             public ClusterState execute(ClusterState currentState) throws Exception {                 // if we are no longer master, fail...                 DiscoveryNodes nodes = currentState.nodes().                 if (!nodes.nodeExists(request.sourceNode)) {                     throw new NodeDoesNotExistOnMasterException().                 }                 return currentState.             }              @Override             public void onNoLongerMaster(String source) {                 onFailure(source, new NotMasterException("local node is not master")).             }              @Override             public void onFailure(String source, @Nullable Exception e) {                 if (e == null) {                     e = new ElasticsearchException("unknown error while processing ping").                 }                 try {                     channel.sendResponse(e).                 } catch (IOException inner) {                     inner.addSuppressed(e).                     logger.warn("error while sending ping response", inner).                 }             }              @Override             public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {                 try {                     channel.sendResponse(new MasterPingResponseResponse()).                 } catch (IOException e) {                     logger.warn("error while sending ping response", e).                 }             }         }).     } else {         // send a response, and note if we are connected to the master or not         channel.sendResponse(new MasterPingResponseResponse()).     } }
false;public;1;7;;@Override public void readFrom(StreamInput in) throws IOException {     super.readFrom(in).     sourceNode = new DiscoveryNode(in).     masterNode = new DiscoveryNode(in).     clusterName = new ClusterName(in). }
false;public;1;7;;@Override public void writeTo(StreamOutput out) throws IOException {     super.writeTo(out).     sourceNode.writeTo(out).     masterNode.writeTo(out).     clusterName.writeTo(out). }
