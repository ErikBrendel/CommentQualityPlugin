commented;modifiers;parameterAmount;loc;comment;code
false;protected;0;13;;@Override protected void doStart() {     ByteSizeValue chunkSize = chunkSize().     if (chunkSize != null && chunkSize.getBytes() <= 0) {         throw new IllegalArgumentException("the chunk size cannot be negative: [" + chunkSize + "]").     }     globalMetaDataFormat = new ChecksumBlobStoreFormat<>(METADATA_CODEC, METADATA_NAME_FORMAT, MetaData::fromXContent, namedXContentRegistry, compress).     indexMetaDataFormat = new ChecksumBlobStoreFormat<>(INDEX_METADATA_CODEC, METADATA_NAME_FORMAT, IndexMetaData::fromXContent, namedXContentRegistry, compress).     snapshotFormat = new ChecksumBlobStoreFormat<>(SNAPSHOT_CODEC, SNAPSHOT_NAME_FORMAT, SnapshotInfo::fromXContentInternal, namedXContentRegistry, compress). }
false;protected;0;2;;@Override protected void doStop() { }
false;protected;0;15;;@Override protected void doClose() {     BlobStore store.     // to close blobStore if blobStore initialization is started during close     synchronized (lock) {         store = blobStore.get().     }     if (store != null) {         try {             store.close().         } catch (Exception t) {             logger.warn("cannot close blob store", t).         }     } }
true;;0;3;// package private, only use for testing ;// package private, only use for testing BlobContainer getBlobContainer() {     return blobContainer.get(). }
true;protected;0;3;// for test purposes only ;// for test purposes only protected BlobStore getBlobStore() {     return blobStore.get(). }
true;protected;0;16;/**  * maintains single lazy instance of {@link BlobContainer}  */ ;/**  * maintains single lazy instance of {@link BlobContainer}  */ protected BlobContainer blobContainer() {     assertSnapshotOrGenericThread().     BlobContainer blobContainer = this.blobContainer.get().     if (blobContainer == null) {         synchronized (lock) {             blobContainer = this.blobContainer.get().             if (blobContainer == null) {                 blobContainer = blobStore().blobContainer(basePath()).                 this.blobContainer.set(blobContainer).             }         }     }     return blobContainer. }
true;protected;0;24;/**  * maintains single lazy instance of {@link BlobStore}  */ ;/**  * maintains single lazy instance of {@link BlobStore}  */ protected BlobStore blobStore() {     assertSnapshotOrGenericThread().     BlobStore store = blobStore.get().     if (store == null) {         synchronized (lock) {             store = blobStore.get().             if (store == null) {                 if (lifecycle.started() == false) {                     throw new RepositoryException(metadata.name(), "repository is not in started state").                 }                 try {                     store = createBlobStore().                 } catch (RepositoryException e) {                     throw e.                 } catch (Exception e) {                     throw new RepositoryException(metadata.name(), "cannot create blob store", e).                 }                 blobStore.set(store).             }         }     }     return store. }
true;protected,abstract;0;1;/**  * Creates new BlobStore to read and write data.  */ ;/**  * Creates new BlobStore to read and write data.  */ protected abstract BlobStore createBlobStore() throws Exception.
true;protected,abstract;0;1;/**  * Returns base path of the repository  */ ;/**  * Returns base path of the repository  */ protected abstract BlobPath basePath().
true;protected,final;0;3;/**  * Returns true if metadata and snapshot files should be compressed  *  * @return true if compression is needed  */ ;/**  * Returns true if metadata and snapshot files should be compressed  *  * @return true if compression is needed  */ protected final boolean isCompress() {     return compress. }
true;protected;0;3;/**  * Returns data file chunk size.  * <p>  * This method should return null if no chunking is needed.  *  * @return chunk size  */ ;/**  * Returns data file chunk size.  * <p>  * This method should return null if no chunking is needed.  *  * @return chunk size  */ protected ByteSizeValue chunkSize() {     return null. }
false;public;0;4;;@Override public RepositoryMetaData getMetadata() {     return metadata. }
false;public;3;30;;@Override public void initializeSnapshot(SnapshotId snapshotId, List<IndexId> indices, MetaData clusterMetaData) {     if (isReadOnly()) {         throw new RepositoryException(metadata.name(), "cannot create snapshot in a readonly repository").     }     try {         final String snapshotName = snapshotId.getName().         // check if the snapshot name already exists in the repository         final RepositoryData repositoryData = getRepositoryData().         if (repositoryData.getAllSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName))) {             throw new InvalidSnapshotNameException(metadata.name(), snapshotId.getName(), "snapshot with the same name already exists").         }         if (snapshotFormat.exists(blobContainer(), snapshotId.getUUID())) {             throw new InvalidSnapshotNameException(metadata.name(), snapshotId.getName(), "snapshot with the same name already exists").         }         // Write Global MetaData         globalMetaDataFormat.write(clusterMetaData, blobContainer(), snapshotId.getUUID()).         // write the index metadata for each index in the snapshot         for (IndexId index : indices) {             final IndexMetaData indexMetaData = clusterMetaData.index(index.getName()).             final BlobPath indexPath = basePath().add("indices").add(index.getId()).             final BlobContainer indexMetaDataBlobContainer = blobStore().blobContainer(indexPath).             indexMetaDataFormat.write(indexMetaData, indexMetaDataBlobContainer, snapshotId.getUUID()).         }     } catch (IOException ex) {         throw new SnapshotCreationException(metadata.name(), snapshotId, ex).     } }
false;public;2;80;;@Override public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId) {     if (isReadOnly()) {         throw new RepositoryException(metadata.name(), "cannot delete snapshot from a readonly repository").     }     final RepositoryData repositoryData = getRepositoryData().     SnapshotInfo snapshot = null.     try {         snapshot = getSnapshotInfo(snapshotId).     } catch (SnapshotMissingException ex) {         throw ex.     } catch (IllegalStateException | SnapshotException | ElasticsearchParseException ex) {         logger.warn(() -> new ParameterizedMessage("cannot read snapshot file [{}]", snapshotId), ex).     }     try {         // Delete snapshot from the index file, since it is the maintainer of truth of active snapshots         final RepositoryData updatedRepositoryData = repositoryData.removeSnapshot(snapshotId).         writeIndexGen(updatedRepositoryData, repositoryStateId).         // delete the snapshot file         deleteSnapshotBlobIgnoringErrors(snapshot, snapshotId.getUUID()).         // delete the global metadata file         deleteGlobalMetaDataBlobIgnoringErrors(snapshot, snapshotId.getUUID()).         // Now delete all indices         if (snapshot != null) {             final List<String> indices = snapshot.indices().             for (String index : indices) {                 final IndexId indexId = repositoryData.resolveIndexId(index).                 IndexMetaData indexMetaData = null.                 try {                     indexMetaData = getSnapshotIndexMetaData(snapshotId, indexId).                 } catch (ElasticsearchParseException | IOException ex) {                     logger.warn(() -> new ParameterizedMessage("[{}] [{}] failed to read metadata for index", snapshotId, index), ex).                 }                 deleteIndexMetaDataBlobIgnoringErrors(snapshot, indexId).                 if (indexMetaData != null) {                     for (int shardId = 0. shardId < indexMetaData.getNumberOfShards(). shardId++) {                         try {                             delete(snapshotId, indexId, new ShardId(indexMetaData.getIndex(), shardId)).                         } catch (SnapshotException ex) {                             final int finalShardId = shardId.                             logger.warn(() -> new ParameterizedMessage("[{}] failed to delete shard data for shard [{}][{}]", snapshotId, index, finalShardId), ex).                         }                     }                 }             }         }         // cleanup indices that are no longer part of the repository         final Collection<IndexId> indicesToCleanUp = Sets.newHashSet(repositoryData.getIndices().values()).         indicesToCleanUp.removeAll(updatedRepositoryData.getIndices().values()).         final BlobContainer indicesBlobContainer = blobStore().blobContainer(basePath().add("indices")).         for (final IndexId indexId : indicesToCleanUp) {             try {                 indicesBlobContainer.deleteBlob(indexId.getId()).             } catch (DirectoryNotEmptyException dnee) {                 // if the directory isn't empty for some reason, it will fail to clean up.                 // we'll ignore that and accept that cleanup didn't fully succeed.                 // since we are using UUIDs for path names, this won't be an issue for                 // snapshotting indices of the same name                 logger.debug(() -> new ParameterizedMessage("[{}] index [{}] no longer part of any snapshots in the repository, " + "but failed to clean up its index folder due to the directory not being empty.", metadata.name(), indexId), dnee).             } catch (IOException ioe) {                 // a different IOException occurred while trying to delete - will just log the issue for now                 logger.debug(() -> new ParameterizedMessage("[{}] index [{}] no longer part of any snapshots in the repository, " + "but failed to clean up its index folder.", metadata.name(), indexId), ioe).             }         }     } catch (IOException | ResourceNotFoundException ex) {         throw new RepositoryException(metadata.name(), "failed to delete snapshot [" + snapshotId + "]", ex).     } }
false;private;2;12;;private void deleteSnapshotBlobIgnoringErrors(final SnapshotInfo snapshotInfo, final String blobId) {     try {         snapshotFormat.delete(blobContainer(), blobId).     } catch (IOException e) {         if (snapshotInfo != null) {             logger.warn(() -> new ParameterizedMessage("[{}] Unable to delete snapshot file [{}]", snapshotInfo.snapshotId(), blobId), e).         } else {             logger.warn(() -> new ParameterizedMessage("Unable to delete snapshot file [{}]", blobId), e).         }     } }
false;private;2;12;;private void deleteGlobalMetaDataBlobIgnoringErrors(final SnapshotInfo snapshotInfo, final String blobId) {     try {         globalMetaDataFormat.delete(blobContainer(), blobId).     } catch (IOException e) {         if (snapshotInfo != null) {             logger.warn(() -> new ParameterizedMessage("[{}] Unable to delete global metadata file [{}]", snapshotInfo.snapshotId(), blobId), e).         } else {             logger.warn(() -> new ParameterizedMessage("Unable to delete global metadata file [{}]", blobId), e).         }     } }
false;private;2;10;;private void deleteIndexMetaDataBlobIgnoringErrors(final SnapshotInfo snapshotInfo, final IndexId indexId) {     final SnapshotId snapshotId = snapshotInfo.snapshotId().     BlobContainer indexMetaDataBlobContainer = blobStore().blobContainer(basePath().add("indices").add(indexId.getId())).     try {         indexMetaDataFormat.delete(indexMetaDataBlobContainer, snapshotId.getUUID()).     } catch (IOException ex) {         logger.warn(() -> new ParameterizedMessage("[{}] failed to delete metadata for index [{}]", snapshotId, indexId.getName()), ex).     } }
true;public;8;28;/**  * {@inheritDoc}  */ ;/**  * {@inheritDoc}  */ @Override public SnapshotInfo finalizeSnapshot(final SnapshotId snapshotId, final List<IndexId> indices, final long startTime, final String failure, final int totalShards, final List<SnapshotShardFailure> shardFailures, final long repositoryStateId, final boolean includeGlobalState) {     SnapshotInfo blobStoreSnapshot = new SnapshotInfo(snapshotId, indices.stream().map(IndexId::getName).collect(Collectors.toList()), startTime, failure, System.currentTimeMillis(), totalShards, shardFailures, includeGlobalState).     try {         snapshotFormat.write(blobStoreSnapshot, blobContainer(), snapshotId.getUUID()).         final RepositoryData repositoryData = getRepositoryData().         writeIndexGen(repositoryData.addSnapshot(snapshotId, blobStoreSnapshot.state(), indices), repositoryStateId).     } catch (FileAlreadyExistsException ex) {         // log a warning here and carry on         throw new RepositoryException(metadata.name(), "Blob already exists while " + "finalizing snapshot, assume the snapshot has already been saved", ex).     } catch (IOException ex) {         throw new RepositoryException(metadata.name(), "failed to update snapshot in repository", ex).     }     return blobStoreSnapshot. }
false;public;1;10;;@Override public SnapshotInfo getSnapshotInfo(final SnapshotId snapshotId) {     try {         return snapshotFormat.read(blobContainer(), snapshotId.getUUID()).     } catch (NoSuchFileException ex) {         throw new SnapshotMissingException(metadata.name(), snapshotId, ex).     } catch (IOException | NotXContentException ex) {         throw new SnapshotException(metadata.name(), snapshotId, "failed to get snapshots", ex).     } }
false;public;1;10;;@Override public MetaData getSnapshotGlobalMetaData(final SnapshotId snapshotId) {     try {         return globalMetaDataFormat.read(blobContainer(), snapshotId.getUUID()).     } catch (NoSuchFileException ex) {         throw new SnapshotMissingException(metadata.name(), snapshotId, ex).     } catch (IOException ex) {         throw new SnapshotException(metadata.name(), snapshotId, "failed to read global metadata", ex).     } }
false;public;2;5;;@Override public IndexMetaData getSnapshotIndexMetaData(final SnapshotId snapshotId, final IndexId index) throws IOException {     final BlobPath indexPath = basePath().add("indices").add(index.getId()).     return indexMetaDataFormat.read(blobStore().blobContainer(indexPath), snapshotId.getUUID()). }
true;private;3;9;/**  * Configures RateLimiter based on repository and global settings  *  * @param repositorySettings repository settings  * @param setting            setting to use to configure rate limiter  * @param defaultRate        default limiting rate  * @return rate limiter or null of no throttling is needed  */ ;/**  * Configures RateLimiter based on repository and global settings  *  * @param repositorySettings repository settings  * @param setting            setting to use to configure rate limiter  * @param defaultRate        default limiting rate  * @return rate limiter or null of no throttling is needed  */ private RateLimiter getRateLimiter(Settings repositorySettings, String setting, ByteSizeValue defaultRate) {     ByteSizeValue maxSnapshotBytesPerSec = repositorySettings.getAsBytesSize(setting, settings.getAsBytesSize(setting, defaultRate)).     if (maxSnapshotBytesPerSec.getBytes() <= 0) {         return null.     } else {         return new RateLimiter.SimpleRateLimiter(maxSnapshotBytesPerSec.getMbFrac()).     } }
false;public;0;4;;@Override public long getSnapshotThrottleTimeInNanos() {     return snapshotRateLimitingTimeInNanos.count(). }
false;public;0;4;;@Override public long getRestoreThrottleTimeInNanos() {     return restoreRateLimitingTimeInNanos.count(). }
false;protected;0;5;;protected void assertSnapshotOrGenericThread() {     assert Thread.currentThread().getName().contains(ThreadPool.Names.SNAPSHOT) || Thread.currentThread().getName().contains(ThreadPool.Names.GENERIC) : "Expected current thread [" + Thread.currentThread() + "] to be the snapshot or generic thread.". }
false;public;0;22;;@Override public String startVerification() {     try {         if (isReadOnly()) {             // It's readonly - so there is not much we can do here to verify it apart from reading the blob store metadata             latestIndexBlobId().             return "read-only".         } else {             String seed = UUIDs.randomBase64UUID().             byte[] testBytes = Strings.toUTF8Bytes(seed).             BlobContainer testContainer = blobStore().blobContainer(basePath().add(testBlobPrefix(seed))).             String blobName = "master.dat".             BytesArray bytes = new BytesArray(testBytes).             try (InputStream stream = bytes.streamInput()) {                 testContainer.writeBlobAtomic(blobName, stream, bytes.length(), true).             }             return seed.         }     } catch (IOException exp) {         throw new RepositoryVerificationException(metadata.name(), "path " + basePath() + " is not accessible on master node", exp).     } }
false;public;1;10;;@Override public void endVerification(String seed) {     if (isReadOnly() == false) {         try {             blobStore().delete(basePath().add(testBlobPrefix(seed))).         } catch (IOException exp) {             throw new RepositoryVerificationException(metadata.name(), "cannot delete test data at " + basePath(), exp).         }     } }
false;public;0;49;;@Override public RepositoryData getRepositoryData() {     try {         final long indexGen = latestIndexBlobId().         final String snapshotsIndexBlobName = INDEX_FILE_PREFIX + Long.toString(indexGen).         RepositoryData repositoryData.         try (InputStream blob = blobContainer().readBlob(snapshotsIndexBlobName)) {             BytesStreamOutput out = new BytesStreamOutput().             Streams.copy(blob, out).             // EMPTY is safe here because RepositoryData#fromXContent calls namedObject             try (XContentParser parser = XContentHelper.createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, out.bytes(), XContentType.JSON)) {                 repositoryData = RepositoryData.snapshotsFromXContent(parser, indexGen).             } catch (NotXContentException e) {                 logger.warn("[{}] index blob is not valid x-content [{} bytes]", snapshotsIndexBlobName, out.bytes().length()).                 throw e.             }         }         // now load the incompatible snapshot ids, if they exist         try (InputStream blob = blobContainer().readBlob(INCOMPATIBLE_SNAPSHOTS_BLOB)) {             BytesStreamOutput out = new BytesStreamOutput().             Streams.copy(blob, out).             try (XContentParser parser = XContentHelper.createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, out.bytes(), XContentType.JSON)) {                 repositoryData = repositoryData.incompatibleSnapshotsFromXContent(parser).             }         } catch (NoSuchFileException e) {             if (isReadOnly()) {                 logger.debug("[{}] Incompatible snapshots blob [{}] does not exist, the likely " + "reason is that there are no incompatible snapshots in the repository", metadata.name(), INCOMPATIBLE_SNAPSHOTS_BLOB).             } else {                 // write an empty incompatible-snapshots blob - we do this so that there                 // is a blob present, which helps speed up some cloud-based repositories                 // (e.g. S3), which retry if a blob is missing with exponential backoff,                 // delaying the read of repository data and sometimes causing a timeout                 writeIncompatibleSnapshots(RepositoryData.EMPTY).             }         }         return repositoryData.     } catch (NoSuchFileException ex) {         // repository doesn't have an index blob, its a new blank repo         return RepositoryData.EMPTY.     } catch (IOException ioe) {         throw new RepositoryException(metadata.name(), "could not read repository data from index blob", ioe).     } }
false;public,static;1;3;;public static String testBlobPrefix(String seed) {     return TESTS_FILE + seed. }
false;public;0;4;;@Override public boolean isReadOnly() {     return readOnly. }
false;protected;2;39;;protected void writeIndexGen(final RepositoryData repositoryData, final long repositoryStateId) throws IOException {     // can not write to a read only repository     assert isReadOnly() == false.     final long currentGen = latestIndexBlobId().     if (currentGen != repositoryStateId) {         // repository data         throw new RepositoryException(metadata.name(), "concurrent modification of the index-N file, expected current generation [" + repositoryStateId + "], actual current generation [" + currentGen + "] - possibly due to simultaneous snapshot deletion requests").     }     final long newGen = currentGen + 1.     final BytesReference snapshotsBytes.     try (BytesStreamOutput bStream = new BytesStreamOutput()) {         try (StreamOutput stream = new OutputStreamStreamOutput(bStream)) {             XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON, stream).             repositoryData.snapshotsToXContent(builder, ToXContent.EMPTY_PARAMS).             builder.close().         }         snapshotsBytes = bStream.bytes().     }     // write the index file     final String indexBlob = INDEX_FILE_PREFIX + Long.toString(newGen).     logger.debug("Repository [{}] writing new index generational blob [{}]", metadata.name(), indexBlob).     writeAtomic(indexBlob, snapshotsBytes, true).     // delete the N-2 index file if it exists, keep the previous one around as a backup     if (isReadOnly() == false && newGen - 2 >= 0) {         final String oldSnapshotIndexFile = INDEX_FILE_PREFIX + Long.toString(newGen - 2).         blobContainer().deleteBlobIgnoringIfNotExists(oldSnapshotIndexFile).     }     // write the current generation to the index-latest file     final BytesReference genBytes.     try (BytesStreamOutput bStream = new BytesStreamOutput()) {         bStream.writeLong(newGen).         genBytes = bStream.bytes().     }     logger.debug("Repository [{}] updating index.latest with generation [{}]", metadata.name(), newGen).     writeAtomic(INDEX_LATEST_BLOB, genBytes, false). }
true;;1;14;/**  * Writes the incompatible snapshot ids list to the `incompatible-snapshots` blob in the repository.  *  * Package private for testing.  */ ;/**  * Writes the incompatible snapshot ids list to the `incompatible-snapshots` blob in the repository.  *  * Package private for testing.  */ void writeIncompatibleSnapshots(RepositoryData repositoryData) throws IOException {     // can not write to a read only repository     assert isReadOnly() == false.     final BytesReference bytes.     try (BytesStreamOutput bStream = new BytesStreamOutput()) {         try (StreamOutput stream = new OutputStreamStreamOutput(bStream)) {             XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON, stream).             repositoryData.incompatibleSnapshotsToXContent(builder, ToXContent.EMPTY_PARAMS).             builder.close().         }         bytes = bStream.bytes().     }     // write the incompatible snapshots blob     writeAtomic(INCOMPATIBLE_SNAPSHOTS_BLOB, bytes, false). }
true;;0;24;/**  * Get the latest snapshot index blob id.  Snapshot index blobs are named index-N, where N is  * the next version number from when the index blob was written.  Each individual index-N blob is  * only written once and never overwritten.  The highest numbered index-N blob is the latest one  * that contains the current snapshots in the repository.  *  * Package private for testing  */ ;/**  * Get the latest snapshot index blob id.  Snapshot index blobs are named index-N, where N is  * the next version number from when the index blob was written.  Each individual index-N blob is  * only written once and never overwritten.  The highest numbered index-N blob is the latest one  * that contains the current snapshots in the repository.  *  * Package private for testing  */ long latestIndexBlobId() throws IOException {     try {         // and because the index.latest blob will never be deleted and re-written.         return listBlobsToGetLatestIndexId().     } catch (UnsupportedOperationException e) {         // in this case, try reading the latest index generation from the index.latest blob         try {             return readSnapshotIndexLatestBlob().         } catch (NoSuchFileException nsfe) {             return RepositoryData.EMPTY_REPO_GEN.         }     } }
true;;0;7;// package private for testing ;// package private for testing long readSnapshotIndexLatestBlob() throws IOException {     try (InputStream blob = blobContainer().readBlob(INDEX_LATEST_BLOB)) {         BytesStreamOutput out = new BytesStreamOutput().         Streams.copy(blob, out).         return Numbers.bytesToLong(out.bytes().toBytesRef()).     } }
false;private;0;20;;private long listBlobsToGetLatestIndexId() throws IOException {     Map<String, BlobMetaData> blobs = blobContainer().listBlobsByPrefix(INDEX_FILE_PREFIX).     long latest = RepositoryData.EMPTY_REPO_GEN.     if (blobs.isEmpty()) {         // no snapshot index blobs have been written yet         return latest.     }     for (final BlobMetaData blobMetaData : blobs.values()) {         final String blobName = blobMetaData.name().         try {             final long curr = Long.parseLong(blobName.substring(INDEX_FILE_PREFIX.length())).             latest = Math.max(latest, curr).         } catch (NumberFormatException nfe) {             // the index- blob wasn't of the format index-N where N is a number,             // no idea what this blob is but it doesn't belong in the repository!             logger.debug("[{}] Unknown blob in the repository: {}", metadata.name(), blobName).         }     }     return latest. }
false;private;3;5;;private void writeAtomic(final String blobName, final BytesReference bytesRef, boolean failIfAlreadyExists) throws IOException {     try (InputStream stream = bytesRef.streamInput()) {         blobContainer().writeBlobAtomic(blobName, stream, bytesRef.length(), failIfAlreadyExists).     } }
false;public;6;15;;@Override public void snapshotShard(IndexShard shard, Store store, SnapshotId snapshotId, IndexId indexId, IndexCommit snapshotIndexCommit, IndexShardSnapshotStatus snapshotStatus) {     SnapshotContext snapshotContext = new SnapshotContext(store, snapshotId, indexId, snapshotStatus, System.currentTimeMillis()).     try {         snapshotContext.snapshot(snapshotIndexCommit).     } catch (Exception e) {         snapshotStatus.moveToFailed(System.currentTimeMillis(), ExceptionsHelper.detailedMessage(e)).         if (e instanceof IndexShardSnapshotFailedException) {             throw (IndexShardSnapshotFailedException) e.         } else {             throw new IndexShardSnapshotFailedException(store.shardId(), e).         }     } }
false;public;6;15;;@Override public void restoreShard(IndexShard shard, SnapshotId snapshotId, Version version, IndexId indexId, ShardId snapshotShardId, RecoveryState recoveryState) {     final Context context = new Context(snapshotId, indexId, shard.shardId(), snapshotShardId).     BlobPath path = basePath().add("indices").add(indexId.getId()).add(Integer.toString(snapshotShardId.getId())).     BlobContainer blobContainer = blobStore().blobContainer(path).     final RestoreContext snapshotContext = new RestoreContext(shard, snapshotId, recoveryState, blobContainer).     try {         BlobStoreIndexShardSnapshot snapshot = context.loadSnapshot().         SnapshotFiles snapshotFiles = new SnapshotFiles(snapshot.snapshot(), snapshot.indexFiles()).         snapshotContext.restore(snapshotFiles).     } catch (Exception e) {         throw new IndexShardRestoreFailedException(shard.shardId(), "failed to restore snapshot [" + snapshotId + "]", e).     } }
false;public;4;8;;@Override public IndexShardSnapshotStatus getShardSnapshotStatus(SnapshotId snapshotId, Version version, IndexId indexId, ShardId shardId) {     Context context = new Context(snapshotId, indexId, shardId).     BlobStoreIndexShardSnapshot snapshot = context.loadSnapshot().     return IndexShardSnapshotStatus.newDone(snapshot.startTime(), snapshot.time(), snapshot.incrementalFileCount(), snapshot.totalFileCount(), snapshot.incrementalSize(), snapshot.totalSize()). }
false;public;2;30;;@Override public void verify(String seed, DiscoveryNode localNode) {     assertSnapshotOrGenericThread().     if (isReadOnly()) {         try {             latestIndexBlobId().         } catch (IOException e) {             throw new RepositoryVerificationException(metadata.name(), "path " + basePath() + " is not accessible on node " + localNode, e).         }     } else {         BlobContainer testBlobContainer = blobStore().blobContainer(basePath().add(testBlobPrefix(seed))).         if (testBlobContainer.blobExists("master.dat")) {             try {                 BytesArray bytes = new BytesArray(seed).                 try (InputStream stream = bytes.streamInput()) {                     testBlobContainer.writeBlob("data-" + localNode.getId() + ".dat", stream, bytes.length(), true).                 }             } catch (IOException exp) {                 throw new RepositoryVerificationException(metadata.name(), "store location [" + blobStore() + "] is not accessible on the node [" + localNode + "]", exp).             }         } else {             throw new RepositoryVerificationException(metadata.name(), "a file written by master to the store [" + blobStore() + "] cannot be accessed on the node [" + localNode + "]. " + "This might indicate that the store [" + blobStore() + "] is not shared between this node and the master node or " + "that permissions on the store don't allow reading files written by the master node").         }     } }
true;private;3;4;/**  * Delete shard snapshot  *  * @param snapshotId snapshot id  * @param shardId    shard id  */ ;/**  * Delete shard snapshot  *  * @param snapshotId snapshot id  * @param shardId    shard id  */ private void delete(SnapshotId snapshotId, IndexId indexId, ShardId shardId) {     Context context = new Context(snapshotId, indexId, shardId, shardId).     context.delete(). }
false;public;0;7;;@Override public String toString() {     return "BlobStoreRepository[" + "[" + metadata.name() + "], [" + blobStore() + ']' + ']'. }
true;public;0;28;/**  * Delete shard snapshot  */ ;/**  * Delete shard snapshot  */ public void delete() {     final Map<String, BlobMetaData> blobs.     try {         blobs = blobContainer.listBlobs().     } catch (IOException e) {         throw new IndexShardSnapshotException(shardId, "Failed to list content of gateway", e).     }     Tuple<BlobStoreIndexShardSnapshots, Integer> tuple = buildBlobStoreIndexShardSnapshots(blobs).     BlobStoreIndexShardSnapshots snapshots = tuple.v1().     int fileListGeneration = tuple.v2().     try {         indexShardSnapshotFormat.delete(blobContainer, snapshotId.getUUID()).     } catch (IOException e) {         logger.debug("[{}] [{}] failed to delete shard snapshot file", shardId, snapshotId).     }     // Build a list of snapshots that should be preserved     List<SnapshotFiles> newSnapshotsList = new ArrayList<>().     for (SnapshotFiles point : snapshots) {         if (!point.snapshot().equals(snapshotId.getName())) {             newSnapshotsList.add(point).         }     }     // finalize the snapshot and rewrite the snapshot index with the next sequential snapshot index     finalize(newSnapshotsList, fileListGeneration + 1, blobs, "snapshot deletion [" + snapshotId + "]"). }
true;;0;7;/**  * Loads information about shard snapshot  */ ;/**  * Loads information about shard snapshot  */ BlobStoreIndexShardSnapshot loadSnapshot() {     try {         return indexShardSnapshotFormat.read(blobContainer, snapshotId.getUUID()).     } catch (IOException ex) {         throw new SnapshotException(metadata.name(), snapshotId, "failed to read shard snapshot file for " + shardId, ex).     } }
true;protected;4;57;/**  * Writes a new index file for the shard and removes all unreferenced files from the repository.  *  * We need to be really careful in handling index files in case of failures to make sure we don't  * have index file that points to files that were deleted.  *  * @param snapshots          list of active snapshots in the container  * @param fileListGeneration the generation number of the snapshot index file  * @param blobs              list of blobs in the container  * @param reason             a reason explaining why the shard index file is written  */ ;/**  * Writes a new index file for the shard and removes all unreferenced files from the repository.  *  * We need to be really careful in handling index files in case of failures to make sure we don't  * have index file that points to files that were deleted.  *  * @param snapshots          list of active snapshots in the container  * @param fileListGeneration the generation number of the snapshot index file  * @param blobs              list of blobs in the container  * @param reason             a reason explaining why the shard index file is written  */ protected void finalize(final List<SnapshotFiles> snapshots, final int fileListGeneration, final Map<String, BlobMetaData> blobs, final String reason) {     final String indexGeneration = Integer.toString(fileListGeneration).     final String currentIndexGen = indexShardSnapshotsFormat.blobName(indexGeneration).     final BlobStoreIndexShardSnapshots updatedSnapshots = new BlobStoreIndexShardSnapshots(snapshots).     try {         // attempt to write an index file with this generation failed mid-way after creating the temporary file.         for (final String blobName : blobs.keySet()) {             if (FsBlobContainer.isTempBlobName(blobName)) {                 try {                     blobContainer.deleteBlobIgnoringIfNotExists(blobName).                 } catch (IOException e) {                     logger.warn(() -> new ParameterizedMessage("[{}][{}] failed to delete index blob [{}] during finalization", snapshotId, shardId, blobName), e).                     throw e.                 }             }         }         // If we deleted all snapshots, we don't need to create a new index file         if (snapshots.size() > 0) {             indexShardSnapshotsFormat.writeAtomic(updatedSnapshots, blobContainer, indexGeneration).         }         // Delete old index files         for (final String blobName : blobs.keySet()) {             if (blobName.startsWith(SNAPSHOT_INDEX_PREFIX)) {                 try {                     blobContainer.deleteBlobIgnoringIfNotExists(blobName).                 } catch (IOException e) {                     logger.warn(() -> new ParameterizedMessage("[{}][{}] failed to delete index blob [{}] during finalization", snapshotId, shardId, blobName), e).                     throw e.                 }             }         }         // Delete all blobs that don't exist in a snapshot         for (final String blobName : blobs.keySet()) {             if (blobName.startsWith(DATA_BLOB_PREFIX) && (updatedSnapshots.findNameFile(canonicalName(blobName)) == null)) {                 try {                     blobContainer.deleteBlobIgnoringIfNotExists(blobName).                 } catch (IOException e) {                     logger.warn(() -> new ParameterizedMessage("[{}][{}] failed to delete data blob [{}] during finalization", snapshotId, shardId, blobName), e).                 }             }         }     } catch (IOException e) {         String message = "Failed to finalize " + reason + " with shard index [" + currentIndexGen + "]".         throw new IndexShardSnapshotFailedException(shardId, message, e).     } }
true;protected;1;3;/**  * Generates blob name  *  * @param generation the blob number  * @return the blob name  */ ;/**  * Generates blob name  *  * @param generation the blob number  * @return the blob name  */ protected String fileNameFromGeneration(long generation) {     return DATA_BLOB_PREFIX + Long.toString(generation, Character.MAX_RADIX). }
true;protected;1;18;/**  * Finds the next available blob number  *  * @param blobs list of blobs in the repository  * @return next available blob number  */ ;/**  * Finds the next available blob number  *  * @param blobs list of blobs in the repository  * @return next available blob number  */ protected long findLatestFileNameGeneration(Map<String, BlobMetaData> blobs) {     long generation = -1.     for (String name : blobs.keySet()) {         if (!name.startsWith(DATA_BLOB_PREFIX)) {             continue.         }         name = canonicalName(name).         try {             long currentGen = Long.parseLong(name.substring(DATA_BLOB_PREFIX.length()), Character.MAX_RADIX).             if (currentGen > generation) {                 generation = currentGen.             }         } catch (NumberFormatException e) {             logger.warn("file [{}] does not conform to the '{}' schema", name, DATA_BLOB_PREFIX).         }     }     return generation. }
true;protected;1;45;/**  * Loads all available snapshots in the repository  *  * @param blobs list of blobs in repository  * @return tuple of BlobStoreIndexShardSnapshots and the last snapshot index generation  */ ;/**  * Loads all available snapshots in the repository  *  * @param blobs list of blobs in repository  * @return tuple of BlobStoreIndexShardSnapshots and the last snapshot index generation  */ protected Tuple<BlobStoreIndexShardSnapshots, Integer> buildBlobStoreIndexShardSnapshots(Map<String, BlobMetaData> blobs) {     int latest = -1.     Set<String> blobKeys = blobs.keySet().     for (String name : blobKeys) {         if (name.startsWith(SNAPSHOT_INDEX_PREFIX)) {             try {                 int gen = Integer.parseInt(name.substring(SNAPSHOT_INDEX_PREFIX.length())).                 if (gen > latest) {                     latest = gen.                 }             } catch (NumberFormatException ex) {                 logger.warn("failed to parse index file name [{}]", name).             }         }     }     if (latest >= 0) {         try {             final BlobStoreIndexShardSnapshots shardSnapshots = indexShardSnapshotsFormat.read(blobContainer, Integer.toString(latest)).             return new Tuple<>(shardSnapshots, latest).         } catch (IOException e) {             final String file = SNAPSHOT_INDEX_PREFIX + latest.             logger.warn(() -> new ParameterizedMessage("failed to read index file [{}]", file), e).         }     } else if (blobKeys.isEmpty() == false) {         logger.debug("Could not find a readable index-N file in a non-empty shard snapshot directory [{}]", blobContainer.path()).     }     // We couldn't load the index file - falling back to loading individual snapshots     List<SnapshotFiles> snapshots = new ArrayList<>().     for (String name : blobKeys) {         try {             BlobStoreIndexShardSnapshot snapshot = null.             if (name.startsWith(SNAPSHOT_PREFIX)) {                 snapshot = indexShardSnapshotFormat.readBlob(blobContainer, name).             }             if (snapshot != null) {                 snapshots.add(new SnapshotFiles(snapshot.snapshot(), snapshot.indexFiles())).             }         } catch (IOException e) {             logger.warn(() -> new ParameterizedMessage("failed to read commit point [{}]", name), e).         }     }     return new Tuple<>(new BlobStoreIndexShardSnapshots(snapshots), -1). }
true;public;1;133;/**  * Create snapshot from index commit point  *  * @param snapshotIndexCommit snapshot commit point  */ ;/**  * Create snapshot from index commit point  *  * @param snapshotIndexCommit snapshot commit point  */ public void snapshot(final IndexCommit snapshotIndexCommit) {     logger.debug("[{}] [{}] snapshot to [{}] ...", shardId, snapshotId, metadata.name()).     final Map<String, BlobMetaData> blobs.     try {         blobs = blobContainer.listBlobs().     } catch (IOException e) {         throw new IndexShardSnapshotFailedException(shardId, "failed to list blobs", e).     }     long generation = findLatestFileNameGeneration(blobs).     Tuple<BlobStoreIndexShardSnapshots, Integer> tuple = buildBlobStoreIndexShardSnapshots(blobs).     BlobStoreIndexShardSnapshots snapshots = tuple.v1().     int fileListGeneration = tuple.v2().     if (snapshots.snapshots().stream().anyMatch(sf -> sf.snapshot().equals(snapshotId.getName()))) {         throw new IndexShardSnapshotFailedException(shardId, "Duplicate snapshot name [" + snapshotId.getName() + "] detected, aborting").     }     final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>().     store.incRef().     int indexIncrementalFileCount = 0.     int indexTotalNumberOfFiles = 0.     long indexIncrementalSize = 0.     long indexTotalFileCount = 0.     try {         ArrayList<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new ArrayList<>().         final Store.MetadataSnapshot metadata.         // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should         final Collection<String> fileNames.         try {             logger.trace("[{}] [{}] Loading store metadata using index commit [{}]", shardId, snapshotId, snapshotIndexCommit).             metadata = store.getMetadata(snapshotIndexCommit).             fileNames = snapshotIndexCommit.getFileNames().         } catch (IOException e) {             throw new IndexShardSnapshotFailedException(shardId, "Failed to get store file metadata", e).         }         for (String fileName : fileNames) {             if (snapshotStatus.isAborted()) {                 logger.debug("[{}] [{}] Aborted on the file [{}], exiting", shardId, snapshotId, fileName).                 throw new IndexShardSnapshotFailedException(shardId, "Aborted").             }             logger.trace("[{}] [{}] Processing [{}]", shardId, snapshotId, fileName).             final StoreFileMetaData md = metadata.get(fileName).             BlobStoreIndexShardSnapshot.FileInfo existingFileInfo = null.             List<BlobStoreIndexShardSnapshot.FileInfo> filesInfo = snapshots.findPhysicalIndexFiles(fileName).             if (filesInfo != null) {                 for (BlobStoreIndexShardSnapshot.FileInfo fileInfo : filesInfo) {                     try {                         // in 1.3.3 we added additional hashes for .si / segments_N files                         // to ensure we don't double the space in the repo since old snapshots                         // don't have this hash we try to read that hash from the blob store                         // in a bwc compatible way.                         maybeRecalculateMetadataHash(blobContainer, fileInfo, metadata).                     } catch (Exception e) {                         logger.warn(() -> new ParameterizedMessage("{} Can't calculate hash from blob for file [{}] [{}]", shardId, fileInfo.physicalName(), fileInfo.metadata()), e).                     }                     if (fileInfo.isSame(md) && snapshotFileExistsInBlobs(fileInfo, blobs)) {                         // a commit point file with the same name, size and checksum was already copied to repository                         // we will reuse it for this snapshot                         existingFileInfo = fileInfo.                         break.                     }                 }             }             indexTotalFileCount += md.length().             indexTotalNumberOfFiles++.             if (existingFileInfo == null) {                 indexIncrementalFileCount++.                 indexIncrementalSize += md.length().                 // create a new FileInfo                 BlobStoreIndexShardSnapshot.FileInfo snapshotFileInfo = new BlobStoreIndexShardSnapshot.FileInfo(fileNameFromGeneration(++generation), md, chunkSize()).                 indexCommitPointFiles.add(snapshotFileInfo).                 filesToSnapshot.add(snapshotFileInfo).             } else {                 indexCommitPointFiles.add(existingFileInfo).             }         }         snapshotStatus.moveToStarted(startTime, indexIncrementalFileCount, indexTotalNumberOfFiles, indexIncrementalSize, indexTotalFileCount).         for (BlobStoreIndexShardSnapshot.FileInfo snapshotFileInfo : filesToSnapshot) {             try {                 snapshotFile(snapshotFileInfo).             } catch (IOException e) {                 throw new IndexShardSnapshotFailedException(shardId, "Failed to perform snapshot (index files)", e).             }         }     } finally {         store.decRef().     }     final IndexShardSnapshotStatus.Copy lastSnapshotStatus = snapshotStatus.moveToFinalize(snapshotIndexCommit.getGeneration()).     // now create and write the commit point     final BlobStoreIndexShardSnapshot snapshot = new BlobStoreIndexShardSnapshot(snapshotId.getName(), lastSnapshotStatus.getIndexVersion(), indexCommitPointFiles, lastSnapshotStatus.getStartTime(), // so it's safe to use with VLong     System.currentTimeMillis() - lastSnapshotStatus.getStartTime(), lastSnapshotStatus.getIncrementalFileCount(), lastSnapshotStatus.getIncrementalSize()).     // TODO: The time stored in snapshot doesn't include cleanup time.     logger.trace("[{}] [{}] writing shard snapshot file", shardId, snapshotId).     try {         indexShardSnapshotFormat.write(snapshot, blobContainer, snapshotId.getUUID()).     } catch (IOException e) {         throw new IndexShardSnapshotFailedException(shardId, "Failed to write commit point", e).     }     // delete all files that are not referenced by any commit point     // build a new BlobStoreIndexShardSnapshot, that includes this one and all the saved ones     List<SnapshotFiles> newSnapshotsList = new ArrayList<>().     newSnapshotsList.add(new SnapshotFiles(snapshot.snapshot(), snapshot.indexFiles())).     for (SnapshotFiles point : snapshots) {         newSnapshotsList.add(point).     }     // finalize the snapshot and rewrite the snapshot index with the next sequential snapshot index     finalize(newSnapshotsList, fileListGeneration + 1, blobs, "snapshot creation [" + snapshotId + "]").     snapshotStatus.moveToDone(System.currentTimeMillis()). }
true;private;1;23;/**  * Snapshot individual file  *  * @param fileInfo file to be snapshotted  */ ;/**  * Snapshot individual file  *  * @param fileInfo file to be snapshotted  */ private void snapshotFile(final BlobStoreIndexShardSnapshot.FileInfo fileInfo) throws IOException {     final String file = fileInfo.physicalName().     try (IndexInput indexInput = store.openVerifyingInput(file, IOContext.READONCE, fileInfo.metadata())) {         for (int i = 0. i < fileInfo.numberOfParts(). i++) {             final long partBytes = fileInfo.partBytes(i).             final InputStreamIndexInput inputStreamIndexInput = new InputStreamIndexInput(indexInput, partBytes).             InputStream inputStream = inputStreamIndexInput.             if (snapshotRateLimiter != null) {                 inputStream = new RateLimitingInputStream(inputStreamIndexInput, snapshotRateLimiter, snapshotRateLimitingTimeInNanos::inc).             }             inputStream = new AbortableInputStream(inputStream, fileInfo.physicalName()).             blobContainer.writeBlob(fileInfo.partName(i), inputStream, partBytes, true).         }         Store.verify(indexInput).         snapshotStatus.addProcessedFile(fileInfo.length()).     } catch (Exception t) {         failStoreIfCorrupted(t).         snapshotStatus.addProcessedFile(0).         throw t.     } }
false;private;1;10;;private void failStoreIfCorrupted(Exception e) {     if (Lucene.isCorruptionException(e)) {         try {             store.markStoreCorrupted((IOException) e).         } catch (IOException inner) {             inner.addSuppressed(e).             logger.warn("store cannot be marked as corrupted", inner).         }     } }
true;private;2;20;/**  * Checks if snapshot file already exists in the list of blobs  *  * @param fileInfo file to check  * @param blobs    list of blobs  * @return true if file exists in the list of blobs  */ ;/**  * Checks if snapshot file already exists in the list of blobs  *  * @param fileInfo file to check  * @param blobs    list of blobs  * @return true if file exists in the list of blobs  */ private boolean snapshotFileExistsInBlobs(BlobStoreIndexShardSnapshot.FileInfo fileInfo, Map<String, BlobMetaData> blobs) {     BlobMetaData blobMetaData = blobs.get(fileInfo.name()).     if (blobMetaData != null) {         return blobMetaData.length() == fileInfo.length().     } else if (blobs.containsKey(fileInfo.partName(0))) {         // multi part file sum up the size and check         int part = 0.         long totalSize = 0.         while (true) {             blobMetaData = blobs.get(fileInfo.partName(part++)).             if (blobMetaData == null) {                 break.             }             totalSize += blobMetaData.length().         }         return totalSize == fileInfo.length().     }     // no file, not exact and not multipart     return false. }
false;public;0;5;;@Override public int read() throws IOException {     checkAborted().     return in.read(). }
false;public;3;5;;@Override public int read(byte[] b, int off, int len) throws IOException {     checkAborted().     return in.read(b, off, len). }
false;private;0;6;;private void checkAborted() {     if (snapshotStatus.isAborted()) {         logger.debug("[{}] [{}] Aborted on the file [{}], exiting", shardId, snapshotId, fileName).         throw new IndexShardSnapshotFailedException(shardId, "Aborted").     } }
true;private,static;3;20;/**  * This is a BWC layer to ensure we update the snapshots metadata with the corresponding hashes before we compare them.  * The new logic for StoreFileMetaData reads the entire {@code .si} and {@code segments.n} files to strengthen the  * comparison of the files on a per-segment / per-commit level.  */ ;/**  * This is a BWC layer to ensure we update the snapshots metadata with the corresponding hashes before we compare them.  * The new logic for StoreFileMetaData reads the entire {@code .si} and {@code segments.n} files to strengthen the  * comparison of the files on a per-segment / per-commit level.  */ private static void maybeRecalculateMetadataHash(final BlobContainer blobContainer, final BlobStoreIndexShardSnapshot.FileInfo fileInfo, Store.MetadataSnapshot snapshot) throws Exception {     final StoreFileMetaData metadata.     if (fileInfo != null && (metadata = snapshot.get(fileInfo.physicalName())) != null) {         if (metadata.hash().length > 0 && fileInfo.metadata().hash().length == 0) {             // we might have multiple parts even though the file is small... make sure we read all of it.             try (InputStream stream = new PartSliceStream(blobContainer, fileInfo)) {                 BytesRefBuilder builder = new BytesRefBuilder().                 Store.MetadataSnapshot.hashFile(builder, stream, fileInfo.length()).                 // reset the file infos metadata hash                 BytesRef hash = fileInfo.metadata().hash().                 assert hash.length == 0.                 hash.bytes = builder.bytes().                 hash.offset = 0.                 hash.length = builder.length().             }         }     } }
false;protected;1;4;;@Override protected InputStream openSlice(long slice) throws IOException {     return container.readBlob(info.partName(slice)). }
false;protected;1;9;;@Override protected InputStream fileInputStream(BlobStoreIndexShardSnapshot.FileInfo fileInfo) {     if (restoreRateLimiter == null) {         return new PartSliceStream(blobContainer, fileInfo).     } else {         RateLimitingInputStream.Listener listener = restoreRateLimitingTimeInNanos::inc.         return new RateLimitingInputStream(new PartSliceStream(blobContainer, fileInfo), restoreRateLimiter, listener).     } }
