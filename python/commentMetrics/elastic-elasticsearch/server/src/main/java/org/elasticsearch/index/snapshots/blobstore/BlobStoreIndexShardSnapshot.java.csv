commented;modifiers;parameterAmount;loc;comment;code
true;public;0;3;/**  * Returns the base file name  *  * @return file name  */ ;/**  * Returns the base file name  *  * @return file name  */ public String name() {     return name. }
true;public;1;7;/**  * Returns part name if file is stored as multiple parts  *  * @param part part number  * @return part name  */ ;/**  * Returns part name if file is stored as multiple parts  *  * @param part part number  * @return part name  */ public String partName(long part) {     if (numberOfParts > 1) {         return name + ".part" + part.     } else {         return name.     } }
true;public,static;1;6;/**  * Returns base file name from part name  *  * @param blobName part name  * @return base file name  */ ;/**  * Returns base file name from part name  *  * @param blobName part name  * @return base file name  */ public static String canonicalName(String blobName) {     if (blobName.contains(".part")) {         return blobName.substring(0, blobName.indexOf(".part")).     }     return blobName. }
true;public;0;3;/**  * Returns original file name  *  * @return original file name  */ ;/**  * Returns original file name  *  * @return original file name  */ public String physicalName() {     return metadata.name(). }
true;public;0;3;/**  * File length  *  * @return file length  */ ;/**  * File length  *  * @return file length  */ public long length() {     return metadata.length(). }
true;public;0;3;/**  * Returns part size  *  * @return part size  */ ;/**  * Returns part size  *  * @return part size  */ public ByteSizeValue partSize() {     return partSize. }
true;public;1;11;/**  * Returns the size (in bytes) of a given part  *  * @return the size (in bytes) of a given part  */ ;/**  * Returns the size (in bytes) of a given part  *  * @return the size (in bytes) of a given part  */ public long partBytes(int part) {     if (numberOfParts == 1) {         return length().     }     // First and last-but-one parts have a size equal to partBytes     if (part < (numberOfParts - 1)) {         return partBytes.     }     // Last part size is deducted from the length and the number of parts     return length() - (partBytes * (numberOfParts - 1)). }
true;public;0;3;/**  * Returns number of parts  *  * @return number of parts  */ ;/**  * Returns number of parts  *  * @return number of parts  */ public long numberOfParts() {     return numberOfParts. }
true;public;0;3;/**  * Returns file md5 checksum provided by {@link org.elasticsearch.index.store.Store}  *  * @return file checksum  */ ;/**  * Returns file md5 checksum provided by {@link org.elasticsearch.index.store.Store}  *  * @return file checksum  */ public String checksum() {     return metadata.checksum(). }
true;public;0;3;/**  * Returns the StoreFileMetaData for this file info.  */ ;/**  * Returns the StoreFileMetaData for this file info.  */ public StoreFileMetaData metadata() {     return metadata. }
true;public;1;3;/**  * Checks if a file in a store is the same file  *  * @param md file in a store  * @return true if file in a store this this file have the same checksum and length  */ ;/**  * Checks if a file in a store is the same file  *  * @param md file in a store  * @return true if file in a store this this file have the same checksum and length  */ public boolean isSame(StoreFileMetaData md) {     return metadata.isSame(md). }
true;public;1;21;/**  * Checks if a file in a store is the same file  *  * @param fileInfo file in a store  * @return true if file in a store this this file have the same checksum and length  */ ;/**  * Checks if a file in a store is the same file  *  * @param fileInfo file in a store  * @return true if file in a store this this file have the same checksum and length  */ public boolean isSame(FileInfo fileInfo) {     if (numberOfParts != fileInfo.numberOfParts) {         return false.     }     if (partBytes != fileInfo.partBytes) {         return false.     }     if (!name.equals(fileInfo.name)) {         return false.     }     if (partSize != null) {         if (!partSize.equals(fileInfo.partSize)) {             return false.         }     } else {         if (fileInfo.partSize != null) {             return false.         }     }     return metadata.isSame(fileInfo.metadata). }
true;public;0;3;/**  * Checks if the checksum for the file is unknown. This only is possible on an empty shard's  * segments_N file which was created in older Lucene versions.  */ ;/**  * Checks if the checksum for the file is unknown. This only is possible on an empty shard's  * segments_N file which was created in older Lucene versions.  */ public boolean hasUnknownChecksum() {     return metadata.checksum().equals(UNKNOWN_CHECKSUM). }
true;public,static;3;22;/**  * Serializes file info into JSON  *  * @param file    file info  * @param builder XContent builder  * @param params  parameters  */ ;/**  * Serializes file info into JSON  *  * @param file    file info  * @param builder XContent builder  * @param params  parameters  */ public static void toXContent(FileInfo file, XContentBuilder builder, ToXContent.Params params) throws IOException {     builder.startObject().     builder.field(NAME, file.name).     builder.field(PHYSICAL_NAME, file.metadata.name()).     builder.field(LENGTH, file.metadata.length()).     if (file.metadata.checksum().equals(UNKNOWN_CHECKSUM) == false) {         builder.field(CHECKSUM, file.metadata.checksum()).     }     if (file.partSize != null) {         builder.field(PART_SIZE, file.partSize.getBytes()).     }     if (file.metadata.writtenBy() != null) {         builder.field(WRITTEN_BY, file.metadata.writtenBy()).     }     if (file.metadata.hash() != null && file.metadata().hash().length > 0) {         BytesRef br = file.metadata.hash().         builder.field(META_HASH, br.bytes, br.offset, br.length).     }     builder.endObject(). }
true;public,static;1;59;/**  * Parses JSON that represents file info  *  * @param parser parser  * @return file info  */ ;/**  * Parses JSON that represents file info  *  * @param parser parser  * @return file info  */ public static FileInfo fromXContent(XContentParser parser) throws IOException {     XContentParser.Token token = parser.currentToken().     String name = null.     String physicalName = null.     long length = -1.     String checksum = null.     ByteSizeValue partSize = null.     Version writtenBy = null.     String writtenByStr = null.     BytesRef metaHash = new BytesRef().     if (token == XContentParser.Token.START_OBJECT) {         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {             if (token == XContentParser.Token.FIELD_NAME) {                 String currentFieldName = parser.currentName().                 token = parser.nextToken().                 if (token.isValue()) {                     if (NAME.equals(currentFieldName)) {                         name = parser.text().                     } else if (PHYSICAL_NAME.equals(currentFieldName)) {                         physicalName = parser.text().                     } else if (LENGTH.equals(currentFieldName)) {                         length = parser.longValue().                     } else if (CHECKSUM.equals(currentFieldName)) {                         checksum = parser.text().                     } else if (PART_SIZE.equals(currentFieldName)) {                         partSize = new ByteSizeValue(parser.longValue()).                     } else if (WRITTEN_BY.equals(currentFieldName)) {                         writtenByStr = parser.text().                         writtenBy = Lucene.parseVersionLenient(writtenByStr, null).                     } else if (META_HASH.equals(currentFieldName)) {                         metaHash.bytes = parser.binaryValue().                         metaHash.offset = 0.                         metaHash.length = metaHash.bytes.length.                     } else {                         throw new ElasticsearchParseException("unknown parameter [{}]", currentFieldName).                     }                 } else {                     throw new ElasticsearchParseException("unexpected token  [{}]", token).                 }             } else {                 throw new ElasticsearchParseException("unexpected token [{}]", token).             }         }     }     // Verify that file information is complete     if (name == null || Strings.validFileName(name) == false) {         throw new ElasticsearchParseException("missing or invalid file name [" + name + "]").     } else if (physicalName == null || Strings.validFileName(physicalName) == false) {         throw new ElasticsearchParseException("missing or invalid physical file name [" + physicalName + "]").     } else if (length < 0) {         throw new ElasticsearchParseException("missing or invalid file length").     } else if (writtenBy == null) {         throw new ElasticsearchParseException("missing or invalid written_by [" + writtenByStr + "]").     } else if (checksum == null) {         throw new ElasticsearchParseException("missing checksum for name [" + name + "]").     }     return new FileInfo(name, new StoreFileMetaData(physicalName, length, checksum, writtenBy, metaHash), partSize). }
false;public;0;8;;@Override public String toString() {     return "[name: " + name + ", numberOfParts: " + numberOfParts + ", partSize: " + partSize + ", partBytes: " + partBytes + ", metadata: " + metadata + "]". }
true;public;0;3;/**  * Returns index version  *  * @return index version  */ ;/**  * Returns index version  *  * @return index version  */ public long indexVersion() {     return indexVersion. }
true;public;0;3;/**  * Returns snapshot id  *  * @return snapshot id  */ ;/**  * Returns snapshot id  *  * @return snapshot id  */ public String snapshot() {     return snapshot. }
true;public;0;3;/**  * Returns list of files in the shard  *  * @return list of files  */ ;/**  * Returns list of files in the shard  *  * @return list of files  */ public List<FileInfo> indexFiles() {     return indexFiles. }
true;public;0;3;/**  * Returns snapshot start time  */ ;/**  * Returns snapshot start time  */ public long startTime() {     return startTime. }
true;public;0;3;/**  * Returns snapshot running time  */ ;/**  * Returns snapshot running time  */ public long time() {     return time. }
true;public;0;3;/**  * Returns incremental of files that were snapshotted  */ ;/**  * Returns incremental of files that were snapshotted  */ public int incrementalFileCount() {     return incrementalFileCount. }
true;public;0;3;/**  * Returns total number of files that are referenced by this snapshot  */ ;/**  * Returns total number of files that are referenced by this snapshot  */ public int totalFileCount() {     return indexFiles.size(). }
true;public;0;3;/**  * Returns incremental of files size that were snapshotted  */ ;/**  * Returns incremental of files size that were snapshotted  */ public long incrementalSize() {     return incrementalSize. }
true;public;0;3;/**  * Returns total size of all files that where snapshotted  */ ;/**  * Returns total size of all files that where snapshotted  */ public long totalSize() {     return indexFiles.stream().mapToLong(fi -> fi.metadata().length()).sum(). }
true;public;2;15;/**  * Serializes shard snapshot metadata info into JSON  *  * @param builder  XContent builder  * @param params   parameters  */ ;/**  * Serializes shard snapshot metadata info into JSON  *  * @param builder  XContent builder  * @param params   parameters  */ @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {     builder.field(NAME, snapshot).     builder.field(INDEX_VERSION, indexVersion).     builder.field(START_TIME, startTime).     builder.field(TIME, time).     builder.field(INCREMENTAL_FILE_COUNT, incrementalFileCount).     builder.field(INCREMENTAL_SIZE, incrementalSize).     builder.startArray(FILES).     for (FileInfo fileInfo : indexFiles) {         FileInfo.toXContent(fileInfo, builder, params).     }     builder.endArray().     return builder. }
true;public,static;1;55;/**  * Parses shard snapshot metadata  *  * @param parser parser  * @return shard snapshot metadata  */ ;/**  * Parses shard snapshot metadata  *  * @param parser parser  * @return shard snapshot metadata  */ public static BlobStoreIndexShardSnapshot fromXContent(XContentParser parser) throws IOException {     String snapshot = null.     long indexVersion = -1.     long startTime = 0.     long time = 0.     int incrementalFileCount = 0.     long incrementalSize = 0.     List<FileInfo> indexFiles = new ArrayList<>().     if (parser.currentToken() == null) {         // fresh parser? move to the first token         parser.nextToken().     }     XContentParser.Token token = parser.currentToken().     if (token == XContentParser.Token.START_OBJECT) {         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {             if (token == XContentParser.Token.FIELD_NAME) {                 String currentFieldName = parser.currentName().                 token = parser.nextToken().                 if (token.isValue()) {                     if (PARSE_NAME.match(currentFieldName, parser.getDeprecationHandler())) {                         snapshot = parser.text().                     } else if (PARSE_INDEX_VERSION.match(currentFieldName, parser.getDeprecationHandler())) {                         // The index-version is needed for backward compatibility with v 1.0                         indexVersion = parser.longValue().                     } else if (PARSE_START_TIME.match(currentFieldName, parser.getDeprecationHandler())) {                         startTime = parser.longValue().                     } else if (PARSE_TIME.match(currentFieldName, parser.getDeprecationHandler())) {                         time = parser.longValue().                     } else if (PARSE_INCREMENTAL_FILE_COUNT.match(currentFieldName, parser.getDeprecationHandler())) {                         incrementalFileCount = parser.intValue().                     } else if (PARSE_INCREMENTAL_SIZE.match(currentFieldName, parser.getDeprecationHandler())) {                         incrementalSize = parser.longValue().                     } else {                         throw new ElasticsearchParseException("unknown parameter [{}]", currentFieldName).                     }                 } else if (token == XContentParser.Token.START_ARRAY) {                     if (PARSE_FILES.match(currentFieldName, parser.getDeprecationHandler())) {                         while ((parser.nextToken()) != XContentParser.Token.END_ARRAY) {                             indexFiles.add(FileInfo.fromXContent(parser)).                         }                     } else {                         throw new ElasticsearchParseException("unknown parameter [{}]", currentFieldName).                     }                 } else {                     throw new ElasticsearchParseException("unexpected token  [{}]", token).                 }             } else {                 throw new ElasticsearchParseException("unexpected token [{}]", token).             }         }     }     return new BlobStoreIndexShardSnapshot(snapshot, indexVersion, Collections.unmodifiableList(indexFiles), startTime, time, incrementalFileCount, incrementalSize). }
