commented;modifiers;parameterAmount;loc;comment;code
false;public;1;5;;@Override public void reset(TokenStream stream) {     super.reset(stream).     typeAttribute = stream.addAttribute(TypeAttribute.class). }
false;public;0;21;;@Override public void nextToken() throws IOException {     anyTokens = true.     BytesRef term = fillBytesRef(termsRef).     if (requireUnigram && typeAttribute.type() == ShingleFilter.DEFAULT_TOKEN_TYPE) {         return.     }     anyUnigram = true.     if (posIncAttr.getPositionIncrement() == 0 && typeAttribute.type() == SynonymFilter.TYPE_SYNONYM) {         assert currentSet != null.         TermStats termStats = generator.termStats(term).         if (termStats.docFreq > 0) {             currentSet.addOneCandidate(generator.createCandidate(BytesRef.deepCopyOf(term), termStats, realWordLikelihood)).         }     } else {         if (currentSet != null) {             candidateSetsList.add(currentSet).         }         currentSet = new CandidateSet(Candidate.EMPTY, generator.createCandidate(BytesRef.deepCopyOf(term), true)).     } }
false;public;0;9;;@Override public void end() {     if (currentSet != null) {         candidateSetsList.add(currentSet).     }     if (requireUnigram && !anyUnigram && anyTokens) {         throw new IllegalStateException("At least one unigram is required but all tokens were ngrams").     } }
false;public;7;71;;public Result getCorrections(TokenStream stream, final CandidateGenerator generator, float maxErrors, int numCorrections, WordScorer wordScorer, float confidence, int gramSize) throws IOException {     final List<CandidateSet> candidateSetsList = new ArrayList<>().     DirectCandidateGenerator.analyze(stream, new DirectCandidateGenerator.TokenConsumer() {          CandidateSet currentSet = null.          private TypeAttribute typeAttribute.          private final BytesRefBuilder termsRef = new BytesRefBuilder().          private boolean anyUnigram = false.          private boolean anyTokens = false.          @Override         public void reset(TokenStream stream) {             super.reset(stream).             typeAttribute = stream.addAttribute(TypeAttribute.class).         }          @Override         public void nextToken() throws IOException {             anyTokens = true.             BytesRef term = fillBytesRef(termsRef).             if (requireUnigram && typeAttribute.type() == ShingleFilter.DEFAULT_TOKEN_TYPE) {                 return.             }             anyUnigram = true.             if (posIncAttr.getPositionIncrement() == 0 && typeAttribute.type() == SynonymFilter.TYPE_SYNONYM) {                 assert currentSet != null.                 TermStats termStats = generator.termStats(term).                 if (termStats.docFreq > 0) {                     currentSet.addOneCandidate(generator.createCandidate(BytesRef.deepCopyOf(term), termStats, realWordLikelihood)).                 }             } else {                 if (currentSet != null) {                     candidateSetsList.add(currentSet).                 }                 currentSet = new CandidateSet(Candidate.EMPTY, generator.createCandidate(BytesRef.deepCopyOf(term), true)).             }         }          @Override         public void end() {             if (currentSet != null) {                 candidateSetsList.add(currentSet).             }             if (requireUnigram && !anyUnigram && anyTokens) {                 throw new IllegalStateException("At least one unigram is required but all tokens were ngrams").             }         }     }).     if (candidateSetsList.isEmpty() || candidateSetsList.size() >= tokenLimit) {         return Result.EMPTY.     }     for (CandidateSet candidateSet : candidateSetsList) {         generator.drawCandidates(candidateSet).     }     double cutoffScore = Double.MIN_VALUE.     CandidateScorer scorer = new CandidateScorer(wordScorer, numCorrections, gramSize).     CandidateSet[] candidateSets = candidateSetsList.toArray(new CandidateSet[candidateSetsList.size()]).     if (confidence > 0.0) {         Candidate[] candidates = new Candidate[candidateSets.length].         for (int i = 0. i < candidates.length. i++) {             candidates[i] = candidateSets[i].originalTerm.         }         double inputPhraseScore = scorer.score(candidates, candidateSets).         cutoffScore = inputPhraseScore * confidence.     }     Correction[] bestCandidates = scorer.findBestCandiates(candidateSets, maxErrors, cutoffScore).     return new Result(bestCandidates, cutoffScore). }
false;public;10;8;;public Result getCorrections(Analyzer analyzer, BytesRef query, CandidateGenerator generator, float maxErrors, int numCorrections, IndexReader reader, String analysisField, WordScorer scorer, float confidence, int gramSize) throws IOException {     return getCorrections(tokenStream(analyzer, query, new CharsRefBuilder(), analysisField), generator, maxErrors, numCorrections, scorer, confidence, gramSize). }
false;public;4;4;;public TokenStream tokenStream(Analyzer analyzer, BytesRef query, CharsRefBuilder spare, String field) throws IOException {     spare.copyUTF8Bytes(query).     return analyzer.tokenStream(field, new CharArrayReader(spare.chars(), 0, spare.length())). }
