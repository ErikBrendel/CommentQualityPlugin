commented;modifiers;parameterAmount;loc;comment;code
false;public;2;5;;@Override public void collect(int doc, long bucket) throws IOException {     super.collect(doc, bucket).     numCollectedDocs++. }
false;public;2;11;;@Override public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, final LeafBucketCollector sub) throws IOException {     return new LeafBucketCollectorBase(super.getLeafCollector(ctx, sub), null) {          @Override         public void collect(int doc, long bucket) throws IOException {             super.collect(doc, bucket).             numCollectedDocs++.         }     }. }
false;public;1;44;;@Override public SignificantLongTerms buildAggregation(long owningBucketOrdinal) throws IOException {     assert owningBucketOrdinal == 0.     final int size = (int) Math.min(bucketOrds.size(), bucketCountThresholds.getShardSize()).     long supersetSize = termsAggFactory.getSupersetNumDocs().     long subsetSize = numCollectedDocs.     BucketSignificancePriorityQueue<SignificantLongTerms.Bucket> ordered = new BucketSignificancePriorityQueue<>(size).     SignificantLongTerms.Bucket spare = null.     for (long i = 0. i < bucketOrds.size(). i++) {         final int docCount = bucketDocCount(i).         if (docCount < bucketCountThresholds.getShardMinDocCount()) {             continue.         }         if (spare == null) {             spare = new SignificantLongTerms.Bucket(0, 0, 0, 0, 0, null, format).         }         spare.term = bucketOrds.get(i).         spare.subsetDf = docCount.         spare.subsetSize = subsetSize.         spare.supersetDf = termsAggFactory.getBackgroundFrequency(spare.term).         spare.supersetSize = supersetSize.         // During shard-local down-selection we use subset/superset stats that are for this shard only         // Back at the central reducer these properties will be updated with global stats         spare.updateScore(significanceHeuristic).         spare.bucketOrd = i.         spare = ordered.insertWithOverflow(spare).         if (spare == null) {             consumeBucketsAndMaybeBreak(1).         }     }     final SignificantLongTerms.Bucket[] list = new SignificantLongTerms.Bucket[ordered.size()].     for (int i = ordered.size() - 1. i >= 0. i--) {         final SignificantLongTerms.Bucket bucket = ordered.pop().         bucket.aggregations = bucketAggregations(bucket.bucketOrd).         list[i] = bucket.     }     return new SignificantLongTerms(name, bucketCountThresholds.getRequiredSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators(), metaData(), format, subsetSize, supersetSize, significanceHeuristic, Arrays.asList(list)). }
false;public;0;9;;@Override public SignificantLongTerms buildEmptyAggregation() {     // We need to account for the significance of a miss in our global stats - provide corpus size as context     ContextIndexSearcher searcher = context.searcher().     IndexReader topReader = searcher.getIndexReader().     int supersetSize = topReader.numDocs().     return new SignificantLongTerms(name, bucketCountThresholds.getRequiredSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators(), metaData(), format, 0, supersetSize, significanceHeuristic, emptyList()). }
false;public;0;4;;@Override public void doClose() {     Releasables.close(bucketOrds, termsAggFactory). }
