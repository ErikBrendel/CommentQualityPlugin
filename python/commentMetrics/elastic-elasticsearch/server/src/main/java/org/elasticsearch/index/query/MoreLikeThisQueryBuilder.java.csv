commented;modifiers;parameterAmount;loc;comment;code
false;public;1;17;;@Override public void writeTo(StreamOutput out) throws IOException {     out.writeOptionalString(index).     out.writeOptionalString(type).     out.writeBoolean(doc != null).     if (doc != null) {         out.writeGenericValue(doc).         out.writeEnum(xContentType).     } else {         out.writeString(id).     }     out.writeOptionalStringArray(fields).     out.writeGenericValue(perFieldAnalyzer).     out.writeOptionalString(routing).     out.writeLong(version).     versionType.writeTo(out). }
false;public;0;3;;public String index() {     return index. }
false;public;1;4;;public Item index(String index) {     this.index = index.     return this. }
true;public;0;4;/**  * @deprecated Types are in the process of being removed.  */ ;/**  * @deprecated Types are in the process of being removed.  */ @Deprecated public String type() {     return type. }
true;public;1;5;/**  * @deprecated Types are in the process of being removed.  */ ;/**  * @deprecated Types are in the process of being removed.  */ @Deprecated public Item type(String type) {     this.type = type.     return this. }
false;public;0;3;;public String id() {     return id. }
false;public;0;3;;public BytesReference doc() {     return doc. }
false;public;0;3;;public String[] fields() {     return fields. }
false;public;1;4;;public Item fields(String... fields) {     this.fields = fields.     return this. }
false;public;0;3;;public Map<String, String> perFieldAnalyzer() {     return perFieldAnalyzer. }
true;public;1;4;/**  * Sets the analyzer(s) to use at any given field.  */ ;/**  * Sets the analyzer(s) to use at any given field.  */ public Item perFieldAnalyzer(Map<String, String> perFieldAnalyzer) {     this.perFieldAnalyzer = perFieldAnalyzer.     return this. }
false;public;0;3;;public String routing() {     return routing. }
false;public;1;4;;public Item routing(String routing) {     this.routing = routing.     return this. }
false;public;0;3;;public long version() {     return version. }
false;public;1;4;;public Item version(long version) {     this.version = version.     return this. }
false;public;0;3;;public VersionType versionType() {     return versionType. }
false;public;1;4;;public Item versionType(VersionType versionType) {     this.versionType = versionType.     return this. }
false;;0;3;;XContentType xContentType() {     return xContentType. }
true;;0;19;/**  * Convert this to a {@link TermVectorsRequest} for fetching the terms of the document.  */ ;/**  * Convert this to a {@link TermVectorsRequest} for fetching the terms of the document.  */ TermVectorsRequest toTermVectorsRequest() {     TermVectorsRequest termVectorsRequest = new TermVectorsRequest(index, type, id).selectedFields(fields).routing(routing).version(version).versionType(versionType).perFieldAnalyzer(perFieldAnalyzer).positions(// ensures these following parameters are never set     false).offsets(false).payloads(false).fieldStatistics(false).termStatistics(false).     // for artificial docs to make sure that the id has changed in the item too     if (doc != null) {         termVectorsRequest.doc(doc, true, xContentType).         this.id = termVectorsRequest.id().     }     return termVectorsRequest. }
true;public,static;2;51;/**  * Parses and returns the given item.  */ ;/**  * Parses and returns the given item.  */ public static Item parse(XContentParser parser, Item item) throws IOException {     XContentParser.Token token.     String currentFieldName = null.     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {         if (token == XContentParser.Token.FIELD_NAME) {             currentFieldName = parser.currentName().         } else if (currentFieldName != null) {             if (INDEX.match(currentFieldName, parser.getDeprecationHandler())) {                 item.index = parser.text().             } else if (TYPE.match(currentFieldName, parser.getDeprecationHandler())) {                 item.type = parser.text().             } else if (ID.match(currentFieldName, parser.getDeprecationHandler())) {                 item.id = parser.text().             } else if (DOC.match(currentFieldName, parser.getDeprecationHandler())) {                 item.doc = BytesReference.bytes(jsonBuilder().copyCurrentStructure(parser)).                 item.xContentType = XContentType.JSON.             } else if (FIELDS.match(currentFieldName, parser.getDeprecationHandler())) {                 if (token == XContentParser.Token.START_ARRAY) {                     List<String> fields = new ArrayList<>().                     while (parser.nextToken() != XContentParser.Token.END_ARRAY) {                         fields.add(parser.text()).                     }                     item.fields(fields.toArray(new String[fields.size()])).                 } else {                     throw new ElasticsearchParseException("failed to parse More Like This item. field [fields] must be an array").                 }             } else if (PER_FIELD_ANALYZER.match(currentFieldName, parser.getDeprecationHandler())) {                 item.perFieldAnalyzer(TermVectorsRequest.readPerFieldAnalyzer(parser.map())).             } else if (ROUTING.match(currentFieldName, parser.getDeprecationHandler())) {                 item.routing = parser.text().             } else if (VERSION.match(currentFieldName, parser.getDeprecationHandler())) {                 item.version = parser.longValue().             } else if (VERSION_TYPE.match(currentFieldName, parser.getDeprecationHandler())) {                 item.versionType = VersionType.fromString(parser.text()).             } else {                 throw new ElasticsearchParseException("failed to parse More Like This item. unknown field [{}]", currentFieldName).             }         }     }     if (item.id != null && item.doc != null) {         throw new ElasticsearchParseException("failed to parse More Like This item. either [id] or [doc] can be specified, but not both!").     }     if (item.id == null && item.doc == null) {         throw new ElasticsearchParseException("failed to parse More Like This item. neither [id] nor [doc] is specified!").     }     return item. }
false;public;2;34;;@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {     builder.startObject().     if (this.index != null) {         builder.field(INDEX.getPreferredName(), this.index).     }     if (this.type != null) {         builder.field(TYPE.getPreferredName(), this.type).     }     if (this.id != null) {         builder.field(ID.getPreferredName(), this.id).     }     if (this.doc != null) {         try (InputStream stream = this.doc.streamInput()) {             builder.rawField(DOC.getPreferredName(), stream, xContentType).         }     }     if (this.fields != null) {         builder.array(FIELDS.getPreferredName(), this.fields).     }     if (this.perFieldAnalyzer != null) {         builder.field(PER_FIELD_ANALYZER.getPreferredName(), this.perFieldAnalyzer).     }     if (this.routing != null) {         builder.field(ROUTING.getPreferredName(), this.routing).     }     if (this.version != Versions.MATCH_ANY) {         builder.field(VERSION.getPreferredName(), this.version).     }     if (this.versionType != VersionType.INTERNAL) {         builder.field(VERSION_TYPE.getPreferredName(), this.versionType.toString().toLowerCase(Locale.ROOT)).     }     return builder.endObject(). }
false;public;0;11;;@Override public String toString() {     try {         XContentBuilder builder = XContentFactory.jsonBuilder().         builder.prettyPrint().         toXContent(builder, EMPTY_PARAMS).         return Strings.toString(builder).     } catch (Exception e) {         return "{ \"error\" : \"" + ExceptionsHelper.detailedMessage(e) + "\"}".     } }
false;public;0;5;;@Override public int hashCode() {     return Objects.hash(index, type, id, doc, Arrays.hashCode(fields), perFieldAnalyzer, routing, version, versionType). }
false;public;1;15;;@Override public boolean equals(Object o) {     if (this == o)         return true.     if (!(o instanceof Item))         return false.     Item other = (Item) o.     return Objects.equals(index, other.index) && Objects.equals(type, other.type) && Objects.equals(id, other.id) && Objects.equals(doc, other.doc) && // otherwise we are comparing pointers     Arrays.equals(fields, other.fields) && Objects.equals(perFieldAnalyzer, other.perFieldAnalyzer) && Objects.equals(routing, other.routing) && Objects.equals(version, other.version) && Objects.equals(versionType, other.versionType). }
false;protected;1;20;;@Override protected void doWriteTo(StreamOutput out) throws IOException {     out.writeOptionalStringArray(fields).     out.writeStringArray(likeTexts).     out.writeList(Arrays.asList(likeItems)).     out.writeStringArray(unlikeTexts).     out.writeList(Arrays.asList(unlikeItems)).     out.writeVInt(maxQueryTerms).     out.writeVInt(minTermFreq).     out.writeVInt(minDocFreq).     out.writeVInt(maxDocFreq).     out.writeVInt(minWordLength).     out.writeVInt(maxWordLength).     out.writeOptionalStringArray(stopWords).     out.writeOptionalString(analyzer).     out.writeString(minimumShouldMatch).     out.writeGenericValue(boostTerms).     out.writeBoolean(include).     out.writeBoolean(failOnUnsupportedField). }
false;public;0;3;;public String[] fields() {     return this.fields. }
false;public;0;3;;public String[] likeTexts() {     return likeTexts. }
false;public;0;3;;public Item[] likeItems() {     return likeItems. }
true;public;1;4;/**  * Sets the text from which the terms should not be selected from.  */ ;/**  * Sets the text from which the terms should not be selected from.  */ public MoreLikeThisQueryBuilder unlike(String[] unlikeTexts) {     this.unlikeTexts = Optional.ofNullable(unlikeTexts).orElse(Strings.EMPTY_ARRAY).     return this. }
false;public;0;3;;public String[] unlikeTexts() {     return unlikeTexts. }
true;public;1;4;/**  * Sets the documents from which the terms should not be selected from.  */ ;/**  * Sets the documents from which the terms should not be selected from.  */ public MoreLikeThisQueryBuilder unlike(Item[] unlikeItems) {     this.unlikeItems = Optional.ofNullable(unlikeItems).orElse(new Item[0]).     return this. }
false;public;0;3;;public Item[] unlikeItems() {     return unlikeItems. }
true;public;1;4;/**  * Sets the maximum number of query terms that will be included in any generated query.  * Defaults to {@code 25}.  */ ;/**  * Sets the maximum number of query terms that will be included in any generated query.  * Defaults to {@code 25}.  */ public MoreLikeThisQueryBuilder maxQueryTerms(int maxQueryTerms) {     this.maxQueryTerms = maxQueryTerms.     return this. }
false;public;0;3;;public int maxQueryTerms() {     return maxQueryTerms. }
true;public;1;4;/**  * The frequency below which terms will be ignored in the source doc. The default  * frequency is {@code 2}.  */ ;/**  * The frequency below which terms will be ignored in the source doc. The default  * frequency is {@code 2}.  */ public MoreLikeThisQueryBuilder minTermFreq(int minTermFreq) {     this.minTermFreq = minTermFreq.     return this. }
false;public;0;3;;public int minTermFreq() {     return minTermFreq. }
true;public;1;4;/**  * Sets the frequency at which words will be ignored which do not occur in at least this  * many docs. Defaults to {@code 5}.  */ ;/**  * Sets the frequency at which words will be ignored which do not occur in at least this  * many docs. Defaults to {@code 5}.  */ public MoreLikeThisQueryBuilder minDocFreq(int minDocFreq) {     this.minDocFreq = minDocFreq.     return this. }
false;public;0;3;;public int minDocFreq() {     return minDocFreq. }
true;public;1;4;/**  * Set the maximum frequency in which words may still appear. Words that appear  * in more than this many docs will be ignored. Defaults to unbounded.  */ ;/**  * Set the maximum frequency in which words may still appear. Words that appear  * in more than this many docs will be ignored. Defaults to unbounded.  */ public MoreLikeThisQueryBuilder maxDocFreq(int maxDocFreq) {     this.maxDocFreq = maxDocFreq.     return this. }
false;public;0;3;;public int maxDocFreq() {     return maxDocFreq. }
true;public;1;4;/**  * Sets the minimum word length below which words will be ignored. Defaults  * to {@code 0}.  */ ;/**  * Sets the minimum word length below which words will be ignored. Defaults  * to {@code 0}.  */ public MoreLikeThisQueryBuilder minWordLength(int minWordLength) {     this.minWordLength = minWordLength.     return this. }
false;public;0;3;;public int minWordLength() {     return minWordLength. }
true;public;1;4;/**  * Sets the maximum word length above which words will be ignored. Defaults to  * unbounded ({@code 0}).  */ ;/**  * Sets the maximum word length above which words will be ignored. Defaults to  * unbounded ({@code 0}).  */ public MoreLikeThisQueryBuilder maxWordLength(int maxWordLength) {     this.maxWordLength = maxWordLength.     return this. }
false;public;0;3;;public int maxWordLength() {     return maxWordLength. }
true;public;1;4;/**  * Set the set of stopwords.  * <p>  * Any word in this set is considered "uninteresting" and ignored. Even if your Analyzer allows stopwords, you  * might want to tell the MoreLikeThis code to ignore them, as for the purposes of document similarity it seems  * reasonable to assume that "a stop word is never interesting".  */ ;/**  * Set the set of stopwords.  * <p>  * Any word in this set is considered "uninteresting" and ignored. Even if your Analyzer allows stopwords, you  * might want to tell the MoreLikeThis code to ignore them, as for the purposes of document similarity it seems  * reasonable to assume that "a stop word is never interesting".  */ public MoreLikeThisQueryBuilder stopWords(String... stopWords) {     this.stopWords = stopWords.     return this. }
false;public;1;7;;public MoreLikeThisQueryBuilder stopWords(List<String> stopWords) {     if (stopWords == null) {         throw new IllegalArgumentException("requires stopwords to be non-null").     }     this.stopWords = stopWords.toArray(new String[stopWords.size()]).     return this. }
false;public;0;3;;public String[] stopWords() {     return stopWords. }
true;public;1;4;/**  * The analyzer that will be used to analyze the text. Defaults to the analyzer associated with the field.  */ ;/**  * The analyzer that will be used to analyze the text. Defaults to the analyzer associated with the field.  */ public MoreLikeThisQueryBuilder analyzer(String analyzer) {     this.analyzer = analyzer.     return this. }
false;public;0;3;;public String analyzer() {     return analyzer. }
true;public;1;7;/**  * Number of terms that must match the generated query expressed in the  * common syntax for minimum should match. Defaults to {@code 30%}.  *  * @see    org.elasticsearch.common.lucene.search.Queries#calculateMinShouldMatch(int, String)  */ ;/**  * Number of terms that must match the generated query expressed in the  * common syntax for minimum should match. Defaults to {@code 30%}.  *  * @see    org.elasticsearch.common.lucene.search.Queries#calculateMinShouldMatch(int, String)  */ public MoreLikeThisQueryBuilder minimumShouldMatch(String minimumShouldMatch) {     if (minimumShouldMatch == null) {         throw new IllegalArgumentException("[" + NAME + "] requires minimum should match to be non-null").     }     this.minimumShouldMatch = minimumShouldMatch.     return this. }
false;public;0;3;;public String minimumShouldMatch() {     return minimumShouldMatch. }
true;public;1;4;/**  * Sets the boost factor to use when boosting terms. Defaults to {@code 0} (deactivated).  */ ;/**  * Sets the boost factor to use when boosting terms. Defaults to {@code 0} (deactivated).  */ public MoreLikeThisQueryBuilder boostTerms(float boostTerms) {     this.boostTerms = boostTerms.     return this. }
false;public;0;3;;public float boostTerms() {     return boostTerms. }
true;public;1;4;/**  * Whether to include the input documents. Defaults to {@code false}  */ ;/**  * Whether to include the input documents. Defaults to {@code false}  */ public MoreLikeThisQueryBuilder include(boolean include) {     this.include = include.     return this. }
false;public;0;3;;public boolean include() {     return include. }
true;public;1;4;/**  * Whether to fail or return no result when this query is run against a field which is not supported such as binary/numeric fields.  */ ;/**  * Whether to fail or return no result when this query is run against a field which is not supported such as binary/numeric fields.  */ public MoreLikeThisQueryBuilder failOnUnsupportedField(boolean fail) {     this.failOnUnsupportedField = fail.     return this. }
false;public;0;3;;public boolean failOnUnsupportedField() {     return failOnUnsupportedField. }
false;protected;2;27;;@Override protected void doXContent(XContentBuilder builder, Params params) throws IOException {     builder.startObject(NAME).     if (fields != null) {         builder.array(FIELDS.getPreferredName(), fields).     }     buildLikeField(builder, LIKE.getPreferredName(), likeTexts, likeItems).     buildLikeField(builder, UNLIKE.getPreferredName(), unlikeTexts, unlikeItems).     builder.field(MAX_QUERY_TERMS.getPreferredName(), maxQueryTerms).     builder.field(MIN_TERM_FREQ.getPreferredName(), minTermFreq).     builder.field(MIN_DOC_FREQ.getPreferredName(), minDocFreq).     builder.field(MAX_DOC_FREQ.getPreferredName(), maxDocFreq).     builder.field(MIN_WORD_LENGTH.getPreferredName(), minWordLength).     builder.field(MAX_WORD_LENGTH.getPreferredName(), maxWordLength).     if (stopWords != null) {         builder.array(STOP_WORDS.getPreferredName(), stopWords).     }     if (analyzer != null) {         builder.field(ANALYZER.getPreferredName(), analyzer).     }     builder.field(MINIMUM_SHOULD_MATCH.getPreferredName(), minimumShouldMatch).     builder.field(BOOST_TERMS.getPreferredName(), boostTerms).     builder.field(INCLUDE.getPreferredName(), include).     builder.field(FAIL_ON_UNSUPPORTED_FIELD.getPreferredName(), failOnUnsupportedField).     printBoostAndQueryName(builder).     builder.endObject(). }
false;public,static;1;138;;public static MoreLikeThisQueryBuilder fromXContent(XContentParser parser) throws IOException {     // document inputs     List<String> fields = null.     List<String> likeTexts = new ArrayList<>().     List<String> unlikeTexts = new ArrayList<>().     List<Item> likeItems = new ArrayList<>().     List<Item> unlikeItems = new ArrayList<>().     // term selection parameters     int maxQueryTerms = MoreLikeThisQueryBuilder.DEFAULT_MAX_QUERY_TERMS.     int minTermFreq = MoreLikeThisQueryBuilder.DEFAULT_MIN_TERM_FREQ.     int minDocFreq = MoreLikeThisQueryBuilder.DEFAULT_MIN_DOC_FREQ.     int maxDocFreq = MoreLikeThisQueryBuilder.DEFAULT_MAX_DOC_FREQ.     int minWordLength = MoreLikeThisQueryBuilder.DEFAULT_MIN_WORD_LENGTH.     int maxWordLength = MoreLikeThisQueryBuilder.DEFAULT_MAX_WORD_LENGTH.     List<String> stopWords = null.     String analyzer = null.     // query formation parameters     String minimumShouldMatch = MoreLikeThisQueryBuilder.DEFAULT_MINIMUM_SHOULD_MATCH.     float boostTerms = MoreLikeThisQueryBuilder.DEFAULT_BOOST_TERMS.     boolean include = MoreLikeThisQueryBuilder.DEFAULT_INCLUDE.     // other parameters     boolean failOnUnsupportedField = MoreLikeThisQueryBuilder.DEFAULT_FAIL_ON_UNSUPPORTED_FIELDS.     float boost = AbstractQueryBuilder.DEFAULT_BOOST.     String queryName = null.     XContentParser.Token token.     String currentFieldName = null.     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {         if (token == XContentParser.Token.FIELD_NAME) {             currentFieldName = parser.currentName().         } else if (token.isValue()) {             if (LIKE.match(currentFieldName, parser.getDeprecationHandler())) {                 parseLikeField(parser, likeTexts, likeItems).             } else if (UNLIKE.match(currentFieldName, parser.getDeprecationHandler())) {                 parseLikeField(parser, unlikeTexts, unlikeItems).             } else if (MAX_QUERY_TERMS.match(currentFieldName, parser.getDeprecationHandler())) {                 maxQueryTerms = parser.intValue().             } else if (MIN_TERM_FREQ.match(currentFieldName, parser.getDeprecationHandler())) {                 minTermFreq = parser.intValue().             } else if (MIN_DOC_FREQ.match(currentFieldName, parser.getDeprecationHandler())) {                 minDocFreq = parser.intValue().             } else if (MAX_DOC_FREQ.match(currentFieldName, parser.getDeprecationHandler())) {                 maxDocFreq = parser.intValue().             } else if (MIN_WORD_LENGTH.match(currentFieldName, parser.getDeprecationHandler())) {                 minWordLength = parser.intValue().             } else if (MAX_WORD_LENGTH.match(currentFieldName, parser.getDeprecationHandler())) {                 maxWordLength = parser.intValue().             } else if (ANALYZER.match(currentFieldName, parser.getDeprecationHandler())) {                 analyzer = parser.text().             } else if (MINIMUM_SHOULD_MATCH.match(currentFieldName, parser.getDeprecationHandler())) {                 minimumShouldMatch = parser.text().             } else if (BOOST_TERMS.match(currentFieldName, parser.getDeprecationHandler())) {                 boostTerms = parser.floatValue().             } else if (INCLUDE.match(currentFieldName, parser.getDeprecationHandler())) {                 include = parser.booleanValue().             } else if (FAIL_ON_UNSUPPORTED_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {                 failOnUnsupportedField = parser.booleanValue().             } else if ("boost".equals(currentFieldName)) {                 boost = parser.floatValue().             } else if ("_name".equals(currentFieldName)) {                 queryName = parser.text().             } else {                 throw new ParsingException(parser.getTokenLocation(), "[mlt] query does not support [" + currentFieldName + "]").             }         } else if (token == XContentParser.Token.START_ARRAY) {             if (FIELDS.match(currentFieldName, parser.getDeprecationHandler())) {                 fields = new ArrayList<>().                 while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {                     fields.add(parser.text()).                 }             } else if (LIKE.match(currentFieldName, parser.getDeprecationHandler())) {                 while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {                     parseLikeField(parser, likeTexts, likeItems).                 }             } else if (UNLIKE.match(currentFieldName, parser.getDeprecationHandler())) {                 while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {                     parseLikeField(parser, unlikeTexts, unlikeItems).                 }             } else if (STOP_WORDS.match(currentFieldName, parser.getDeprecationHandler())) {                 stopWords = new ArrayList<>().                 while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {                     stopWords.add(parser.text()).                 }             } else {                 throw new ParsingException(parser.getTokenLocation(), "[mlt] query does not support [" + currentFieldName + "]").             }         } else if (token == XContentParser.Token.START_OBJECT) {             if (LIKE.match(currentFieldName, parser.getDeprecationHandler())) {                 parseLikeField(parser, likeTexts, likeItems).             } else if (UNLIKE.match(currentFieldName, parser.getDeprecationHandler())) {                 parseLikeField(parser, unlikeTexts, unlikeItems).             } else {                 throw new ParsingException(parser.getTokenLocation(), "[mlt] query does not support [" + currentFieldName + "]").             }         }     }     if (likeTexts.isEmpty() && likeItems.isEmpty()) {         throw new ParsingException(parser.getTokenLocation(), "more_like_this requires 'like' to be specified").     }     if (fields != null && fields.isEmpty()) {         throw new ParsingException(parser.getTokenLocation(), "more_like_this requires 'fields' to be non-empty").     }     String[] fieldsArray = fields == null ? null : fields.toArray(new String[fields.size()]).     String[] likeTextsArray = likeTexts.isEmpty() ? null : likeTexts.toArray(new String[likeTexts.size()]).     String[] unlikeTextsArray = unlikeTexts.isEmpty() ? null : unlikeTexts.toArray(new String[unlikeTexts.size()]).     Item[] likeItemsArray = likeItems.isEmpty() ? null : likeItems.toArray(new Item[likeItems.size()]).     Item[] unlikeItemsArray = unlikeItems.isEmpty() ? null : unlikeItems.toArray(new Item[unlikeItems.size()]).     MoreLikeThisQueryBuilder moreLikeThisQueryBuilder = new MoreLikeThisQueryBuilder(fieldsArray, likeTextsArray, likeItemsArray).unlike(unlikeTextsArray).unlike(unlikeItemsArray).maxQueryTerms(maxQueryTerms).minTermFreq(minTermFreq).minDocFreq(minDocFreq).maxDocFreq(maxDocFreq).minWordLength(minWordLength).maxWordLength(maxWordLength).analyzer(analyzer).minimumShouldMatch(minimumShouldMatch).boostTerms(boostTerms).include(include).failOnUnsupportedField(failOnUnsupportedField).boost(boost).queryName(queryName).     if (stopWords != null) {         moreLikeThisQueryBuilder.stopWords(stopWords).     }     if (moreLikeThisQueryBuilder.isTypeless() == false) {         deprecationLogger.deprecatedAndMaybeLog("more_like_this_query_with_types", TYPES_DEPRECATION_MESSAGE).     }     return moreLikeThisQueryBuilder. }
false;public;0;4;;public boolean isTypeless() {     return Stream.concat(Arrays.stream(likeItems), Arrays.stream(unlikeItems)).allMatch(item -> item.type == null). }
false;private,static;3;9;;private static void parseLikeField(XContentParser parser, List<String> texts, List<Item> items) throws IOException {     if (parser.currentToken().isValue()) {         texts.add(parser.text()).     } else if (parser.currentToken() == XContentParser.Token.START_OBJECT) {         items.add(Item.parse(parser, new Item())).     } else {         throw new IllegalArgumentException("Content of 'like' parameter should either be a string or an object").     } }
false;private,static;4;12;;private static void buildLikeField(XContentBuilder builder, String fieldName, String[] texts, Item[] items) throws IOException {     if (texts.length > 0 || items.length > 0) {         builder.startArray(fieldName).         for (String text : texts) {             builder.value(text).         }         for (Item item : items) {             builder.value(item).         }         builder.endArray().     } }
false;public;0;4;;@Override public String getWriteableName() {     return NAME. }
false;protected;1;81;;@Override protected Query doToQuery(QueryShardContext context) throws IOException {     Item[] likeItems = new Item[this.likeItems.length].     for (int i = 0. i < likeItems.length. i++) {         likeItems[i] = new Item(this.likeItems[i]).     }     Item[] unlikeItems = new Item[this.unlikeItems.length].     for (int i = 0. i < unlikeItems.length. i++) {         unlikeItems[i] = new Item(this.unlikeItems[i]).     }     MoreLikeThisQuery mltQuery = new MoreLikeThisQuery().     // set similarity     mltQuery.setSimilarity(context.getSearchSimilarity()).     // set query parameters     mltQuery.setMaxQueryTerms(maxQueryTerms).     mltQuery.setMinTermFrequency(minTermFreq).     mltQuery.setMinDocFreq(minDocFreq).     mltQuery.setMaxDocFreq(maxDocFreq).     mltQuery.setMinWordLen(minWordLength).     mltQuery.setMaxWordLen(maxWordLength).     mltQuery.setMinimumShouldMatch(minimumShouldMatch).     if (stopWords != null) {         mltQuery.setStopWords(new HashSet<>(Arrays.asList(stopWords))).     }     // sets boost terms     if (boostTerms != 0) {         mltQuery.setBoostTerms(true).         mltQuery.setBoostTermsFactor(boostTerms).     }     // set analyzer     Analyzer analyzerObj = context.getIndexAnalyzers().get(analyzer).     if (analyzerObj == null) {         analyzerObj = context.getMapperService().searchAnalyzer().     }     mltQuery.setAnalyzer(analyzerObj).     // set like text fields     boolean useDefaultField = (fields == null).     List<String> moreLikeFields = new ArrayList<>().     if (useDefaultField) {         moreLikeFields = context.defaultFields().     } else {         for (String field : fields) {             MappedFieldType fieldType = context.fieldMapper(field).             if (fieldType != null && SUPPORTED_FIELD_TYPES.contains(fieldType.getClass()) == false) {                 if (failOnUnsupportedField) {                     throw new IllegalArgumentException("more_like_this only supports text/keyword fields: [" + field + "]").                 } else {                     // skip                     continue.                 }             }             moreLikeFields.add(fieldType == null ? field : fieldType.name()).         }     }     if (moreLikeFields.isEmpty()) {         return null.     }     mltQuery.setMoreLikeFields(moreLikeFields.toArray(new String[moreLikeFields.size()])).     // handle like texts     if (likeTexts.length > 0) {         mltQuery.setLikeText(likeTexts).     }     if (unlikeTexts.length > 0) {         mltQuery.setUnlikeText(unlikeTexts).     }     // handle items     if (likeItems.length > 0) {         return handleItems(context, mltQuery, likeItems, unlikeItems, include, moreLikeFields, useDefaultField).     } else {         return mltQuery.     } }
false;private;7;33;;private Query handleItems(QueryShardContext context, MoreLikeThisQuery mltQuery, Item[] likeItems, Item[] unlikeItems, boolean include, List<String> moreLikeFields, boolean useDefaultField) throws IOException {     // set default index, type and fields if not specified     for (Item item : likeItems) {         setDefaultIndexTypeFields(context, item, moreLikeFields, useDefaultField).     }     for (Item item : unlikeItems) {         setDefaultIndexTypeFields(context, item, moreLikeFields, useDefaultField).     }     // fetching the items with multi-termvectors API     MultiTermVectorsResponse likeItemsResponse = fetchResponse(context.getClient(), likeItems).     // getting the Fields for liked items     mltQuery.setLikeFields(getFieldsFor(likeItemsResponse)).     // getting the Fields for unliked items     if (unlikeItems.length > 0) {         MultiTermVectorsResponse unlikeItemsResponse = fetchResponse(context.getClient(), unlikeItems).         org.apache.lucene.index.Fields[] unlikeFields = getFieldsFor(unlikeItemsResponse).         if (unlikeFields.length > 0) {             mltQuery.setUnlikeFields(unlikeFields).         }     }     BooleanQuery.Builder boolQuery = new BooleanQuery.Builder().     boolQuery.add(mltQuery, BooleanClause.Occur.SHOULD).     // exclude the items from the search     if (!include) {         handleExclude(boolQuery, likeItems, context).     }     return boolQuery.build(). }
false;private,static;4;17;;private static void setDefaultIndexTypeFields(QueryShardContext context, Item item, List<String> moreLikeFields, boolean useDefaultField) {     if (item.index() == null) {         item.index(context.index().getName()).     }     if (item.type() == null) {         item.type(MapperService.SINGLE_MAPPING_NAME).     }     // default fields if not present but don't override for artificial docs     if ((item.fields() == null || item.fields().length == 0) && item.doc() == null) {         if (useDefaultField) {             item.fields("*").         } else {             item.fields(moreLikeFields.toArray(new String[moreLikeFields.size()])).         }     } }
false;private;2;8;;private MultiTermVectorsResponse fetchResponse(Client client, Item[] items) throws IOException {     MultiTermVectorsRequest request = new MultiTermVectorsRequest().     for (Item item : items) {         request.add(item.toTermVectorsRequest()).     }     return client.multiTermVectors(request).actionGet(). }
false;private,static;1;16;;private static Fields[] getFieldsFor(MultiTermVectorsResponse responses) throws IOException {     List<Fields> likeFields = new ArrayList<>().     for (MultiTermVectorsItemResponse response : responses) {         if (response.isFailed()) {             checkRoutingMissingException(response).             continue.         }         TermVectorsResponse getResponse = response.getResponse().         if (!getResponse.isExists()) {             continue.         }         likeFields.add(getResponse.getFields()).     }     return likeFields.toArray(Fields.EMPTY_ARRAY). }
false;private,static;1;6;;private static void checkRoutingMissingException(MultiTermVectorsItemResponse response) {     Throwable cause = ExceptionsHelper.unwrap(response.getFailure().getCause(), RoutingMissingException.class).     if (cause != null) {         throw ((RoutingMissingException) cause).     } }
false;private,static;3;19;;private static void handleExclude(BooleanQuery.Builder boolQuery, Item[] likeItems, QueryShardContext context) {     MappedFieldType idField = context.fieldMapper(IdFieldMapper.NAME).     if (idField == null) {         // no mappings, nothing to exclude         return.     }     // artificial docs get assigned a random id and should be disregarded     List<String> ids = new ArrayList<>().     for (Item item : likeItems) {         if (item.doc() != null) {             continue.         }         ids.add(item.id()).     }     if (!ids.isEmpty()) {         Query query = idField.termsQuery(ids, context).         boolQuery.add(query, BooleanClause.Occur.MUST_NOT).     } }
false;protected;0;7;;@Override protected int doHashCode() {     return Objects.hash(Arrays.hashCode(fields), Arrays.hashCode(likeTexts), Arrays.hashCode(unlikeTexts), Arrays.hashCode(likeItems), Arrays.hashCode(unlikeItems), maxQueryTerms, minTermFreq, minDocFreq, maxDocFreq, minWordLength, maxWordLength, Arrays.hashCode(stopWords), analyzer, minimumShouldMatch, boostTerms, include, failOnUnsupportedField). }
false;protected;1;20;;@Override protected boolean doEquals(MoreLikeThisQueryBuilder other) {     return Arrays.equals(fields, other.fields) && Arrays.equals(likeTexts, other.likeTexts) && Arrays.equals(unlikeTexts, other.unlikeTexts) && Arrays.equals(likeItems, other.likeItems) && Arrays.equals(unlikeItems, other.unlikeItems) && Objects.equals(maxQueryTerms, other.maxQueryTerms) && Objects.equals(minTermFreq, other.minTermFreq) && Objects.equals(minDocFreq, other.minDocFreq) && Objects.equals(maxDocFreq, other.maxDocFreq) && Objects.equals(minWordLength, other.minWordLength) && Objects.equals(maxWordLength, other.maxWordLength) && // otherwise we are comparing pointers     Arrays.equals(stopWords, other.stopWords) && Objects.equals(analyzer, other.analyzer) && Objects.equals(minimumShouldMatch, other.minimumShouldMatch) && Objects.equals(boostTerms, other.boostTerms) && Objects.equals(include, other.include) && Objects.equals(failOnUnsupportedField, other.failOnUnsupportedField). }
false;protected;1;5;;@Override protected QueryBuilder doRewrite(QueryRewriteContext queryRewriteContext) {     // TODO this needs heavy cleanups before we can rewrite it     return this. }
