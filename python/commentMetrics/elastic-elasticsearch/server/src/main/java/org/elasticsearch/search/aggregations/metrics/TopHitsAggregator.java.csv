commented;modifiers;parameterAmount;loc;comment;code
false;public;0;10;;@Override public ScoreMode scoreMode() {     SortAndFormats sort = subSearchContext.sort().     if (sort != null) {         return sort.sort.needsScores() || subSearchContext.trackScores() ? ScoreMode.COMPLETE : ScoreMode.COMPLETE_NO_SCORES.     } else {         // sort by score         return ScoreMode.COMPLETE.     } }
false;public;1;8;;@Override public void setScorer(Scorable scorer) throws IOException {     this.scorer = scorer.     super.setScorer(scorer).     for (ObjectCursor<LeafCollector> cursor : leafCollectors.values()) {         cursor.value.setScorer(scorer).     } }
false;public;2;39;;@Override public void collect(int docId, long bucket) throws IOException {     Collectors collectors = topDocsCollectors.get(bucket).     if (collectors == null) {         SortAndFormats sort = subSearchContext.sort().         int topN = subSearchContext.from() + subSearchContext.size().         if (sort == null) {             for (RescoreContext rescoreContext : context.rescore()) {                 topN = Math.max(rescoreContext.getWindowSize(), topN).             }         }         // In the QueryPhase we don't need this protection, because it is build into the IndexSearcher,         // but here we create collectors ourselves and we need prevent OOM because of crazy an offset and size.         topN = Math.min(topN, subSearchContext.searcher().getIndexReader().maxDoc()).         if (sort == null) {             collectors = new Collectors(TopScoreDocCollector.create(topN, Integer.MAX_VALUE), null).         } else {             // TODO: can we pass trackTotalHits=subSearchContext.trackTotalHits(){             // Note that this would require to catch CollectionTerminatedException             collectors = new Collectors(TopFieldCollector.create(sort.sort, topN, Integer.MAX_VALUE), subSearchContext.trackScores() ? new MaxScoreCollector() : null).         }         topDocsCollectors.put(bucket, collectors).     }     final LeafCollector leafCollector.     final int key = leafCollectors.indexOf(bucket).     if (key < 0) {         leafCollector = collectors.collector.getLeafCollector(ctx).         if (scorer != null) {             leafCollector.setScorer(scorer).         }         leafCollectors.indexInsert(key, bucket, leafCollector).     } else {         leafCollector = leafCollectors.indexGet(key).     }     leafCollector.collect(docId). }
false;public;2;61;;@Override public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {     // Create leaf collectors here instead of at the aggregator level. Otherwise in case this collector get invoked     // when post collecting then we have already replaced the leaf readers on the aggregator level have already been     // replaced with the next leaf readers and then post collection pushes docids of the previous segement, which     // then causes assertions to trip or incorrect top docs to be computed.     final LongObjectHashMap<LeafCollector> leafCollectors = new LongObjectHashMap<>(1).     return new LeafBucketCollectorBase(sub, null) {          Scorable scorer.          @Override         public void setScorer(Scorable scorer) throws IOException {             this.scorer = scorer.             super.setScorer(scorer).             for (ObjectCursor<LeafCollector> cursor : leafCollectors.values()) {                 cursor.value.setScorer(scorer).             }         }          @Override         public void collect(int docId, long bucket) throws IOException {             Collectors collectors = topDocsCollectors.get(bucket).             if (collectors == null) {                 SortAndFormats sort = subSearchContext.sort().                 int topN = subSearchContext.from() + subSearchContext.size().                 if (sort == null) {                     for (RescoreContext rescoreContext : context.rescore()) {                         topN = Math.max(rescoreContext.getWindowSize(), topN).                     }                 }                 // In the QueryPhase we don't need this protection, because it is build into the IndexSearcher,                 // but here we create collectors ourselves and we need prevent OOM because of crazy an offset and size.                 topN = Math.min(topN, subSearchContext.searcher().getIndexReader().maxDoc()).                 if (sort == null) {                     collectors = new Collectors(TopScoreDocCollector.create(topN, Integer.MAX_VALUE), null).                 } else {                     // TODO: can we pass trackTotalHits=subSearchContext.trackTotalHits(){                     // Note that this would require to catch CollectionTerminatedException                     collectors = new Collectors(TopFieldCollector.create(sort.sort, topN, Integer.MAX_VALUE), subSearchContext.trackScores() ? new MaxScoreCollector() : null).                 }                 topDocsCollectors.put(bucket, collectors).             }             final LeafCollector leafCollector.             final int key = leafCollectors.indexOf(bucket).             if (key < 0) {                 leafCollector = collectors.collector.getLeafCollector(ctx).                 if (scorer != null) {                     leafCollector.setScorer(scorer).                 }                 leafCollectors.indexInsert(key, bucket, leafCollector).             } else {                 leafCollector = leafCollectors.indexGet(key).             }             leafCollector.collect(docId).         }     }. }
false;public;1;48;;@Override public InternalAggregation buildAggregation(long owningBucketOrdinal) throws IOException {     Collectors collectors = topDocsCollectors.get(owningBucketOrdinal).     if (collectors == null) {         return buildEmptyAggregation().     }     TopDocsCollector<?> topDocsCollector = collectors.topDocsCollector.     TopDocs topDocs = topDocsCollector.topDocs().     float maxScore = Float.NaN.     if (subSearchContext.sort() == null) {         for (RescoreContext ctx : context().rescore()) {             try {                 topDocs = ctx.rescorer().rescore(topDocs, context.searcher(), ctx).             } catch (IOException e) {                 throw new ElasticsearchException("Rescore TopHits Failed", e).             }         }         if (topDocs.scoreDocs.length > 0) {             maxScore = topDocs.scoreDocs[0].score.         }     } else if (subSearchContext.trackScores()) {         TopFieldCollector.populateScores(topDocs.scoreDocs, subSearchContext.searcher(), subSearchContext.query()).         maxScore = collectors.maxScoreCollector.getMaxScore().     }     final TopDocsAndMaxScore topDocsAndMaxScore = new TopDocsAndMaxScore(topDocs, maxScore).     subSearchContext.queryResult().topDocs(topDocsAndMaxScore, subSearchContext.sort() == null ? null : subSearchContext.sort().formats).     int[] docIdsToLoad = new int[topDocs.scoreDocs.length].     for (int i = 0. i < topDocs.scoreDocs.length. i++) {         docIdsToLoad[i] = topDocs.scoreDocs[i].doc.     }     subSearchContext.docIdsToLoad(docIdsToLoad, 0, docIdsToLoad.length).     fetchPhase.execute(subSearchContext).     FetchSearchResult fetchResult = subSearchContext.fetchResult().     SearchHit[] internalHits = fetchResult.fetchResult().hits().getHits().     for (int i = 0. i < internalHits.length. i++) {         ScoreDoc scoreDoc = topDocs.scoreDocs[i].         SearchHit searchHitFields = internalHits[i].         searchHitFields.shard(subSearchContext.shardTarget()).         searchHitFields.score(scoreDoc.score).         if (scoreDoc instanceof FieldDoc) {             FieldDoc fieldDoc = (FieldDoc) scoreDoc.             searchHitFields.sortValues(fieldDoc.fields, subSearchContext.sort().formats).         }     }     return new InternalTopHits(name, subSearchContext.from(), subSearchContext.size(), topDocsAndMaxScore, fetchResult.hits(), pipelineAggregators(), metaData()). }
false;public;0;12;;@Override public InternalTopHits buildEmptyAggregation() {     TopDocs topDocs.     if (subSearchContext.sort() != null) {         topDocs = new TopFieldDocs(new TotalHits(0, TotalHits.Relation.EQUAL_TO), new FieldDoc[0], subSearchContext.sort().sort.getSort()).     } else {         topDocs = Lucene.EMPTY_TOP_DOCS.     }     return new InternalTopHits(name, subSearchContext.from(), subSearchContext.size(), new TopDocsAndMaxScore(topDocs, Float.NaN), SearchHits.empty(), pipelineAggregators(), metaData()). }
false;protected;0;4;;@Override protected void doClose() {     Releasables.close(topDocsCollectors). }
