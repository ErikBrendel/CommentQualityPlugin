commented;modifiers;parameterAmount;loc;comment;code
false;public;0;4;;@Override public ScoreMode scoreMode() {     return ScoreMode.COMPLETE. }
true;public;1;4;/**  * Set the deferred collectors.  */ ;/**  * Set the deferred collectors.  */ @Override public void setDeferredCollector(Iterable<BucketCollector> deferredCollectors) {     this.deferred = MultiBucketCollector.wrap(deferredCollectors). }
false;public;1;4;;@Override public void setScorer(Scorable scorer) throws IOException {     perSegCollector.setScorer(scorer). }
false;public;2;4;;@Override public void collect(int doc, long bucket) throws IOException {     perSegCollector.collect(doc, bucket). }
false;public;1;18;;@Override public LeafBucketCollector getLeafCollector(LeafReaderContext ctx) throws IOException {     perSegCollector = new PerSegmentCollects(ctx).     entries.add(perSegCollector).     // Deferring collector     return new LeafBucketCollector() {          @Override         public void setScorer(Scorable scorer) throws IOException {             perSegCollector.setScorer(scorer).         }          @Override         public void collect(int doc, long bucket) throws IOException {             perSegCollector.collect(doc, bucket).         }     }. }
true;protected;1;3;// other than Lucene score ;// Designed to be overridden by subclasses that may score docs by criteria // other than Lucene score protected TopDocsCollector<? extends ScoreDoc> createTopDocsCollector(int size) throws IOException {     return TopScoreDocCollector.create(size, Integer.MAX_VALUE). }
false;public;0;4;;@Override public void preCollection() throws IOException {     deferred.preCollection(). }
false;public;0;4;;@Override public void postCollection() throws IOException {     runDeferredAggs(). }
false;public;1;4;;@Override public void prepareSelectedBuckets(long... selectedBuckets) throws IOException { // no-op - deferred aggs processed in postCollection call }
false;private;0;27;;private void runDeferredAggs() throws IOException {     List<ScoreDoc> allDocs = new ArrayList<>(shardSize).     for (int i = 0. i < perBucketSamples.size(). i++) {         PerParentBucketSamples perBucketSample = perBucketSamples.get(i).         if (perBucketSample == null) {             continue.         }         perBucketSample.getMatches(allDocs).     }     // Sort the top matches by docID for the benefit of deferred collector     ScoreDoc[] docsArr = allDocs.toArray(new ScoreDoc[allDocs.size()]).     Arrays.sort(docsArr, (o1, o2) -> {         if (o1.doc == o2.doc) {             return o1.shardIndex - o2.shardIndex.         }         return o1.doc - o2.doc.     }).     try {         for (PerSegmentCollects perSegDocs : entries) {             perSegDocs.replayRelatedMatches(docsArr).         }     } catch (IOException e) {         throw new ElasticsearchException("IOException collecting best scoring results", e).     }     deferred.postCollection(). }
false;public;1;13;;public void getMatches(List<ScoreDoc> allDocs) {     TopDocs topDocs = tdc.topDocs().     ScoreDoc[] sd = topDocs.scoreDocs.     matchedDocs = sd.length.     for (ScoreDoc scoreDoc : sd) {         // A bit of a hack to (ab)use shardIndex property here to         // hold a bucket ID but avoids allocating extra data structures         // and users should have bigger concerns if bucket IDs         // exceed int capacity..         scoreDoc.shardIndex = (int) parentBucket.     }     allDocs.addAll(Arrays.asList(sd)). }
false;public;1;3;;public void collect(int doc) throws IOException {     currentLeafCollector.collect(doc). }
false;public;1;3;;public void setScorer(Scorable scorer) throws IOException {     currentLeafCollector.setScorer(scorer). }
false;public;1;3;;public void changeSegment(LeafReaderContext readerContext) throws IOException {     currentLeafCollector = tdc.getLeafCollector(readerContext). }
false;public;0;3;;public int getDocCount() {     return matchedDocs. }
false;public;1;10;;public void setScorer(Scorable scorer) throws IOException {     this.currentScorer = scorer.     for (int i = 0. i < perBucketSamples.size(). i++) {         PerParentBucketSamples perBucketSample = perBucketSamples.get(i).         if (perBucketSample == null) {             continue.         }         perBucketSample.setScorer(scorer).     } }
false;public;1;23;;public void replayRelatedMatches(ScoreDoc[] sd) throws IOException {     final LeafBucketCollector leafCollector = deferred.getLeafCollector(readerContext).     leafCollector.setScorer(this).     currentScore = 0.     currentDocId = -1.     if (maxDocId < 0) {         return.     }     for (ScoreDoc scoreDoc : sd) {         // Doc ids from TopDocCollector are root-level Reader so         // need rebasing         int rebased = scoreDoc.doc - readerContext.docBase.         if ((rebased >= 0) && (rebased <= maxDocId)) {             currentScore = scoreDoc.score.             currentDocId = rebased.             // We stored the bucket ID in Lucene's shardIndex property             // for convenience.             leafCollector.collect(rebased, scoreDoc.shardIndex).         }     } }
false;public;0;4;;@Override public float score() throws IOException {     return currentScore. }
false;public;0;4;;@Override public int docID() {     return currentDocId. }
false;public;2;10;;public void collect(int docId, long parentBucket) throws IOException {     perBucketSamples = bigArrays.grow(perBucketSamples, parentBucket + 1).     PerParentBucketSamples sampler = perBucketSamples.get((int) parentBucket).     if (sampler == null) {         sampler = new PerParentBucketSamples(parentBucket, currentScorer, readerContext).         perBucketSamples.set((int) parentBucket, sampler).     }     sampler.collect(docId).     maxDocId = Math.max(maxDocId, docId). }
false;public;1;9;;public int getDocCount(long parentBucket) {     PerParentBucketSamples sampler = perBucketSamples.get((int) parentBucket).     if (sampler == null) {         // framework still asks for doc count.         return 0.     }     return sampler.getDocCount(). }
false;public;0;4;;@Override public void close() throws ElasticsearchException {     Releasables.close(perBucketSamples). }
