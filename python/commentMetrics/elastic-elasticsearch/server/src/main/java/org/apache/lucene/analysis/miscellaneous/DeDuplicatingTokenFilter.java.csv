commented;modifiers;parameterAmount;loc;comment;code
false;protected;0;4;;@Override protected boolean accept() throws IOException {     return emitDuplicates || seqAtt.getNumPriorUsesInASequence() < 1. }
false;public,final;0;15;;@Override public final boolean incrementToken() throws IOException {     if (allTokens == null) {         loadAllTokens().     }     clearAttributes().     if (pos < allTokens.size()) {         State earlierToken = allTokens.get(pos).         pos++.         restoreState(earlierToken).         return true.     } else {         return false.     } }
false;public;0;66;;public void loadAllTokens() throws IOException {     // TODO consider changing this implementation to emit tokens as-we-go     // rather than buffering all. However this array is perhaps not the     // bulk of memory usage (in practice the dupSequenceSpotter requires     // ~5x the original content size in its internal tree ).     allTokens = new ArrayList<State>(256).     /*              * Given the bytes 123456123456 and a duplicate sequence size of 6              * the byteStreamDuplicateSpotter will only flag the final byte as              * part of a duplicate sequence due to the byte-at-a-time streaming              * nature of its assessments. When this happens we retain a buffer              * of the last 6 tokens so that we can mark the states of prior              * tokens (bytes 7 to 11) as also being duplicates              */     pos = 0.     boolean isWrapped = false.     State[] priorStatesBuffer = new State[windowSize].     short[] priorMaxNumSightings = new short[windowSize].     int cursor = 0.     while (input.incrementToken()) {         BytesRef bytesRef = termBytesAtt.getBytesRef().         long tokenHash = MurmurHash3.hash128(bytesRef.bytes, bytesRef.offset, bytesRef.length, 0, seed).h1.         byte tokenByte = (byte) (tokenHash & 0xFF).         short numSightings = byteStreamDuplicateSpotter.addByte(tokenByte).         priorStatesBuffer[cursor] = captureState().         // token is marked as a duplicate         if (numSightings >= 1) {             int numLengthsToRecord = windowSize.             int pos = cursor.             while (numLengthsToRecord > 0) {                 if (pos < 0) {                     pos = windowSize - 1.                 }                 priorMaxNumSightings[pos] = (short) Math.max(priorMaxNumSightings[pos], numSightings).                 numLengthsToRecord--.                 pos--.             }         }         // Reposition cursor to next free slot         cursor++.         if (cursor >= windowSize) {             // wrap around the buffer             cursor = 0.             isWrapped = true.         }         // next iteration adds a new head         if (isWrapped) {             // tokens we may about to overwrite in the next iteration             if (priorStatesBuffer[cursor] != null) {                 recordLengthInfoState(priorMaxNumSightings, priorStatesBuffer, cursor).             }         }     }     // end loop reading all tokens from stream     // Flush the buffered tokens     int pos = isWrapped ? nextAfter(cursor) : 0.     while (pos != cursor) {         recordLengthInfoState(priorMaxNumSightings, priorStatesBuffer, pos).         pos = nextAfter(pos).     } }
false;private;1;7;;private int nextAfter(int pos) {     pos++.     if (pos >= windowSize) {         pos = 0.     }     return pos. }
false;private;3;12;;private void recordLengthInfoState(short[] maxNumSightings, State[] tokenStates, int cursor) {     if (maxNumSightings[cursor] > 0) {         // We need to patch in the max sequence length we recorded at         // this position into the token state         restoreState(tokenStates[cursor]).         seqAtt.setNumPriorUsesInASequence(maxNumSightings[cursor]).         maxNumSightings[cursor] = 0.         // record the patched state         tokenStates[cursor] = captureState().     }     allTokens.add(tokenStates[cursor]). }
