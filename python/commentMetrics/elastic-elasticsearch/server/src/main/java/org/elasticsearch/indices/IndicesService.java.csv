commented;modifiers;parameterAmount;loc;comment;code
false;protected;0;5;;@Override protected void doStart() {     // Start thread that will manage cleaning the field data cache periodically     threadPool.schedule(this.cacheCleaner, this.cleanInterval, ThreadPool.Names.SAME). }
false;public;4;6;;@Override public void onRemoval(ShardId shardId, String fieldName, boolean wasEvicted, long sizeInBytes) {     assert sizeInBytes >= 0 : "When reducing circuit breaker, it should be adjusted with a number higher or " + "equal to 0 and not [" + sizeInBytes + "]".     circuitBreakerService.getBreaker(CircuitBreaker.FIELDDATA).addWithoutBreaking(-sizeInBytes). }
false;protected;0;14;;@Override protected void closeInternal() {     try {         IOUtils.close(analysisRegistry, indexingMemoryController, indicesFieldDataCache, cacheCleaner, indicesRequestCache, indicesQueryCache).     } catch (IOException e) {         throw new UncheckedIOException(e).     } }
false;protected;0;27;;@Override protected void doStop() {     ExecutorService indicesStopExecutor = Executors.newFixedThreadPool(5, EsExecutors.daemonThreadFactory(settings, "indices_shutdown")).     // Copy indices because we modify it asynchronously in the body of the loop     final Set<Index> indices = this.indices.values().stream().map(s -> s.index()).collect(Collectors.toSet()).     final CountDownLatch latch = new CountDownLatch(indices.size()).     for (final Index index : indices) {         indicesStopExecutor.execute(() -> {             try {                 removeIndex(index, IndexRemovalReason.NO_LONGER_ASSIGNED, "shutdown").             } finally {                 latch.countDown().             }         }).     }     try {         if (latch.await(shardsClosedTimeout.seconds(), TimeUnit.SECONDS) == false) {             logger.warn("Not all shards are closed yet, waited {}sec - stopping service", shardsClosedTimeout.seconds()).         }     } catch (InterruptedException e) {     // ignore     } finally {         indicesStopExecutor.shutdown().     } }
false;protected;0;4;;@Override protected void doClose() throws IOException {     indicesRefCount.decRef(). }
true;public;1;3;/**  * Returns the node stats indices stats. The {@code includePrevious} flag controls  * if old shards stats will be aggregated as well (only for relevant stats, such as  * refresh and indexing, not for docs/store).  */ ;/**  * Returns the node stats indices stats. The {@code includePrevious} flag controls  * if old shards stats will be aggregated as well (only for relevant stats, such as  * refresh and indexing, not for docs/store).  */ public NodeIndicesStats stats(boolean includePrevious) {     return stats(includePrevious, new CommonStatsFlags().all()). }
false;public;2;34;;public NodeIndicesStats stats(boolean includePrevious, CommonStatsFlags flags) {     CommonStats oldStats = new CommonStats(flags).     if (includePrevious) {         Flag[] setFlags = flags.getFlags().         for (Flag flag : setFlags) {             switch(flag) {                 case Get:                     oldStats.get.add(oldShardsStats.getStats).                     break.                 case Indexing:                     oldStats.indexing.add(oldShardsStats.indexingStats).                     break.                 case Search:                     oldStats.search.add(oldShardsStats.searchStats).                     break.                 case Merge:                     oldStats.merge.add(oldShardsStats.mergeStats).                     break.                 case Refresh:                     oldStats.refresh.add(oldShardsStats.refreshStats).                     break.                 case Recovery:                     oldStats.recoveryStats.add(oldShardsStats.recoveryStats).                     break.                 case Flush:                     oldStats.flush.add(oldShardsStats.flushStats).                     break.             }         }     }     return new NodeIndicesStats(oldStats, statsByShard(this, flags)). }
false;;2;26;;Map<Index, List<IndexShardStats>> statsByShard(final IndicesService indicesService, final CommonStatsFlags flags) {     final Map<Index, List<IndexShardStats>> statsByShard = new HashMap<>().     for (final IndexService indexService : indicesService) {         for (final IndexShard indexShard : indexService) {             try {                 final IndexShardStats indexShardStats = indicesService.indexShardStats(indicesService, indexShard, flags).                 if (indexShardStats == null) {                     continue.                 }                 if (statsByShard.containsKey(indexService.index()) == false) {                     statsByShard.put(indexService.index(), arrayAsArrayList(indexShardStats)).                 } else {                     statsByShard.get(indexService.index()).add(indexShardStats).                 }             } catch (IllegalIndexShardStateException | AlreadyClosedException e) {                 // we can safely ignore illegal state on ones that are closing for example                 logger.trace(() -> new ParameterizedMessage("{} ignoring shard stats", indexShard.shardId()), e).             }         }     }     return statsByShard. }
false;;3;31;;IndexShardStats indexShardStats(final IndicesService indicesService, final IndexShard indexShard, final CommonStatsFlags flags) {     if (indexShard.routingEntry() == null) {         return null.     }     CommitStats commitStats.     SeqNoStats seqNoStats.     RetentionLeaseStats retentionLeaseStats.     try {         commitStats = indexShard.commitStats().         seqNoStats = indexShard.seqNoStats().         retentionLeaseStats = indexShard.getRetentionLeaseStats().     } catch (AlreadyClosedException e) {         // shard is closed - no stats is fine         commitStats = null.         seqNoStats = null.         retentionLeaseStats = null.     }     return new IndexShardStats(indexShard.shardId(), new ShardStats[] { new ShardStats(indexShard.routingEntry(), indexShard.shardPath(), new CommonStats(indicesService.getIndicesQueryCache(), indexShard, flags), commitStats, seqNoStats, retentionLeaseStats) }). }
true;private;0;5;/**  * Checks if changes (adding / removing) indices, shards and so on are allowed.  *  * @throws IllegalStateException if no changes allowed.  */ ;/**  * Checks if changes (adding / removing) indices, shards and so on are allowed.  *  * @throws IllegalStateException if no changes allowed.  */ private void ensureChangesAllowed() {     if (lifecycle.started() == false) {         throw new IllegalStateException("Can't make changes to indices service, node is closed").     } }
false;public;0;4;;@Override public Iterator<IndexService> iterator() {     return indices.values().iterator(). }
false;public;1;3;;public boolean hasIndex(Index index) {     return indices.containsKey(index.getUUID()). }
true;public;1;5;/**  * Returns an IndexService for the specified index if exists otherwise returns <code>null</code>.  */ ;/**  * Returns an IndexService for the specified index if exists otherwise returns <code>null</code>.  */ @Override @Nullable public IndexService indexService(Index index) {     return indices.get(index.getUUID()). }
true;public;1;9;/**  * Returns an IndexService for the specified index if exists otherwise a {@link IndexNotFoundException} is thrown.  */ ;/**  * Returns an IndexService for the specified index if exists otherwise a {@link IndexNotFoundException} is thrown.  */ public IndexService indexServiceSafe(Index index) {     IndexService indexService = indices.get(index.getUUID()).     if (indexService == null) {         throw new IndexNotFoundException(index).     }     assert indexService.indexUUID().equals(index.getUUID()) : "uuid mismatch local: " + indexService.indexUUID() + " incoming: " + index.getUUID().     return indexService. }
false;public;1;4;;@Override public void onStoreCreated(ShardId shardId) {     indicesRefCount.incRef(). }
false;public;1;8;;@Override public void onStoreClosed(ShardId shardId) {     try {         indicesRefCount.decRef().     } finally {         indicesQueryCache.onClose(shardId).     } }
true;public,synchronized;2;48;/**  * Creates a new {@link IndexService} for the given metadata.  *  * @param indexMetaData          the index metadata to create the index for  * @param builtInListeners       a list of built-in lifecycle {@link IndexEventListener} that should should be used along side with the  *                               per-index listeners  * @throws ResourceAlreadyExistsException if the index already exists.  */ ;/**  * Creates a new {@link IndexService} for the given metadata.  *  * @param indexMetaData          the index metadata to create the index for  * @param builtInListeners       a list of built-in lifecycle {@link IndexEventListener} that should should be used along side with the  *                               per-index listeners  * @throws ResourceAlreadyExistsException if the index already exists.  */ @Override public synchronized IndexService createIndex(final IndexMetaData indexMetaData, final List<IndexEventListener> builtInListeners) throws IOException {     ensureChangesAllowed().     if (indexMetaData.getIndexUUID().equals(IndexMetaData.INDEX_UUID_NA_VALUE)) {         throw new IllegalArgumentException("index must have a real UUID found value: [" + indexMetaData.getIndexUUID() + "]").     }     final Index index = indexMetaData.getIndex().     if (hasIndex(index)) {         throw new ResourceAlreadyExistsException(index).     }     List<IndexEventListener> finalListeners = new ArrayList<>(builtInListeners).     final IndexEventListener onStoreClose = new IndexEventListener() {          @Override         public void onStoreCreated(ShardId shardId) {             indicesRefCount.incRef().         }          @Override         public void onStoreClosed(ShardId shardId) {             try {                 indicesRefCount.decRef().             } finally {                 indicesQueryCache.onClose(shardId).             }         }     }.     finalListeners.add(onStoreClose).     finalListeners.add(oldShardsStats).     final IndexService indexService = createIndexService("create index", indexMetaData, indicesQueryCache, indicesFieldDataCache, finalListeners, indexingMemoryController).     boolean success = false.     try {         indexService.getIndexEventListener().afterIndexCreated(indexService).         indices = newMapBuilder(indices).put(index.getUUID(), indexService).immutableMap().         success = true.         return indexService.     } finally {         if (success == false) {             indexService.close("plugins_failed", true).         }     } }
true;private,synchronized;6;38;/**  * This creates a new IndexService without registering it  */ ;/**  * This creates a new IndexService without registering it  */ private synchronized IndexService createIndexService(final String reason, IndexMetaData indexMetaData, IndicesQueryCache indicesQueryCache, IndicesFieldDataCache indicesFieldDataCache, List<IndexEventListener> builtInListeners, IndexingOperationListener... indexingOperationListeners) throws IOException {     final IndexSettings idxSettings = new IndexSettings(indexMetaData, settings, indexScopedSettings).     // we ignore private settings since they are not registered settings     indexScopedSettings.validate(indexMetaData.getSettings(), true, true, true).     logger.debug("creating Index [{}], shards [{}]/[{}] - reason [{}]", indexMetaData.getIndex(), idxSettings.getNumberOfShards(), idxSettings.getNumberOfReplicas(), reason).     final IndexModule indexModule = new IndexModule(idxSettings, analysisRegistry, getEngineFactory(idxSettings), indexStoreFactories).     for (IndexingOperationListener operationListener : indexingOperationListeners) {         indexModule.addIndexOperationListener(operationListener).     }     pluginsService.onIndexModule(indexModule).     for (IndexEventListener listener : builtInListeners) {         indexModule.addIndexEventListener(listener).     }     return indexModule.newIndexService(nodeEnv, xContentRegistry, this, circuitBreakerService, bigArrays, threadPool, scriptService, client, indicesQueryCache, mapperRegistry, indicesFieldDataCache, namedWriteableRegistry). }
false;private;1;33;;private EngineFactory getEngineFactory(final IndexSettings idxSettings) {     final IndexMetaData indexMetaData = idxSettings.getIndexMetaData().     if (indexMetaData != null && indexMetaData.getState() == IndexMetaData.State.CLOSE) {         // NoOpEngine takes precedence as long as the index is closed         return NoOpEngine::new.     }     final List<Optional<EngineFactory>> engineFactories = engineFactoryProviders.stream().map(engineFactoryProvider -> engineFactoryProvider.apply(idxSettings)).filter(maybe -> Objects.requireNonNull(maybe).isPresent()).collect(Collectors.toList()).     if (engineFactories.isEmpty()) {         return new InternalEngineFactory().     } else if (engineFactories.size() == 1) {         assert engineFactories.get(0).isPresent().         return engineFactories.get(0).get().     } else {         final String message = String.format(Locale.ROOT, "multiple engine factories provided for %s: %s", idxSettings.getIndex(), engineFactories.stream().map(t -> {             assert t.isPresent().             return "[" + t.get().getClass().getName() + "]".         }).collect(Collectors.joining(","))).         throw new IllegalStateException(message).     } }
true;public,synchronized;1;6;/**  * creates a new mapper service for the given index, in order to do administrative work like mapping updates.  * This *should not* be used for document parsing. Doing so will result in an exception.  *  * Note: the returned {@link MapperService} should be closed when unneeded.  */ ;/**  * creates a new mapper service for the given index, in order to do administrative work like mapping updates.  * This *should not* be used for document parsing. Doing so will result in an exception.  *  * Note: the returned {@link MapperService} should be closed when unneeded.  */ public synchronized MapperService createIndexMapperService(IndexMetaData indexMetaData) throws IOException {     final IndexSettings idxSettings = new IndexSettings(indexMetaData, this.settings, indexScopedSettings).     final IndexModule indexModule = new IndexModule(idxSettings, analysisRegistry, getEngineFactory(idxSettings), indexStoreFactories).     pluginsService.onIndexModule(indexModule).     return indexModule.newIndexMapperService(xContentRegistry, mapperRegistry, scriptService). }
true;public,synchronized;2;19;/**  * This method verifies that the given {@code metaData} holds sane values to create an {@link IndexService}.  * This method tries to update the meta data of the created {@link IndexService} if the given {@code metaDataUpdate}  * is different from the given {@code metaData}.  * This method will throw an exception if the creation or the update fails.  * The created {@link IndexService} will not be registered and will be closed immediately.  */ ;/**  * This method verifies that the given {@code metaData} holds sane values to create an {@link IndexService}.  * This method tries to update the meta data of the created {@link IndexService} if the given {@code metaDataUpdate}  * is different from the given {@code metaData}.  * This method will throw an exception if the creation or the update fails.  * The created {@link IndexService} will not be registered and will be closed immediately.  */ public synchronized void verifyIndexMetadata(IndexMetaData metaData, IndexMetaData metaDataUpdate) throws IOException {     final List<Closeable> closeables = new ArrayList<>().     try {         IndicesFieldDataCache indicesFieldDataCache = new IndicesFieldDataCache(settings, new IndexFieldDataCache.Listener() {         }).         closeables.add(indicesFieldDataCache).         IndicesQueryCache indicesQueryCache = new IndicesQueryCache(settings).         closeables.add(indicesQueryCache).         // this will also fail if some plugin fails etc. which is nice since we can verify that early         final IndexService service = createIndexService("metadata verification", metaData, indicesQueryCache, indicesFieldDataCache, emptyList()).         closeables.add(() -> service.close("metadata verification", false)).         service.mapperService().merge(metaData, MapperService.MergeReason.MAPPING_RECOVERY).         if (metaData.equals(metaDataUpdate) == false) {             service.updateMetaData(metaData, metaDataUpdate).         }     } finally {         IOUtils.close(closeables).     } }
false;public;8;27;;@Override public IndexShard createShard(final ShardRouting shardRouting, final RecoveryState recoveryState, final PeerRecoveryTargetService recoveryTargetService, final PeerRecoveryTargetService.RecoveryListener recoveryListener, final RepositoriesService repositoriesService, final Consumer<IndexShard.ShardFailure> onShardFailure, final Consumer<ShardId> globalCheckpointSyncer, final RetentionLeaseSyncer retentionLeaseSyncer) throws IOException {     Objects.requireNonNull(retentionLeaseSyncer).     ensureChangesAllowed().     IndexService indexService = indexService(shardRouting.index()).     IndexShard indexShard = indexService.createShard(shardRouting, globalCheckpointSyncer, retentionLeaseSyncer).     indexShard.addShardFailureCallback(onShardFailure).     indexShard.startRecovery(recoveryState, recoveryTargetService, recoveryListener, repositoriesService, (type, mapping) -> {         assert recoveryState.getRecoverySource().getType() == RecoverySource.Type.LOCAL_SHARDS : "mapping update consumer only required by local shards recovery".         client.admin().indices().preparePutMapping().setConcreteIndex(// concrete index - no name clash, it uses uuid         shardRouting.index()).setType(type).setSource(mapping.source().string(), XContentType.JSON).get().     }, this).     return indexShard. }
false;public;3;33;;@Override public void removeIndex(final Index index, final IndexRemovalReason reason, final String extraInfo) {     final String indexName = index.getName().     try {         final IndexService indexService.         final IndexEventListener listener.         synchronized (this) {             if (hasIndex(index) == false) {                 return.             }             logger.debug("[{}] closing ... (reason [{}])", indexName, reason).             Map<String, IndexService> newIndices = new HashMap<>(indices).             indexService = newIndices.remove(index.getUUID()).             assert indexService != null : "IndexService is null for index: " + index.             indices = unmodifiableMap(newIndices).             listener = indexService.getIndexEventListener().         }         listener.beforeIndexRemoved(indexService, reason).         logger.debug("{} closing index service (reason [{}][{}])", index, reason, extraInfo).         indexService.close(extraInfo, reason == IndexRemovalReason.DELETED).         logger.debug("{} closed... (reason [{}][{}])", index, reason, extraInfo).         final IndexSettings indexSettings = indexService.getIndexSettings().         listener.afterIndexRemoved(indexService.index(), indexSettings, reason).         if (reason == IndexRemovalReason.DELETED) {             // now we are done - try to wipe data on disk if possible             deleteIndexStore(extraInfo, indexService.index(), indexSettings).         }     } catch (Exception e) {         logger.warn(() -> new ParameterizedMessage("failed to remove index {} ([{}][{}])", index, reason, extraInfo), e).     } }
false;public;0;3;;public IndicesFieldDataCache getIndicesFieldDataCache() {     return indicesFieldDataCache. }
false;public;0;3;;public CircuitBreakerService getCircuitBreakerService() {     return circuitBreakerService. }
false;public;0;3;;public IndicesQueryCache getIndicesQueryCache() {     return indicesQueryCache. }
false;public,synchronized;3;12;;@Override public synchronized void beforeIndexShardClosed(ShardId shardId, @Nullable IndexShard indexShard, Settings indexSettings) {     if (indexShard != null) {         getStats.addTotals(indexShard.getStats()).         indexingStats.addTotals(indexShard.indexingStats()).         searchStats.addTotals(indexShard.searchStats()).         mergeStats.addTotals(indexShard.mergeStats()).         refreshStats.addTotals(indexShard.refreshStats()).         flushStats.addTotals(indexShard.flushStats()).         recoveryStats.addTotals(indexShard.recoveryStats()).     } }
true;public;3;17;/**  * Deletes an index that is not assigned to this node. This method cleans up all disk folders relating to the index  * but does not deal with in-memory structures. For those call {@link #removeIndex(Index, IndexRemovalReason, String)}  */ ;/**  * Deletes an index that is not assigned to this node. This method cleans up all disk folders relating to the index  * but does not deal with in-memory structures. For those call {@link #removeIndex(Index, IndexRemovalReason, String)}  */ @Override public void deleteUnassignedIndex(String reason, IndexMetaData metaData, ClusterState clusterState) {     if (nodeEnv.hasNodeFile()) {         String indexName = metaData.getIndex().getName().         try {             if (clusterState.metaData().hasIndex(indexName)) {                 final IndexMetaData index = clusterState.metaData().index(indexName).                 throw new IllegalStateException("Can't delete unassigned index store for [" + indexName + "] - it's still part of " + "the cluster state [" + index.getIndexUUID() + "] [" + metaData.getIndexUUID() + "]").             }             deleteIndexStore(reason, metaData, clusterState).         } catch (Exception e) {             logger.warn(() -> new ParameterizedMessage("[{}] failed to delete unassigned index (reason [{}])", metaData.getIndex(), reason), e).         }     } }
true;;3;23;/**  * Deletes the index store trying to acquire all shards locks for this index.  * This method will delete the metadata for the index even if the actual shards can't be locked.  *  * Package private for testing  */ ;/**  * Deletes the index store trying to acquire all shards locks for this index.  * This method will delete the metadata for the index even if the actual shards can't be locked.  *  * Package private for testing  */ void deleteIndexStore(String reason, IndexMetaData metaData, ClusterState clusterState) throws IOException {     if (nodeEnv.hasNodeFile()) {         synchronized (this) {             Index index = metaData.getIndex().             if (hasIndex(index)) {                 String localUUid = indexService(index).indexUUID().                 throw new IllegalStateException("Can't delete index store for [" + index.getName() + "] - it's still part of the indices service [" + localUUid + "] [" + metaData.getIndexUUID() + "]").             }             if (clusterState.metaData().hasIndex(index.getName()) && (clusterState.nodes().getLocalNode().isMasterNode() == true)) {                 // we do not delete the store if it is a master eligible node and the index is still in the cluster state                 // because we want to keep the meta data for indices around even if no shards are left here                 final IndexMetaData idxMeta = clusterState.metaData().index(index.getName()).                 throw new IllegalStateException("Can't delete index store for [" + index.getName() + "] - it's still part of the " + "cluster state [" + idxMeta.getIndexUUID() + "] [" + metaData.getIndexUUID() + "], " + "we are master eligible, so will keep the index metadata even if no shards are left.").             }         }         final IndexSettings indexSettings = buildIndexSettings(metaData).         deleteIndexStore(reason, indexSettings.getIndex(), indexSettings).     } }
false;private;3;3;;private void deleteIndexStore(String reason, Index index, IndexSettings indexSettings) throws IOException {     deleteIndexStoreIfDeletionAllowed(reason, index, indexSettings, DEFAULT_INDEX_DELETION_PREDICATE). }
false;private;4;27;;private void deleteIndexStoreIfDeletionAllowed(final String reason, final Index index, final IndexSettings indexSettings, final IndexDeletionAllowedPredicate predicate) throws IOException {     boolean success = false.     try {         // we are trying to delete the index store here - not a big deal if the lock can't be obtained         // the store metadata gets wiped anyway even without the lock this is just best effort since         // every shards deletes its content under the shard lock it owns.         logger.debug("{} deleting index store reason [{}]", index, reason).         if (predicate.apply(index, indexSettings)) {             // its safe to delete all index metadata and shard data             nodeEnv.deleteIndexDirectorySafe(index, 0, indexSettings).         }         success = true.     } catch (LockObtainFailedException ex) {         logger.debug(() -> new ParameterizedMessage("{} failed to delete index store - at least one shards is still locked", index), ex).     } catch (Exception ex) {         logger.warn(() -> new ParameterizedMessage("{} failed to delete index", index), ex).     } finally {         if (success == false) {             addPendingDelete(index, indexSettings).         }         // this is a pure protection to make sure this index doesn't get re-imported as a dangling index.         // we should in the future rather write a tombstone rather than wiping the metadata.         MetaDataStateFormat.deleteMetaState(nodeEnv.indexPaths(index)).     } }
true;public;3;6;/**  * Deletes the shard with an already acquired shard lock.  * @param reason the reason for the shard deletion  * @param lock the lock of the shard to delete  * @param indexSettings the shards index settings.  * @throws IOException if an IOException occurs  */ ;/**  * Deletes the shard with an already acquired shard lock.  * @param reason the reason for the shard deletion  * @param lock the lock of the shard to delete  * @param indexSettings the shards index settings.  * @throws IOException if an IOException occurs  */ @Override public void deleteShardStore(String reason, ShardLock lock, IndexSettings indexSettings) throws IOException {     ShardId shardId = lock.getShardId().     logger.trace("{} deleting shard reason [{}]", shardId, reason).     nodeEnv.deleteShardDirectoryUnderLock(lock, indexSettings). }
true;public;3;28;/**  * This method deletes the shard contents on disk for the given shard ID. This method will fail if the shard deleting  * is prevented by {@link #canDeleteShardContent(ShardId, IndexSettings)}  * of if the shards lock can not be acquired.  *  * On data nodes, if the deleted shard is the last shard folder in its index, the method will attempt to remove  * the index folder as well.  *  * @param reason the reason for the shard deletion  * @param shardId the shards ID to delete  * @param clusterState . This is required to access the indexes settings etc.  * @throws IOException if an IOException occurs  */ ;/**  * This method deletes the shard contents on disk for the given shard ID. This method will fail if the shard deleting  * is prevented by {@link #canDeleteShardContent(ShardId, IndexSettings)}  * of if the shards lock can not be acquired.  *  * On data nodes, if the deleted shard is the last shard folder in its index, the method will attempt to remove  * the index folder as well.  *  * @param reason the reason for the shard deletion  * @param shardId the shards ID to delete  * @param clusterState . This is required to access the indexes settings etc.  * @throws IOException if an IOException occurs  */ public void deleteShardStore(String reason, ShardId shardId, ClusterState clusterState) throws IOException, ShardLockObtainFailedException {     final IndexMetaData metaData = clusterState.getMetaData().indices().get(shardId.getIndexName()).     final IndexSettings indexSettings = buildIndexSettings(metaData).     ShardDeletionCheckResult shardDeletionCheckResult = canDeleteShardContent(shardId, indexSettings).     if (shardDeletionCheckResult != ShardDeletionCheckResult.FOLDER_FOUND_CAN_DELETE) {         throw new IllegalStateException("Can't delete shard " + shardId + " (cause: " + shardDeletionCheckResult + ")").     }     nodeEnv.deleteShardDirectorySafe(shardId, indexSettings).     logger.debug("{} deleted shard reason [{}]", shardId, reason).     // master nodes keep the index meta data, even if having no shards..     if (clusterState.nodes().getLocalNode().isMasterNode() == false && canDeleteIndexContents(shardId.getIndex(), indexSettings)) {         if (nodeEnv.findAllShardIds(shardId.getIndex()).isEmpty()) {             try {                 // note that deleteIndexStore have more safety checks and may throw an exception if index was concurrently created.                 deleteIndexStore("no longer used", metaData, clusterState).             } catch (Exception e) {                 // wrap the exception to indicate we already deleted the shard                 throw new ElasticsearchException("failed to delete unused index after deleting its last shard (" + shardId + ")", e).             }         } else {             logger.trace("[{}] still has shard stores, leaving as is", shardId.getIndex()).         }     } }
true;public;2;9;/**  * This method returns true if the current node is allowed to delete the given index.  * This is the case if the index is deleted in the metadata or there is no allocation  * on the local node and the index isn't on a shared file system.  * @param index {@code Index} to check whether deletion is allowed  * @param indexSettings {@code IndexSettings} for the given index  * @return true if the index can be deleted on this node  */ ;/**  * This method returns true if the current node is allowed to delete the given index.  * This is the case if the index is deleted in the metadata or there is no allocation  * on the local node and the index isn't on a shared file system.  * @param index {@code Index} to check whether deletion is allowed  * @param indexSettings {@code IndexSettings} for the given index  * @return true if the index can be deleted on this node  */ public boolean canDeleteIndexContents(Index index, IndexSettings indexSettings) {     // index contents can be deleted if its an already closed index (so all its resources have     // already been relinquished)     final IndexService indexService = indexService(index).     if (indexService == null && nodeEnv.hasNodeFile()) {         return true.     }     return false. }
true;public;2;31;/**  * Verify that the contents on disk for the given index is deleted. if not, delete the contents.  * This method assumes that an index is already deleted in the cluster state and/or explicitly  * through index tombstones.  * @param index {@code Index} to make sure its deleted from disk  * @param clusterState {@code ClusterState} to ensure the index is not part of it  * @return IndexMetaData for the index loaded from disk  */ ;/**  * Verify that the contents on disk for the given index is deleted. if not, delete the contents.  * This method assumes that an index is already deleted in the cluster state and/or explicitly  * through index tombstones.  * @param index {@code Index} to make sure its deleted from disk  * @param clusterState {@code ClusterState} to ensure the index is not part of it  * @return IndexMetaData for the index loaded from disk  */ @Override @Nullable public IndexMetaData verifyIndexIsDeleted(final Index index, final ClusterState clusterState) {     // this method should only be called when we know the index (name + uuid) is not part of the cluster state     if (clusterState.metaData().index(index) != null) {         throw new IllegalStateException("Cannot delete index [" + index + "], it is still part of the cluster state.").     }     if (nodeEnv.hasNodeFile() && FileSystemUtils.exists(nodeEnv.indexPaths(index))) {         final IndexMetaData metaData.         try {             metaData = metaStateService.loadIndexState(index).             if (metaData == null) {                 return null.             }         } catch (Exception e) {             logger.warn(() -> new ParameterizedMessage("[{}] failed to load state file from a stale deleted index, " + "folders will be left on disk", index), e).             return null.         }         final IndexSettings indexSettings = buildIndexSettings(metaData).         try {             deleteIndexStoreIfDeletionAllowed("stale deleted index", index, indexSettings, ALWAYS_TRUE).         } catch (Exception e) {             // we just warn about the exception here because if deleteIndexStoreIfDeletionAllowed             // throws an exception, it gets added to the list of pending deletes to be tried again             logger.warn(() -> new ParameterizedMessage("[{}] failed to delete index on disk", metaData.getIndex()), e).         }         return metaData.     }     return null. }
true;public;2;24;/**  * Returns <code>ShardDeletionCheckResult</code> signaling whether the shards content for the given shard can be deleted.  *  * @param shardId the shard to delete.  * @param indexSettings the shards's relevant {@link IndexSettings}. This is required to access the indexes settings etc.  */ ;/**  * Returns <code>ShardDeletionCheckResult</code> signaling whether the shards content for the given shard can be deleted.  *  * @param shardId the shard to delete.  * @param indexSettings the shards's relevant {@link IndexSettings}. This is required to access the indexes settings etc.  */ public ShardDeletionCheckResult canDeleteShardContent(ShardId shardId, IndexSettings indexSettings) {     assert shardId.getIndex().equals(indexSettings.getIndex()).     final IndexService indexService = indexService(shardId.getIndex()).     if (nodeEnv.hasNodeFile()) {         final boolean isAllocated = indexService != null && indexService.hasShard(shardId.id()).         if (isAllocated) {             // we are allocated - can't delete the shard             return ShardDeletionCheckResult.STILL_ALLOCATED.         } else if (indexSettings.hasCustomDataPath()) {             // we don't need to delete anything that is not there             return Files.exists(nodeEnv.resolveCustomLocation(indexSettings, shardId)) ? ShardDeletionCheckResult.FOLDER_FOUND_CAN_DELETE : ShardDeletionCheckResult.NO_FOLDER_FOUND.         } else {             // we don't need to delete anything that is not there             return FileSystemUtils.exists(nodeEnv.availableShardPaths(shardId)) ? ShardDeletionCheckResult.FOLDER_FOUND_CAN_DELETE : ShardDeletionCheckResult.NO_FOLDER_FOUND.         }     } else {         return ShardDeletionCheckResult.NO_LOCAL_STORAGE.     } }
false;private;1;6;;private IndexSettings buildIndexSettings(IndexMetaData metaData) {     // actual content.     return new IndexSettings(metaData, settings). }
true;public;2;11;/**  * Adds a pending delete for the given index shard.  */ ;/**  * Adds a pending delete for the given index shard.  */ @Override public void addPendingDelete(ShardId shardId, IndexSettings settings) {     if (shardId == null) {         throw new IllegalArgumentException("shardId must not be null").     }     if (settings == null) {         throw new IllegalArgumentException("settings must not be null").     }     PendingDelete pendingDelete = new PendingDelete(shardId, settings).     addPendingDelete(shardId.getIndex(), pendingDelete). }
true;public;2;4;/**  * Adds a pending delete for the given index.  */ ;/**  * Adds a pending delete for the given index.  */ public void addPendingDelete(Index index, IndexSettings settings) {     PendingDelete pendingDelete = new PendingDelete(index, settings).     addPendingDelete(index, pendingDelete). }
false;private;2;11;;private void addPendingDelete(Index index, PendingDelete pendingDelete) {     synchronized (pendingDeletes) {         List<PendingDelete> list = pendingDeletes.get(index).         if (list == null) {             list = new ArrayList<>().             pendingDeletes.put(index, list).         }         list.add(pendingDelete).         numUncompletedDeletes.incrementAndGet().     } }
false;public;0;9;;@Override public String toString() {     StringBuilder sb = new StringBuilder().     sb.append("[").append(index).append("]").     if (shardId != -1) {         sb.append("[").append(shardId).append("]").     }     return sb.toString(). }
false;public;1;4;;@Override public int compareTo(PendingDelete o) {     return Integer.compare(shardId, o.shardId). }
true;public;3;70;/**  * Processes all pending deletes for the given index. This method will acquire all locks for the given index and will  * process all pending deletes for this index. Pending deletes might occur if the OS doesn't allow deletion of files because  * they are used by a different process ie. on Windows where files might still be open by a virus scanner. On a shared  * filesystem a replica might not have been closed when the primary is deleted causing problems on delete calls so we  * schedule there deletes later.  * @param index the index to process the pending deletes for  * @param timeout the timeout used for processing pending deletes  */ ;/**  * Processes all pending deletes for the given index. This method will acquire all locks for the given index and will  * process all pending deletes for this index. Pending deletes might occur if the OS doesn't allow deletion of files because  * they are used by a different process ie. on Windows where files might still be open by a virus scanner. On a shared  * filesystem a replica might not have been closed when the primary is deleted causing problems on delete calls so we  * schedule there deletes later.  * @param index the index to process the pending deletes for  * @param timeout the timeout used for processing pending deletes  */ @Override public void processPendingDeletes(Index index, IndexSettings indexSettings, TimeValue timeout) throws IOException, InterruptedException, ShardLockObtainFailedException {     logger.debug("{} processing pending deletes", index).     final long startTimeNS = System.nanoTime().     final List<ShardLock> shardLocks = nodeEnv.lockAllForIndex(index, indexSettings, "process pending deletes", timeout.millis()).     int numRemoved = 0.     try {         Map<ShardId, ShardLock> locks = new HashMap<>().         for (ShardLock lock : shardLocks) {             locks.put(lock.getShardId(), lock).         }         final List<PendingDelete> remove.         synchronized (pendingDeletes) {             remove = pendingDeletes.remove(index).         }         if (remove != null && remove.isEmpty() == false) {             numRemoved = remove.size().             // make sure we delete indices first             CollectionUtil.timSort(remove).             // ensure we retry after 10 sec             final long maxSleepTimeMs = 10 * 1000.             long sleepTime = 10.             do {                 if (remove.isEmpty()) {                     break.                 }                 Iterator<PendingDelete> iterator = remove.iterator().                 while (iterator.hasNext()) {                     PendingDelete delete = iterator.next().                     if (delete.deleteIndex) {                         assert delete.shardId == -1.                         logger.debug("{} deleting index store reason [{}]", index, "pending delete").                         try {                             nodeEnv.deleteIndexDirectoryUnderLock(index, indexSettings).                             iterator.remove().                         } catch (IOException ex) {                             logger.debug(() -> new ParameterizedMessage("{} retry pending delete", index), ex).                         }                     } else {                         assert delete.shardId != -1.                         ShardLock shardLock = locks.get(new ShardId(delete.index, delete.shardId)).                         if (shardLock != null) {                             try {                                 deleteShardStore("pending delete", shardLock, delete.settings).                                 iterator.remove().                             } catch (IOException ex) {                                 logger.debug(() -> new ParameterizedMessage("{} retry pending delete", shardLock.getShardId()), ex).                             }                         } else {                             logger.warn("{} no shard lock for pending delete", delete.shardId).                             iterator.remove().                         }                     }                 }                 if (remove.isEmpty() == false) {                     logger.warn("{} still pending deletes present for shards {} - retrying", index, remove.toString()).                     Thread.sleep(sleepTime).                     // increase the sleep time gradually                     sleepTime = Math.min(maxSleepTimeMs, sleepTime * 2).                     logger.debug("{} schedule pending delete retry after {} ms", index, sleepTime).                 }             } while ((System.nanoTime() - startTimeNS) < timeout.nanos()).         }     } finally {         IOUtils.close(shardLocks).         if (numRemoved > 0) {             int remainingUncompletedDeletes = numUncompletedDeletes.addAndGet(-numRemoved).             assert remainingUncompletedDeletes >= 0.         }     } }
false;;1;9;;int numPendingDeletes(Index index) {     synchronized (pendingDeletes) {         List<PendingDelete> deleteList = pendingDeletes.get(index).         if (deleteList == null) {             return 0.         }         return deleteList.size().     } }
true;public;0;3;/**  * Checks if all pending deletes have completed. Used by tests to ensure we don't check directory contents  * while deletion still ongoing. * The reason is that, on Windows, browsing the directory contents can interfere  * with the deletion process and delay it unnecessarily.  */ ;/**  * Checks if all pending deletes have completed. Used by tests to ensure we don't check directory contents  * while deletion still ongoing. * The reason is that, on Windows, browsing the directory contents can interfere  * with the deletion process and delay it unnecessarily.  */ public boolean hasUncompletedPendingDeletes() {     return numUncompletedDeletes.get() > 0. }
false;public;0;3;;public AnalysisRegistry getAnalysis() {     return analysisRegistry. }
false;public;0;26;;@Override public void run() {     long startTimeNS = System.nanoTime().     if (logger.isTraceEnabled()) {         logger.trace("running periodic field data cache cleanup").     }     try {         this.cache.getCache().refresh().     } catch (Exception e) {         logger.warn("Exception during periodic field data cache cleanup:", e).     }     if (logger.isTraceEnabled()) {         logger.trace("periodic field data cache cleanup finished in {} milliseconds", TimeValue.nsecToMSec(System.nanoTime() - startTimeNS)).     }     try {         this.requestCache.cleanCache().     } catch (Exception e) {         logger.warn("Exception during periodic request cache cleanup:", e).     }     // Reschedule itself to run again if not closed     if (closed.get() == false) {         threadPool.schedule(this, interval, ThreadPool.Names.SAME).     } }
false;public;0;4;;@Override public void close() {     closed.compareAndSet(false, true). }
true;public;2;40;/**  * Can the shard request be cached at all?  */ ;/**  * Can the shard request be cached at all?  */ public boolean canCache(ShardSearchRequest request, SearchContext context) {     // may invalidate the scroll for the next query.     if (request.scroll() != null) {         return false.     }     // (think about top_hits aggs or scripts using the score)     if (SearchType.QUERY_THEN_FETCH != context.searchType()) {         return false.     }     IndexSettings settings = context.indexShard().indexSettings().     // if not explicitly set in the request, use the index setting, if not, use the request     if (request.requestCache() == null) {         if (settings.getValue(IndicesRequestCache.INDEX_CACHE_REQUEST_ENABLED_SETTING) == false) {             return false.         } else if (context.size() != 0) {             // is enabled in settings don't cache for requests with size > 0             return false.         }     } else if (request.requestCache() == false) {         return false.     }     // We use the cacheKey of the index reader as a part of a key of the IndicesRequestCache.     assert context.searcher().getIndexReader().getReaderCacheHelper() != null.     // then we can't cache based on "now" key within the search request, as it is not deterministic     if (context.getQueryShardContext().isCacheable() == false) {         return false.     }     return true. }
true;public;3;39;/**  * Loads the cache result, computing it if needed by executing the query phase and otherwise deserializing the cached  * value into the {@link SearchContext#queryResult() context's query result}. The combination of load + compute allows  * to have a single load operation that will cause other requests with the same key to wait till its loaded an reuse  * the same cache.  */ ;/**  * Loads the cache result, computing it if needed by executing the query phase and otherwise deserializing the cached  * value into the {@link SearchContext#queryResult() context's query result}. The combination of load + compute allows  * to have a single load operation that will cause other requests with the same key to wait till its loaded an reuse  * the same cache.  */ public void loadIntoContext(ShardSearchRequest request, SearchContext context, QueryPhase queryPhase) throws Exception {     assert canCache(request, context).     final DirectoryReader directoryReader = context.searcher().getDirectoryReader().     boolean[] loadedFromCache = new boolean[] { true }.     BytesReference bytesReference = cacheShardLevelResult(context.indexShard(), directoryReader, request.cacheKey(), () -> "Shard: " + request.shardId() + "\nSource:\n" + request.source(), out -> {         queryPhase.execute(context).         try {             context.queryResult().writeToNoId(out).         } catch (IOException e) {             throw new AssertionError("Could not serialize response", e).         }         loadedFromCache[0] = false.     }).     if (loadedFromCache[0]) {         // restore the cached query result into the context         final QuerySearchResult result = context.queryResult().         StreamInput in = new NamedWriteableAwareStreamInput(bytesReference.streamInput(), namedWriteableRegistry).         result.readFromWithId(context.id(), in).         result.setSearchShardTarget(context.shardTarget()).     } else if (context.queryResult().searchTimedOut()) {         // we have to invalidate the cache entry if we cached a query result form a request that timed out.         // we can't really throw exceptions in the loading part to signal a timed out search to the outside world since if there are         // multiple requests that wait for the cache entry to be calculated they'd fail all with the same exception.         // instead we all caching such a result for the time being, return the timed out result for all other searches with that cache         // key invalidate the result in the thread that caused the timeout. This will end up to be simpler and eventually correct since         // running a search that times out concurrently will likely timeout again if it's run while we have this `stale` result in the         // cache. One other option is to not cache requests with a timeout at all...         indicesRequestCache.invalidate(new IndexShardCacheEntity(context.indexShard()), directoryReader, request.cacheKey()).         if (logger.isTraceEnabled()) {             logger.trace("Query timed out, invalidating cache entry for request on shard [{}]:\n {}", request.shardId(), request.source()).         }     } }
false;public;0;3;;public ByteSizeValue getTotalIndexingBufferBytes() {     return indexingMemoryController.indexingBufferSize(). }
true;private;5;21;/**  * Cache something calculated at the shard level.  * @param shard the shard this item is part of  * @param reader a reader for this shard. Used to invalidate the cache when there are changes.  * @param cacheKey key for the thing being cached within this shard  * @param loader loads the data into the cache if needed  * @return the contents of the cache or the result of calling the loader  */ ;/**  * Cache something calculated at the shard level.  * @param shard the shard this item is part of  * @param reader a reader for this shard. Used to invalidate the cache when there are changes.  * @param cacheKey key for the thing being cached within this shard  * @param loader loads the data into the cache if needed  * @return the contents of the cache or the result of calling the loader  */ private BytesReference cacheShardLevelResult(IndexShard shard, DirectoryReader reader, BytesReference cacheKey, Supplier<String> cacheKeyRenderer, Consumer<StreamOutput> loader) throws Exception {     IndexShardCacheEntity cacheEntity = new IndexShardCacheEntity(shard).     Supplier<BytesReference> supplier = () -> {         /* BytesStreamOutput allows to pass the expected size but by default uses              * BigArrays.PAGE_SIZE_IN_BYTES which is 16k. A common cached result ie.              * a date histogram with 3 buckets is ~100byte so 16k might be very wasteful              * since we don't shrink to the actual size once we are done serializing.              * By passing 512 as the expected size we will resize the byte array in the stream              * slowly until we hit the page size and don't waste too much memory for small query              * results.*/         final int expectedSizeInBytes = 512.         try (BytesStreamOutput out = new BytesStreamOutput(expectedSizeInBytes)) {             loader.accept(out).             // the memory properly paged instead of having varied sized bytes             return out.bytes().         }     }.     return indicesRequestCache.getOrCompute(cacheEntity, supplier, reader, cacheKey, cacheKeyRenderer). }
false;protected;0;4;;@Override protected ShardRequestCache stats() {     return indexShard.requestCache(). }
false;public;0;4;;@Override public boolean isOpen() {     return indexShard.state() != IndexShardState.CLOSED. }
false;public;0;4;;@Override public Object getCacheIdentity() {     return indexShard. }
false;public;0;6;;@Override public long ramBytesUsed() {     // across many entities     return BASE_RAM_BYTES_USED. }
false;;2;1;;boolean apply(Index index, IndexSettings indexSettings).
false;public;3;13;;public AliasFilter buildAliasFilter(ClusterState state, String index, String... expressions) {     /* Being static, parseAliasFilter doesn't have access to whatever guts it needs to parse a query. Instead of passing in a bunch          * of dependencies we pass in a function that can perform the parsing. */     CheckedFunction<byte[], QueryBuilder, IOException> filterParser = bytes -> {         try (XContentParser parser = XContentFactory.xContent(bytes).createParser(xContentRegistry, LoggingDeprecationHandler.INSTANCE, bytes)) {             return parseInnerQueryBuilder(parser).         }     }.     String[] aliases = indexNameExpressionResolver.filteringAliases(state, index, expressions).     IndexMetaData indexMetaData = state.metaData().index(index).     return new AliasFilter(ShardSearchRequest.parseAliasFilter(filterParser, indexMetaData, aliases), aliases). }
true;public;1;3;/**  * Returns a new {@link QueryRewriteContext} with the given {@code now} provider  */ ;/**  * Returns a new {@link QueryRewriteContext} with the given {@code now} provider  */ public QueryRewriteContext getRewriteContext(LongSupplier nowInMillis) {     return new QueryRewriteContext(xContentRegistry, namedWriteableRegistry, client, nowInMillis). }
true;public;5;11;/**  * Clears the caches for the given shard id if the shard is still allocated on this node  */ ;/**  * Clears the caches for the given shard id if the shard is still allocated on this node  */ public void clearIndexShardCache(ShardId shardId, boolean queryCache, boolean fieldDataCache, boolean requestCache, String... fields) {     final IndexService service = indexService(shardId.getIndex()).     if (service != null) {         IndexShard shard = service.getShardOrNull(shardId.id()).         final boolean clearedAtLeastOne = service.clearCaches(queryCache, fieldDataCache, fields).         if ((requestCache || (clearedAtLeastOne == false && fields.length == 0)) && shard != null) {             indicesRequestCache.clear(new IndexShardCacheEntity(shard)).         }     } }
true;public;0;3;/**  * Returns a function which given an index name, returns a predicate which fields must match in order to be returned by get mappings,  * get index, get field mappings and field capabilities API. Useful to filter the fields that such API return.  * The predicate receives the field name as input argument. In case multiple plugins register a field filter through  * {@link org.elasticsearch.plugins.MapperPlugin#getFieldFilter()}, only fields that match all the registered filters will be  * returned by get mappings, get index, get field mappings and field capabilities API.  */ ;/**  * Returns a function which given an index name, returns a predicate which fields must match in order to be returned by get mappings,  * get index, get field mappings and field capabilities API. Useful to filter the fields that such API return.  * The predicate receives the field name as input argument. In case multiple plugins register a field filter through  * {@link org.elasticsearch.plugins.MapperPlugin#getFieldFilter()}, only fields that match all the registered filters will be  * returned by get mappings, get index, get field mappings and field capabilities API.  */ public Function<String, Predicate<String>> getFieldFilter() {     return mapperRegistry.getFieldFilter(). }
true;public;2;3;/**  * Returns true if the provided field is a registered metadata field (including ones registered via plugins), false otherwise.  */ ;/**  * Returns true if the provided field is a registered metadata field (including ones registered via plugins), false otherwise.  */ public boolean isMetaDataField(Version indexCreatedVersion, String field) {     return mapperRegistry.isMetaDataField(indexCreatedVersion, field). }
true;public,static;2;20;/**  * Checks to see if an operation can be performed without taking the cluster over the cluster-wide shard limit. Adds a deprecation  * warning or returns an error message as appropriate  *  * @param newShards         The number of shards to be added by this operation  * @param state             The current cluster state  * @return If present, an error message to be given as the reason for failing  * an operation. If empty, a sign that the operation is valid.  */ ;/**  * Checks to see if an operation can be performed without taking the cluster over the cluster-wide shard limit. Adds a deprecation  * warning or returns an error message as appropriate  *  * @param newShards         The number of shards to be added by this operation  * @param state             The current cluster state  * @return If present, an error message to be given as the reason for failing  * an operation. If empty, a sign that the operation is valid.  */ public static Optional<String> checkShardLimit(int newShards, ClusterState state) {     Settings theseSettings = state.metaData().settings().     int nodeCount = state.getNodes().getDataNodes().size().     // index creation during cluster setup     if (nodeCount == 0 || newShards < 0) {         return Optional.empty().     }     int maxShardsPerNode = MetaData.SETTING_CLUSTER_MAX_SHARDS_PER_NODE.get(theseSettings).     int maxShardsInCluster = maxShardsPerNode * nodeCount.     int currentOpenShards = state.getMetaData().getTotalOpenIndexShards().     if ((currentOpenShards + newShards) > maxShardsInCluster) {         String errorMessage = "this action would add [" + newShards + "] total shards, but this cluster currently has [" + currentOpenShards + "]/[" + maxShardsInCluster + "] maximum shards open".         return Optional.of(errorMessage).     }     return Optional.empty(). }
