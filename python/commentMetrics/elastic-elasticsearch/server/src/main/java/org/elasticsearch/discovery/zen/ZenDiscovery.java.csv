commented;modifiers;parameterAmount;loc;comment;code
true;protected;4;4;// protected to allow overriding in tests ;// protected to allow overriding in tests protected ZenPing newZenPing(Settings settings, ThreadPool threadPool, TransportService transportService, SeedHostsProvider hostsProvider) {     return new UnicastZenPing(settings, threadPool, transportService, hostsProvider, this). }
false;protected;0;22;;@Override protected void doStart() {     DiscoveryNode localNode = transportService.getLocalNode().     assert localNode != null.     synchronized (stateMutex) {         // set initial state         assert committedState.get() == null.         assert localNode != null.         ClusterState.Builder builder = ClusterState.builder(clusterName).         ClusterState initialState = builder.blocks(ClusterBlocks.builder().addGlobalBlock(STATE_NOT_RECOVERED_BLOCK).addGlobalBlock(noMasterBlockService.getNoMasterBlock())).nodes(DiscoveryNodes.builder().add(localNode).localNodeId(localNode.getId())).build().         committedState.set(initialState).         clusterApplier.setInitialState(initialState).         nodesFD.setLocalNode(localNode).         joinThreadControl.start().     }     zenPing.start(). }
false;public;0;8;;@Override public void startInitialJoin() {     // start the join thread from a cluster state update. See {@link JoinThreadControl} for details.     synchronized (stateMutex) {         // do the join on a different thread, the caller of this method waits for 30s anyhow till it is discovered         joinThreadControl.startNewThreadIfNotRunning().     } }
false;protected;0;33;;@Override protected void doStop() {     joinThreadControl.stop().     masterFD.stop("zen disco stop").     nodesFD.stop().     // stop any ongoing pinging     Releasables.close(zenPing).     DiscoveryNodes nodes = clusterState().nodes().     if (sendLeaveRequest) {         if (nodes.getMasterNode() == null) {         // if we don't know who the master is, nothing to do here         } else if (!nodes.isLocalNodeElectedMaster()) {             try {                 membership.sendLeaveRequestBlocking(nodes.getMasterNode(), nodes.getLocalNode(), TimeValue.timeValueSeconds(1)).             } catch (Exception e) {                 logger.debug(() -> new ParameterizedMessage("failed to send leave request to master [{}]", nodes.getMasterNode()), e).             }         } else {             // we're master -> let other potential master we left and start a master election now rather then wait for masterFD             DiscoveryNode[] possibleMasters = electMaster.nextPossibleMasters(nodes.getNodes().values(), 5).             for (DiscoveryNode possibleMaster : possibleMasters) {                 if (nodes.getLocalNode().equals(possibleMaster)) {                     continue.                 }                 try {                     membership.sendLeaveRequest(nodes.getLocalNode(), possibleMaster).                 } catch (Exception e) {                     logger.debug(() -> new ParameterizedMessage("failed to send leave request from master [{}] to possible master [{}]", nodes.getMasterNode(), possibleMaster), e).                 }             }         }     } }
false;protected;0;4;;@Override protected void doClose() throws IOException {     IOUtils.close(masterFD, nodesFD). }
false;public;0;6;;@Override public ClusterState clusterState() {     ClusterState clusterState = committedState.get().     assert clusterState != null : "accessing cluster state before it is set".     return clusterState. }
false;public;0;6;;@Override public void onNewClusterStateProcessed() {     processedOrFailed.set(true).     publishListener.onResponse(null).     ackListener.onNodeAck(localNode, null). }
false;public;1;8;;@Override public void onNewClusterStateFailed(Exception e) {     processedOrFailed.set(true).     publishListener.onFailure(e).     ackListener.onNodeAck(localNode, e).     logger.warn(() -> new ParameterizedMessage("failed while applying cluster state locally [{}]", clusterChangedEvent.source()), e). }
false;public;3;71;;@Override public void publish(ClusterChangedEvent clusterChangedEvent, ActionListener<Void> publishListener, AckListener ackListener) {     ClusterState newState = clusterChangedEvent.state().     assert newState.getNodes().isLocalNodeElectedMaster() : "Shouldn't publish state when not master " + clusterChangedEvent.source().     try {         // state got changed locally (maybe because another master published to us)         if (clusterChangedEvent.previousState() != this.committedState.get()) {             throw new FailedToCommitClusterStateException("state was mutated while calculating new CS update").         }         pendingStatesQueue.addPending(newState).         publishClusterState.publish(clusterChangedEvent, electMaster.minimumMasterNodes(), ackListener).     } catch (FailedToCommitClusterStateException t) {         // cluster service logs a WARN message         logger.debug("failed to publish cluster state version [{}] (not enough nodes acknowledged, min master nodes [{}])", newState.version(), electMaster.minimumMasterNodes()).         synchronized (stateMutex) {             pendingStatesQueue.failAllStatesAndClear(new ElasticsearchException("failed to publish cluster state")).             rejoin("zen-disco-failed-to-publish").         }         publishListener.onFailure(t).         return.     }     final DiscoveryNode localNode = newState.getNodes().getLocalNode().     final AtomicBoolean processedOrFailed = new AtomicBoolean().     pendingStatesQueue.markAsCommitted(newState.stateUUID(), new PendingClusterStatesQueue.StateProcessedListener() {          @Override         public void onNewClusterStateProcessed() {             processedOrFailed.set(true).             publishListener.onResponse(null).             ackListener.onNodeAck(localNode, null).         }          @Override         public void onNewClusterStateFailed(Exception e) {             processedOrFailed.set(true).             publishListener.onFailure(e).             ackListener.onNodeAck(localNode, e).             logger.warn(() -> new ParameterizedMessage("failed while applying cluster state locally [{}]", clusterChangedEvent.source()), e).         }     }).     synchronized (stateMutex) {         if (clusterChangedEvent.previousState() != this.committedState.get()) {             publishListener.onFailure(new FailedToCommitClusterStateException("local state was mutated while CS update was published to other nodes")).             return.         }         boolean sentToApplier = processNextCommittedClusterState("master " + newState.nodes().getMasterNode() + " committed version [" + newState.version() + "] source [" + clusterChangedEvent.source() + "]").         if (sentToApplier == false && processedOrFailed.get() == false) {             assert false : "cluster state published locally neither processed nor failed: " + newState.             logger.warn("cluster state with version [{}] that is published locally has neither been processed nor failed", newState.version()).             publishListener.onFailure(new FailedToCommitClusterStateException("cluster state that is published locally has neither " + "been processed nor failed")).         }     } }
true;;0;3;/**  * Gets the current set of nodes involved in the node fault detection.  * NB: for testing purposes  */ ;/**  * Gets the current set of nodes involved in the node fault detection.  * NB: for testing purposes  */ Set<DiscoveryNode> getFaultDetectionNodes() {     return nodesFD.getNodes(). }
false;public;0;4;;@Override public DiscoveryStats stats() {     return new DiscoveryStats(pendingStatesQueue.stats(), publishClusterState.stats()). }
false;public;0;3;;public DiscoverySettings getDiscoverySettings() {     return discoverySettings. }
true;public;0;3;/**  * returns true if zen discovery is started and there is a currently a background thread active for (re)joining  * the cluster used for testing.  */ ;/**  * returns true if zen discovery is started and there is a currently a background thread active for (re)joining  * the cluster used for testing.  */ public boolean joiningCluster() {     return joinThreadControl.joinThreadActive(). }
true;public;0;3;// used for testing ;// used for testing public ClusterState[] pendingClusterStates() {     return pendingStatesQueue.pendingClusterStates(). }
false;;0;3;;PendingClusterStatesQueue pendingClusterStatesQueue() {     return pendingStatesQueue. }
false;public;1;6;;@Override public void onElectedAsMaster(ClusterState state) {     synchronized (stateMutex) {         joinThreadControl.markThreadAsDone(currentThread).     } }
false;public;1;7;;@Override public void onFailure(Throwable t) {     logger.trace("failed while waiting for nodes to join, rejoining", t).     synchronized (stateMutex) {         joinThreadControl.markThreadAsDoneAndStartNew(currentThread).     } }
true;private;0;63;/**  * the main function of a join thread. This function is guaranteed to join the cluster  * or spawn a new join thread upon failure to do so.  */ ;/**  * the main function of a join thread. This function is guaranteed to join the cluster  * or spawn a new join thread upon failure to do so.  */ private void innerJoinCluster() {     DiscoveryNode masterNode = null.     final Thread currentThread = Thread.currentThread().     nodeJoinController.startElectionContext().     while (masterNode == null && joinThreadControl.joinThreadActive(currentThread)) {         masterNode = findMaster().     }     if (!joinThreadControl.joinThreadActive(currentThread)) {         logger.trace("thread is no longer in currentJoinThread. Stopping.").         return.     }     if (transportService.getLocalNode().equals(masterNode)) {         // we count as one         final int requiredJoins = Math.max(0, electMaster.minimumMasterNodes() - 1).         logger.debug("elected as master, waiting for incoming joins ([{}] needed)", requiredJoins).         nodeJoinController.waitToBeElectedAsMaster(requiredJoins, masterElectionWaitForJoinsTimeout, new NodeJoinController.ElectionCallback() {              @Override             public void onElectedAsMaster(ClusterState state) {                 synchronized (stateMutex) {                     joinThreadControl.markThreadAsDone(currentThread).                 }             }              @Override             public void onFailure(Throwable t) {                 logger.trace("failed while waiting for nodes to join, rejoining", t).                 synchronized (stateMutex) {                     joinThreadControl.markThreadAsDoneAndStartNew(currentThread).                 }             }         }).     } else {         // process any incoming joins (they will fail because we are not the master)         nodeJoinController.stopElectionContext(masterNode + " elected").         // send join request         final boolean success = joinElectedMaster(masterNode).         synchronized (stateMutex) {             if (success) {                 DiscoveryNode currentMasterNode = this.clusterState().getNodes().getMasterNode().                 if (currentMasterNode == null) {                     // Post 1.3.0, the master should publish a new cluster state before acking our join request. we now should have                     // a valid master.                     logger.debug("no master node is set, despite of join request completing. retrying pings.").                     joinThreadControl.markThreadAsDoneAndStartNew(currentThread).                 } else if (currentMasterNode.equals(masterNode) == false) {                     // update cluster state                     joinThreadControl.stopRunningThreadAndRejoin("master_switched_while_finalizing_join").                 }                 joinThreadControl.markThreadAsDone(currentThread).             } else {                 // failed to join. Try again...                 joinThreadControl.markThreadAsDoneAndStartNew(currentThread).             }         }     } }
true;private;1;43;/**  * Join a newly elected master.  *  * @return true if successful  */ ;/**  * Join a newly elected master.  *  * @return true if successful  */ private boolean joinElectedMaster(DiscoveryNode masterNode) {     try {         // first, make sure we can connect to the master         transportService.connectToNode(masterNode).     } catch (Exception e) {         logger.warn(() -> new ParameterizedMessage("failed to connect to master [{}], retrying...", masterNode), e).         return false.     }     // we retry on illegal state if the master is not yet ready     int joinAttempt = 0.     while (true) {         try {             logger.trace("joining master {}", masterNode).             membership.sendJoinRequestBlocking(masterNode, transportService.getLocalNode(), joinTimeout).             return true.         } catch (Exception e) {             final Throwable unwrap = ExceptionsHelper.unwrapCause(e).             if (unwrap instanceof NotMasterException) {                 if (++joinAttempt == this.joinRetryAttempts) {                     logger.info("failed to send join request to master [{}], reason [{}], tried [{}] times", masterNode, ExceptionsHelper.detailedMessage(e), joinAttempt).                     return false.                 } else {                     logger.trace("master {} failed with [{}]. retrying... (attempts done: [{}])", masterNode, ExceptionsHelper.detailedMessage(e), joinAttempt).                 }             } else {                 if (logger.isTraceEnabled()) {                     logger.trace(() -> new ParameterizedMessage("failed to send join request to master [{}]", masterNode), e).                 } else {                     logger.info("failed to send join request to master [{}], reason [{}]", masterNode, ExceptionsHelper.detailedMessage(e)).                 }                 return false.             }         }         try {             Thread.sleep(this.joinRetryDelay.millis()).         } catch (InterruptedException e) {             Thread.currentThread().interrupt().         }     } }
false;private;1;5;;private void submitRejoin(String source) {     synchronized (stateMutex) {         rejoin(source).     } }
true;;1;5;// visible for testing ;// visible for testing void setCommittedState(ClusterState clusterState) {     synchronized (stateMutex) {         committedState.set(clusterState).     } }
false;private;3;8;;private void removeNode(final DiscoveryNode node, final String source, final String reason) {     masterService.submitStateUpdateTask(source + "(" + node + "), reason(" + reason + ")", new NodeRemovalClusterStateTaskExecutor.Task(node, reason), ClusterStateTaskConfig.build(Priority.IMMEDIATE), nodeRemovalExecutor, nodeRemovalExecutor). }
false;private;1;11;;private void handleLeaveRequest(final DiscoveryNode node) {     if (lifecycleState() != Lifecycle.State.STARTED) {         // not started, ignore a node failure         return.     }     if (localNodeMaster()) {         removeNode(node, "zen-disco-node-left", "left").     } else if (node.equals(clusterState().nodes().getMasterNode())) {         handleMasterGone(node, null, "shut_down").     } }
false;private;2;11;;private void handleNodeFailure(final DiscoveryNode node, final String reason) {     if (lifecycleState() != Lifecycle.State.STARTED) {         // not started, ignore a node failure         return.     }     if (!localNodeMaster()) {         // nothing to do here...         return.     }     removeNode(node, "zen-disco-node-failed", reason). }
false;private;1;19;;private void handleMinimumMasterNodesChanged(final int minimumMasterNodes) {     if (lifecycleState() != Lifecycle.State.STARTED) {         // not started, ignore a node failure         return.     }     final int prevMinimumMasterNode = ZenDiscovery.this.electMaster.minimumMasterNodes().     ZenDiscovery.this.electMaster.minimumMasterNodes(minimumMasterNodes).     if (!localNodeMaster()) {         // We only set the new value. If the master doesn't see enough nodes it will revoke it's mastership.         return.     }     synchronized (stateMutex) {         // check if we have enough master nodes, if not, we need to move into joining the cluster again         if (!electMaster.hasEnoughMasterNodes(committedState.get().nodes())) {             rejoin("not enough master nodes on change of minimum_master_nodes from [" + prevMinimumMasterNode + "] to [" + minimumMasterNodes + "]").         }     } }
false;private;3;20;;private void handleMasterGone(final DiscoveryNode masterNode, final Throwable cause, final String reason) {     if (lifecycleState() != Lifecycle.State.STARTED) {         // not started, ignore a master failure         return.     }     if (localNodeMaster()) {         // we might get this on both a master telling us shutting down, and then the disconnect failure         return.     }     logger.info(() -> new ParameterizedMessage("master_left [{}], reason [{}]", masterNode, reason), cause).     synchronized (stateMutex) {         if (localNodeMaster() == false && masterNode.equals(committedState.get().nodes().getMasterNode())) {             // flush any pending cluster states from old master, so it will not be set as master again             pendingStatesQueue.failAllStatesAndClear(new ElasticsearchException("master left [{}]", reason)).             rejoin("master left (reason = " + reason + ")").         }     } }
false;public;1;8;;@Override public void onSuccess(String source) {     try {         pendingStatesQueue.markAsProcessed(newClusterState).     } catch (Exception e) {         onFailure(source, e).     } }
false;public;2;12;;@Override public void onFailure(String source, Exception e) {     logger.error(() -> new ParameterizedMessage("unexpected failure applying [{}]", reason), e).     try {         // TODO: use cluster state uuid instead of full cluster state so that we don't keep reference to CS around         // for too long.         pendingStatesQueue.markAsFailed(newClusterState, e).     } catch (Exception inner) {         inner.addSuppressed(e).         logger.error(() -> new ParameterizedMessage("unexpected exception while failing [{}]", reason), inner).     } }
true;;1;93;// return true if state has been sent to applier ;// return true if state has been sent to applier boolean processNextCommittedClusterState(String reason) {     assert Thread.holdsLock(stateMutex).     final ClusterState newClusterState = pendingStatesQueue.getNextClusterStateToProcess().     final ClusterState currentState = committedState.get().     // all pending states have been processed     if (newClusterState == null) {         return false.     }     assert newClusterState.nodes().getMasterNode() != null : "received a cluster state without a master".     assert !newClusterState.blocks().hasGlobalBlock(noMasterBlockService.getNoMasterBlock()) : "received a cluster state with a master block".     if (currentState.nodes().isLocalNodeElectedMaster() && newClusterState.nodes().isLocalNodeElectedMaster() == false) {         handleAnotherMaster(currentState, newClusterState.nodes().getMasterNode(), newClusterState.version(), "via a new cluster state").         return false.     }     try {         if (shouldIgnoreOrRejectNewClusterState(logger, currentState, newClusterState)) {             String message = String.format(Locale.ROOT, "rejecting cluster state version [%d] uuid [%s] received from [%s]", newClusterState.version(), newClusterState.stateUUID(), newClusterState.nodes().getMasterNodeId()).             throw new IllegalStateException(message).         }     } catch (Exception e) {         try {             pendingStatesQueue.markAsFailed(newClusterState, e).         } catch (Exception inner) {             inner.addSuppressed(e).             logger.error(() -> new ParameterizedMessage("unexpected exception while failing [{}]", reason), inner).         }         return false.     }     if (currentState.blocks().hasGlobalBlock(noMasterBlockService.getNoMasterBlock())) {         // its a fresh update from the master as we transition from a start of not having a master to having one         logger.debug("got first state from fresh master [{}]", newClusterState.nodes().getMasterNodeId()).     }     if (currentState == newClusterState) {         return false.     }     committedState.set(newClusterState).     // and handleNodeFailure as those check the current state to determine whether the failure is to be handled by this node     if (newClusterState.nodes().isLocalNodeElectedMaster()) {         // update the set of nodes to ping         nodesFD.updateNodesAndPing(newClusterState).     } else {         // check to see that we monitor the correct master of the cluster         if (masterFD.masterNode() == null || !masterFD.masterNode().equals(newClusterState.nodes().getMasterNode())) {             masterFD.restart(newClusterState.nodes().getMasterNode(), "new cluster state received and we are monitoring the wrong master [" + masterFD.masterNode() + "]").         }     }     clusterApplier.onNewClusterState("apply cluster state (from master [" + reason + "])", this::clusterState, new ClusterApplyListener() {          @Override         public void onSuccess(String source) {             try {                 pendingStatesQueue.markAsProcessed(newClusterState).             } catch (Exception e) {                 onFailure(source, e).             }         }          @Override         public void onFailure(String source, Exception e) {             logger.error(() -> new ParameterizedMessage("unexpected failure applying [{}]", reason), e).             try {                 // TODO: use cluster state uuid instead of full cluster state so that we don't keep reference to CS around                 // for too long.                 pendingStatesQueue.markAsFailed(newClusterState, e).             } catch (Exception inner) {                 inner.addSuppressed(e).                 logger.error(() -> new ParameterizedMessage("unexpected exception while failing [{}]", reason), inner).             }         }     }).     return true. }
true;public,static;3;21;/**  * In the case we follow an elected master the new cluster state needs to have the same elected master and  * the new cluster state version needs to be equal or higher than our cluster state version.  * If the first condition fails we reject the cluster state and throw an error.  * If the second condition fails we ignore the cluster state.  */ ;/**  * In the case we follow an elected master the new cluster state needs to have the same elected master and  * the new cluster state version needs to be equal or higher than our cluster state version.  * If the first condition fails we reject the cluster state and throw an error.  * If the second condition fails we ignore the cluster state.  */ public static boolean shouldIgnoreOrRejectNewClusterState(Logger logger, ClusterState currentState, ClusterState newClusterState) {     validateStateIsFromCurrentMaster(logger, currentState.nodes(), newClusterState).     // reject cluster states that are not new from the same master     if (currentState.supersedes(newClusterState) || (newClusterState.nodes().getMasterNodeId().equals(currentState.nodes().getMasterNodeId()) && currentState.version() == newClusterState.version())) {         // if the new state has a smaller version, and it has the same master node, then no need to process it         logger.debug("received a cluster state that is not newer than the current one, ignoring (received {}, current {})", newClusterState.version(), currentState.version()).         return true.     }     // reject older cluster states if we are following a master     if (currentState.nodes().getMasterNodeId() != null && newClusterState.version() < currentState.version()) {         logger.debug("received a cluster state that has a lower version than the current one, ignoring (received {}, current {})", newClusterState.version(), currentState.version()).         return true.     }     return false. }
false;public,static;3;11;;/**  * In the case we follow an elected master the new cluster state needs to have the same elected master  * This method checks for this and throws an exception if needed  */ public static void validateStateIsFromCurrentMaster(Logger logger, DiscoveryNodes currentNodes, ClusterState newClusterState) {     if (currentNodes.getMasterNodeId() == null) {         return.     }     if (!currentNodes.getMasterNodeId().equals(newClusterState.nodes().getMasterNodeId())) {         logger.warn("received a cluster state from a different master than the current one, rejecting (received {}, current {})", newClusterState.nodes().getMasterNode(), currentNodes.getMasterNode()).         throw new IllegalStateException("cluster state from a different master than the current one, rejecting (received " + newClusterState.nodes().getMasterNode() + ", current " + currentNodes.getMasterNode() + ")").     } }
false;;3;26;;void handleJoinRequest(final DiscoveryNode node, final ClusterState state, final MembershipAction.JoinCallback callback) {     if (nodeJoinController == null) {         throw new IllegalStateException("discovery module is not yet started").     } else {         // we do this in a couple of places including the cluster update thread. This one here is really just best effort         // to ensure we fail as fast as possible.         onJoinValidators.stream().forEach(a -> a.accept(node, state)).         if (state.getBlocks().hasGlobalBlock(STATE_NOT_RECOVERED_BLOCK) == false) {             JoinTaskExecutor.ensureMajorVersionBarrier(node.getVersion(), state.getNodes().getMinNodeVersion()).         }         // try and connect to the node, if it fails, we can raise an exception back to the client...         transportService.connectToNode(node).         // node calling the join request         try {             membership.sendValidateJoinRequestBlocking(node, state, joinTimeout).         } catch (Exception e) {             logger.warn(() -> new ParameterizedMessage("failed to validate incoming join request from node [{}]", node), e).             callback.onFailure(new IllegalStateException("failure when sending a validation request to node", e)).             return.         }         nodeJoinController.handleJoinRequest(node, callback).     } }
false;private;0;65;;private DiscoveryNode findMaster() {     logger.trace("starting to ping").     List<ZenPing.PingResponse> fullPingResponses = pingAndWait(pingTimeout).toList().     if (fullPingResponses == null) {         logger.trace("No full ping responses").         return null.     }     if (logger.isTraceEnabled()) {         StringBuilder sb = new StringBuilder().         if (fullPingResponses.size() == 0) {             sb.append(" {none}").         } else {             for (ZenPing.PingResponse pingResponse : fullPingResponses) {                 sb.append("\n\t--> ").append(pingResponse).             }         }         logger.trace("full ping responses:{}", sb).     }     final DiscoveryNode localNode = transportService.getLocalNode().     // add our selves     assert fullPingResponses.stream().map(ZenPing.PingResponse::node).filter(n -> n.equals(localNode)).findAny().isPresent() == false.     fullPingResponses.add(new ZenPing.PingResponse(localNode, null, this.clusterState())).     // filter responses     final List<ZenPing.PingResponse> pingResponses = filterPingResponses(fullPingResponses, masterElectionIgnoreNonMasters, logger).     List<DiscoveryNode> activeMasters = new ArrayList<>().     for (ZenPing.PingResponse pingResponse : pingResponses) {         // any check / verifications from other nodes in ZenDiscover#innerJoinCluster()         if (pingResponse.master() != null && !localNode.equals(pingResponse.master())) {             activeMasters.add(pingResponse.master()).         }     }     // nodes discovered during pinging     List<ElectMasterService.MasterCandidate> masterCandidates = new ArrayList<>().     for (ZenPing.PingResponse pingResponse : pingResponses) {         if (pingResponse.node().isMasterNode()) {             masterCandidates.add(new ElectMasterService.MasterCandidate(pingResponse.node(), pingResponse.getClusterStateVersion())).         }     }     if (activeMasters.isEmpty()) {         if (electMaster.hasEnoughCandidates(masterCandidates)) {             final ElectMasterService.MasterCandidate winner = electMaster.electMaster(masterCandidates).             logger.trace("candidate {} won election", winner).             return winner.getNode().         } else {             // if we don't have enough master nodes, we bail, because there are not enough master to elect from             logger.warn("not enough master nodes discovered during pinging (found [{}], but needed [{}]), pinging again", masterCandidates, electMaster.minimumMasterNodes()).             return null.         }     } else {         assert !activeMasters.contains(localNode) : "local node should never be elected as master when other nodes indicate an active master".         // lets tie break between discovered nodes         return electMaster.tieBreakActiveMasters(activeMasters).     } }
false;static;3;22;;static List<ZenPing.PingResponse> filterPingResponses(List<ZenPing.PingResponse> fullPingResponses, boolean masterElectionIgnoreNonMasters, Logger logger) {     List<ZenPing.PingResponse> pingResponses.     if (masterElectionIgnoreNonMasters) {         pingResponses = fullPingResponses.stream().filter(ping -> ping.node().isMasterNode()).collect(Collectors.toList()).     } else {         pingResponses = fullPingResponses.     }     if (logger.isDebugEnabled()) {         StringBuilder sb = new StringBuilder().         if (pingResponses.isEmpty()) {             sb.append(" {none}").         } else {             for (ZenPing.PingResponse pingResponse : pingResponses) {                 sb.append("\n\t--> ").append(pingResponse).             }         }         logger.debug("filtered ping responses: (ignore_non_masters [{}]){}", masterElectionIgnoreNonMasters, sb).     }     return pingResponses. }
false;protected;1;30;;protected void rejoin(String reason) {     assert Thread.holdsLock(stateMutex).     ClusterState clusterState = committedState.get().     logger.warn("{}, current nodes: {}", reason, clusterState.nodes()).     nodesFD.stop().     masterFD.stop(reason).     // TODO: do we want to force a new thread if we actively removed the master? this is to give a full pinging cycle     // before a decision is made.     joinThreadControl.startNewThreadIfNotRunning().     if (clusterState.nodes().getMasterNodeId() != null) {         // remove block if it already exists before adding new one         assert clusterState.blocks().hasGlobalBlockWithId(noMasterBlockService.getNoMasterBlock().id()) == false : "NO_MASTER_BLOCK should only be added by ZenDiscovery".         ClusterBlocks clusterBlocks = ClusterBlocks.builder().blocks(clusterState.blocks()).addGlobalBlock(noMasterBlockService.getNoMasterBlock()).build().         DiscoveryNodes discoveryNodes = new DiscoveryNodes.Builder(clusterState.nodes()).masterNodeId(null).build().         clusterState = ClusterState.builder(clusterState).blocks(clusterBlocks).nodes(discoveryNodes).build().         committedState.set(clusterState).         // don't wait for state to be applied         clusterApplier.onNewClusterState(reason, this::clusterState, (source, e) -> {         }).     } }
false;private;0;3;;private boolean localNodeMaster() {     return clusterState().nodes().isLocalNodeElectedMaster(). }
false;public;1;4;;@Override public void handleException(TransportException exp) {     logger.warn(() -> new ParameterizedMessage("failed to send rejoin request to [{}]", otherMaster), exp). }
false;private;4;30;;private void handleAnotherMaster(ClusterState localClusterState, final DiscoveryNode otherMaster, long otherClusterStateVersion, String reason) {     assert localClusterState.nodes().isLocalNodeElectedMaster() : "handleAnotherMaster called but current node is not a master".     assert Thread.holdsLock(stateMutex).     if (otherClusterStateVersion > localClusterState.version()) {         rejoin("zen-disco-discovered another master with a new cluster_state [" + otherMaster + "][" + reason + "]").     } else {         // TODO: do this outside mutex         logger.warn("discovered [{}] which is also master but with an older cluster_state, telling [{}] to rejoin the cluster ([{}])", otherMaster, otherMaster, reason).         try {             // make sure we're connected to this node (connect to node does nothing if we're already connected)             // since the network connections are asymmetric, it may be that we received a state but have disconnected from the node             // in the past (after a master failure, for example)             transportService.connectToNode(otherMaster).             transportService.sendRequest(otherMaster, DISCOVERY_REJOIN_ACTION_NAME, new RejoinClusterRequest(localClusterState.nodes().getLocalNodeId()), new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {                  @Override                 public void handleException(TransportException exp) {                     logger.warn(() -> new ParameterizedMessage("failed to send rejoin request to [{}]", otherMaster), exp).                 }             }).         } catch (Exception e) {             logger.warn(() -> new ParameterizedMessage("failed to send rejoin request to [{}]", otherMaster), e).         }     } }
false;private;1;19;;private ZenPing.PingCollection pingAndWait(TimeValue timeout) {     final CompletableFuture<ZenPing.PingCollection> response = new CompletableFuture<>().     try {         zenPing.ping(response::complete, timeout).     } catch (Exception ex) {         // logged later         response.completeExceptionally(ex).     }     try {         return response.get().     } catch (InterruptedException e) {         logger.trace("pingAndWait interrupted").         return new ZenPing.PingCollection().     } catch (ExecutionException e) {         logger.warn("Ping execution failed", e).         return new ZenPing.PingCollection().     } }
false;public;1;5;;@Override public void onIncomingClusterState(ClusterState incomingState) {     validateIncomingState(logger, incomingState, committedState.get()).     pendingStatesQueue.addPending(incomingState). }
false;public;0;4;;@Override public void onNewClusterStateProcessed() {     processedListener.onResponse(null). }
false;public;1;4;;@Override public void onNewClusterStateFailed(Exception e) {     processedListener.onFailure(e). }
false;public;2;21;;@Override public void onClusterStateCommitted(String stateUUID, ActionListener<Void> processedListener) {     final ClusterState state = pendingStatesQueue.markAsCommitted(stateUUID, new PendingClusterStatesQueue.StateProcessedListener() {          @Override         public void onNewClusterStateProcessed() {             processedListener.onResponse(null).         }          @Override         public void onNewClusterStateFailed(Exception e) {             processedListener.onFailure(e).         }     }).     if (state != null) {         synchronized (stateMutex) {             processNextCommittedClusterState("master " + state.nodes().getMasterNode() + " committed version [" + state.version() + "]").         }     } }
true;static;3;26;/**  * does simple sanity check of the incoming cluster state. Throws an exception on rejections.  */ ;/**  * does simple sanity check of the incoming cluster state. Throws an exception on rejections.  */ static void validateIncomingState(Logger logger, ClusterState incomingState, ClusterState lastState) {     final ClusterName incomingClusterName = incomingState.getClusterName().     if (!incomingClusterName.equals(lastState.getClusterName())) {         logger.warn("received cluster state from [{}] which is also master but with a different cluster name [{}]", incomingState.nodes().getMasterNode(), incomingClusterName).         throw new IllegalStateException("received state from a node that is not part of the cluster").     }     if (lastState.nodes().getLocalNode().equals(incomingState.nodes().getLocalNode()) == false) {         logger.warn("received a cluster state from [{}] and not part of the cluster, should not happen", incomingState.nodes().getMasterNode()).         throw new IllegalStateException("received state with a local node that does not match the current local node").     }     if (shouldIgnoreOrRejectNewClusterState(logger, lastState, incomingState)) {         String message = String.format(Locale.ROOT, "rejecting cluster state version [%d] uuid [%s] received from [%s]", incomingState.version(), incomingState.stateUUID(), incomingState.nodes().getMasterNodeId()).         logger.warn(message).         throw new IllegalStateException(message).     } }
false;public;2;4;;@Override public void onJoin(DiscoveryNode node, MembershipAction.JoinCallback callback) {     handleJoinRequest(node, ZenDiscovery.this.clusterState(), callback). }
false;public;1;4;;@Override public void onLeave(DiscoveryNode node) {     handleLeaveRequest(node). }
false;public;2;4;;@Override public void onNodeFailure(DiscoveryNode node, String reason) {     handleNodeFailure(node, reason). }
false;public;1;24;;@Override public void onPingReceived(final NodesFaultDetection.PingRequest pingRequest) {     // means we potentially have two masters in the cluster.     if (!localNodeMaster()) {         pingsWhileMaster.set(0).         return.     }     if (pingsWhileMaster.incrementAndGet() < maxPingsFromAnotherMaster) {         logger.trace("got a ping from another master {}. current ping count: [{}]", pingRequest.masterNode(), pingsWhileMaster.get()).         return.     }     logger.debug("got a ping from another master {}. resolving who should rejoin. current ping count: [{}]", pingRequest.masterNode(), pingsWhileMaster.get()).     synchronized (stateMutex) {         ClusterState currentState = committedState.get().         if (currentState.nodes().isLocalNodeElectedMaster()) {             pingsWhileMaster.set(0).             handleAnotherMaster(currentState, pingRequest.masterNode(), pingRequest.clusterStateVersion(), "node fd ping").         }     } }
false;public;3;4;;@Override public void onMasterFailure(DiscoveryNode masterNode, Throwable cause, String reason) {     handleMasterGone(masterNode, cause, reason). }
false;public;1;5;;@Override public void readFrom(StreamInput in) throws IOException {     super.readFrom(in).     fromNodeId = in.readOptionalString(). }
false;public;1;5;;@Override public void writeTo(StreamOutput out) throws IOException {     super.writeTo(out).     out.writeOptionalString(fromNodeId). }
false;public;3;11;;@Override public void messageReceived(final RejoinClusterRequest request, final TransportChannel channel, Task task) throws Exception {     try {         channel.sendResponse(TransportResponse.Empty.INSTANCE).     } catch (Exception e) {         logger.warn("failed to send response on rejoin cluster request handling", e).     }     synchronized (stateMutex) {         rejoin("received a request to rejoin the cluster from [" + request.fromNodeId + "]").     } }
true;public;0;4;/**  * returns true if join thread control is started and there is currently an active join thread  */ ;/**  * returns true if join thread control is started and there is currently an active join thread  */ public boolean joinThreadActive() {     Thread currentThread = currentJoinThread.get().     return running.get() && currentThread != null && currentThread.isAlive(). }
true;public;1;3;/**  * returns true if join thread control is started and the supplied thread is the currently active joinThread  */ ;/**  * returns true if join thread control is started and the supplied thread is the currently active joinThread  */ public boolean joinThreadActive(Thread joinThread) {     return running.get() && joinThread.equals(currentJoinThread.get()). }
true;public;1;5;/**  * cleans any running joining thread and calls {@link #rejoin}  */ ;/**  * cleans any running joining thread and calls {@link #rejoin}  */ public void stopRunningThreadAndRejoin(String reason) {     assert Thread.holdsLock(stateMutex).     currentJoinThread.set(null).     rejoin(reason). }
false;public;0;20;;@Override public void run() {     Thread currentThread = Thread.currentThread().     if (!currentJoinThread.compareAndSet(null, currentThread)) {         return.     }     while (running.get() && joinThreadActive(currentThread)) {         try {             innerJoinCluster().             return.         } catch (Exception e) {             logger.error("unexpected error while joining cluster, trying again", e).             // leak detection can catch this. In practise no uncaught exception should leak             assert ExceptionsHelper.reThrowIfNotNull(e).         }     } // cleaning the current thread from currentJoinThread is done by explicit calls. }
true;public;0;28;/**  * starts a new joining thread if there is no currently active one and join thread controlling is started  */ ;/**  * starts a new joining thread if there is no currently active one and join thread controlling is started  */ public void startNewThreadIfNotRunning() {     assert Thread.holdsLock(stateMutex).     if (joinThreadActive()) {         return.     }     threadPool.generic().execute(new Runnable() {          @Override         public void run() {             Thread currentThread = Thread.currentThread().             if (!currentJoinThread.compareAndSet(null, currentThread)) {                 return.             }             while (running.get() && joinThreadActive(currentThread)) {                 try {                     innerJoinCluster().                     return.                 } catch (Exception e) {                     logger.error("unexpected error while joining cluster, trying again", e).                     // leak detection can catch this. In practise no uncaught exception should leak                     assert ExceptionsHelper.reThrowIfNotNull(e).                 }             }         // cleaning the current thread from currentJoinThread is done by explicit calls.         }     }). }
true;public;1;7;/**  * marks the given joinThread as completed and makes sure another thread is running (starting one if needed)  * If the given thread is not the currently running join thread, the command is ignored.  */ ;/**  * marks the given joinThread as completed and makes sure another thread is running (starting one if needed)  * If the given thread is not the currently running join thread, the command is ignored.  */ public void markThreadAsDoneAndStartNew(Thread joinThread) {     assert Thread.holdsLock(stateMutex).     if (!markThreadAsDone(joinThread)) {         return.     }     startNewThreadIfNotRunning(). }
true;public;1;4;/**  * marks the given joinThread as completed. Returns false if the supplied thread is not the currently active join thread  */ ;/**  * marks the given joinThread as completed. Returns false if the supplied thread is not the currently active join thread  */ public boolean markThreadAsDone(Thread joinThread) {     assert Thread.holdsLock(stateMutex).     return currentJoinThread.compareAndSet(joinThread, null). }
false;public;0;7;;public void stop() {     running.set(false).     Thread joinThread = currentJoinThread.getAndSet(null).     if (joinThread != null) {         joinThread.interrupt().     } }
false;public;0;3;;public void start() {     running.set(true). }
false;public,final;0;3;;public final Collection<BiConsumer<DiscoveryNode, ClusterState>> getOnJoinValidators() {     return onJoinValidators. }
false;protected;3;13;;@Override protected ClusterTasksResult<Task> getTaskClusterTasksResult(ClusterState currentState, List<Task> tasks, ClusterState remainingNodesClusterState) {     if (electMasterService.hasEnoughMasterNodes(remainingNodesClusterState.nodes()) == false) {         final ClusterTasksResult.Builder<Task> resultBuilder = ClusterTasksResult.<Task>builder().successes(tasks).         final int masterNodes = electMasterService.countMasterNodes(remainingNodesClusterState.nodes()).         rejoin.accept(LoggerMessageFormat.format("not enough master nodes (has [{}], but needed [{}])", masterNodes, electMasterService.minimumMasterNodes())).         return resultBuilder.build(currentState).     } else {         return super.getTaskClusterTasksResult(currentState, tasks, remainingNodesClusterState).     } }
