commented;modifiers;parameterAmount;loc;comment;code
true;public;0;3;/**  * Returns a fresh recovery target to retry recovery from the same source node onto the same shard and using the same listener.  *  * @return a copy of this recovery target  */ ;/**  * Returns a fresh recovery target to retry recovery from the same source node onto the same shard and using the same listener.  *  * @return a copy of this recovery target  */ public RecoveryTarget retryCopy() {     return new RecoveryTarget(indexShard, sourceNode, listener, ensureClusterStateVersionCallback). }
false;public;0;3;;public long recoveryId() {     return recoveryId. }
false;public;0;3;;public ShardId shardId() {     return shardId. }
false;public;0;4;;public IndexShard indexShard() {     ensureRefCount().     return indexShard. }
false;public;0;3;;public DiscoveryNode sourceNode() {     return this.sourceNode. }
false;public;0;3;;public RecoveryState state() {     return indexShard.recoveryState(). }
false;public;0;3;;public CancellableThreads cancellableThreads() {     return cancellableThreads. }
true;public;0;3;/**  * return the last time this RecoveryStatus was used (based on System.nanoTime()  */ ;/**  * return the last time this RecoveryStatus was used (based on System.nanoTime()  */ public long lastAccessTime() {     return lastAccessTime. }
true;public;0;3;/**  * sets the lasAccessTime flag to now  */ ;/**  * sets the lasAccessTime flag to now  */ public void setLastAccessTime() {     lastAccessTime = System.nanoTime(). }
false;public;0;4;;public Store store() {     ensureRefCount().     return store. }
false;public;0;3;;public RecoveryState.Stage stage() {     return state().getStage(). }
true;;1;29;/**  * Closes the current recovery target and waits up to a certain timeout for resources to be freed.  * Returns true if resetting the recovery was successful, false if the recovery target is already cancelled / failed or marked as done.  */ ;/**  * Closes the current recovery target and waits up to a certain timeout for resources to be freed.  * Returns true if resetting the recovery was successful, false if the recovery target is already cancelled / failed or marked as done.  */ boolean resetRecovery(CancellableThreads newTargetCancellableThreads) throws IOException {     if (finished.compareAndSet(false, true)) {         try {             logger.debug("reset of recovery with shard {} and id [{}]", shardId, recoveryId).         } finally {             // release the initial reference. recovery files will be cleaned as soon as ref count goes to zero, potentially now.             decRef().         }         try {             newTargetCancellableThreads.execute(closedLatch::await).         } catch (CancellableThreads.ExecutionCancelledException e) {             logger.trace("new recovery target cancelled for shard {} while waiting on old recovery target with id [{}] to close", shardId, recoveryId).             return false.         }         RecoveryState.Stage stage = indexShard.recoveryState().getStage().         if (indexShard.recoveryState().getPrimary() && (stage == RecoveryState.Stage.FINALIZE || stage == RecoveryState.Stage.DONE)) {             // documents indexed and acknowledged before the reset.             assert stage != RecoveryState.Stage.DONE : "recovery should not have completed when it's being reset".             throw new IllegalStateException("cannot reset recovery as previous attempt made it past finalization step").         }         indexShard.performRecoveryRestart().         return true.     }     return false. }
true;public;1;11;/**  * cancel the recovery. calling this method will clean temporary files and release the store  * unless this object is in use (in which case it will be cleaned once all ongoing users call  * {@link #decRef()}  * <p>  * if {@link #cancellableThreads()} was used, the threads will be interrupted.  */ ;/**  * cancel the recovery. calling this method will clean temporary files and release the store  * unless this object is in use (in which case it will be cleaned once all ongoing users call  * {@link #decRef()}  * <p>  * if {@link #cancellableThreads()} was used, the threads will be interrupted.  */ public void cancel(String reason) {     if (finished.compareAndSet(false, true)) {         try {             logger.debug("recovery canceled (reason: [{}])", reason).             cancellableThreads.cancel(reason).         } finally {             // release the initial reference. recovery files will be cleaned as soon as ref count goes to zero, potentially now             decRef().         }     } }
true;public;2;14;/**  * fail the recovery and call listener  *  * @param e                exception that encapsulating the failure  * @param sendShardFailure indicates whether to notify the master of the shard failure  */ ;/**  * fail the recovery and call listener  *  * @param e                exception that encapsulating the failure  * @param sendShardFailure indicates whether to notify the master of the shard failure  */ public void fail(RecoveryFailedException e, boolean sendShardFailure) {     if (finished.compareAndSet(false, true)) {         try {             notifyListener(e, sendShardFailure).         } finally {             try {                 cancellableThreads.cancel("failed recovery [" + ExceptionsHelper.stackTrace(e) + "]").             } finally {                 // release the initial reference. recovery files will be cleaned as soon as ref count goes to zero, potentially now                 decRef().             }         }     } }
false;public;2;3;;public void notifyListener(RecoveryFailedException e, boolean sendShardFailure) {     listener.onRecoveryFailure(state(), e, sendShardFailure). }
true;public;0;14;/**  * mark the current recovery as done  */ ;/**  * mark the current recovery as done  */ public void markAsDone() {     if (finished.compareAndSet(false, true)) {         assert multiFileWriter.tempFileNames.isEmpty() : "not all temporary files are renamed".         try {             // this might still throw an exception ie. if the shard is CLOSED due to some other event.             // it's safer to decrement the reference in a try finally here.             indexShard.postRecovery("peer recovery done").         } finally {             // release the initial reference. recovery files will be cleaned as soon as ref count goes to zero, potentially now             decRef().         }         listener.onRecoveryDone(state()).     } }
false;protected;0;11;;@Override protected void closeInternal() {     try {         multiFileWriter.close().     } finally {         // free store. increment happens in constructor         store.decRef().         indexShard.recoveryStats().decCurrentAsTarget().         closedLatch.countDown().     } }
false;public;0;4;;@Override public String toString() {     return shardId + " [" + recoveryId + "]". }
false;private;0;6;;private void ensureRefCount() {     if (refCount() <= 0) {         throw new ElasticsearchException("RecoveryStatus is used but it's refcount is 0. Probably a mismatch between incRef/decRef " + "calls").     } }
false;public;3;8;;/**  * Implementation of {@link RecoveryTargetHandler }  */ @Override public void prepareForTranslogOperations(boolean fileBasedRecovery, int totalTranslogOps, ActionListener<Void> listener) {     ActionListener.completeWith(listener, () -> {         state().getTranslog().totalOperations(totalTranslogOps).         indexShard().openEngineAndSkipTranslogRecovery().         return null.     }). }
false;public;2;12;;@Override public void finalizeRecovery(final long globalCheckpoint, ActionListener<Void> listener) {     ActionListener.completeWith(listener, () -> {         final IndexShard indexShard = indexShard().         indexShard.updateGlobalCheckpointOnReplica(globalCheckpoint, "finalizing recovery").         // Persist the global checkpoint.         indexShard.sync().         indexShard.persistRetentionLeases().         indexShard.finalizeRecovery().         return null.     }). }
false;public;1;4;;@Override public void ensureClusterStateVersion(long clusterStateVersion) {     ensureClusterStateVersionCallback.accept(clusterStateVersion). }
false;public;1;4;;@Override public void handoffPrimaryContext(final ReplicationTracker.PrimaryContext primaryContext) {     indexShard.activateWithPrimaryContext(primaryContext). }
false;public;6;52;;@Override public void indexTranslogOperations(final List<Translog.Operation> operations, final int totalTranslogOps, final long maxSeenAutoIdTimestampOnPrimary, final long maxSeqNoOfDeletesOrUpdatesOnPrimary, final RetentionLeases retentionLeases, final ActionListener<Long> listener) {     ActionListener.completeWith(listener, () -> {         final RecoveryState.Translog translog = state().getTranslog().         translog.totalOperations(totalTranslogOps).         assert indexShard().recoveryState() == state().         if (indexShard().state() != IndexShardState.RECOVERING) {             throw new IndexShardNotRecoveringException(shardId, indexShard().state()).         }         /*              * The maxSeenAutoIdTimestampOnPrimary received from the primary is at least the highest auto_id_timestamp from any operation              * will be replayed. Bootstrapping this timestamp here will disable the optimization for original append-only requests              * (source of these operations) replicated via replication. Without this step, we may have duplicate documents if we              * replay these operations first (without timestamp), then optimize append-only requests (with timestamp).              */         indexShard().updateMaxUnsafeAutoIdTimestamp(maxSeenAutoIdTimestampOnPrimary).         /*              * Bootstrap the max_seq_no_of_updates from the primary to make sure that the max_seq_no_of_updates on this replica when              * replaying any of these operations will be at least the max_seq_no_of_updates on the primary when that op was executed on.              */         indexShard().advanceMaxSeqNoOfUpdatesOrDeletes(maxSeqNoOfDeletesOrUpdatesOnPrimary).         /*              * We have to update the retention leases before we start applying translog operations to ensure we are retaining according to              * the policy.              */         indexShard().updateRetentionLeasesOnReplica(retentionLeases).         for (Translog.Operation operation : operations) {             Engine.Result result = indexShard().applyTranslogOperation(operation, Engine.Operation.Origin.PEER_RECOVERY).             if (result.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) {                 throw new MapperException("mapping updates are not allowed [" + operation + "]").             }             if (result.getFailure() != null) {                 if (Assertions.ENABLED) {                     throw new AssertionError("unexpected failure while replicating translog entry", result.getFailure()).                 }                 ExceptionsHelper.reThrowIfNotNull(result.getFailure()).             }         }         // update stats only after all operations completed (to ensure that mapping updates don't mess with stats)         translog.incrementRecoveredOperations(operations.size()).         indexShard().sync().         // roll over / flush / trim if needed         indexShard().afterWriteOperation().         return indexShard().getLocalCheckpoint().     }). }
false;public;5;17;;@Override public void receiveFileInfo(List<String> phase1FileNames, List<Long> phase1FileSizes, List<String> phase1ExistingFileNames, List<Long> phase1ExistingFileSizes, int totalTranslogOps) {     final RecoveryState.Index index = state().getIndex().     for (int i = 0. i < phase1ExistingFileNames.size(). i++) {         index.addFileDetail(phase1ExistingFileNames.get(i), phase1ExistingFileSizes.get(i), true).     }     for (int i = 0. i < phase1FileNames.size(). i++) {         index.addFileDetail(phase1FileNames.get(i), phase1FileSizes.get(i), false).     }     state().getTranslog().totalOperations(totalTranslogOps).     state().getTranslog().totalOperationsOnStart(totalTranslogOps). }
false;public;2;46;;@Override public void cleanFiles(int totalTranslogOps, Store.MetadataSnapshot sourceMetaData) throws IOException {     state().getTranslog().totalOperations(totalTranslogOps).     // first, we go and move files that were created with the recovery id suffix to     // the actual names, its ok if we have a corrupted index here, since we have replicas     // to recover from in case of a full cluster shutdown just when this code executes...     multiFileWriter.renameAllTempFiles().     final Store store = store().     store.incRef().     try {         store.cleanupAndVerify("recovery CleanFilesRequestHandler", sourceMetaData).         if (indexShard.indexSettings().getIndexVersionCreated().before(Version.V_6_0_0_rc1)) {             store.ensureIndexHasHistoryUUID().         }         // TODO: Assign the global checkpoint to the max_seqno of the safe commit if the index version >= 6.2         final String translogUUID = Translog.createEmptyTranslog(indexShard.shardPath().resolveTranslog(), SequenceNumbers.UNASSIGNED_SEQ_NO, shardId, indexShard.getPendingPrimaryTerm()).         store.associateIndexWithNewTranslog(translogUUID).     } catch (CorruptIndexException | IndexFormatTooNewException | IndexFormatTooOldException ex) {         // its content on disk if possible.         try {             try {                 store.removeCorruptionMarker().             } finally {                 // clean up and delete all files                 Lucene.cleanLuceneIndex(store.directory()).             }         } catch (Exception e) {             logger.debug("Failed to clean lucene index", e).             ex.addSuppressed(e).         }         RecoveryFailedException rfe = new RecoveryFailedException(state(), "failed to clean after recovery", ex).         fail(rfe, true).         throw rfe.     } catch (Exception ex) {         RecoveryFailedException rfe = new RecoveryFailedException(state(), "failed to clean after recovery", ex).         fail(rfe, true).         throw rfe.     } finally {         store.decRef().     } }
false;public;6;11;;@Override public void writeFileChunk(StoreFileMetaData fileMetaData, long position, BytesReference content, boolean lastChunk, int totalTranslogOps, ActionListener<Void> listener) {     try {         state().getTranslog().totalOperations(totalTranslogOps).         multiFileWriter.writeFileChunk(fileMetaData, position, content, lastChunk).         listener.onResponse(null).     } catch (Exception e) {         listener.onFailure(e).     } }
true;public;1;3;/**  * Get a temporary name for the provided file name.  */ ;/**  * Get a temporary name for the provided file name.  */ public String getTempNameForFile(String origFile) {     return multiFileWriter.getTempNameForFile(origFile). }
false;;0;3;;Path translogLocation() {     return indexShard().shardPath().resolveTranslog(). }
