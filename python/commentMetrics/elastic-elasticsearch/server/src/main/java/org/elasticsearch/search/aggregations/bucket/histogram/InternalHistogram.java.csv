commented;modifiers;parameterAmount;loc;comment;code
false;public;1;12;;@Override public boolean equals(Object obj) {     if (obj == null || obj.getClass() != Bucket.class) {         return false.     }     Bucket that = (Bucket) obj.     // they are already stored and tested on the InternalHistogram object     return key == that.key && docCount == that.docCount && Objects.equals(aggregations, that.aggregations). }
false;public;0;4;;@Override public int hashCode() {     return Objects.hash(getClass(), key, docCount, aggregations). }
false;public;1;6;;@Override public void writeTo(StreamOutput out) throws IOException {     out.writeDouble(key).     out.writeVLong(docCount).     aggregations.writeTo(out). }
false;public;0;4;;@Override public String getKeyAsString() {     return format.format(key).toString(). }
false;public;0;4;;@Override public Object getKey() {     return key. }
false;public;0;4;;@Override public long getDocCount() {     return docCount. }
false;public;0;4;;@Override public Aggregations getAggregations() {     return aggregations. }
false;;2;10;;Bucket reduce(List<Bucket> buckets, ReduceContext context) {     List<InternalAggregations> aggregations = new ArrayList<>(buckets.size()).     long docCount = 0.     for (Bucket bucket : buckets) {         docCount += bucket.docCount.         aggregations.add((InternalAggregations) bucket.getAggregations()).     }     InternalAggregations aggs = InternalAggregations.reduce(aggregations, context).     return new InternalHistogram.Bucket(key, docCount, keyed, format, aggs). }
false;public;2;17;;@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {     String keyAsString = format.format(key).toString().     if (keyed) {         builder.startObject(keyAsString).     } else {         builder.startObject().     }     if (format != DocValueFormat.RAW) {         builder.field(CommonFields.KEY_AS_STRING.getPreferredName(), keyAsString).     }     builder.field(CommonFields.KEY.getPreferredName(), key).     builder.field(CommonFields.DOC_COUNT.getPreferredName(), docCount).     aggregations.toXContentInternal(builder, params).     builder.endObject().     return builder. }
false;public;1;4;;@Override public int compareKey(Bucket other) {     return Double.compare(key, other.key). }
false;public;0;3;;public DocValueFormat getFormatter() {     return format. }
false;public;0;3;;public boolean getKeyed() {     return keyed. }
false;public;1;7;;public void writeTo(StreamOutput out) throws IOException {     out.writeDouble(interval).     out.writeDouble(offset).     out.writeDouble(minBound).     out.writeDouble(maxBound).     subAggregations.writeTo(out). }
false;public;1;12;;@Override public boolean equals(Object obj) {     if (obj == null || getClass() != obj.getClass()) {         return false.     }     EmptyBucketInfo that = (EmptyBucketInfo) obj.     return interval == that.interval && offset == that.offset && minBound == that.minBound && maxBound == that.maxBound && Objects.equals(subAggregations, that.subAggregations). }
false;public;0;4;;@Override public int hashCode() {     return Objects.hash(getClass(), interval, offset, minBound, maxBound, subAggregations). }
false;protected;1;11;;@Override protected void doWriteTo(StreamOutput out) throws IOException {     InternalOrder.Streams.writeHistogramOrder(order, out, false).     out.writeVLong(minDocCount).     if (minDocCount == 0) {         emptyBucketInfo.writeTo(out).     }     out.writeNamedWriteable(format).     out.writeBoolean(keyed).     out.writeList(buckets). }
false;public;0;4;;@Override public String getWriteableName() {     return HistogramAggregationBuilder.NAME. }
false;public;0;4;;@Override public List<InternalHistogram.Bucket> getBuckets() {     return Collections.unmodifiableList(buckets). }
false;;0;3;;long getMinDocCount() {     return minDocCount. }
false;;0;3;;BucketOrder getOrder() {     return order. }
false;public;1;4;;@Override public InternalHistogram create(List<Bucket> buckets) {     return new InternalHistogram(name, buckets, order, minDocCount, emptyBucketInfo, format, keyed, pipelineAggregators(), metaData). }
false;public;2;4;;@Override public Bucket createBucket(InternalAggregations aggregations, Bucket prototype) {     return new Bucket(prototype.key, prototype.docCount, prototype.keyed, prototype.format, aggregations). }
false;protected;2;4;;@Override protected boolean lessThan(IteratorAndCurrent a, IteratorAndCurrent b) {     return Double.compare(a.current.key, b.current.key) < 0. }
false;private;2;63;;private List<Bucket> reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext) {     final PriorityQueue<IteratorAndCurrent> pq = new PriorityQueue<IteratorAndCurrent>(aggregations.size()) {          @Override         protected boolean lessThan(IteratorAndCurrent a, IteratorAndCurrent b) {             return Double.compare(a.current.key, b.current.key) < 0.         }     }.     for (InternalAggregation aggregation : aggregations) {         InternalHistogram histogram = (InternalHistogram) aggregation.         if (histogram.buckets.isEmpty() == false) {             pq.add(new IteratorAndCurrent(histogram.buckets.iterator())).         }     }     List<Bucket> reducedBuckets = new ArrayList<>().     if (pq.size() > 0) {         // list of buckets coming from different shards that have the same key         List<Bucket> currentBuckets = new ArrayList<>().         double key = pq.top().current.key.         do {             final IteratorAndCurrent top = pq.top().             if (Double.compare(top.current.key, key) != 0) {                 // The key changes, reduce what we already buffered and reset the buffer for current buckets.                 // Using Double.compare instead of != to handle NaN correctly.                 final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceContext).                 if (reduced.getDocCount() >= minDocCount || reduceContext.isFinalReduce() == false) {                     reduceContext.consumeBucketsAndMaybeBreak(1).                     reducedBuckets.add(reduced).                 } else {                     reduceContext.consumeBucketsAndMaybeBreak(-countInnerBucket(reduced)).                 }                 currentBuckets.clear().                 key = top.current.key.             }             currentBuckets.add(top.current).             if (top.iterator.hasNext()) {                 final Bucket next = top.iterator.next().                 assert Double.compare(next.key, top.current.key) > 0 : "shards must return data sorted by key".                 top.current = next.                 pq.updateTop().             } else {                 pq.pop().             }         } while (pq.size() > 0).         if (currentBuckets.isEmpty() == false) {             final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceContext).             if (reduced.getDocCount() >= minDocCount || reduceContext.isFinalReduce() == false) {                 reduceContext.consumeBucketsAndMaybeBreak(1).                 reducedBuckets.add(reduced).             } else {                 reduceContext.consumeBucketsAndMaybeBreak(-countInnerBucket(reduced)).             }         }     }     return reducedBuckets. }
false;private;1;3;;private double nextKey(double key) {     return round(key + emptyBucketInfo.interval + emptyBucketInfo.interval / 2). }
false;private;1;3;;private double round(double key) {     return Math.floor((key - emptyBucketInfo.offset) / emptyBucketInfo.interval) * emptyBucketInfo.interval + emptyBucketInfo.offset. }
false;private;2;48;;private void addEmptyBuckets(List<Bucket> list, ReduceContext reduceContext) {     ListIterator<Bucket> iter = list.listIterator().     // first adding all the empty buckets *before* the actual data (based on th extended_bounds.min the user requested)     InternalAggregations reducedEmptySubAggs = InternalAggregations.reduce(Collections.singletonList(emptyBucketInfo.subAggregations), reduceContext).     if (iter.hasNext() == false) {         // fill with empty buckets         for (double key = round(emptyBucketInfo.minBound). key <= emptyBucketInfo.maxBound. key = nextKey(key)) {             reduceContext.consumeBucketsAndMaybeBreak(1).             iter.add(new Bucket(key, 0, keyed, format, reducedEmptySubAggs)).         }     } else {         Bucket first = list.get(iter.nextIndex()).         if (Double.isFinite(emptyBucketInfo.minBound)) {             // fill with empty buckets until the first key             for (double key = round(emptyBucketInfo.minBound). key < first.key. key = nextKey(key)) {                 reduceContext.consumeBucketsAndMaybeBreak(1).                 iter.add(new Bucket(key, 0, keyed, format, reducedEmptySubAggs)).             }         }         // now adding the empty buckets within the actual data,         // e.g. if the data series is [1,2,3,7] there're 3 empty buckets that will be created for 4,5,6         Bucket lastBucket = null.         do {             Bucket nextBucket = list.get(iter.nextIndex()).             if (lastBucket != null) {                 double key = nextKey(lastBucket.key).                 while (key < nextBucket.key) {                     reduceContext.consumeBucketsAndMaybeBreak(1).                     iter.add(new Bucket(key, 0, keyed, format, reducedEmptySubAggs)).                     key = nextKey(key).                 }                 assert key == nextBucket.key || Double.isNaN(nextBucket.key) : "key: " + key + ", nextBucket.key: " + nextBucket.key.             }             lastBucket = iter.next().         } while (iter.hasNext()).         // finally, adding the empty buckets *after* the actual data (based on the extended_bounds.max requested by the user)         for (double key = nextKey(lastBucket.key). key <= emptyBucketInfo.maxBound. key = nextKey(key)) {             reduceContext.consumeBucketsAndMaybeBreak(1).             iter.add(new Bucket(key, 0, keyed, format, reducedEmptySubAggs)).         }     } }
false;public;2;22;;@Override public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) {     List<Bucket> reducedBuckets = reduceBuckets(aggregations, reduceContext).     if (reduceContext.isFinalReduce()) {         if (minDocCount == 0) {             addEmptyBuckets(reducedBuckets, reduceContext).         }         if (InternalOrder.isKeyDesc(order)) {             // we just need to reverse here...             List<Bucket> reverse = new ArrayList<>(reducedBuckets).             Collections.reverse(reverse).             reducedBuckets = reverse.         } else if (InternalOrder.isKeyAsc(order) == false) {             // nothing to do when sorting by key ascending, as data is already sorted since shards return             // sorted buckets and the merge-sort performed by reduceBuckets maintains order.             // otherwise, sorted by compound order or sub-aggregation, we need to fall back to a costly n*log(n) sort             CollectionUtil.introSort(reducedBuckets, order.comparator(null)).         }     }     return new InternalHistogram(getName(), reducedBuckets, order, minDocCount, emptyBucketInfo, format, keyed, pipelineAggregators(), getMetaData()). }
false;public;2;17;;@Override public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {     if (keyed) {         builder.startObject(CommonFields.BUCKETS.getPreferredName()).     } else {         builder.startArray(CommonFields.BUCKETS.getPreferredName()).     }     for (Bucket bucket : buckets) {         bucket.toXContent(builder, params).     }     if (keyed) {         builder.endObject().     } else {         builder.endArray().     }     return builder. }
false;public;1;4;;// HistogramFactory method impls @Override public Number getKey(MultiBucketsAggregation.Bucket bucket) {     return ((Bucket) bucket).key. }
false;public;1;4;;@Override public Number nextKey(Number key) {     return nextKey(key.doubleValue()). }
false;public;1;11;;@Override public InternalAggregation createAggregation(List<MultiBucketsAggregation.Bucket> buckets) {     // convert buckets to the right type     List<Bucket> buckets2 = new ArrayList<>(buckets.size()).     for (Object b : buckets) {         buckets2.add((Bucket) b).     }     buckets2 = Collections.unmodifiableList(buckets2).     return new InternalHistogram(name, buckets2, order, minDocCount, emptyBucketInfo, format, keyed, pipelineAggregators(), getMetaData()). }
false;public;3;4;;@Override public Bucket createBucket(Number key, long docCount, InternalAggregations aggregations) {     return new Bucket(key.doubleValue(), docCount, keyed, format, aggregations). }
false;protected;1;10;;@Override protected boolean doEquals(Object obj) {     InternalHistogram that = (InternalHistogram) obj.     return Objects.equals(buckets, that.buckets) && Objects.equals(emptyBucketInfo, that.emptyBucketInfo) && Objects.equals(format, that.format) && Objects.equals(keyed, that.keyed) && Objects.equals(minDocCount, that.minDocCount) && Objects.equals(order, that.order). }
false;protected;0;4;;@Override protected int doHashCode() {     return Objects.hash(buckets, emptyBucketInfo, format, keyed, minDocCount, order). }
