commented;modifiers;parameterAmount;loc;comment;code
false;public;2;4;;@Override public int compare(TextFragment o1, TextFragment o2) {     return Math.round(o2.getScore() - o1.getScore()). }
false;public;1;133;;@Override public HighlightField highlight(HighlighterContext highlighterContext) {     SearchContextHighlight.Field field = highlighterContext.field.     SearchContext context = highlighterContext.context.     FetchSubPhase.HitContext hitContext = highlighterContext.hitContext.     MappedFieldType fieldType = highlighterContext.fieldType.     Encoder encoder = field.fieldOptions().encoder().equals("html") ? HighlightUtils.Encoders.HTML : HighlightUtils.Encoders.DEFAULT.     if (!hitContext.cache().containsKey(CACHE_KEY)) {         hitContext.cache().put(CACHE_KEY, new HashMap<>()).     }     @SuppressWarnings("unchecked")     Map<MappedFieldType, org.apache.lucene.search.highlight.Highlighter> cache = (Map<MappedFieldType, org.apache.lucene.search.highlight.Highlighter>) hitContext.cache().get(CACHE_KEY).     org.apache.lucene.search.highlight.Highlighter entry = cache.get(fieldType).     if (entry == null) {         QueryScorer queryScorer = new CustomQueryScorer(highlighterContext.query, field.fieldOptions().requireFieldMatch() ? fieldType.name() : null).         queryScorer.setExpandMultiTermQuery(true).         Fragmenter fragmenter.         if (field.fieldOptions().numberOfFragments() == 0) {             fragmenter = new NullFragmenter().         } else if (field.fieldOptions().fragmenter() == null) {             fragmenter = new SimpleSpanFragmenter(queryScorer, field.fieldOptions().fragmentCharSize()).         } else if ("simple".equals(field.fieldOptions().fragmenter())) {             fragmenter = new SimpleFragmenter(field.fieldOptions().fragmentCharSize()).         } else if ("span".equals(field.fieldOptions().fragmenter())) {             fragmenter = new SimpleSpanFragmenter(queryScorer, field.fieldOptions().fragmentCharSize()).         } else {             throw new IllegalArgumentException("unknown fragmenter option [" + field.fieldOptions().fragmenter() + "] for the field [" + highlighterContext.fieldName + "]").         }         Formatter formatter = new SimpleHTMLFormatter(field.fieldOptions().preTags()[0], field.fieldOptions().postTags()[0]).         entry = new org.apache.lucene.search.highlight.Highlighter(formatter, encoder, queryScorer).         entry.setTextFragmenter(fragmenter).         // always highlight across all data         entry.setMaxDocCharsToAnalyze(Integer.MAX_VALUE).         cache.put(fieldType, entry).     }     // a HACK to make highlighter do highlighting, even though its using the single frag list builder     int numberOfFragments = field.fieldOptions().numberOfFragments() == 0 ? 1 : field.fieldOptions().numberOfFragments().     ArrayList<TextFragment> fragsList = new ArrayList<>().     List<Object> textsToHighlight.     Analyzer analyzer = HighlightUtils.getAnalyzer(context.mapperService().documentMapper(hitContext.hit().getType()), fieldType).     final int maxAnalyzedOffset = context.indexShard().indexSettings().getHighlightMaxAnalyzedOffset().     try {         textsToHighlight = HighlightUtils.loadFieldValues(field, fieldType, context, hitContext).         for (Object textToHighlight : textsToHighlight) {             String text = convertFieldValue(fieldType, textToHighlight).             if (text.length() > maxAnalyzedOffset) {                 throw new IllegalArgumentException("The length of [" + highlighterContext.fieldName + "] field of [" + hitContext.hit().getId() + "] doc of [" + context.indexShard().shardId().getIndexName() + "] index " + "has exceeded [" + maxAnalyzedOffset + "] - maximum allowed to be analyzed for highlighting. " + "This maximum can be set by changing the [" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() + "] index level setting. " + "For large texts, indexing with offsets or term vectors, and highlighting " + "with unified or fvh highlighter is recommended!").             }             try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {                 if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {                     // can't perform highlighting if the stream has no terms (binary token stream) or no offsets                     continue.                 }                 TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments).                 for (TextFragment bestTextFragment : bestTextFragments) {                     if (bestTextFragment != null && bestTextFragment.getScore() > 0) {                         fragsList.add(bestTextFragment).                     }                 }             }         }     } catch (Exception e) {         if (ExceptionsHelper.unwrap(e, BytesRefHash.MaxBytesLengthExceededException.class) != null) {             // the plain highlighter will parse the source and try to analyze it.             return null.         } else {             throw new FetchPhaseExecutionException(context, "Failed to highlight field [" + highlighterContext.fieldName + "]", e).         }     }     if (field.fieldOptions().scoreOrdered()) {         CollectionUtil.introSort(fragsList, new Comparator<TextFragment>() {              @Override             public int compare(TextFragment o1, TextFragment o2) {                 return Math.round(o2.getScore() - o1.getScore()).             }         }).     }     String[] fragments.     // number_of_fragments is set to 0 but we have a multivalued field     if (field.fieldOptions().numberOfFragments() == 0 && textsToHighlight.size() > 1 && fragsList.size() > 0) {         fragments = new String[fragsList.size()].         for (int i = 0. i < fragsList.size(). i++) {             fragments[i] = fragsList.get(i).toString().         }     } else {         // refine numberOfFragments if needed         numberOfFragments = fragsList.size() < numberOfFragments ? fragsList.size() : numberOfFragments.         fragments = new String[numberOfFragments].         for (int i = 0. i < fragments.length. i++) {             fragments[i] = fragsList.get(i).toString().         }     }     if (fragments.length > 0) {         return new HighlightField(highlighterContext.fieldName, Text.convertFromStringArray(fragments)).     }     int noMatchSize = highlighterContext.field.fieldOptions().noMatchSize().     if (noMatchSize > 0 && textsToHighlight.size() > 0) {         // Pull an excerpt from the beginning of the string but make sure to split the string on a term boundary.         String fieldContents = textsToHighlight.get(0).toString().         int end.         try {             end = findGoodEndForNoHighlightExcerpt(noMatchSize, analyzer, fieldType.name(), fieldContents).         } catch (Exception e) {             throw new FetchPhaseExecutionException(context, "Failed to highlight field [" + highlighterContext.fieldName + "]", e).         }         if (end > 0) {             return new HighlightField(highlighterContext.fieldName, new Text[] { new Text(fieldContents.substring(0, end)) }).         }     }     return null. }
false;public;1;4;;@Override public boolean canHighlight(MappedFieldType fieldType) {     return true. }
false;private,static;4;25;;private static int findGoodEndForNoHighlightExcerpt(int noMatchSize, Analyzer analyzer, String fieldName, String contents) throws IOException {     try (TokenStream tokenStream = analyzer.tokenStream(fieldName, contents)) {         if (!tokenStream.hasAttribute(OffsetAttribute.class)) {             // Can't split on term boundaries without offsets             return -1.         }         int end = -1.         tokenStream.reset().         while (tokenStream.incrementToken()) {             OffsetAttribute attr = tokenStream.getAttribute(OffsetAttribute.class).             if (attr.endOffset() >= noMatchSize) {                 // Jump to the end of this token if it wouldn't put us past the boundary                 if (attr.endOffset() == noMatchSize) {                     end = noMatchSize.                 }                 return end.             }             end = attr.endOffset().         }         tokenStream.end().         // We've exhausted the token stream so we should just highlight everything.         return end.     } }
