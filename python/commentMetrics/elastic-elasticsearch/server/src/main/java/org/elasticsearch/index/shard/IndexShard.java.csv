commented;modifiers;parameterAmount;loc;comment;code
false;;0;3;;Runnable getGlobalCheckpointSyncer() {     return globalCheckpointSyncer. }
false;public;1;4;;@Override public void onUse(Query query) { }
false;public;1;4;;@Override public boolean shouldCache(Query query) {     return true. }
false;public;0;3;;public ThreadPool getThreadPool() {     return this.threadPool. }
false;public;0;3;;public Store store() {     return this.store. }
true;public;0;3;/**  * Return the sort order of this index, or null if the index has no sort.  */ ;/**  * Return the sort order of this index, or null if the index has no sort.  */ public Sort getIndexSort() {     return indexSortSupplier.get(). }
false;public;0;3;;public ShardGetService getService() {     return this.getService. }
false;public;0;3;;public ShardBitsetFilterCache shardBitsetFilterCache() {     return shardBitsetFilterCache. }
false;public;0;3;;public MapperService mapperService() {     return mapperService. }
false;public;0;3;;public SearchOperationListener getSearchOperationListener() {     return this.searchOperationListener. }
false;public;0;3;;public ShardIndexWarmerService warmerService() {     return this.shardWarmerService. }
false;public;0;3;;public ShardRequestCache requestCache() {     return this.requestCacheStats. }
false;public;0;3;;public ShardFieldData fieldData() {     return this.shardFieldData. }
true;public;0;3;/**  * USE THIS METHOD WITH CARE!  * Returns the primary term the index shard is supposed to be on. In case of primary promotion or when a replica learns about  * a new term due to a new primary, the term that's exposed here will not be the term that the shard internally uses to assign  * to operations. The shard will auto-correct its internal operation term, but this might take time.  * See {@link org.elasticsearch.cluster.metadata.IndexMetaData#primaryTerm(int)}  */ ;/**  * USE THIS METHOD WITH CARE!  * Returns the primary term the index shard is supposed to be on. In case of primary promotion or when a replica learns about  * a new term due to a new primary, the term that's exposed here will not be the term that the shard internally uses to assign  * to operations. The shard will auto-correct its internal operation term, but this might take time.  * See {@link org.elasticsearch.cluster.metadata.IndexMetaData#primaryTerm(int)}  */ public long getPendingPrimaryTerm() {     return this.pendingPrimaryTerm. }
true;public;0;3;/**  * Returns the primary term that is currently being used to assign to operations  */ ;/**  * Returns the primary term that is currently being used to assign to operations  */ public long getOperationPrimaryTerm() {     return replicationTracker.getOperationPrimaryTerm(). }
true;public;0;4;/**  * Returns the latest cluster routing entry received with this shard.  */ ;/**  * Returns the latest cluster routing entry received with this shard.  */ @Override public ShardRouting routingEntry() {     return this.shardRouting. }
false;public;0;3;;public QueryCachingPolicy getQueryCachingPolicy() {     return cachingPolicy. }
false;public;1;8;;@Override public void onResponse(ResyncTask resyncTask) {     logger.info("primary-replica resync completed with {} operations", resyncTask.getResyncedOperations()).     boolean resyncCompleted = primaryReplicaResyncInProgress.compareAndSet(true, false).     assert resyncCompleted : "primary-replica resync finished but was not started". }
false;public;1;11;;@Override public void onFailure(Exception e) {     boolean resyncCompleted = primaryReplicaResyncInProgress.compareAndSet(true, false).     assert resyncCompleted : "primary-replica resync finished but was not started".     if (state == IndexShardState.CLOSED) {     // ignore, shutting down     } else {         failShard("exception during primary-replica resync", e).     } }
false;public;7;166;;@Override public void updateShardState(final ShardRouting newRouting, final long newPrimaryTerm, final BiConsumer<IndexShard, ActionListener<ResyncTask>> primaryReplicaSyncer, final long applyingClusterStateVersion, final Set<String> inSyncAllocationIds, final IndexShardRoutingTable routingTable, final Set<String> pre60AllocationIds) throws IOException {     final ShardRouting currentRouting.     synchronized (mutex) {         currentRouting = this.shardRouting.         if (!newRouting.shardId().equals(shardId())) {             throw new IllegalArgumentException("Trying to set a routing entry with shardId " + newRouting.shardId() + " on a shard with shardId " + shardId()).         }         if ((currentRouting == null || newRouting.isSameAllocation(currentRouting)) == false) {             throw new IllegalArgumentException("Trying to set a routing entry with a different allocation. Current " + currentRouting + ", new " + newRouting).         }         if (currentRouting != null && currentRouting.primary() && newRouting.primary() == false) {             throw new IllegalArgumentException("illegal state: trying to move shard from primary mode to replica mode. Current " + currentRouting + ", new " + newRouting).         }         if (newRouting.primary()) {             replicationTracker.updateFromMaster(applyingClusterStateVersion, inSyncAllocationIds, routingTable, pre60AllocationIds).         }         if (state == IndexShardState.POST_RECOVERY && newRouting.active()) {             assert currentRouting.active() == false : "we are in POST_RECOVERY, but our shard routing is active " + currentRouting.             assert currentRouting.isRelocationTarget() == false || currentRouting.primary() == false || replicationTracker.isPrimaryMode() : "a primary relocation is completed by the master, but primary mode is not active " + currentRouting.             changeState(IndexShardState.STARTED, "global state is [" + newRouting.state() + "]").         } else if (currentRouting.primary() && currentRouting.relocating() && replicationTracker.isRelocated() && (newRouting.relocating() == false || newRouting.equalsIgnoringMetaData(currentRouting) == false)) {             // reactivate primary mode without risking two active primaries.             throw new IndexShardRelocatedException(shardId(), "Shard is marked as relocated, cannot safely move to state " + newRouting.state()).         }         assert newRouting.active() == false || state == IndexShardState.STARTED || state == IndexShardState.CLOSED : "routing is active, but local shard state isn't. routing: " + newRouting + ", local state: " + state.         persistMetadata(path, indexSettings, newRouting, currentRouting, logger).         final CountDownLatch shardStateUpdated = new CountDownLatch(1).         if (newRouting.primary()) {             if (newPrimaryTerm == pendingPrimaryTerm) {                 if (currentRouting.initializing() && currentRouting.isRelocationTarget() == false && newRouting.active()) {                     // the master started a recovering primary, activate primary mode.                     replicationTracker.activatePrimaryMode(getLocalCheckpoint()).                 }             } else {                 assert currentRouting.primary() == false : "term is only increased as part of primary promotion".                 /* Note that due to cluster state batching an initializing primary shard term can failed and re-assigned                      * in one state causing it's term to be incremented. Note that if both current shard state and new                      * shard state are initializing, we could replace the current shard and reinitialize it. It is however                      * possible that this shard is being started. This can happen if:                      * 1) Shard is post recovery and sends shard started to the master                      * 2) Node gets disconnected and rejoins                      * 3) Master assigns the shard back to the node                      * 4) Master processes the shard started and starts the shard                      * 5) The node process the cluster state where the shard is both started and primary term is incremented.                      *                      * We could fail the shard in that case, but this will cause it to be removed from the insync allocations list                      * potentially preventing re-allocation.                      */                 assert newRouting.initializing() == false : "a started primary shard should never update its term. " + "shard " + newRouting + ", " + "current term [" + pendingPrimaryTerm + "], " + "new term [" + newPrimaryTerm + "]".                 assert newPrimaryTerm > pendingPrimaryTerm : "primary terms can only go up. current term [" + pendingPrimaryTerm + "], new term [" + newPrimaryTerm + "]".                 /*                      * Before this call returns, we are guaranteed that all future operations are delayed and so this happens before we                      * increment the primary term. The latch is needed to ensure that we do not unblock operations before the primary                      * term is incremented.                      */                 // to prevent primary relocation handoff while resync is not completed                 boolean resyncStarted = primaryReplicaResyncInProgress.compareAndSet(false, true).                 if (resyncStarted == false) {                     throw new IllegalStateException("cannot start resync while it's already in progress").                 }                 bumpPrimaryTerm(newPrimaryTerm, () -> {                     shardStateUpdated.await().                     assert pendingPrimaryTerm == newPrimaryTerm : "shard term changed on primary. expected [" + newPrimaryTerm + "] but was [" + pendingPrimaryTerm + "]" + ", current routing: " + currentRouting + ", new routing: " + newRouting.                     assert getOperationPrimaryTerm() == newPrimaryTerm.                     try {                         replicationTracker.activatePrimaryMode(getLocalCheckpoint()).                         /*                                  * If this shard was serving as a replica shard when another shard was promoted to primary then                                  * its Lucene index was reset during the primary term transition. In particular, the Lucene index                                  * on this shard was reset to the global checkpoint and the operations above the local checkpoint                                  * were reverted. If the other shard that was promoted to primary subsequently fails before the                                  * primary/replica re-sync completes successfully and we are now being promoted, we have to restore                                  * the reverted operations on this shard by replaying the translog to avoid losing acknowledged writes.                                  */                         final Engine engine = getEngine().                         if (getMaxSeqNoOfUpdatesOrDeletes() == UNASSIGNED_SEQ_NO) {                             // we need to bootstrap it manually from its local history.                             assert indexSettings.getIndexVersionCreated().before(Version.V_6_5_0).                             engine.advanceMaxSeqNoOfUpdatesOrDeletes(seqNoStats().getMaxSeqNo()).                         }                         engine.restoreLocalHistoryFromTranslog((resettingEngine, snapshot) -> runTranslogRecovery(resettingEngine, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> {                         })).                         /* Rolling the translog generation is not strictly needed here (as we will never have collisions between                                  * sequence numbers in a translog generation in a new primary as it takes the last known sequence number                                  * as a starting point), but it simplifies reasoning about the relationship between primary terms and                                  * translog generations.                                  */                         engine.rollTranslogGeneration().                         engine.fillSeqNoGaps(newPrimaryTerm).                         replicationTracker.updateLocalCheckpoint(currentRouting.allocationId().getId(), getLocalCheckpoint()).                         primaryReplicaSyncer.accept(this, new ActionListener<ResyncTask>() {                              @Override                             public void onResponse(ResyncTask resyncTask) {                                 logger.info("primary-replica resync completed with {} operations", resyncTask.getResyncedOperations()).                                 boolean resyncCompleted = primaryReplicaResyncInProgress.compareAndSet(true, false).                                 assert resyncCompleted : "primary-replica resync finished but was not started".                             }                              @Override                             public void onFailure(Exception e) {                                 boolean resyncCompleted = primaryReplicaResyncInProgress.compareAndSet(true, false).                                 assert resyncCompleted : "primary-replica resync finished but was not started".                                 if (state == IndexShardState.CLOSED) {                                 // ignore, shutting down                                 } else {                                     failShard("exception during primary-replica resync", e).                                 }                             }                         }).                     } catch (final AlreadyClosedException e) {                     // okay, the index was deleted                     }                 }, null).             }         }         // set this last, once we finished updating all internal state.         this.shardRouting = newRouting.         assert this.shardRouting.primary() == false || // note that we use started and not active to avoid relocating shards         this.shardRouting.started() == false || // if permits are blocked, we are still transitioning         this.indexShardOperationPermits.isBlocked() || this.replicationTracker.isPrimaryMode() : "a started primary with non-pending operation term must be in primary mode " + this.shardRouting.         shardStateUpdated.countDown().     }     if (currentRouting != null && currentRouting.active() == false && newRouting.active()) {         indexEventListener.afterIndexShardStarted(this).     }     if (newRouting.equals(currentRouting) == false) {         indexEventListener.shardRoutingChanged(this, currentRouting, newRouting).     } }
true;public;2;19;/**  * Marks the shard as recovering based on a recovery state, fails with exception is recovering is not allowed to be set.  */ ;/**  * Marks the shard as recovering based on a recovery state, fails with exception is recovering is not allowed to be set.  */ public IndexShardState markAsRecovering(String reason, RecoveryState recoveryState) throws IndexShardStartedException, IndexShardRelocatedException, IndexShardRecoveringException, IndexShardClosedException {     synchronized (mutex) {         if (state == IndexShardState.CLOSED) {             throw new IndexShardClosedException(shardId).         }         if (state == IndexShardState.STARTED) {             throw new IndexShardStartedException(shardId).         }         if (state == IndexShardState.RECOVERING) {             throw new IndexShardRecoveringException(shardId).         }         if (state == IndexShardState.POST_RECOVERY) {             throw new IndexShardRecoveringException(shardId).         }         this.recoveryState = recoveryState.         return changeState(IndexShardState.RECOVERING, reason).     } }
true;public;1;41;/**  * Completes the relocation. Operations are blocked and current operations are drained before changing state to relocated. The provided  * {@link Runnable} is executed after all operations are successfully blocked.  *  * @param consumer a {@link Runnable} that is executed after operations are blocked  * @throws IllegalIndexShardStateException if the shard is not relocating due to concurrent cancellation  * @throws InterruptedException            if blocking operations is interrupted  */ ;/**  * Completes the relocation. Operations are blocked and current operations are drained before changing state to relocated. The provided  * {@link Runnable} is executed after all operations are successfully blocked.  *  * @param consumer a {@link Runnable} that is executed after operations are blocked  * @throws IllegalIndexShardStateException if the shard is not relocating due to concurrent cancellation  * @throws InterruptedException            if blocking operations is interrupted  */ public void relocated(final Consumer<ReplicationTracker.PrimaryContext> consumer) throws IllegalIndexShardStateException, InterruptedException {     assert shardRouting.primary() : "only primaries can be marked as relocated: " + shardRouting.     final Releasable forceRefreshes = refreshListeners.forceRefreshes().     try {         indexShardOperationPermits.blockOperations(30, TimeUnit.MINUTES, () -> {             forceRefreshes.close().             // no shard operation permits are being held here, move state from started to relocated             assert indexShardOperationPermits.getActiveOperationsCount() == 0 : "in-flight operations in progress while moving shard state to relocated".             /*                  * We should not invoke the runnable under the mutex as the expected implementation is to handoff the primary context via a                  * network operation. Doing this under the mutex can implicitly block the cluster state update thread on network operations.                  */             verifyRelocatingState().             final ReplicationTracker.PrimaryContext primaryContext = replicationTracker.startRelocationHandoff().             try {                 consumer.accept(primaryContext).                 synchronized (mutex) {                     verifyRelocatingState().                     // make changes to primaryMode and relocated flag only under mutex                     replicationTracker.completeRelocationHandoff().                 }             } catch (final Exception e) {                 try {                     replicationTracker.abortRelocationHandoff().                 } catch (final Exception inner) {                     e.addSuppressed(inner).                 }                 throw e.             }         }).     } catch (TimeoutException e) {         logger.warn("timed out waiting for relocation hand-off to complete").         // This is really bad as ongoing replication operations are preventing this shard from completing relocation hand-off.         // Fail primary relocation source and target shards.         failShard("timed out waiting for relocation hand-off to complete", null).         throw new IndexShardClosedException(shardId(), "timed out waiting for relocation hand-off to complete").     } finally {         forceRefreshes.close().     } }
false;private;0;20;;private void verifyRelocatingState() {     if (state != IndexShardState.STARTED) {         throw new IndexShardNotStartedException(shardId, state).     }     if (shardRouting.relocating() == false) {         throw new IllegalIndexShardStateException(shardId, IndexShardState.STARTED, ": shard is no longer relocating " + shardRouting).     }     if (primaryReplicaResyncInProgress.get()) {         throw new IllegalIndexShardStateException(shardId, IndexShardState.STARTED, ": primary relocation is forbidden while primary-replica resync is in progress " + shardRouting).     } }
false;public;0;4;;@Override public IndexShardState state() {     return state. }
true;private;2;8;/**  * Changes the state of the current shard  *  * @param newState the new shard state  * @param reason   the reason for the state change  * @return the previous shard state  */ ;/**  * Changes the state of the current shard  *  * @param newState the new shard state  * @param reason   the reason for the state change  * @return the previous shard state  */ private IndexShardState changeState(IndexShardState newState, String reason) {     assert Thread.holdsLock(mutex).     logger.debug("state: [{}]->[{}], reason [{}]", state, newState, reason).     IndexShardState previousState = state.     state = newState.     this.indexEventListener.indexShardStateChanged(this, previousState, newState, reason).     return previousState. }
false;public;7;8;;public Engine.IndexResult applyIndexOperationOnPrimary(long version, VersionType versionType, SourceToParse sourceToParse, long ifSeqNo, long ifPrimaryTerm, long autoGeneratedTimestamp, boolean isRetry) throws IOException {     assert versionType.validateVersionForWrites(version).     return applyIndexOperation(getEngine(), UNASSIGNED_SEQ_NO, getOperationPrimaryTerm(), version, versionType, ifSeqNo, ifPrimaryTerm, autoGeneratedTimestamp, isRetry, Engine.Operation.Origin.PRIMARY, sourceToParse). }
false;public;5;6;;public Engine.IndexResult applyIndexOperationOnReplica(long seqNo, long version, long autoGeneratedTimeStamp, boolean isRetry, SourceToParse sourceToParse) throws IOException {     return applyIndexOperation(getEngine(), seqNo, getOperationPrimaryTerm(), version, null, UNASSIGNED_SEQ_NO, 0, autoGeneratedTimeStamp, isRetry, Engine.Operation.Origin.REPLICA, sourceToParse). }
false;private;11;34;;private Engine.IndexResult applyIndexOperation(Engine engine, long seqNo, long opPrimaryTerm, long version, @Nullable VersionType versionType, long ifSeqNo, long ifPrimaryTerm, long autoGeneratedTimeStamp, boolean isRetry, Engine.Operation.Origin origin, SourceToParse sourceToParse) throws IOException {     assert opPrimaryTerm <= getOperationPrimaryTerm() : "op term [ " + opPrimaryTerm + " ] > shard term [" + getOperationPrimaryTerm() + "]".     ensureWriteAllowed(origin).     Engine.Index operation.     try {         final String resolvedType = mapperService.resolveDocumentType(sourceToParse.type()).         final SourceToParse sourceWithResolvedType.         if (resolvedType.equals(sourceToParse.type())) {             sourceWithResolvedType = sourceToParse.         } else {             sourceWithResolvedType = new SourceToParse(sourceToParse.index(), resolvedType, sourceToParse.id(), sourceToParse.source(), sourceToParse.getXContentType(), sourceToParse.routing()).         }         operation = prepareIndex(docMapper(resolvedType), indexSettings.getIndexVersionCreated(), sourceWithResolvedType, seqNo, opPrimaryTerm, version, versionType, origin, autoGeneratedTimeStamp, isRetry, ifSeqNo, ifPrimaryTerm).         Mapping update = operation.parsedDoc().dynamicMappingsUpdate().         if (update != null) {             return new Engine.IndexResult(update).         }     } catch (Exception e) {         // We treat any exception during parsing and or mapping update as a document level failure         // with the exception side effects of closing the shard. Since we don't have the shard, we         // can not raise an exception that may block any replication of previous operations to the         // replicas         verifyNotClosed(e).         return new Engine.IndexResult(e, version, opPrimaryTerm, seqNo).     }     return index(engine, operation). }
false;public,static;12;13;;public static Engine.Index prepareIndex(DocumentMapperForType docMapper, Version indexCreatedVersion, SourceToParse source, long seqNo, long primaryTerm, long version, VersionType versionType, Engine.Operation.Origin origin, long autoGeneratedIdTimestamp, boolean isRetry, long ifSeqNo, long ifPrimaryTerm) {     long startTime = System.nanoTime().     ParsedDocument doc = docMapper.getDocumentMapper().parse(source).     if (docMapper.getMapping() != null) {         doc.addDynamicMappingsUpdate(docMapper.getMapping()).     }     Term uid = new Term(IdFieldMapper.NAME, Uid.encodeId(doc.id())).     return new Engine.Index(uid, doc, seqNo, primaryTerm, version, versionType, origin, startTime, autoGeneratedIdTimestamp, isRetry, ifSeqNo, ifPrimaryTerm). }
false;private;2;18;;private Engine.IndexResult index(Engine engine, Engine.Index index) throws IOException {     active.set(true).     final Engine.IndexResult result.     index = indexingOperationListeners.preIndex(shardId, index).     try {         if (logger.isTraceEnabled()) {             // don't use index.source().utf8ToString() here source might not be valid UTF-8             logger.trace("index [{}][{}] seq# [{}] allocation-id {}", index.type(), index.id(), index.seqNo(), routingEntry().allocationId()).         }         result = engine.index(index).     } catch (Exception e) {         indexingOperationListeners.postIndex(shardId, index, e).         throw e.     }     indexingOperationListeners.postIndex(shardId, index, result).     return result. }
false;public;2;3;;public Engine.NoOpResult markSeqNoAsNoop(long seqNo, String reason) throws IOException {     return markSeqNoAsNoop(getEngine(), seqNo, getOperationPrimaryTerm(), reason, Engine.Operation.Origin.REPLICA). }
false;private;5;9;;private Engine.NoOpResult markSeqNoAsNoop(Engine engine, long seqNo, long opPrimaryTerm, String reason, Engine.Operation.Origin origin) throws IOException {     assert opPrimaryTerm <= getOperationPrimaryTerm() : "op term [ " + opPrimaryTerm + " ] > shard term [" + getOperationPrimaryTerm() + "]".     long startTime = System.nanoTime().     ensureWriteAllowed(origin).     final Engine.NoOp noOp = new Engine.NoOp(seqNo, opPrimaryTerm, origin, startTime, reason).     return noOp(engine, noOp). }
false;private;2;7;;private Engine.NoOpResult noOp(Engine engine, Engine.NoOp noOp) throws IOException {     active.set(true).     if (logger.isTraceEnabled()) {         logger.trace("noop (seq# [{}])", noOp.seqNo()).     }     return engine.noOp(noOp). }
false;public;2;3;;public Engine.IndexResult getFailedIndexResult(Exception e, long version) {     return new Engine.IndexResult(e, version, getOperationPrimaryTerm()). }
false;public;2;3;;public Engine.DeleteResult getFailedDeleteResult(Exception e, long version) {     return new Engine.DeleteResult(e, version, getOperationPrimaryTerm()). }
false;public;6;7;;public Engine.DeleteResult applyDeleteOperationOnPrimary(long version, String type, String id, VersionType versionType, long ifSeqNo, long ifPrimaryTerm) throws IOException {     assert versionType.validateVersionForWrites(version).     return applyDeleteOperation(getEngine(), UNASSIGNED_SEQ_NO, getOperationPrimaryTerm(), version, type, id, versionType, ifSeqNo, ifPrimaryTerm, Engine.Operation.Origin.PRIMARY). }
false;public;4;4;;public Engine.DeleteResult applyDeleteOperationOnReplica(long seqNo, long version, String type, String id) throws IOException {     return applyDeleteOperation(getEngine(), seqNo, getOperationPrimaryTerm(), version, type, id, null, UNASSIGNED_SEQ_NO, 0, Engine.Operation.Origin.REPLICA). }
false;private;10;35;;private Engine.DeleteResult applyDeleteOperation(Engine engine, long seqNo, long opPrimaryTerm, long version, String type, String id, @Nullable VersionType versionType, long ifSeqNo, long ifPrimaryTerm, Engine.Operation.Origin origin) throws IOException {     assert opPrimaryTerm <= getOperationPrimaryTerm() : "op term [ " + opPrimaryTerm + " ] > shard term [" + getOperationPrimaryTerm() + "]".     ensureWriteAllowed(origin).     // TODO: clean this up when types are gone     try {         Mapping update = docMapper(type).getMapping().         if (update != null) {             return new Engine.DeleteResult(update).         }     } catch (MapperParsingException | IllegalArgumentException | TypeMissingException e) {         return new Engine.DeleteResult(e, version, getOperationPrimaryTerm(), seqNo, false).     }     if (mapperService.resolveDocumentType(type).equals(mapperService.documentMapper().type()) == false) {         // document in the wrong type.         throw new IllegalStateException("Deleting document from type [" + mapperService.resolveDocumentType(type) + "] while current type is [" + mapperService.documentMapper().type() + "]").     }     final Term uid = new Term(IdFieldMapper.NAME, Uid.encodeId(id)).     final Engine.Delete delete = prepareDelete(type, id, uid, seqNo, opPrimaryTerm, version, versionType, origin, ifSeqNo, ifPrimaryTerm).     return delete(engine, delete). }
false;private;10;7;;private Engine.Delete prepareDelete(String type, String id, Term uid, long seqNo, long primaryTerm, long version, VersionType versionType, Engine.Operation.Origin origin, long ifSeqNo, long ifPrimaryTerm) {     long startTime = System.nanoTime().     return new Engine.Delete(mapperService.resolveDocumentType(type), id, uid, seqNo, primaryTerm, version, versionType, origin, startTime, ifSeqNo, ifPrimaryTerm). }
false;private;2;16;;private Engine.DeleteResult delete(Engine engine, Engine.Delete delete) throws IOException {     active.set(true).     final Engine.DeleteResult result.     delete = indexingOperationListeners.preDelete(shardId, delete).     try {         if (logger.isTraceEnabled()) {             logger.trace("delete [{}] (seq no [{}])", delete.uid().text(), delete.seqNo()).         }         result = engine.delete(delete).     } catch (Exception e) {         indexingOperationListeners.postDelete(shardId, delete, e).         throw e.     }     indexingOperationListeners.postDelete(shardId, delete, result).     return result. }
false;public;1;8;;public Engine.GetResult get(Engine.Get get) {     readAllowed().     DocumentMapper mapper = mapperService.documentMapper().     if (mapper == null || mapper.type().equals(mapperService.resolveDocumentType(get.type())) == false) {         return GetResult.NOT_EXISTS.     }     return getEngine().get(get, this::acquireSearcher). }
true;public;1;7;/**  * Writes all indexing changes to disk and opens a new searcher reflecting all changes.  This can throw {@link AlreadyClosedException}.  */ ;/**  * Writes all indexing changes to disk and opens a new searcher reflecting all changes.  This can throw {@link AlreadyClosedException}.  */ public void refresh(String source) {     verifyNotClosed().     if (logger.isTraceEnabled()) {         logger.trace("refresh with source [{}]", source).     }     getEngine().refresh(source). }
true;public;0;7;/**  * Returns how many bytes we are currently moving from heap to disk  */ ;/**  * Returns how many bytes we are currently moving from heap to disk  */ public long getWritingBytes() {     Engine engine = getEngineOrNull().     if (engine == null) {         return 0.     }     return engine.getWritingBytes(). }
false;public;0;4;;public RefreshStats refreshStats() {     int listeners = refreshListeners.pendingCount().     return new RefreshStats(refreshMetric.count(), TimeUnit.NANOSECONDS.toMillis(refreshMetric.sum()), listeners). }
false;public;0;3;;public FlushStats flushStats() {     return new FlushStats(flushMetric.count(), periodicFlushMetric.count(), TimeUnit.NANOSECONDS.toMillis(flushMetric.sum())). }
false;public;0;6;;public DocsStats docStats() {     readAllowed().     DocsStats docsStats = getEngine().docStats().     markSearcherAccessed().     return docsStats. }
true;public;0;3;/**  * @return {@link CommitStats}  * @throws AlreadyClosedException if shard is closed  */ ;/**  * @return {@link CommitStats}  * @throws AlreadyClosedException if shard is closed  */ public CommitStats commitStats() {     return getEngine().commitStats(). }
true;public;0;3;/**  * @return {@link SeqNoStats}  * @throws AlreadyClosedException if shard is closed  */ ;/**  * @return {@link SeqNoStats}  * @throws AlreadyClosedException if shard is closed  */ public SeqNoStats seqNoStats() {     return getEngine().getSeqNoStats(replicationTracker.getGlobalCheckpoint()). }
false;public;1;13;;public IndexingStats indexingStats(String... types) {     Engine engine = getEngineOrNull().     final boolean throttled.     final long throttleTimeInMillis.     if (engine == null) {         throttled = false.         throttleTimeInMillis = 0.     } else {         throttled = engine.isThrottled().         throttleTimeInMillis = engine.getIndexThrottleTimeInMillis().     }     return internalIndexingStats.stats(throttled, throttleTimeInMillis, types). }
false;public;1;3;;public SearchStats searchStats(String... groups) {     return searchStats.stats(groups). }
false;public;0;3;;public GetStats getStats() {     return getService.stats(). }
false;public;0;8;;public StoreStats storeStats() {     try {         return store.stats().     } catch (IOException e) {         failShard("Failing shard because of exception during storeStats", e).         throw new ElasticsearchException("io exception while building 'store stats'", e).     } }
false;public;0;7;;public MergeStats mergeStats() {     final Engine engine = getEngineOrNull().     if (engine == null) {         return new MergeStats().     }     return engine.getMergeStats(). }
false;public;1;5;;public SegmentsStats segmentStats(boolean includeSegmentFileSizes) {     SegmentsStats segmentsStats = getEngine().segmentsStats(includeSegmentFileSizes).     segmentsStats.addBitsetMemoryInBytes(shardBitsetFilterCache.getMemorySizeInBytes()).     return segmentsStats. }
false;public;0;3;;public WarmerStats warmerStats() {     return shardWarmerService.stats(). }
false;public;1;3;;public FieldDataStats fieldDataStats(String... fields) {     return shardFieldData.stats(fields). }
false;public;0;3;;public TranslogStats translogStats() {     return getEngine().getTranslogStats(). }
false;public;1;12;;public CompletionStats completionStats(String... fields) {     readAllowed().     try {         CompletionStats stats = getEngine().completionStats(fields).         // we don't wait for a pending refreshes here since it's a stats call instead we mark it as accessed only which will cause         // the next scheduled refresh to go through and refresh the stats as well         markSearcherAccessed().         return stats.     } catch (IOException e) {         throw new UncheckedIOException(e).     } }
false;public;2;10;;public Engine.SyncedFlushResult syncFlush(String syncId, Engine.CommitId expectedCommitId) {     verifyNotClosed().     logger.trace("trying to sync flush. sync id [{}]. expected commit id [{}]]", syncId, expectedCommitId).     Engine engine = getEngine().     if (engine.isRecovering()) {         throw new IllegalIndexShardStateException(shardId(), state, "syncFlush is only allowed if the engine is not recovery" + " from translog").     }     return engine.syncFlush(syncId, expectedCommitId). }
true;public;1;22;/**  * Executes the given flush request against the engine.  *  * @param request the flush request  * @return the commit ID  */ ;/**  * Executes the given flush request against the engine.  *  * @param request the flush request  * @return the commit ID  */ public Engine.CommitId flush(FlushRequest request) {     final boolean waitIfOngoing = request.waitIfOngoing().     final boolean force = request.force().     logger.trace("flush with {}", request).     /*          * We allow flushes while recovery since we allow operations to happen while recovering and we want to keep the translog under          * control (up to deletes, which we do not GC). Yet, we do not use flush internally to clear deletes and flush the index writer          * since we use Engine#writeIndexingBuffer for this now.          */     verifyNotClosed().     final Engine engine = getEngine().     if (engine.isRecovering()) {         throw new IllegalIndexShardStateException(shardId(), state, "flush is only allowed if the engine is not recovery from translog").     }     final long time = System.nanoTime().     final Engine.CommitId commitId = engine.flush(force, waitIfOngoing).     flushMetric.inc(System.nanoTime() - time).     return commitId. }
true;public;0;5;/**  * checks and removes translog files that no longer need to be retained. See  * {@link org.elasticsearch.index.translog.TranslogDeletionPolicy} for details  */ ;/**  * checks and removes translog files that no longer need to be retained. See  * {@link org.elasticsearch.index.translog.TranslogDeletionPolicy} for details  */ public void trimTranslog() {     verifyNotClosed().     final Engine engine = getEngine().     engine.trimUnreferencedTranslogFiles(). }
true;private;0;4;/**  * Rolls the tranlog generation and cleans unneeded.  */ ;/**  * Rolls the tranlog generation and cleans unneeded.  */ private void rollTranslogGeneration() {     final Engine engine = getEngine().     engine.rollTranslogGeneration(). }
false;public;1;9;;public void forceMerge(ForceMergeRequest forceMerge) throws IOException {     verifyActive().     if (logger.isTraceEnabled()) {         logger.trace("force merge with {}", forceMerge).     }     Engine engine = getEngine().     engine.forceMerge(forceMerge.flush(), forceMerge.maxNumSegments(), forceMerge.onlyExpungeDeletes(), false, false). }
true;public;1;18;/**  * Upgrades the shard to the current version of Lucene and returns the minimum segment version  */ ;/**  * Upgrades the shard to the current version of Lucene and returns the minimum segment version  */ public org.apache.lucene.util.Version upgrade(UpgradeRequest upgrade) throws IOException {     verifyActive().     if (logger.isTraceEnabled()) {         logger.trace("upgrade with {}", upgrade).     }     org.apache.lucene.util.Version previousVersion = minimumCompatibleVersion().     // we just want to upgrade the segments, not actually forge merge to a single segment     final Engine engine = getEngine().     // we need to flush at the end to make sure the upgrade is durable     engine.forceMerge(// we need to flush at the end to make sure the upgrade is durable     true, // we just want to upgrade the segments, not actually optimize to a single segment     Integer.MAX_VALUE, false, true, upgrade.upgradeOnlyAncientSegments()).     org.apache.lucene.util.Version version = minimumCompatibleVersion().     if (logger.isTraceEnabled()) {         logger.trace("upgraded segments for {} from version {} to version {}", shardId, previousVersion, version).     }     return version. }
false;public;0;9;;public org.apache.lucene.util.Version minimumCompatibleVersion() {     org.apache.lucene.util.Version luceneVersion = null.     for (Segment segment : getEngine().segments(false)) {         if (luceneVersion == null || luceneVersion.onOrAfter(segment.getVersion())) {             luceneVersion = segment.getVersion().         }     }     return luceneVersion == null ? indexSettings.getIndexVersionCreated().luceneVersion : luceneVersion. }
true;public;1;9;/**  * Creates a new {@link IndexCommit} snapshot from the currently running engine. All resources referenced by this  * commit won't be freed until the commit / snapshot is closed.  *  * @param flushFirst <code>true</code> if the index should first be flushed to disk / a low level lucene commit should be executed  */ ;/**  * Creates a new {@link IndexCommit} snapshot from the currently running engine. All resources referenced by this  * commit won't be freed until the commit / snapshot is closed.  *  * @param flushFirst <code>true</code> if the index should first be flushed to disk / a low level lucene commit should be executed  */ public Engine.IndexCommitRef acquireLastIndexCommit(boolean flushFirst) throws EngineException {     // one time volatile read     final IndexShardState state = this.state.     // we allow snapshot on closed index shard, since we want to do one after we close the shard and before we close the engine     if (state == IndexShardState.STARTED || state == IndexShardState.CLOSED) {         return getEngine().acquireLastIndexCommit(flushFirst).     } else {         throw new IllegalIndexShardStateException(shardId, state, "snapshot is not allowed").     } }
true;public;0;9;/**  * Snapshots the most recent safe index commit from the currently running engine.  * All index files referenced by this index commit won't be freed until the commit/snapshot is closed.  */ ;/**  * Snapshots the most recent safe index commit from the currently running engine.  * All index files referenced by this index commit won't be freed until the commit/snapshot is closed.  */ public Engine.IndexCommitRef acquireSafeIndexCommit() throws EngineException {     // one time volatile read     final IndexShardState state = this.state.     // we allow snapshot on closed index shard, since we want to do one after we close the shard and before we close the engine     if (state == IndexShardState.STARTED || state == IndexShardState.CLOSED) {         return getEngine().acquireSafeIndexCommit().     } else {         throw new IllegalIndexShardStateException(shardId, state, "snapshot is not allowed").     } }
true;public;0;21;/**  * gets a {@link Store.MetadataSnapshot} for the current directory. This method is safe to call in all lifecycle of the index shard,  * without having to worry about the current state of the engine and concurrent flushes.  *  * @throws org.apache.lucene.index.IndexNotFoundException     if no index is found in the current directory  * @throws org.apache.lucene.index.CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum  *                                                            mismatch or an unexpected exception when opening the index reading the  *                                                            segments file.  * @throws org.apache.lucene.index.IndexFormatTooOldException if the lucene index is too old to be opened.  * @throws org.apache.lucene.index.IndexFormatTooNewException if the lucene index is too new to be opened.  * @throws java.io.FileNotFoundException                      if one or more files referenced by a commit are not present.  * @throws java.nio.file.NoSuchFileException                  if one or more files referenced by a commit are not present.  */ ;/**  * gets a {@link Store.MetadataSnapshot} for the current directory. This method is safe to call in all lifecycle of the index shard,  * without having to worry about the current state of the engine and concurrent flushes.  *  * @throws org.apache.lucene.index.IndexNotFoundException     if no index is found in the current directory  * @throws org.apache.lucene.index.CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum  *                                                            mismatch or an unexpected exception when opening the index reading the  *                                                            segments file.  * @throws org.apache.lucene.index.IndexFormatTooOldException if the lucene index is too old to be opened.  * @throws org.apache.lucene.index.IndexFormatTooNewException if the lucene index is too new to be opened.  * @throws java.io.FileNotFoundException                      if one or more files referenced by a commit are not present.  * @throws java.nio.file.NoSuchFileException                  if one or more files referenced by a commit are not present.  */ public Store.MetadataSnapshot snapshotStoreMetadata() throws IOException {     Engine.IndexCommitRef indexCommit = null.     store.incRef().     try {         Engine engine.         synchronized (mutex) {             // if the engine is not running, we can access the store directly, but we need to make sure no one starts             // the engine on us. If the engine is running, we can get a snapshot via the deletion policy which is initialized.             // That can be done out of mutex, since the engine can be closed half way.             engine = getEngineOrNull().             if (engine == null) {                 return store.getMetadata(null, true).             }         }         indexCommit = engine.acquireLastIndexCommit(false).         return store.getMetadata(indexCommit.getIndexCommit()).     } finally {         store.decRef().         IOUtils.close(indexCommit).     } }
true;public;2;4;/**  * Fails the shard and marks the shard store as corrupted if  * <code>e</code> is caused by index corruption  */ ;/**  * Fails the shard and marks the shard store as corrupted if  * <code>e</code> is caused by index corruption  */ public void failShard(String reason, @Nullable Exception e) {     // fail the engine. This will cause this shard to also be removed from the node's index service.     getEngine().failEngine(reason, e). }
false;public;1;3;;public Engine.Searcher acquireSearcher(String source) {     return acquireSearcher(source, Engine.SearcherScope.EXTERNAL). }
false;private;0;3;;private void markSearcherAccessed() {     lastSearcherAccess.lazySet(threadPool.relativeTimeInMillis()). }
false;private;2;20;;private Engine.Searcher acquireSearcher(String source, Engine.SearcherScope scope) {     readAllowed().     final Engine engine = getEngine().     final Engine.Searcher searcher = engine.acquireSearcher(source, scope).     assert ElasticsearchDirectoryReader.unwrap(searcher.getDirectoryReader()) != null : "DirectoryReader must be an instance or ElasticsearchDirectoryReader".     boolean success = false.     try {         final Engine.Searcher wrappedSearcher = searcherWrapper == null ? searcher : searcherWrapper.wrap(searcher).         assert wrappedSearcher != null.         success = true.         return wrappedSearcher.     } catch (IOException ex) {         throw new ElasticsearchException("failed to wrap searcher", ex).     } finally {         if (success == false) {             Releasables.close(success, searcher).         }     } }
false;public;2;19;;public void close(String reason, boolean flushEngine) throws IOException {     synchronized (mutex) {         try {             changeState(IndexShardState.CLOSED, reason).         } finally {             final Engine engine = this.currentEngineReference.getAndSet(null).             try {                 if (engine != null && flushEngine) {                     engine.flushAndClose().                 }             } finally {                 // playing safe here and close the engine even if the above succeeds - close can be called multiple times                 // Also closing refreshListeners to prevent us from accumulating any more listeners                 IOUtils.close(engine, globalCheckpointListeners, refreshListeners).                 indexShardOperationPermits.close().             }         }     } }
false;public;1;18;;public IndexShard postRecovery(String reason) throws IndexShardStartedException, IndexShardRelocatedException, IndexShardClosedException {     synchronized (mutex) {         if (state == IndexShardState.CLOSED) {             throw new IndexShardClosedException(shardId).         }         if (state == IndexShardState.STARTED) {             throw new IndexShardStartedException(shardId).         }         // we need to refresh again to expose all operations that were index until now. Otherwise         // we may not expose operations that were indexed with a refresh listener that was immediately         // responded to in addRefreshListener.         getEngine().refresh("post_recovery").         recoveryState.setStage(RecoveryState.Stage.DONE).         changeState(IndexShardState.POST_RECOVERY, reason).     }     return this. }
true;public;0;7;/**  * called before starting to copy index files over  */ ;/**  * called before starting to copy index files over  */ public void prepareForIndexRecovery() {     if (state != IndexShardState.RECOVERING) {         throw new IndexShardNotRecoveringException(shardId, state).     }     recoveryState.setStage(RecoveryState.Stage.INDEX).     assert currentEngineReference.get() == null. }
false;public;1;3;;public void trimOperationOfPreviousPrimaryTerms(long aboveSeqNo) {     getEngine().trimOperationsFromTranslog(getOperationPrimaryTerm(), aboveSeqNo). }
true;public;0;3;/**  * Returns the maximum auto_id_timestamp of all append-only requests have been processed by this shard or the auto_id_timestamp received  * from the primary via {@link #updateMaxUnsafeAutoIdTimestamp(long)} at the beginning of a peer-recovery or a primary-replica resync.  *  * @see #updateMaxUnsafeAutoIdTimestamp(long)  */ ;/**  * Returns the maximum auto_id_timestamp of all append-only requests have been processed by this shard or the auto_id_timestamp received  * from the primary via {@link #updateMaxUnsafeAutoIdTimestamp(long)} at the beginning of a peer-recovery or a primary-replica resync.  *  * @see #updateMaxUnsafeAutoIdTimestamp(long)  */ public long getMaxSeenAutoIdTimestamp() {     return getEngine().getMaxSeenAutoIdTimestamp(). }
true;public;1;3;/**  * Since operations stored in soft-deletes do not have max_auto_id_timestamp, the primary has to propagate its max_auto_id_timestamp  * (via {@link #getMaxSeenAutoIdTimestamp()} of all processed append-only requests to replicas at the beginning of a peer-recovery  * or a primary-replica resync to force a replica to disable optimization for all append-only requests which are replicated via  * replication while its retry variants are replicated via recovery without auto_id_timestamp.  * <p>  * Without this force-update, a replica can generate duplicate documents (for the same id) if it first receives  * a retry append-only (without timestamp) via recovery, then an original append-only (with timestamp) via replication.  */ ;/**  * Since operations stored in soft-deletes do not have max_auto_id_timestamp, the primary has to propagate its max_auto_id_timestamp  * (via {@link #getMaxSeenAutoIdTimestamp()} of all processed append-only requests to replicas at the beginning of a peer-recovery  * or a primary-replica resync to force a replica to disable optimization for all append-only requests which are replicated via  * replication while its retry variants are replicated via recovery without auto_id_timestamp.  * <p>  * Without this force-update, a replica can generate duplicate documents (for the same id) if it first receives  * a retry append-only (without timestamp) via recovery, then an original append-only (with timestamp) via replication.  */ public void updateMaxUnsafeAutoIdTimestamp(long maxSeenAutoIdTimestampFromPrimary) {     getEngine().updateMaxUnsafeAutoIdTimestamp(maxSeenAutoIdTimestampFromPrimary). }
false;public;2;3;;public Engine.Result applyTranslogOperation(Translog.Operation operation, Engine.Operation.Origin origin) throws IOException {     return applyTranslogOperation(getEngine(), operation, origin). }
false;private;3;29;;private Engine.Result applyTranslogOperation(Engine engine, Translog.Operation operation, Engine.Operation.Origin origin) throws IOException {     // If a translog op is replayed on the primary (eg. ccr), we need to use external instead of null for its version type.     final VersionType versionType = (origin == Engine.Operation.Origin.PRIMARY) ? VersionType.EXTERNAL : null.     final Engine.Result result.     switch(operation.opType()) {         case INDEX:             final Translog.Index index = (Translog.Index) operation.             // we set canHaveDuplicates to true all the time such that we de-optimze the translog case and ensure that all             // autoGeneratedID docs that are coming from the primary are updated correctly.             result = applyIndexOperation(engine, index.seqNo(), index.primaryTerm(), index.version(), versionType, UNASSIGNED_SEQ_NO, 0, index.getAutoGeneratedIdTimestamp(), true, origin, new SourceToParse(shardId.getIndexName(), index.type(), index.id(), index.source(), XContentHelper.xContentType(index.source()), index.routing())).             break.         case DELETE:             final Translog.Delete delete = (Translog.Delete) operation.             result = applyDeleteOperation(engine, delete.seqNo(), delete.primaryTerm(), delete.version(), delete.type(), delete.id(), versionType, UNASSIGNED_SEQ_NO, 0, origin).             break.         case NO_OP:             final Translog.NoOp noOp = (Translog.NoOp) operation.             result = markSeqNoAsNoop(engine, noOp.seqNo(), noOp.primaryTerm(), noOp.reason(), origin).             break.         default:             throw new IllegalStateException("No operation defined for [" + operation + "]").     }     return result. }
true;;4;32;/**  * Replays translog operations from the provided translog {@code snapshot} to the current engine using the given {@code origin}.  * The callback {@code onOperationRecovered} is notified after each translog operation is replayed successfully.  */ ;/**  * Replays translog operations from the provided translog {@code snapshot} to the current engine using the given {@code origin}.  * The callback {@code onOperationRecovered} is notified after each translog operation is replayed successfully.  */ int runTranslogRecovery(Engine engine, Translog.Snapshot snapshot, Engine.Operation.Origin origin, Runnable onOperationRecovered) throws IOException {     int opsRecovered = 0.     Translog.Operation operation.     while ((operation = snapshot.next()) != null) {         try {             logger.trace("[translog] recover op {}", operation).             Engine.Result result = applyTranslogOperation(engine, operation, origin).             switch(result.getResultType()) {                 case FAILURE:                     throw result.getFailure().                 case MAPPING_UPDATE_REQUIRED:                     throw new IllegalArgumentException("unexpected mapping update: " + result.getRequiredMappingUpdate()).                 case SUCCESS:                     break.                 default:                     throw new AssertionError("Unknown result type [" + result.getResultType() + "]").             }             opsRecovered++.             onOperationRecovered.run().         } catch (Exception e) {             if (ExceptionsHelper.status(e) == RestStatus.BAD_REQUEST) {                 // mainly for MapperParsingException and Failure to detect xcontent                 logger.info("ignoring recovery of a corrupt translog entry", e).             } else {                 throw ExceptionsHelper.convertToRuntime(e).             }         }     }     return opsRecovered. }
true;public;0;13;/**  * opens the engine on top of the existing lucene engine and translog.  * Operations from the translog will be replayed to bring lucene up to date.  */ ;/**  * opens the engine on top of the existing lucene engine and translog.  * Operations from the translog will be replayed to bring lucene up to date.  */ public void openEngineAndRecoverFromTranslog() throws IOException {     final RecoveryState.Translog translogRecoveryStats = recoveryState.getTranslog().     final Engine.TranslogRecoveryRunner translogRecoveryRunner = (engine, snapshot) -> {         translogRecoveryStats.totalOperations(snapshot.totalOperations()).         translogRecoveryStats.totalOperationsOnStart(snapshot.totalOperations()).         return runTranslogRecovery(engine, snapshot, Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY, translogRecoveryStats::incrementRecoveredOperations).     }.     innerOpenEngineAndTranslog().     final Engine engine = getEngine().     engine.initializeMaxSeqNoOfUpdatesOrDeletes().     engine.recoverFromTranslog(translogRecoveryRunner, Long.MAX_VALUE). }
true;public;0;4;/**  * Opens the engine on top of the existing lucene engine and translog.  * The translog is kept but its operations won't be replayed.  */ ;/**  * Opens the engine on top of the existing lucene engine and translog.  * The translog is kept but its operations won't be replayed.  */ public void openEngineAndSkipTranslogRecovery() throws IOException {     innerOpenEngineAndTranslog().     getEngine().skipTranslogRecovery(). }
false;private;0;45;;private void innerOpenEngineAndTranslog() throws IOException {     if (state != IndexShardState.RECOVERING) {         throw new IndexShardNotRecoveringException(shardId, state).     }     recoveryState.setStage(RecoveryState.Stage.VERIFY_INDEX).     // also check here, before we apply the translog     if (Booleans.isTrue(checkIndexOnStartup) || "checksum".equals(checkIndexOnStartup)) {         try {             checkIndex().         } catch (IOException ex) {             throw new RecoveryFailedException(recoveryState, "check index failed", ex).         }     }     recoveryState.setStage(RecoveryState.Stage.TRANSLOG).     final EngineConfig config = newEngineConfig().     // we disable deletes since we allow for operations to be executed against the shard while recovering     // but we need to make sure we don't loose deletes until we are done recovering     config.setEnableGcDeletes(false).     // we have to set it before we open an engine and recover from the translog because     // acquiring a snapshot from the translog causes a sync which causes the global checkpoint to be pulled in,     // and an engine can be forced to close in ctor which also causes the global checkpoint to be pulled in.     final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY).     final long globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID).     replicationTracker.updateGlobalCheckpointOnReplica(globalCheckpoint, "read from translog checkpoint").     updateRetentionLeasesOnReplica(loadRetentionLeases()).     trimUnsafeCommits().     synchronized (mutex) {         verifyNotClosed().         assert currentEngineReference.get() == null : "engine is running".         // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata).         final Engine newEngine = engineFactory.newReadWriteEngine(config).         onNewEngine(newEngine).         currentEngineReference.set(newEngine).         // We set active because we are now writing operations to the engine. this way,         // if we go idle after some time and become inactive, we still give sync'd flush a chance to run.         active.set(true).     }     // time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during     // which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine.     onSettingsChanged().     assertSequenceNumbersInCommit().     assert recoveryState.getStage() == RecoveryState.Stage.TRANSLOG : "TRANSLOG stage expected but was: " + recoveryState.getStage(). }
false;private;0;8;;private void trimUnsafeCommits() throws IOException {     assert currentEngineReference.get() == null || currentEngineReference.get() instanceof ReadOnlyEngine : "a write engine is running".     final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY).     final long globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID).     final long minRetainedTranslogGen = Translog.readMinTranslogGeneration(translogConfig.getTranslogPath(), translogUUID).     assertMaxUnsafeAutoIdInCommit().     store.trimUnsafeCommits(globalCheckpoint, minRetainedTranslogGen, indexSettings.getIndexVersionCreated()). }
false;private;0;9;;private boolean assertSequenceNumbersInCommit() throws IOException {     final Map<String, String> userData = SegmentInfos.readLatestCommit(store.directory()).getUserData().     assert userData.containsKey(SequenceNumbers.LOCAL_CHECKPOINT_KEY) : "commit point doesn't contains a local checkpoint".     assert userData.containsKey(SequenceNumbers.MAX_SEQ_NO) : "commit point doesn't contains a maximum sequence number".     assert userData.containsKey(Engine.HISTORY_UUID_KEY) : "commit point doesn't contains a history uuid".     assert userData.get(Engine.HISTORY_UUID_KEY).equals(getHistoryUUID()) : "commit point history uuid [" + userData.get(Engine.HISTORY_UUID_KEY) + "] is different than engine [" + getHistoryUUID() + "]".     return true. }
false;private;0;7;;private boolean assertMaxUnsafeAutoIdInCommit() throws IOException {     final Map<String, String> userData = SegmentInfos.readLatestCommit(store.directory()).getUserData().     assert userData.containsKey(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID) : "opening index which was created post 5.5.0 but " + Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID + " is not found in commit".     return true. }
false;protected;1;3;;protected void onNewEngine(Engine newEngine) {     refreshListeners.setCurrentRefreshLocationSupplier(newEngine::getTranslogLastWriteLocation). }
true;public;0;11;/**  * called if recovery has to be restarted after network error / delay **  */ ;/**  * called if recovery has to be restarted after network error / delay **  */ public void performRecoveryRestart() throws IOException {     synchronized (mutex) {         if (state != IndexShardState.RECOVERING) {             throw new IndexShardNotRecoveringException(shardId, state).         }         assert refreshListeners.pendingCount() == 0 : "we can't restart with pending listeners".         final Engine engine = this.currentEngineReference.getAndSet(null).         IOUtils.close(engine).         recoveryState().setStage(RecoveryState.Stage.INIT).     } }
true;public;0;3;/**  * returns stats about ongoing recoveries, both source and target  */ ;/**  * returns stats about ongoing recoveries, both source and target  */ public RecoveryStats recoveryStats() {     return recoveryStats. }
true;public;0;4;/**  * Returns the current {@link RecoveryState} if this shard is recovering or has been recovering.  * Returns null if the recovery has not yet started or shard was not recovered (created via an API).  */ ;/**  * Returns the current {@link RecoveryState} if this shard is recovering or has been recovering.  * Returns null if the recovery has not yet started or shard was not recovered (created via an API).  */ @Override public RecoveryState recoveryState() {     return this.recoveryState. }
true;public;0;6;/**  * perform the last stages of recovery once all translog operations are done.  * note that you should still call {@link #postRecovery(String)}.  */ ;/**  * perform the last stages of recovery once all translog operations are done.  * note that you should still call {@link #postRecovery(String)}.  */ public void finalizeRecovery() {     recoveryState().setStage(RecoveryState.Stage.FINALIZE).     Engine engine = getEngine().     engine.refresh("recovery_finalization").     engine.config().setEnableGcDeletes(true). }
true;public;0;5;/**  * Returns {@code true} if this shard can ignore a recovery attempt made to it (since the already doing/done it)  */ ;/**  * Returns {@code true} if this shard can ignore a recovery attempt made to it (since the already doing/done it)  */ public boolean ignoreRecoveryAttempt() {     // one time volatile read     IndexShardState state = state().     return state == IndexShardState.POST_RECOVERY || state == IndexShardState.RECOVERING || state == IndexShardState.STARTED || state == IndexShardState.CLOSED. }
false;public;0;7;;public void readAllowed() throws IllegalIndexShardStateException {     // one time volatile read     IndexShardState state = this.state.     if (readAllowedStates.contains(state) == false) {         throw new IllegalIndexShardStateException(shardId, state, "operations only allowed when shard state is one of " + readAllowedStates.toString()).     } }
true;public;0;3;/**  * returns true if the {@link IndexShardState} allows reading  */ ;/**  * returns true if the {@link IndexShardState} allows reading  */ public boolean isReadAllowed() {     return readAllowedStates.contains(state). }
false;private;1;23;;private void ensureWriteAllowed(Engine.Operation.Origin origin) throws IllegalIndexShardStateException {     // one time volatile read     IndexShardState state = this.state.     if (origin.isRecovery()) {         if (state != IndexShardState.RECOVERING) {             throw new IllegalIndexShardStateException(shardId, state, "operation only allowed when recovering, origin [" + origin + "]").         }     } else {         if (origin == Engine.Operation.Origin.PRIMARY) {             assert assertPrimaryMode().         } else if (origin == Engine.Operation.Origin.REPLICA) {             assert assertReplicationTarget().         } else {             assert origin == Engine.Operation.Origin.LOCAL_RESET.             assert getActiveOperationsCount() == 0 : "Ongoing writes [" + getActiveOperations() + "]".         }         if (writeAllowedStates.contains(state) == false) {             throw new IllegalIndexShardStateException(shardId, state, "operation only allowed when shard state is one of " + writeAllowedStates + ", origin [" + origin + "]").         }     } }
false;private;0;5;;private boolean assertPrimaryMode() {     assert shardRouting.primary() && replicationTracker.isPrimaryMode() : "shard " + shardRouting + " is not a primary shard in primary mode".     return true. }
false;private;0;4;;private boolean assertReplicationTarget() {     assert replicationTracker.isPrimaryMode() == false : "shard " + shardRouting + " in primary mode cannot be a replication target".     return true. }
false;private;0;3;;private void verifyNotClosed() throws IllegalIndexShardStateException {     verifyNotClosed(null). }
false;private;1;10;;private void verifyNotClosed(Exception suppressed) throws IllegalIndexShardStateException {     // one time volatile read     IndexShardState state = this.state.     if (state == IndexShardState.CLOSED) {         final IllegalIndexShardStateException exc = new IndexShardClosedException(shardId, "operation only allowed when not closed").         if (suppressed != null) {             exc.addSuppressed(suppressed).         }         throw exc.     } }
false;protected,final;0;6;;protected final void verifyActive() throws IllegalIndexShardStateException {     // one time volatile read     IndexShardState state = this.state.     if (state != IndexShardState.STARTED) {         throw new IllegalIndexShardStateException(shardId, state, "operation only allowed when shard is active").     } }
true;public;0;11;/**  * Returns number of heap bytes used by the indexing buffer for this shard, or 0 if the shard is closed  */ ;/**  * Returns number of heap bytes used by the indexing buffer for this shard, or 0 if the shard is closed  */ public long getIndexBufferRAMBytesUsed() {     Engine engine = getEngineOrNull().     if (engine == null) {         return 0.     }     try {         return engine.getIndexBufferRAMBytesUsed().     } catch (AlreadyClosedException ex) {         return 0.     } }
false;public;1;3;;public void addShardFailureCallback(Consumer<ShardFailure> onShardFailure) {     this.shardEventListener.delegates.add(onShardFailure). }
true;public;1;14;/**  * Called by {@link IndexingMemoryController} to check whether more than {@code inactiveTimeNS} has passed since the last  * indexing operation, and notify listeners that we are now inactive so e.g. sync'd flush can happen.  */ ;/**  * Called by {@link IndexingMemoryController} to check whether more than {@code inactiveTimeNS} has passed since the last  * indexing operation, and notify listeners that we are now inactive so e.g. sync'd flush can happen.  */ public void checkIdle(long inactiveTimeNS) {     Engine engineOrNull = getEngineOrNull().     if (engineOrNull != null && System.nanoTime() - engineOrNull.getLastWriteNanos() >= inactiveTimeNS) {         boolean wasActive = active.getAndSet(false).         if (wasActive) {             logger.debug("shard is now inactive").             try {                 indexEventListener.onShardInactive(this).             } catch (Exception e) {                 logger.warn("failed to notify index event listener", e).             }         }     } }
false;public;0;3;;public boolean isActive() {     return active.get(). }
false;public;0;3;;public ShardPath shardPath() {     return path. }
false;public;2;20;;public boolean recoverFromLocalShards(BiConsumer<String, MappingMetaData> mappingUpdateConsumer, List<IndexShard> localShards) throws IOException {     assert shardRouting.primary() : "recover from local shards only makes sense if the shard is a primary shard".     assert recoveryState.getRecoverySource().getType() == RecoverySource.Type.LOCAL_SHARDS : "invalid recovery type: " + recoveryState.getRecoverySource().     final List<LocalShardSnapshot> snapshots = new ArrayList<>().     try {         for (IndexShard shard : localShards) {             snapshots.add(new LocalShardSnapshot(shard)).         }         // if its post api allocation, the index should exists         assert shardRouting.primary() : "recover from local shards only makes sense if the shard is a primary shard".         StoreRecovery storeRecovery = new StoreRecovery(shardId, logger).         return storeRecovery.recoverFromLocalShards(mappingUpdateConsumer, this, snapshots).     } finally {         IOUtils.close(snapshots).     } }
false;public;0;8;;public boolean recoverFromStore() {     // if its post api allocation, the index should exists     assert shardRouting.primary() : "recover from store only makes sense if the shard is a primary shard".     assert shardRouting.initializing() : "can only start recovery on initializing shard".     StoreRecovery storeRecovery = new StoreRecovery(shardId, logger).     return storeRecovery.recoverFromStore(this). }
false;public;1;7;;public boolean restoreFromRepository(Repository repository) {     assert shardRouting.primary() : "recover from store only makes sense if the shard is a primary shard".     assert recoveryState.getRecoverySource().getType() == RecoverySource.Type.SNAPSHOT : "invalid recovery type: " + recoveryState.getRecoverySource().     StoreRecovery storeRecovery = new StoreRecovery(shardId, logger).     return storeRecovery.recoverFromRepository(this, repository). }
true;;0;11;/**  * Tests whether or not the engine should be flushed periodically.  * This test is based on the current size of the translog compared to the configured flush threshold size.  *  * @return {@code true} if the engine should be flushed  */ ;/**  * Tests whether or not the engine should be flushed periodically.  * This test is based on the current size of the translog compared to the configured flush threshold size.  *  * @return {@code true} if the engine should be flushed  */ boolean shouldPeriodicallyFlush() {     final Engine engine = getEngineOrNull().     if (engine != null) {         try {             return engine.shouldPeriodicallyFlush().         } catch (final AlreadyClosedException e) {         // we are already closed, no need to flush or roll         }     }     return false. }
true;;0;11;/**  * Tests whether or not the translog generation should be rolled to a new generation. This test is based on the size of the current  * generation compared to the configured generation threshold size.  *  * @return {@code true} if the current generation should be rolled to a new generation  */ ;/**  * Tests whether or not the translog generation should be rolled to a new generation. This test is based on the size of the current  * generation compared to the configured generation threshold size.  *  * @return {@code true} if the current generation should be rolled to a new generation  */ boolean shouldRollTranslogGeneration() {     final Engine engine = getEngineOrNull().     if (engine != null) {         try {             return engine.shouldRollTranslogGeneration().         } catch (final AlreadyClosedException e) {         // we are already closed, no need to flush or roll         }     }     return false. }
false;public;0;6;;public void onSettingsChanged() {     Engine engineOrNull = getEngineOrNull().     if (engineOrNull != null) {         engineOrNull.onSettingsChanged().     } }
true;public;0;3;/**  * Acquires a lock on the translog files and Lucene soft-deleted documents to prevent them from being trimmed  */ ;/**  * Acquires a lock on the translog files and Lucene soft-deleted documents to prevent them from being trimmed  */ public Closeable acquireRetentionLock() {     return getEngine().acquireRetentionLock(). }
true;public;2;3;/**  * Returns the estimated number of history operations whose seq# at least the provided seq# in this shard.  */ ;/**  * Returns the estimated number of history operations whose seq# at least the provided seq# in this shard.  */ public int estimateNumberOfHistoryOperations(String source, long startingSeqNo) throws IOException {     return getEngine().estimateNumberOfHistoryOperations(source, mapperService, startingSeqNo). }
true;public;2;3;/**  * Creates a new history snapshot for reading operations since the provided starting seqno (inclusive).  * The returned snapshot can be retrieved from either Lucene index or translog files.  */ ;/**  * Creates a new history snapshot for reading operations since the provided starting seqno (inclusive).  * The returned snapshot can be retrieved from either Lucene index or translog files.  */ public Translog.Snapshot getHistoryOperations(String source, long startingSeqNo) throws IOException {     return getEngine().readHistoryOperations(source, mapperService, startingSeqNo). }
true;public;2;3;/**  * Checks if we have a completed history of operations since the given starting seqno (inclusive).  * This method should be called after acquiring the retention lock. See {@link #acquireRetentionLock()}  */ ;/**  * Checks if we have a completed history of operations since the given starting seqno (inclusive).  * This method should be called after acquiring the retention lock. See {@link #acquireRetentionLock()}  */ public boolean hasCompleteHistoryOperations(String source, long startingSeqNo) throws IOException {     return getEngine().hasCompleteOperationHistory(source, mapperService, startingSeqNo). }
true;public;0;3;/**  * Gets the minimum retained sequence number for this engine.  *  * @return the minimum retained sequence number  */ ;/**  * Gets the minimum retained sequence number for this engine.  *  * @return the minimum retained sequence number  */ public long getMinRetainedSeqNo() {     return getEngine().getMinRetainedSeqNo(). }
true;public;4;4;/**  * Creates a new changes snapshot for reading operations whose seq_no are between {@code fromSeqNo}(inclusive)  * and {@code toSeqNo}(inclusive). The caller has to close the returned snapshot after finishing the reading.  *  * @param source            the source of the request  * @param fromSeqNo         the from seq_no (inclusive) to read  * @param toSeqNo           the to seq_no (inclusive) to read  * @param requiredFullRange if {@code true} then {@link Translog.Snapshot#next()} will throw {@link IllegalStateException}  *                          if any operation between {@code fromSeqNo} and {@code toSeqNo} is missing.  *                          This parameter should be only enabled when the entire requesting range is below the global checkpoint.  */ ;/**  * Creates a new changes snapshot for reading operations whose seq_no are between {@code fromSeqNo}(inclusive)  * and {@code toSeqNo}(inclusive). The caller has to close the returned snapshot after finishing the reading.  *  * @param source            the source of the request  * @param fromSeqNo         the from seq_no (inclusive) to read  * @param toSeqNo           the to seq_no (inclusive) to read  * @param requiredFullRange if {@code true} then {@link Translog.Snapshot#next()} will throw {@link IllegalStateException}  *                          if any operation between {@code fromSeqNo} and {@code toSeqNo} is missing.  *                          This parameter should be only enabled when the entire requesting range is below the global checkpoint.  */ public Translog.Snapshot newChangesSnapshot(String source, long fromSeqNo, long toSeqNo, boolean requiredFullRange) throws IOException {     return getEngine().newChangesSnapshot(source, mapperService, fromSeqNo, toSeqNo, requiredFullRange). }
false;public;1;3;;public List<Segment> segments(boolean verbose) {     return getEngine().segments(verbose). }
false;public;0;3;;public void flushAndCloseEngine() throws IOException {     getEngine().flushAndClose(). }
false;public;0;3;;public String getHistoryUUID() {     return getEngine().getHistoryUUID(). }
false;public;0;3;;public IndexEventListener getIndexEventListener() {     return indexEventListener. }
false;public;0;7;;public void activateThrottling() {     try {         getEngine().activateThrottling().     } catch (AlreadyClosedException ex) {     // ignore     } }
false;public;0;7;;public void deactivateThrottling() {     try {         getEngine().deactivateThrottling().     } catch (AlreadyClosedException ex) {     // ignore     } }
false;private;1;22;;private void handleRefreshException(Exception e) {     if (e instanceof AlreadyClosedException) {     // ignore     } else if (e instanceof RefreshFailedEngineException) {         RefreshFailedEngineException rfee = (RefreshFailedEngineException) e.         if (rfee.getCause() instanceof InterruptedException) {         // ignore, we are being shutdown         } else if (rfee.getCause() instanceof ClosedByInterruptException) {         // ignore, we are being shutdown         } else if (rfee.getCause() instanceof ThreadInterruptedException) {         // ignore, we are being shutdown         } else {             if (state != IndexShardState.CLOSED) {                 logger.warn("Failed to perform engine refresh", e).             }         }     } else {         if (state != IndexShardState.CLOSED) {             logger.warn("Failed to perform engine refresh", e).         }     } }
true;public;0;8;/**  * Called when our shard is using too much heap and should move buffered indexed/deleted documents to disk.  */ ;/**  * Called when our shard is using too much heap and should move buffered indexed/deleted documents to disk.  */ public void writeIndexingBuffer() {     try {         Engine engine = getEngine().         engine.writeIndexingBuffer().     } catch (Exception e) {         handleRefreshException(e).     } }
true;public;2;5;/**  * Notifies the service to update the local checkpoint for the shard with the provided allocation ID. See  * {@link ReplicationTracker#updateLocalCheckpoint(String, long)} for  * details.  *  * @param allocationId the allocation ID of the shard to update the local checkpoint for  * @param checkpoint   the local checkpoint for the shard  */ ;/**  * Notifies the service to update the local checkpoint for the shard with the provided allocation ID. See  * {@link ReplicationTracker#updateLocalCheckpoint(String, long)} for  * details.  *  * @param allocationId the allocation ID of the shard to update the local checkpoint for  * @param checkpoint   the local checkpoint for the shard  */ public void updateLocalCheckpointForShard(final String allocationId, final long checkpoint) {     assert assertPrimaryMode().     verifyNotClosed().     replicationTracker.updateLocalCheckpoint(allocationId, checkpoint). }
true;public;2;5;/**  * Update the local knowledge of the global checkpoint for the specified allocation ID.  *  * @param allocationId     the allocation ID to update the global checkpoint for  * @param globalCheckpoint the global checkpoint  */ ;/**  * Update the local knowledge of the global checkpoint for the specified allocation ID.  *  * @param allocationId     the allocation ID to update the global checkpoint for  * @param globalCheckpoint the global checkpoint  */ public void updateGlobalCheckpointForShard(final String allocationId, final long globalCheckpoint) {     assert assertPrimaryMode().     verifyNotClosed().     replicationTracker.updateGlobalCheckpointForShard(allocationId, globalCheckpoint). }
true;public;3;6;/**  * Add a global checkpoint listener. If the global checkpoint is equal to or above the global checkpoint the listener is waiting for,  * then the listener will be notified immediately via an executor (so possibly not on the current thread). If the specified timeout  * elapses before the listener is notified, the listener will be notified with an {@link TimeoutException}. A caller may pass null to  * specify no timeout.  *  * @param waitingForGlobalCheckpoint the global checkpoint the listener is waiting for  * @param listener                   the listener  * @param timeout                    the timeout  */ ;/**  * Add a global checkpoint listener. If the global checkpoint is equal to or above the global checkpoint the listener is waiting for,  * then the listener will be notified immediately via an executor (so possibly not on the current thread). If the specified timeout  * elapses before the listener is notified, the listener will be notified with an {@link TimeoutException}. A caller may pass null to  * specify no timeout.  *  * @param waitingForGlobalCheckpoint the global checkpoint the listener is waiting for  * @param listener                   the listener  * @param timeout                    the timeout  */ public void addGlobalCheckpointListener(final long waitingForGlobalCheckpoint, final GlobalCheckpointListeners.GlobalCheckpointListener listener, final TimeValue timeout) {     this.globalCheckpointListeners.add(waitingForGlobalCheckpoint, listener, timeout). }
true;public;0;3;/**  * Get all retention leases tracked on this shard.  *  * @return the retention leases  */ ;/**  * Get all retention leases tracked on this shard.  *  * @return the retention leases  */ public RetentionLeases getRetentionLeases() {     return getRetentionLeases(false).v2(). }
true;public;1;5;/**  * If the expire leases parameter is false, gets all retention leases tracked on this shard and otherwise first calculates  * expiration of existing retention leases, and then gets all non-expired retention leases tracked on this shard. Note that only the  * primary shard calculates which leases are expired, and if any have expired, syncs the retention leases to any replicas. If the  * expire leases parameter is true, this replication tracker must be in primary mode.  *  * @return a tuple indicating whether or not any retention leases were expired, and the non-expired retention leases  */ ;/**  * If the expire leases parameter is false, gets all retention leases tracked on this shard and otherwise first calculates  * expiration of existing retention leases, and then gets all non-expired retention leases tracked on this shard. Note that only the  * primary shard calculates which leases are expired, and if any have expired, syncs the retention leases to any replicas. If the  * expire leases parameter is true, this replication tracker must be in primary mode.  *  * @return a tuple indicating whether or not any retention leases were expired, and the non-expired retention leases  */ public Tuple<Boolean, RetentionLeases> getRetentionLeases(final boolean expireLeases) {     assert expireLeases == false || assertPrimaryMode().     verifyNotClosed().     return replicationTracker.getRetentionLeases(expireLeases). }
false;public;0;4;;public RetentionLeaseStats getRetentionLeaseStats() {     verifyNotClosed().     return new RetentionLeaseStats(getRetentionLeases()). }
true;public;4;16;/**  * Adds a new retention lease.  *  * @param id                      the identifier of the retention lease  * @param retainingSequenceNumber the retaining sequence number  * @param source                  the source of the retention lease  * @param listener                the callback when the retention lease is successfully added and synced to replicas  * @return the new retention lease  * @throws IllegalArgumentException if the specified retention lease already exists  */ ;/**  * Adds a new retention lease.  *  * @param id                      the identifier of the retention lease  * @param retainingSequenceNumber the retaining sequence number  * @param source                  the source of the retention lease  * @param listener                the callback when the retention lease is successfully added and synced to replicas  * @return the new retention lease  * @throws IllegalArgumentException if the specified retention lease already exists  */ public RetentionLease addRetentionLease(final String id, final long retainingSequenceNumber, final String source, final ActionListener<ReplicationResponse> listener) {     Objects.requireNonNull(listener).     assert assertPrimaryMode().     verifyNotClosed().     try (Closeable ignore = acquireRetentionLock()) {         final long actualRetainingSequenceNumber = retainingSequenceNumber == RETAIN_ALL ? getMinRetainedSeqNo() : retainingSequenceNumber.         return replicationTracker.addRetentionLease(id, actualRetainingSequenceNumber, source, listener).     } catch (final IOException e) {         throw new AssertionError(e).     } }
true;public;3;11;/**  * Renews an existing retention lease.  *  * @param id                      the identifier of the retention lease  * @param retainingSequenceNumber the retaining sequence number  * @param source                  the source of the retention lease  * @return the renewed retention lease  * @throws IllegalArgumentException if the specified retention lease does not exist  */ ;/**  * Renews an existing retention lease.  *  * @param id                      the identifier of the retention lease  * @param retainingSequenceNumber the retaining sequence number  * @param source                  the source of the retention lease  * @return the renewed retention lease  * @throws IllegalArgumentException if the specified retention lease does not exist  */ public RetentionLease renewRetentionLease(final String id, final long retainingSequenceNumber, final String source) {     assert assertPrimaryMode().     verifyNotClosed().     try (Closeable ignore = acquireRetentionLock()) {         final long actualRetainingSequenceNumber = retainingSequenceNumber == RETAIN_ALL ? getMinRetainedSeqNo() : retainingSequenceNumber.         return replicationTracker.renewRetentionLease(id, actualRetainingSequenceNumber, source).     } catch (final IOException e) {         throw new AssertionError(e).     } }
true;public;2;6;/**  * Removes an existing retention lease.  *  * @param id       the identifier of the retention lease  * @param listener the callback when the retention lease is successfully removed and synced to replicas  */ ;/**  * Removes an existing retention lease.  *  * @param id       the identifier of the retention lease  * @param listener the callback when the retention lease is successfully removed and synced to replicas  */ public void removeRetentionLease(final String id, final ActionListener<ReplicationResponse> listener) {     Objects.requireNonNull(listener).     assert assertPrimaryMode().     verifyNotClosed().     replicationTracker.removeRetentionLease(id, listener). }
true;public;1;5;/**  * Updates retention leases on a replica.  *  * @param retentionLeases the retention leases  */ ;/**  * Updates retention leases on a replica.  *  * @param retentionLeases the retention leases  */ public void updateRetentionLeasesOnReplica(final RetentionLeases retentionLeases) {     assert assertReplicationTarget().     verifyNotClosed().     replicationTracker.updateRetentionLeasesOnReplica(retentionLeases). }
true;public;0;4;/**  * Loads the latest retention leases from their dedicated state file.  *  * @return the retention leases  * @throws IOException if an I/O exception occurs reading the retention leases  */ ;/**  * Loads the latest retention leases from their dedicated state file.  *  * @return the retention leases  * @throws IOException if an I/O exception occurs reading the retention leases  */ public RetentionLeases loadRetentionLeases() throws IOException {     verifyNotClosed().     return replicationTracker.loadRetentionLeases(path.getShardStatePath()). }
true;public;0;4;/**  * Persists the current retention leases to their dedicated state file.  *  * @throws WriteStateException if an exception occurs writing the state file  */ ;/**  * Persists the current retention leases to their dedicated state file.  *  * @throws WriteStateException if an exception occurs writing the state file  */ public void persistRetentionLeases() throws WriteStateException {     verifyNotClosed().     replicationTracker.persistRetentionLeases(path.getShardStatePath()). }
true;public;0;20;/**  * Syncs the current retention leases to all replicas.  */ ;/**  * Syncs the current retention leases to all replicas.  */ public void syncRetentionLeases() {     assert assertPrimaryMode().     verifyNotClosed().     final Tuple<Boolean, RetentionLeases> retentionLeases = getRetentionLeases(true).     if (retentionLeases.v1()) {         logger.trace("syncing retention leases [{}] after expiration check", retentionLeases.v2()).         retentionLeaseSyncer.sync(shardId, retentionLeases.v2(), ActionListener.wrap(r -> {         }, e -> logger.warn(new ParameterizedMessage("failed to sync retention leases [{}] after expiration check", retentionLeases), e))).     } else {         logger.trace("background syncing retention leases [{}] after expiration check", retentionLeases.v2()).         retentionLeaseSyncer.backgroundSync(shardId, retentionLeases.v2()).     } }
true;public;1;4;/**  * Called when the recovery process for a shard has opened the engine on the target shard. Ensures that the right data structures  * have been set up locally to track local checkpoint information for the shard and that the shard is added to the replication group.  *  * @param allocationId  the allocation ID of the shard for which recovery was initiated  */ ;/**  * Called when the recovery process for a shard has opened the engine on the target shard. Ensures that the right data structures  * have been set up locally to track local checkpoint information for the shard and that the shard is added to the replication group.  *  * @param allocationId  the allocation ID of the shard for which recovery was initiated  */ public void initiateTracking(final String allocationId) {     assert assertPrimaryMode().     replicationTracker.initiateTracking(allocationId). }
true;public;2;4;/**  * Marks the shard with the provided allocation ID as in-sync with the primary shard. See  * {@link ReplicationTracker#markAllocationIdAsInSync(String, long)}  * for additional details.  *  * @param allocationId    the allocation ID of the shard to mark as in-sync  * @param localCheckpoint the current local checkpoint on the shard  */ ;/**  * Marks the shard with the provided allocation ID as in-sync with the primary shard. See  * {@link ReplicationTracker#markAllocationIdAsInSync(String, long)}  * for additional details.  *  * @param allocationId    the allocation ID of the shard to mark as in-sync  * @param localCheckpoint the current local checkpoint on the shard  */ public void markAllocationIdAsInSync(final String allocationId, final long localCheckpoint) throws InterruptedException {     assert assertPrimaryMode().     replicationTracker.markAllocationIdAsInSync(allocationId, localCheckpoint). }
true;public;0;3;/**  * Returns the local checkpoint for the shard.  *  * @return the local checkpoint  */ ;/**  * Returns the local checkpoint for the shard.  *  * @return the local checkpoint  */ public long getLocalCheckpoint() {     return getEngine().getLocalCheckpoint(). }
true;public;0;3;/**  * Returns the global checkpoint for the shard.  *  * @return the global checkpoint  */ ;/**  * Returns the global checkpoint for the shard.  *  * @return the global checkpoint  */ public long getGlobalCheckpoint() {     return replicationTracker.getGlobalCheckpoint(). }
true;public;0;3;/**  * Returns the latest global checkpoint value that has been persisted in the underlying storage (i.e. translog's checkpoint)  */ ;/**  * Returns the latest global checkpoint value that has been persisted in the underlying storage (i.e. translog's checkpoint)  */ public long getLastSyncedGlobalCheckpoint() {     return getEngine().getLastSyncedGlobalCheckpoint(). }
true;public;0;5;/**  * Get the local knowledge of the global checkpoints for all in-sync allocation IDs.  *  * @return a map from allocation ID to the local knowledge of the global checkpoint for that allocation ID  */ ;/**  * Get the local knowledge of the global checkpoints for all in-sync allocation IDs.  *  * @return a map from allocation ID to the local knowledge of the global checkpoint for that allocation ID  */ public ObjectLongMap<String> getInSyncGlobalCheckpoints() {     assert assertPrimaryMode().     verifyNotClosed().     return replicationTracker.getInSyncGlobalCheckpoints(). }
true;public;1;25;/**  * Syncs the global checkpoint to the replicas if the global checkpoint on at least one replica is behind the global checkpoint on the  * primary.  */ ;/**  * Syncs the global checkpoint to the replicas if the global checkpoint on at least one replica is behind the global checkpoint on the  * primary.  */ public void maybeSyncGlobalCheckpoint(final String reason) {     verifyNotClosed().     assert shardRouting.primary() : "only call maybeSyncGlobalCheckpoint on primary shard".     if (replicationTracker.isPrimaryMode() == false) {         return.     }     assert assertPrimaryMode().     // only sync if there are not operations in flight     final SeqNoStats stats = getEngine().getSeqNoStats(replicationTracker.getGlobalCheckpoint()).     if (stats.getMaxSeqNo() == stats.getGlobalCheckpoint()) {         final ObjectLongMap<String> globalCheckpoints = getInSyncGlobalCheckpoints().         final String allocationId = routingEntry().allocationId().getId().         assert globalCheckpoints.containsKey(allocationId).         final long globalCheckpoint = globalCheckpoints.get(allocationId).         final boolean syncNeeded = StreamSupport.stream(globalCheckpoints.values().spliterator(), false).anyMatch(v -> v.value < globalCheckpoint).         // only sync if there is a shard lagging the primary         if (syncNeeded) {             logger.trace("syncing global checkpoint for [{}]", reason).             globalCheckpointSyncer.run().         }     } }
true;public;0;5;/**  * Returns the current replication group for the shard.  *  * @return the replication group  */ ;/**  * Returns the current replication group for the shard.  *  * @return the replication group  */ public ReplicationGroup getReplicationGroup() {     assert assertPrimaryMode().     verifyNotClosed().     return replicationTracker.getReplicationGroup(). }
true;public;2;21;/**  * Updates the global checkpoint on a replica shard after it has been updated by the primary.  *  * @param globalCheckpoint the global checkpoint  * @param reason           the reason the global checkpoint was updated  */ ;/**  * Updates the global checkpoint on a replica shard after it has been updated by the primary.  *  * @param globalCheckpoint the global checkpoint  * @param reason           the reason the global checkpoint was updated  */ public void updateGlobalCheckpointOnReplica(final long globalCheckpoint, final String reason) {     assert assertReplicationTarget().     final long localCheckpoint = getLocalCheckpoint().     if (globalCheckpoint > localCheckpoint) {         /*              * This can happen during recovery when the shard has started its engine but recovery is not finalized and is receiving global              * checkpoint updates. However, since this shard is not yet contributing to calculating the global checkpoint, it can be the              * case that the global checkpoint update from the primary is ahead of the local checkpoint on this shard. In this case, we              * ignore the global checkpoint update. This can happen if we are in the translog stage of recovery. Prior to this, the engine              * is not opened and this shard will not receive global checkpoint updates, and after this the shard will be contributing to              * calculations of the global checkpoint. However, we can not assert that we are in the translog stage of recovery here as              * while the global checkpoint update may have emanated from the primary when we were in that state, we could subsequently move              * to recovery finalization, or even finished recovery before the update arrives here.              */         assert state() != IndexShardState.POST_RECOVERY && state() != IndexShardState.STARTED : "supposedly in-sync shard copy received a global checkpoint [" + globalCheckpoint + "] " + "that is higher than its local checkpoint [" + localCheckpoint + "]".         return.     }     replicationTracker.updateGlobalCheckpointOnReplica(globalCheckpoint, reason). }
true;public;1;15;/**  * Updates the known allocation IDs and the local checkpoints for the corresponding allocations from a primary relocation source.  *  * @param primaryContext the sequence number context  */ ;/**  * Updates the known allocation IDs and the local checkpoints for the corresponding allocations from a primary relocation source.  *  * @param primaryContext the sequence number context  */ public void activateWithPrimaryContext(final ReplicationTracker.PrimaryContext primaryContext) {     assert shardRouting.primary() && shardRouting.isRelocationTarget() : "only primary relocation target can update allocation IDs from primary context: " + shardRouting.     assert primaryContext.getCheckpointStates().containsKey(routingEntry().allocationId().getId()) && getLocalCheckpoint() == primaryContext.getCheckpointStates().get(routingEntry().allocationId().getId()).getLocalCheckpoint().     synchronized (mutex) {         // make changes to primaryMode flag only under mutex         replicationTracker.activateWithPrimaryContext(primaryContext).         if (getMaxSeqNoOfUpdatesOrDeletes() == UNASSIGNED_SEQ_NO) {             // we need to bootstrap it manually from its local history.             assert indexSettings.getIndexVersionCreated().before(Version.V_6_5_0).             getEngine().advanceMaxSeqNoOfUpdatesOrDeletes(seqNoStats().getMaxSeqNo()).         }     } }
true;public;0;4;/**  * Check if there are any recoveries pending in-sync.  *  * @return {@code true} if there is at least one shard pending in-sync, otherwise false  */ ;/**  * Check if there are any recoveries pending in-sync.  *  * @return {@code true} if there is at least one shard pending in-sync, otherwise false  */ public boolean pendingInSync() {     assert assertPrimaryMode().     return replicationTracker.pendingInSync(). }
true;public;1;3;/**  * Should be called for each no-op update operation to increment relevant statistics.  *  * @param type the doc type of the update  */ ;/**  * Should be called for each no-op update operation to increment relevant statistics.  *  * @param type the doc type of the update  */ public void noopUpdate(String type) {     internalIndexingStats.noopUpdate(type). }
false;;0;12;;void checkIndex() throws IOException {     if (store.tryIncRef()) {         try {             doCheckIndex().         } catch (IOException e) {             store.markStoreCorrupted(e).             throw e.         } finally {             store.decRef().         }     } }
false;private;0;47;;private void doCheckIndex() throws IOException {     long timeNS = System.nanoTime().     if (!Lucene.indexExists(store.directory())) {         return.     }     BytesStreamOutput os = new BytesStreamOutput().     PrintStream out = new PrintStream(os, false, StandardCharsets.UTF_8.name()).     if ("checksum".equals(checkIndexOnStartup)) {         // physical verification only: verify all checksums for the latest commit         IOException corrupt = null.         MetadataSnapshot metadata = snapshotStoreMetadata().         for (Map.Entry<String, StoreFileMetaData> entry : metadata.asMap().entrySet()) {             try {                 Store.checkIntegrity(entry.getValue(), store.directory()).                 out.println("checksum passed: " + entry.getKey()).             } catch (IOException exc) {                 out.println("checksum failed: " + entry.getKey()).                 exc.printStackTrace(out).                 corrupt = exc.             }         }         out.flush().         if (corrupt != null) {             logger.warn("check index [failure]\n{}", os.bytes().utf8ToString()).             throw corrupt.         }     } else {         // full checkindex         final CheckIndex.Status status = store.checkIndex(out).         out.flush().         if (!status.clean) {             if (state == IndexShardState.CLOSED) {                 // ignore if closed....                 return.             }             logger.warn("check index [failure]\n{}", os.bytes().utf8ToString()).             throw new IOException("index check failure").         }     }     if (logger.isDebugEnabled()) {         logger.debug("check index [success]\n{}", os.bytes().utf8ToString()).     }     recoveryState.getVerifyIndex().checkIndexTime(Math.max(0, TimeValue.nsecToMSec(System.nanoTime() - timeNS))). }
false;;0;7;;Engine getEngine() {     Engine engine = getEngineOrNull().     if (engine == null) {         throw new AlreadyClosedException("engine is closed").     }     return engine. }
true;protected;0;3;/**  * NOTE: returns null if engine is not yet started (e.g. recovery phase 1, copying over index files, is still running), or if engine is  * closed.  */ ;/**  * NOTE: returns null if engine is not yet started (e.g. recovery phase 1, copying over index files, is still running), or if engine is  * closed.  */ protected Engine getEngineOrNull() {     return this.currentEngineReference.get(). }
false;public;6;112;;public void startRecovery(RecoveryState recoveryState, PeerRecoveryTargetService recoveryTargetService, PeerRecoveryTargetService.RecoveryListener recoveryListener, RepositoriesService repositoriesService, BiConsumer<String, MappingMetaData> mappingUpdateConsumer, IndicesService indicesService) {     // }     assert recoveryState.getRecoverySource().equals(shardRouting.recoverySource()).     switch(recoveryState.getRecoverySource().getType()) {         case EMPTY_STORE:         case EXISTING_STORE:             // mark the shard as recovering on the cluster state thread             markAsRecovering("from store", recoveryState).             threadPool.generic().execute(() -> {                 try {                     if (recoverFromStore()) {                         recoveryListener.onRecoveryDone(recoveryState).                     }                 } catch (Exception e) {                     recoveryListener.onRecoveryFailure(recoveryState, new RecoveryFailedException(recoveryState, null, e), true).                 }             }).             break.         case PEER:             try {                 markAsRecovering("from " + recoveryState.getSourceNode(), recoveryState).                 recoveryTargetService.startRecovery(this, recoveryState.getSourceNode(), recoveryListener).             } catch (Exception e) {                 failShard("corrupted preexisting index", e).                 recoveryListener.onRecoveryFailure(recoveryState, new RecoveryFailedException(recoveryState, null, e), true).             }             break.         case SNAPSHOT:             // mark the shard as recovering on the cluster state thread             markAsRecovering("from snapshot", recoveryState).             SnapshotRecoverySource recoverySource = (SnapshotRecoverySource) recoveryState.getRecoverySource().             threadPool.generic().execute(() -> {                 try {                     final Repository repository = repositoriesService.repository(recoverySource.snapshot().getRepository()).                     if (restoreFromRepository(repository)) {                         recoveryListener.onRecoveryDone(recoveryState).                     }                 } catch (Exception e) {                     recoveryListener.onRecoveryFailure(recoveryState, new RecoveryFailedException(recoveryState, null, e), true).                 }             }).             break.         case LOCAL_SHARDS:             final IndexMetaData indexMetaData = indexSettings().getIndexMetaData().             final Index resizeSourceIndex = indexMetaData.getResizeSourceIndex().             final List<IndexShard> startedShards = new ArrayList<>().             final IndexService sourceIndexService = indicesService.indexService(resizeSourceIndex).             final Set<ShardId> requiredShards.             final int numShards.             if (sourceIndexService != null) {                 requiredShards = IndexMetaData.selectRecoverFromShards(shardId().id(), sourceIndexService.getMetaData(), indexMetaData.getNumberOfShards()).                 for (IndexShard shard : sourceIndexService) {                     if (shard.state() == IndexShardState.STARTED && requiredShards.contains(shard.shardId())) {                         startedShards.add(shard).                     }                 }                 numShards = requiredShards.size().             } else {                 numShards = -1.                 requiredShards = Collections.emptySet().             }             if (numShards == startedShards.size()) {                 assert requiredShards.isEmpty() == false.                 // mark the shard as recovering on the cluster state thread                 markAsRecovering("from local shards", recoveryState).                 threadPool.generic().execute(() -> {                     try {                         if (recoverFromLocalShards(mappingUpdateConsumer, startedShards.stream().filter((s) -> requiredShards.contains(s.shardId())).collect(Collectors.toList()))) {                             recoveryListener.onRecoveryDone(recoveryState).                         }                     } catch (Exception e) {                         recoveryListener.onRecoveryFailure(recoveryState, new RecoveryFailedException(recoveryState, null, e), true).                     }                 }).             } else {                 final RuntimeException e.                 if (numShards == -1) {                     e = new IndexNotFoundException(resizeSourceIndex).                 } else {                     e = new IllegalStateException("not all required shards of index " + resizeSourceIndex + " are started yet, expected " + numShards + " found " + startedShards.size() + " can't recover shard " + shardId()).                 }                 throw e.             }             break.         default:             throw new IllegalArgumentException("Unknown recovery source " + recoveryState.getRecoverySource()).     } }
true;public;0;4;/**  * Returns whether the shard is a relocated primary, i.e. not in charge anymore of replicating changes (see {@link ReplicationTracker}).  */ ;/**  * Returns whether the shard is a relocated primary, i.e. not in charge anymore of replicating changes (see {@link ReplicationTracker}).  */ public boolean isRelocatedPrimary() {     assert shardRouting.primary() : "only call isRelocatedPrimary on primary shard".     return replicationTracker.isRelocated(). }
true;public;2;12;// called by the current engine ;// called by the current engine @Override public void onFailedEngine(String reason, @Nullable Exception failure) {     final ShardFailure shardFailure = new ShardFailure(shardRouting, reason, failure).     for (Consumer<ShardFailure> listener : delegates) {         try {             listener.accept(shardFailure).         } catch (Exception inner) {             inner.addSuppressed(failure).             logger.warn("exception while notifying engine failure", inner).         }     } }
false;private,static;5;28;;private static void persistMetadata(final ShardPath shardPath, final IndexSettings indexSettings, final ShardRouting newRouting, @Nullable final ShardRouting currentRouting, final Logger logger) throws IOException {     assert newRouting != null : "newRouting must not be null".     // only persist metadata if routing information that is persisted in shard state metadata actually changed     final ShardId shardId = newRouting.shardId().     if (currentRouting == null || currentRouting.primary() != newRouting.primary() || currentRouting.allocationId().equals(newRouting.allocationId()) == false) {         assert currentRouting == null || currentRouting.isSameAllocation(newRouting).         final String writeReason.         if (currentRouting == null) {             writeReason = "initial state with allocation id [" + newRouting.allocationId() + "]".         } else {             writeReason = "routing changed from " + currentRouting + " to " + newRouting.         }         logger.trace("{} writing shard state, reason [{}]", shardId, writeReason).         final ShardStateMetaData newShardStateMetadata = new ShardStateMetaData(newRouting.primary(), indexSettings.getUUID(), newRouting.allocationId()).         ShardStateMetaData.FORMAT.writeAndCleanup(newShardStateMetadata, shardPath.getShardStatePath()).     } else {         logger.trace("{} skip writing shard state, has been written before", shardId).     } }
false;private;1;4;;private DocumentMapperForType docMapper(String type) {     return mapperService.documentMapperWithAutoCreate(mapperService.resolveDocumentType(type)). }
false;private;0;12;;private EngineConfig newEngineConfig() {     Sort indexSort = indexSortSupplier.get().     return new EngineConfig(shardId, shardRouting.allocationId().getId(), threadPool, indexSettings, warmer, store, indexSettings.getMergePolicy(), mapperService.indexAnalyzer(), similarityService.similarity(mapperService), codecService, shardEventListener, indexCache.query(), cachingPolicy, translogConfig, IndexingMemoryController.SHARD_INACTIVE_TIME_SETTING.get(indexSettings.getSettings()), Collections.singletonList(refreshListeners), Collections.singletonList(new RefreshMetricUpdater(refreshMetric)), indexSort, circuitBreakerService, replicationTracker, replicationTracker::getRetentionLeases, () -> getOperationPrimaryTerm(), tombstoneDocSupplier()). }
true;public;3;6;/**  * Acquire a primary operation permit whenever the shard is ready for indexing. If a permit is directly available, the provided  * ActionListener will be called on the calling thread. During relocation hand-off, permit acquisition can be delayed. The provided  * ActionListener will then be called using the provided executor.  *  * @param debugInfo an extra information that can be useful when tracing an unreleased permit. When assertions are enabled  *                  the tracing will capture the supplied object's {@link Object#toString()} value. Otherwise the object  *                  isn't used  */ ;/**  * Acquire a primary operation permit whenever the shard is ready for indexing. If a permit is directly available, the provided  * ActionListener will be called on the calling thread. During relocation hand-off, permit acquisition can be delayed. The provided  * ActionListener will then be called using the provided executor.  *  * @param debugInfo an extra information that can be useful when tracing an unreleased permit. When assertions are enabled  *                  the tracing will capture the supplied object's {@link Object#toString()} value. Otherwise the object  *                  isn't used  */ public void acquirePrimaryOperationPermit(ActionListener<Releasable> onPermitAcquired, String executorOnDelay, Object debugInfo) {     verifyNotClosed().     assert shardRouting.primary() : "acquirePrimaryOperationPermit should only be called on primary shard: " + shardRouting.     indexShardOperationPermits.acquire(onPermitAcquired, executorOnDelay, false, debugInfo). }
true;public;2;6;/**  * Acquire all primary operation permits. Once all permits are acquired, the provided ActionListener is called.  * It is the responsibility of the caller to close the {@link Releasable}.  */ ;/**  * Acquire all primary operation permits. Once all permits are acquired, the provided ActionListener is called.  * It is the responsibility of the caller to close the {@link Releasable}.  */ public void acquireAllPrimaryOperationsPermits(final ActionListener<Releasable> onPermitAcquired, final TimeValue timeout) {     verifyNotClosed().     assert shardRouting.primary() : "acquireAllPrimaryOperationsPermits should only be called on primary shard: " + shardRouting.     asyncBlockOperations(onPermitAcquired, timeout.duration(), timeout.timeUnit()). }
false;private;3;16;;private void asyncBlockOperations(ActionListener<Releasable> onPermitAcquired, long timeout, TimeUnit timeUnit) {     final Releasable forceRefreshes = refreshListeners.forceRefreshes().     final ActionListener<Releasable> wrappedListener = ActionListener.wrap(r -> {         forceRefreshes.close().         onPermitAcquired.onResponse(r).     }, e -> {         forceRefreshes.close().         onPermitAcquired.onFailure(e).     }).     try {         indexShardOperationPermits.asyncBlockOperations(wrappedListener, timeout, timeUnit).     } catch (Exception e) {         forceRefreshes.close().         throw e.     } }
true;public;4;16;/**  * Runs the specified runnable under a permit and otherwise calling back the specified failure callback. This method is really a  * convenience for {@link #acquirePrimaryOperationPermit(ActionListener, String, Object)} where the listener equates to  * try-with-resources closing the releasable after executing the runnable on successfully acquiring the permit, an otherwise calling  * back the failure callback.  *  * @param runnable the runnable to execute under permit  * @param onFailure the callback on failure  * @param executorOnDelay the executor to execute the runnable on if permit acquisition is blocked  * @param debugInfo debug info  */ ;/**  * Runs the specified runnable under a permit and otherwise calling back the specified failure callback. This method is really a  * convenience for {@link #acquirePrimaryOperationPermit(ActionListener, String, Object)} where the listener equates to  * try-with-resources closing the releasable after executing the runnable on successfully acquiring the permit, an otherwise calling  * back the failure callback.  *  * @param runnable the runnable to execute under permit  * @param onFailure the callback on failure  * @param executorOnDelay the executor to execute the runnable on if permit acquisition is blocked  * @param debugInfo debug info  */ public void runUnderPrimaryPermit(final Runnable runnable, final Consumer<Exception> onFailure, final String executorOnDelay, final Object debugInfo) {     verifyNotClosed().     assert shardRouting.primary() : "runUnderPrimaryPermit should only be called on primary shard but was " + shardRouting.     final ActionListener<Releasable> onPermitAcquired = ActionListener.wrap(releasable -> {         try (Releasable ignore = releasable) {             runnable.run().         }     }, onFailure).     acquirePrimaryOperationPermit(onPermitAcquired, executorOnDelay, debugInfo). }
false;public;1;10;;@Override public void onFailure(final Exception e) {     try {         innerFail(e).     } finally {         if (combineWithAction != null) {             combineWithAction.onFailure(e).         }     } }
false;private;1;7;;private void innerFail(final Exception e) {     try {         failShard("exception during primary term transition", e).     } catch (AlreadyClosedException ace) {     // ignore, shard is already closed     } }
false;public;1;26;;@Override public void onResponse(final Releasable releasable) {     final RunOnce releaseOnce = new RunOnce(releasable::close).     try {         assert getOperationPrimaryTerm() <= pendingPrimaryTerm.         termUpdated.await().         // in the order submitted. We need to guard against another term bump         if (getOperationPrimaryTerm() < newPrimaryTerm) {             replicationTracker.setOperationPrimaryTerm(newPrimaryTerm).             onBlocked.run().         }     } catch (final Exception e) {         if (combineWithAction == null) {             // otherwise leave it to combineWithAction to release the permit             releaseOnce.run().         }         innerFail(e).     } finally {         if (combineWithAction != null) {             combineWithAction.onResponse(releasable).         } else {             releaseOnce.run().         }     } }
false;private;3;57;;private <E extends Exception> void bumpPrimaryTerm(final long newPrimaryTerm, final CheckedRunnable<E> onBlocked, @Nullable ActionListener<Releasable> combineWithAction) {     assert Thread.holdsLock(mutex).     assert newPrimaryTerm > pendingPrimaryTerm || (newPrimaryTerm >= pendingPrimaryTerm && combineWithAction != null).     assert getOperationPrimaryTerm() <= pendingPrimaryTerm.     final CountDownLatch termUpdated = new CountDownLatch(1).     asyncBlockOperations(new ActionListener<Releasable>() {          @Override         public void onFailure(final Exception e) {             try {                 innerFail(e).             } finally {                 if (combineWithAction != null) {                     combineWithAction.onFailure(e).                 }             }         }          private void innerFail(final Exception e) {             try {                 failShard("exception during primary term transition", e).             } catch (AlreadyClosedException ace) {             // ignore, shard is already closed             }         }          @Override         public void onResponse(final Releasable releasable) {             final RunOnce releaseOnce = new RunOnce(releasable::close).             try {                 assert getOperationPrimaryTerm() <= pendingPrimaryTerm.                 termUpdated.await().                 // in the order submitted. We need to guard against another term bump                 if (getOperationPrimaryTerm() < newPrimaryTerm) {                     replicationTracker.setOperationPrimaryTerm(newPrimaryTerm).                     onBlocked.run().                 }             } catch (final Exception e) {                 if (combineWithAction == null) {                     // otherwise leave it to combineWithAction to release the permit                     releaseOnce.run().                 }                 innerFail(e).             } finally {                 if (combineWithAction != null) {                     combineWithAction.onResponse(releasable).                 } else {                     releaseOnce.run().                 }             }         }     }, 30, TimeUnit.MINUTES).     pendingPrimaryTerm = newPrimaryTerm.     termUpdated.countDown(). }
true;public;6;6;/**  * Acquire a replica operation permit whenever the shard is ready for indexing (see  * {@link #acquirePrimaryOperationPermit(ActionListener, String, Object)}). If the given primary term is lower than then one in  * {@link #shardRouting}, the {@link ActionListener#onFailure(Exception)} method of the provided listener is invoked with an  * {@link IllegalStateException}. If permit acquisition is delayed, the listener will be invoked on the executor with the specified  * name.  *  * @param opPrimaryTerm              the operation primary term  * @param globalCheckpoint           the global checkpoint associated with the request  * @param maxSeqNoOfUpdatesOrDeletes the max seq_no of updates (index operations overwrite Lucene) or deletes captured on the primary  *                                   after this replication request was executed on it (see {@link #getMaxSeqNoOfUpdatesOrDeletes()}  * @param onPermitAcquired           the listener for permit acquisition  * @param executorOnDelay            the name of the executor to invoke the listener on if permit acquisition is delayed  * @param debugInfo                  an extra information that can be useful when tracing an unreleased permit. When assertions are  *                                   enabled the tracing will capture the supplied object's {@link Object#toString()} value.  *                                   Otherwise the object isn't used  */ ;/**  * Acquire a replica operation permit whenever the shard is ready for indexing (see  * {@link #acquirePrimaryOperationPermit(ActionListener, String, Object)}). If the given primary term is lower than then one in  * {@link #shardRouting}, the {@link ActionListener#onFailure(Exception)} method of the provided listener is invoked with an  * {@link IllegalStateException}. If permit acquisition is delayed, the listener will be invoked on the executor with the specified  * name.  *  * @param opPrimaryTerm              the operation primary term  * @param globalCheckpoint           the global checkpoint associated with the request  * @param maxSeqNoOfUpdatesOrDeletes the max seq_no of updates (index operations overwrite Lucene) or deletes captured on the primary  *                                   after this replication request was executed on it (see {@link #getMaxSeqNoOfUpdatesOrDeletes()}  * @param onPermitAcquired           the listener for permit acquisition  * @param executorOnDelay            the name of the executor to invoke the listener on if permit acquisition is delayed  * @param debugInfo                  an extra information that can be useful when tracing an unreleased permit. When assertions are  *                                   enabled the tracing will capture the supplied object's {@link Object#toString()} value.  *                                   Otherwise the object isn't used  */ public void acquireReplicaOperationPermit(final long opPrimaryTerm, final long globalCheckpoint, final long maxSeqNoOfUpdatesOrDeletes, final ActionListener<Releasable> onPermitAcquired, final String executorOnDelay, final Object debugInfo) {     innerAcquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, onPermitAcquired, false, (listener) -> indexShardOperationPermits.acquire(listener, executorOnDelay, true, debugInfo)). }
true;public;5;10;/**  * Acquire all replica operation permits whenever the shard is ready for indexing (see  * {@link #acquireAllPrimaryOperationsPermits(ActionListener, TimeValue)}. If the given primary term is lower than then one in  * {@link #shardRouting}, the {@link ActionListener#onFailure(Exception)} method of the provided listener is invoked with an  * {@link IllegalStateException}.  *  * @param opPrimaryTerm              the operation primary term  * @param globalCheckpoint           the global checkpoint associated with the request  * @param maxSeqNoOfUpdatesOrDeletes the max seq_no of updates (index operations overwrite Lucene) or deletes captured on the primary  *                                   after this replication request was executed on it (see {@link #getMaxSeqNoOfUpdatesOrDeletes()}  * @param onPermitAcquired           the listener for permit acquisition  * @param timeout                    the maximum time to wait for the in-flight operations block  */ ;/**  * Acquire all replica operation permits whenever the shard is ready for indexing (see  * {@link #acquireAllPrimaryOperationsPermits(ActionListener, TimeValue)}. If the given primary term is lower than then one in  * {@link #shardRouting}, the {@link ActionListener#onFailure(Exception)} method of the provided listener is invoked with an  * {@link IllegalStateException}.  *  * @param opPrimaryTerm              the operation primary term  * @param globalCheckpoint           the global checkpoint associated with the request  * @param maxSeqNoOfUpdatesOrDeletes the max seq_no of updates (index operations overwrite Lucene) or deletes captured on the primary  *                                   after this replication request was executed on it (see {@link #getMaxSeqNoOfUpdatesOrDeletes()}  * @param onPermitAcquired           the listener for permit acquisition  * @param timeout                    the maximum time to wait for the in-flight operations block  */ public void acquireAllReplicaOperationsPermits(final long opPrimaryTerm, final long globalCheckpoint, final long maxSeqNoOfUpdatesOrDeletes, final ActionListener<Releasable> onPermitAcquired, final TimeValue timeout) {     innerAcquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, onPermitAcquired, true, listener -> asyncBlockOperations(listener, timeout.duration(), timeout.timeUnit())). }
false;public;1;24;;@Override public void onResponse(final Releasable releasable) {     if (opPrimaryTerm < getOperationPrimaryTerm()) {         releasable.close().         final String message = String.format(Locale.ROOT, "%s operation primary term [%d] is too old (current [%d])", shardId, opPrimaryTerm, getOperationPrimaryTerm()).         onPermitAcquired.onFailure(new IllegalStateException(message)).     } else {         assert assertReplicationTarget().         try {             updateGlobalCheckpointOnReplica(globalCheckpoint, "operation").             advanceMaxSeqNoOfUpdatesOrDeletes(maxSeqNoOfUpdatesOrDeletes).         } catch (Exception e) {             releasable.close().             onPermitAcquired.onFailure(e).             return.         }         onPermitAcquired.onResponse(releasable).     } }
false;public;1;4;;@Override public void onFailure(final Exception e) {     onPermitAcquired.onFailure(e). }
false;private;6;82;;private void innerAcquireReplicaOperationPermit(final long opPrimaryTerm, final long globalCheckpoint, final long maxSeqNoOfUpdatesOrDeletes, final ActionListener<Releasable> onPermitAcquired, final boolean allowCombineOperationWithPrimaryTermUpdate, final Consumer<ActionListener<Releasable>> operationExecutor) {     verifyNotClosed().     // This listener is used for the execution of the operation. If the operation requires all the permits for its     // execution and the primary term must be updated first, we can combine the operation execution with the     // primary term update. Since indexShardOperationPermits doesn't guarantee that async submissions are executed     // in the order submitted, combining both operations ensure that the term is updated before the operation is     // executed. It also has the side effect of acquiring all the permits one time instead of two.     final ActionListener<Releasable> operationListener = new ActionListener<Releasable>() {          @Override         public void onResponse(final Releasable releasable) {             if (opPrimaryTerm < getOperationPrimaryTerm()) {                 releasable.close().                 final String message = String.format(Locale.ROOT, "%s operation primary term [%d] is too old (current [%d])", shardId, opPrimaryTerm, getOperationPrimaryTerm()).                 onPermitAcquired.onFailure(new IllegalStateException(message)).             } else {                 assert assertReplicationTarget().                 try {                     updateGlobalCheckpointOnReplica(globalCheckpoint, "operation").                     advanceMaxSeqNoOfUpdatesOrDeletes(maxSeqNoOfUpdatesOrDeletes).                 } catch (Exception e) {                     releasable.close().                     onPermitAcquired.onFailure(e).                     return.                 }                 onPermitAcquired.onResponse(releasable).             }         }          @Override         public void onFailure(final Exception e) {             onPermitAcquired.onFailure(e).         }     }.     if (requirePrimaryTermUpdate(opPrimaryTerm, allowCombineOperationWithPrimaryTermUpdate)) {         synchronized (mutex) {             if (requirePrimaryTermUpdate(opPrimaryTerm, allowCombineOperationWithPrimaryTermUpdate)) {                 final IndexShardState shardState = state().                 // We abort early here to prevent an ongoing recovery from the failed primary to mess with the global / local checkpoint                 if (shardState != IndexShardState.POST_RECOVERY && shardState != IndexShardState.STARTED) {                     throw new IndexShardNotStartedException(shardId, shardState).                 }                 bumpPrimaryTerm(opPrimaryTerm, () -> {                     updateGlobalCheckpointOnReplica(globalCheckpoint, "primary term transition").                     final long currentGlobalCheckpoint = getGlobalCheckpoint().                     final long maxSeqNo = seqNoStats().getMaxSeqNo().                     logger.info("detected new primary with primary term [{}], global checkpoint [{}], max_seq_no [{}]", opPrimaryTerm, currentGlobalCheckpoint, maxSeqNo).                     if (currentGlobalCheckpoint < maxSeqNo) {                         resetEngineToGlobalCheckpoint().                     } else {                         getEngine().rollTranslogGeneration().                     }                 }, allowCombineOperationWithPrimaryTermUpdate ? operationListener : null).                 if (allowCombineOperationWithPrimaryTermUpdate) {                     logger.debug("operation execution has been combined with primary term update").                     return.                 }             }         }     }     assert opPrimaryTerm <= pendingPrimaryTerm : "operation primary term [" + opPrimaryTerm + "] should be at most [" + pendingPrimaryTerm + "]".     operationExecutor.accept(operationListener). }
false;private;2;3;;private boolean requirePrimaryTermUpdate(final long opPrimaryTerm, final boolean allPermits) {     return (opPrimaryTerm > pendingPrimaryTerm) || (allPermits && opPrimaryTerm > getOperationPrimaryTerm()). }
false;public;0;4;;public int getActiveOperationsCount() {     // refCount is incremented on successful acquire and decremented on close     return indexShardOperationPermits.getActiveOperationsCount(). }
true;public;0;3;/**  * @return a list of describing each permit that wasn't released yet. The description consist of the debugInfo supplied  *         when the permit was acquired plus a stack traces that was captured when the permit was request.  */ ;/**  * @return a list of describing each permit that wasn't released yet. The description consist of the debugInfo supplied  *         when the permit was acquired plus a stack traces that was captured when the permit was request.  */ public List<String> getActiveOperations() {     return indexShardOperationPermits.getActiveOperations(). }
false;protected;1;12;;@Override protected void write(List<Tuple<Translog.Location, Consumer<Exception>>> candidates) throws IOException {     try {         getEngine().ensureTranslogSynced(candidates.stream().map(Tuple::v1)).     } catch (AlreadyClosedException ex) {     // that's fine since we already synced everything on engine close - this also is conform with the methods     // documentation     } catch (IOException ex) {         // if this fails we are in deep shit - fail the request         logger.debug("failed to sync translog", ex).         throw ex.     } }
true;public,final;2;4;/**  * Syncs the given location with the underlying storage unless already synced. This method might return immediately without  * actually fsyncing the location until the sync listener is called. Yet, unless there is already another thread fsyncing  * the transaction log the caller thread will be hijacked to run the fsync for all pending fsync operations.  * This method allows indexing threads to continue indexing without blocking on fsync calls. We ensure that there is only  * one thread blocking on the sync an all others can continue indexing.  * NOTE: if the syncListener throws an exception when it's processed the exception will only be logged. Users should make sure that the  * listener handles all exception cases internally.  */ ;/**  * Syncs the given location with the underlying storage unless already synced. This method might return immediately without  * actually fsyncing the location until the sync listener is called. Yet, unless there is already another thread fsyncing  * the transaction log the caller thread will be hijacked to run the fsync for all pending fsync operations.  * This method allows indexing threads to continue indexing without blocking on fsync calls. We ensure that there is only  * one thread blocking on the sync an all others can continue indexing.  * NOTE: if the syncListener throws an exception when it's processed the exception will only be logged. Users should make sure that the  * listener handles all exception cases internally.  */ public final void sync(Translog.Location location, Consumer<Exception> syncListener) {     verifyNotClosed().     translogSyncProcessor.put(location, syncListener). }
false;public;0;4;;public void sync() throws IOException {     verifyNotClosed().     getEngine().syncTranslog(). }
true;public;0;3;/**  * Checks if the underlying storage sync is required.  */ ;/**  * Checks if the underlying storage sync is required.  */ public boolean isSyncNeeded() {     return getEngine().isTranslogSyncNeeded(). }
true;public;0;3;/**  * Returns the current translog durability mode  */ ;/**  * Returns the current translog durability mode  */ public Translog.Durability getTranslogDurability() {     return indexSettings.getTranslogDurability(). }
false;public;1;6;;@Override public void onFailure(final Exception e) {     if (state != IndexShardState.CLOSED) {         logger.warn("failed to roll translog generation", e).     } }
false;protected;0;4;;@Override protected void doRun() throws Exception {     rollTranslogGeneration(). }
false;public;0;5;;@Override public void onAfter() {     flushOrRollRunning.compareAndSet(true, false).     afterWriteOperation(). }
false;public;1;6;;@Override public void onFailure(final Exception e) {     if (state != IndexShardState.CLOSED) {         logger.warn("failed to flush index", e).     } }
false;protected;0;5;;@Override protected void doRun() throws IOException {     flush(new FlushRequest()).     periodicFlushMetric.inc(). }
false;public;0;5;;@Override public void onAfter() {     flushOrRollRunning.compareAndSet(true, false).     afterWriteOperation(). }
true;public;0;62;/**  * Schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. The operation will be  * executed asynchronously on the flush thread pool.  */ ;/**  * Schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. The operation will be  * executed asynchronously on the flush thread pool.  */ public void afterWriteOperation() {     if (shouldPeriodicallyFlush() || shouldRollTranslogGeneration()) {         if (flushOrRollRunning.compareAndSet(false, true)) {             /*                  * We have to check again since otherwise there is a race when a thread passes the first check next to another thread which                  * performs the operation quickly enough to  finish before the current thread could flip the flag. In that situation, we                  * have an extra operation.                  *                  * Additionally, a flush implicitly executes a translog generation roll so if we execute a flush then we do not need to                  * check if we should roll the translog generation.                  */             if (shouldPeriodicallyFlush()) {                 logger.debug("submitting async flush request").                 final AbstractRunnable flush = new AbstractRunnable() {                      @Override                     public void onFailure(final Exception e) {                         if (state != IndexShardState.CLOSED) {                             logger.warn("failed to flush index", e).                         }                     }                      @Override                     protected void doRun() throws IOException {                         flush(new FlushRequest()).                         periodicFlushMetric.inc().                     }                      @Override                     public void onAfter() {                         flushOrRollRunning.compareAndSet(true, false).                         afterWriteOperation().                     }                 }.                 threadPool.executor(ThreadPool.Names.FLUSH).execute(flush).             } else if (shouldRollTranslogGeneration()) {                 logger.debug("submitting async roll translog generation request").                 final AbstractRunnable roll = new AbstractRunnable() {                      @Override                     public void onFailure(final Exception e) {                         if (state != IndexShardState.CLOSED) {                             logger.warn("failed to roll translog generation", e).                         }                     }                      @Override                     protected void doRun() throws Exception {                         rollTranslogGeneration().                     }                      @Override                     public void onAfter() {                         flushOrRollRunning.compareAndSet(true, false).                         afterWriteOperation().                     }                 }.                 threadPool.executor(ThreadPool.Names.FLUSH).execute(roll).             } else {                 flushOrRollRunning.compareAndSet(true, false).             }         }     } }
true;private;0;7;/**  * Build {@linkplain RefreshListeners} for this shard.  */ ;/**  * Build {@linkplain RefreshListeners} for this shard.  */ private RefreshListeners buildRefreshListeners() {     return new RefreshListeners(indexSettings::getMaxRefreshListeners, () -> refresh("too_many_listeners"), threadPool.executor(ThreadPool.Names.LISTENER)::execute, logger, threadPool.getThreadContext()). }
false;;0;3;;EngineFactory getEngineFactory() {     return engineFactory. }
true;;0;3;// for tests ;// for tests ReplicationTracker getReplicationTracker() {     return replicationTracker. }
true;public;0;26;/**  * Executes a scheduled refresh if necessary.  *  * @return <code>true</code> iff the engine got refreshed otherwise <code>false</code>  */ ;/**  * Executes a scheduled refresh if necessary.  *  * @return <code>true</code> iff the engine got refreshed otherwise <code>false</code>  */ public boolean scheduledRefresh() {     verifyNotClosed().     boolean listenerNeedsRefresh = refreshListeners.refreshNeeded().     if (isReadAllowed() && (listenerNeedsRefresh || getEngine().refreshNeeded())) {         if (// if we have a listener that is waiting for a refresh we need to force it         listenerNeedsRefresh == false && isSearchIdle() && indexSettings.isExplicitRefresh() == false && active.get()) {             // it must be active otherwise we might not free up segment memory once the shard became inactive             // lets skip this refresh since we are search idle and             // don't necessarily need to refresh. the next searcher access will register a refreshListener and that will             // cause the next schedule to refresh.             final Engine engine = getEngine().             // try to prune the deletes in the engine if we accumulated some             engine.maybePruneDeletes().             setRefreshPending(engine).             return false.         } else {             if (logger.isTraceEnabled()) {                 logger.trace("refresh with source [schedule]").             }             return getEngine().maybeRefresh("schedule").         }     }     final Engine engine = getEngine().     // try to prune the deletes in the engine if we accumulated some     engine.maybePruneDeletes().     return false. }
true;final;0;3;/**  * Returns true if this shards is search idle  */ ;/**  * Returns true if this shards is search idle  */ final boolean isSearchIdle() {     return (threadPool.relativeTimeInMillis() - lastSearcherAccess.get()) >= indexSettings.getSearchIdleAfter().getMillis(). }
true;final;0;3;/**  * Returns the last timestamp the searcher was accessed. This is a relative timestamp in milliseconds.  */ ;/**  * Returns the last timestamp the searcher was accessed. This is a relative timestamp in milliseconds.  */ final long getLastSearcherAccess() {     return lastSearcherAccess.get(). }
false;private;1;10;;private void setRefreshPending(Engine engine) {     Translog.Location lastWriteLocation = engine.getTranslogLastWriteLocation().     Translog.Location location.     do {         location = this.pendingRefreshLocation.get().         if (location != null && lastWriteLocation.compareTo(location) <= 0) {             break.         }     } while (pendingRefreshLocation.compareAndSet(location, lastWriteLocation) == false). }
true;public,final;1;14;/**  * Registers the given listener and invokes it once the shard is active again and all  * pending refresh translog location has been refreshed. If there is no pending refresh location registered the listener will be  * invoked immediately.  * @param listener the listener to invoke once the pending refresh location is visible. The listener will be called with  *                 <code>true</code> if the listener was registered to wait for a refresh.  */ ;/**  * Registers the given listener and invokes it once the shard is active again and all  * pending refresh translog location has been refreshed. If there is no pending refresh location registered the listener will be  * invoked immediately.  * @param listener the listener to invoke once the pending refresh location is visible. The listener will be called with  *                 <code>true</code> if the listener was registered to wait for a refresh.  */ public final void awaitShardSearchActive(Consumer<Boolean> listener) {     if (isSearchIdle()) {         // move the shard into non-search idle         markSearcherAccessed().     }     final Translog.Location location = pendingRefreshLocation.get().     if (location != null) {         addRefreshListener(location, (b) -> {             pendingRefreshLocation.compareAndSet(location, null).             listener.accept(true).         }).     } else {         listener.accept(false).     } }
true;public;2;19;/**  * Add a listener for refreshes.  *  * @param location the location to listen for  * @param listener for the refresh. Called with true if registering the listener ran it out of slots and forced a refresh. Called with  *        false otherwise.  */ ;/**  * Add a listener for refreshes.  *  * @param location the location to listen for  * @param listener for the refresh. Called with true if registering the listener ran it out of slots and forced a refresh. Called with  *        false otherwise.  */ public void addRefreshListener(Translog.Location location, Consumer<Boolean> listener) {     final boolean readAllowed.     if (isReadAllowed()) {         readAllowed = true.     } else {         // to a listener before a refresh actually happened that contained that operation.         synchronized (mutex) {             readAllowed = isReadAllowed().         }     }     if (readAllowed) {         refreshListeners.addOrNotify(location, listener).     } else {         // we're not yet ready fo ready for reads, just ignore refresh cycles         listener.accept(false).     } }
false;public;0;9;;@Override public void beforeRefresh() throws IOException {     if (Assertions.ENABLED) {         assert callingThread == null : "beforeRefresh was called by " + callingThread.getName() + " without a corresponding call to afterRefresh".         callingThread = Thread.currentThread().     }     currentRefreshStartTime = System.nanoTime(). }
false;public;1;10;;@Override public void afterRefresh(boolean didRefresh) throws IOException {     if (Assertions.ENABLED) {         assert callingThread != null : "afterRefresh called but not beforeRefresh".         assert callingThread == Thread.currentThread() : "beforeRefreshed called by a different thread. current [" + Thread.currentThread().getName() + "], thread that called beforeRefresh [" + callingThread.getName() + "]".         callingThread = null.     }     refreshMetric.inc(System.nanoTime() - currentRefreshStartTime). }
false;public;2;4;;@Override public ParsedDocument newDeleteTombstoneDoc(String type, String id) {     return docMapper(type).getDocumentMapper().createDeleteTombstoneDoc(shardId.getIndexName(), type, id). }
false;public;1;4;;@Override public ParsedDocument newNoopTombstoneDoc(String reason) {     return noopDocumentMapper.createNoopTombstoneDoc(shardId.getIndexName(), reason). }
false;private;0;14;;private EngineConfig.TombstoneDocSupplier tombstoneDocSupplier() {     final RootObjectMapper.Builder noopRootMapper = new RootObjectMapper.Builder("__noop").     final DocumentMapper noopDocumentMapper = new DocumentMapper.Builder(noopRootMapper, mapperService).build(mapperService).     return new EngineConfig.TombstoneDocSupplier() {          @Override         public ParsedDocument newDeleteTombstoneDoc(String type, String id) {             return docMapper(type).getDocumentMapper().createDeleteTombstoneDoc(shardId.getIndexName(), type, id).         }          @Override         public ParsedDocument newNoopTombstoneDoc(String reason) {             return noopDocumentMapper.createNoopTombstoneDoc(shardId.getIndexName(), reason).         }     }. }
true;;0;45;/**  * Rollback the current engine to the safe commit, then replay local translog up to the global checkpoint.  */ ;/**  * Rollback the current engine to the safe commit, then replay local translog up to the global checkpoint.  */ void resetEngineToGlobalCheckpoint() throws IOException {     assert getActiveOperationsCount() == 0 : "Ongoing writes [" + getActiveOperations() + "]".     // persist the global checkpoint to disk     sync().     final SeqNoStats seqNoStats = seqNoStats().     final TranslogStats translogStats = translogStats().     // flush to make sure the latest commit, which will be opened by the read-only engine, includes all operations.     flush(new FlushRequest().waitIfOngoing(true)).     synchronized (mutex) {         verifyNotClosed().         // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata).         final Engine readOnlyEngine = new ReadOnlyEngine(newEngineConfig(), seqNoStats, translogStats, false, Function.identity()).         IOUtils.close(currentEngineReference.getAndSet(readOnlyEngine)).     }     Engine newEngine = null.     try {         final long globalCheckpoint = getGlobalCheckpoint().         trimUnsafeCommits().         synchronized (mutex) {             verifyNotClosed().             // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata).             newEngine = engineFactory.newReadWriteEngine(newEngineConfig()).             onNewEngine(newEngine).         }         newEngine.advanceMaxSeqNoOfUpdatesOrDeletes(globalCheckpoint).         final Engine.TranslogRecoveryRunner translogRunner = (engine, snapshot) -> runTranslogRecovery(engine, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> {         // TODO: add a dedicate recovery stats for the reset translog         }).         newEngine.recoverFromTranslog(translogRunner, globalCheckpoint).         synchronized (mutex) {             verifyNotClosed().             IOUtils.close(currentEngineReference.getAndSet(newEngine)).             // We set active because we are now writing operations to the engine. this way,             // if we go idle after some time and become inactive, we still give sync'd flush a chance to run.             active.set(true).             newEngine = null.         }         // time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during         // which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine.         onSettingsChanged().     } finally {         IOUtils.close(newEngine).     } }
true;public;0;3;/**  * Returns the maximum sequence number of either update or delete operations have been processed in this shard  * or the sequence number from {@link #advanceMaxSeqNoOfUpdatesOrDeletes(long)}. An index request is considered  * as an update operation if it overwrites the existing documents in Lucene index with the same document id.  * <p>  * The primary captures this value after executes a replication request, then transfers it to a replica before  * executing that replication request on a replica.  */ ;/**  * Returns the maximum sequence number of either update or delete operations have been processed in this shard  * or the sequence number from {@link #advanceMaxSeqNoOfUpdatesOrDeletes(long)}. An index request is considered  * as an update operation if it overwrites the existing documents in Lucene index with the same document id.  * <p>  * The primary captures this value after executes a replication request, then transfers it to a replica before  * executing that replication request on a replica.  */ public long getMaxSeqNoOfUpdatesOrDeletes() {     return getEngine().getMaxSeqNoOfUpdatesOrDeletes(). }
true;public;1;7;/**  * A replica calls this method to advance the max_seq_no_of_updates marker of its engine to at least the max_seq_no_of_updates  * value (piggybacked in a replication request) that it receives from its primary before executing that replication request.  * The receiving value is at least as high as the max_seq_no_of_updates on the primary was when any of the operations of that  * replication request were processed on it.  * <p>  * A replica shard also calls this method to bootstrap the max_seq_no_of_updates marker with the value that it received from  * the primary in peer-recovery, before it replays remote translog operations from the primary. The receiving value is at least  * as high as the max_seq_no_of_updates on the primary was when any of these operations were processed on it.  * <p>  * These transfers guarantee that every index/delete operation when executing on a replica engine will observe this marker a value  * which is at least the value of the max_seq_no_of_updates marker on the primary after that operation was executed on the primary.  *  * @see #acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)  * @see RecoveryTarget#indexTranslogOperations(List, int, long, long, RetentionLeases, ActionListener)  */ ;/**  * A replica calls this method to advance the max_seq_no_of_updates marker of its engine to at least the max_seq_no_of_updates  * value (piggybacked in a replication request) that it receives from its primary before executing that replication request.  * The receiving value is at least as high as the max_seq_no_of_updates on the primary was when any of the operations of that  * replication request were processed on it.  * <p>  * A replica shard also calls this method to bootstrap the max_seq_no_of_updates marker with the value that it received from  * the primary in peer-recovery, before it replays remote translog operations from the primary. The receiving value is at least  * as high as the max_seq_no_of_updates on the primary was when any of these operations were processed on it.  * <p>  * These transfers guarantee that every index/delete operation when executing on a replica engine will observe this marker a value  * which is at least the value of the max_seq_no_of_updates marker on the primary after that operation was executed on the primary.  *  * @see #acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)  * @see RecoveryTarget#indexTranslogOperations(List, int, long, long, RetentionLeases, ActionListener)  */ public void advanceMaxSeqNoOfUpdatesOrDeletes(long seqNo) {     assert seqNo != UNASSIGNED_SEQ_NO || getMaxSeqNoOfUpdatesOrDeletes() == UNASSIGNED_SEQ_NO : "replica has max_seq_no_of_updates=" + getMaxSeqNoOfUpdatesOrDeletes() + " but primary does not".     getEngine().advanceMaxSeqNoOfUpdatesOrDeletes(seqNo).     assert seqNo <= getMaxSeqNoOfUpdatesOrDeletes() : getMaxSeqNoOfUpdatesOrDeletes() + " < " + seqNo. }
true;public;0;3;/**  * Performs the pre-closing checks on the {@link IndexShard}.  *  * @throws IllegalStateException if the sanity checks failed  */ ;/**  * Performs the pre-closing checks on the {@link IndexShard}.  *  * @throws IllegalStateException if the sanity checks failed  */ public void verifyShardBeforeIndexClosing() throws IllegalStateException {     getEngine().verifyEngineBeforeIndexClosing(). }
false;;0;3;;RetentionLeaseSyncer getRetentionLeaseSyncer() {     return retentionLeaseSyncer. }
