commented;modifiers;parameterAmount;loc;comment;code
false;public;1;12;;@Override public boolean equals(Object obj) {     if (obj == null || obj.getClass() != InternalDateHistogram.Bucket.class) {         return false.     }     InternalDateHistogram.Bucket that = (InternalDateHistogram.Bucket) obj.     // they are already stored and tested on the InternalDateHistogram object     return key == that.key && docCount == that.docCount && Objects.equals(aggregations, that.aggregations). }
false;public;0;4;;@Override public int hashCode() {     return Objects.hash(getClass(), key, docCount, aggregations). }
false;public;1;6;;@Override public void writeTo(StreamOutput out) throws IOException {     out.writeLong(key).     out.writeVLong(docCount).     aggregations.writeTo(out). }
false;public;0;4;;@Override public String getKeyAsString() {     return format.format(key).toString(). }
false;public;0;4;;@Override public Object getKey() {     return Instant.ofEpochMilli(key).atZone(ZoneOffset.UTC). }
false;public;0;4;;@Override public long getDocCount() {     return docCount. }
false;public;0;4;;@Override public Aggregations getAggregations() {     return aggregations. }
false;;2;10;;Bucket reduce(List<Bucket> buckets, ReduceContext context) {     List<InternalAggregations> aggregations = new ArrayList<>(buckets.size()).     long docCount = 0.     for (Bucket bucket : buckets) {         docCount += bucket.docCount.         aggregations.add((InternalAggregations) bucket.getAggregations()).     }     InternalAggregations aggs = InternalAggregations.reduce(aggregations, context).     return new InternalDateHistogram.Bucket(key, docCount, keyed, format, aggs). }
false;public;2;17;;@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {     String keyAsString = format.format(key).toString().     if (keyed) {         builder.startObject(keyAsString).     } else {         builder.startObject().     }     if (format != DocValueFormat.RAW) {         builder.field(CommonFields.KEY_AS_STRING.getPreferredName(), keyAsString).     }     builder.field(CommonFields.KEY.getPreferredName(), key).     builder.field(CommonFields.DOC_COUNT.getPreferredName(), docCount).     aggregations.toXContentInternal(builder, params).     builder.endObject().     return builder. }
false;public;1;4;;@Override public int compareKey(Bucket other) {     return Long.compare(key, other.key). }
false;public;0;3;;public DocValueFormat getFormatter() {     return format. }
false;public;0;3;;public boolean getKeyed() {     return keyed. }
false;;1;5;;void writeTo(StreamOutput out) throws IOException {     rounding.writeTo(out).     subAggregations.writeTo(out).     out.writeOptionalWriteable(bounds). }
false;public;1;10;;@Override public boolean equals(Object obj) {     if (obj == null || getClass() != obj.getClass()) {         return false.     }     EmptyBucketInfo that = (EmptyBucketInfo) obj.     return Objects.equals(rounding, that.rounding) && Objects.equals(bounds, that.bounds) && Objects.equals(subAggregations, that.subAggregations). }
false;public;0;4;;@Override public int hashCode() {     return Objects.hash(getClass(), rounding, bounds, subAggregations). }
false;protected;1;12;;@Override protected void doWriteTo(StreamOutput out) throws IOException {     InternalOrder.Streams.writeHistogramOrder(order, out, false).     out.writeVLong(minDocCount).     if (minDocCount == 0) {         emptyBucketInfo.writeTo(out).     }     out.writeLong(offset).     out.writeNamedWriteable(format).     out.writeBoolean(keyed).     out.writeList(buckets). }
false;public;0;4;;@Override public String getWriteableName() {     return DateHistogramAggregationBuilder.NAME. }
false;public;0;4;;@Override public List<InternalDateHistogram.Bucket> getBuckets() {     return Collections.unmodifiableList(buckets). }
false;;0;3;;DocValueFormat getFormatter() {     return format. }
false;;0;3;;long getMinDocCount() {     return minDocCount. }
false;;0;3;;long getOffset() {     return offset. }
false;;0;3;;BucketOrder getOrder() {     return order. }
false;public;1;5;;@Override public InternalDateHistogram create(List<Bucket> buckets) {     return new InternalDateHistogram(name, buckets, order, minDocCount, offset, emptyBucketInfo, format, keyed, pipelineAggregators(), metaData). }
false;public;2;4;;@Override public Bucket createBucket(InternalAggregations aggregations, Bucket prototype) {     return new Bucket(prototype.key, prototype.docCount, prototype.keyed, prototype.format, aggregations). }
false;protected;2;4;;@Override protected boolean lessThan(IteratorAndCurrent a, IteratorAndCurrent b) {     return a.current.key < b.current.key. }
false;private;2;62;;private List<Bucket> reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext) {     final PriorityQueue<IteratorAndCurrent> pq = new PriorityQueue<IteratorAndCurrent>(aggregations.size()) {          @Override         protected boolean lessThan(IteratorAndCurrent a, IteratorAndCurrent b) {             return a.current.key < b.current.key.         }     }.     for (InternalAggregation aggregation : aggregations) {         InternalDateHistogram histogram = (InternalDateHistogram) aggregation.         if (histogram.buckets.isEmpty() == false) {             pq.add(new IteratorAndCurrent(histogram.buckets.iterator())).         }     }     List<Bucket> reducedBuckets = new ArrayList<>().     if (pq.size() > 0) {         // list of buckets coming from different shards that have the same key         List<Bucket> currentBuckets = new ArrayList<>().         double key = pq.top().current.key.         do {             final IteratorAndCurrent top = pq.top().             if (top.current.key != key) {                 // the key changes, reduce what we already buffered and reset the buffer for current buckets                 final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceContext).                 if (reduced.getDocCount() >= minDocCount || reduceContext.isFinalReduce() == false) {                     reduceContext.consumeBucketsAndMaybeBreak(1).                     reducedBuckets.add(reduced).                 } else {                     reduceContext.consumeBucketsAndMaybeBreak(-countInnerBucket(reduced)).                 }                 currentBuckets.clear().                 key = top.current.key.             }             currentBuckets.add(top.current).             if (top.iterator.hasNext()) {                 final Bucket next = top.iterator.next().                 assert next.key > top.current.key : "shards must return data sorted by key".                 top.current = next.                 pq.updateTop().             } else {                 pq.pop().             }         } while (pq.size() > 0).         if (currentBuckets.isEmpty() == false) {             final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceContext).             if (reduced.getDocCount() >= minDocCount || reduceContext.isFinalReduce() == false) {                 reduceContext.consumeBucketsAndMaybeBreak(1).                 reducedBuckets.add(reduced).             } else {                 reduceContext.consumeBucketsAndMaybeBreak(-countInnerBucket(reduced)).             }         }     }     return reducedBuckets. }
false;private;2;61;;private void addEmptyBuckets(List<Bucket> list, ReduceContext reduceContext) {     Bucket lastBucket = null.     ExtendedBounds bounds = emptyBucketInfo.bounds.     ListIterator<Bucket> iter = list.listIterator().     // first adding all the empty buckets *before* the actual data (based on th extended_bounds.min the user requested)     InternalAggregations reducedEmptySubAggs = InternalAggregations.reduce(Collections.singletonList(emptyBucketInfo.subAggregations), reduceContext).     if (bounds != null) {         Bucket firstBucket = iter.hasNext() ? list.get(iter.nextIndex()) : null.         if (firstBucket == null) {             if (bounds.getMin() != null && bounds.getMax() != null) {                 long key = bounds.getMin() + offset.                 long max = bounds.getMax() + offset.                 while (key <= max) {                     reduceContext.consumeBucketsAndMaybeBreak(1).                     iter.add(new InternalDateHistogram.Bucket(key, 0, keyed, format, reducedEmptySubAggs)).                     key = nextKey(key).longValue().                 }             }         } else {             if (bounds.getMin() != null) {                 long key = bounds.getMin() + offset.                 if (key < firstBucket.key) {                     while (key < firstBucket.key) {                         reduceContext.consumeBucketsAndMaybeBreak(1).                         iter.add(new InternalDateHistogram.Bucket(key, 0, keyed, format, reducedEmptySubAggs)).                         key = nextKey(key).longValue().                     }                 }             }         }     }     // e.g. if the data series is [1,2,3,7] there're 3 empty buckets that will be created for 4,5,6     while (iter.hasNext()) {         Bucket nextBucket = list.get(iter.nextIndex()).         if (lastBucket != null) {             long key = nextKey(lastBucket.key).longValue().             while (key < nextBucket.key) {                 reduceContext.consumeBucketsAndMaybeBreak(1).                 iter.add(new InternalDateHistogram.Bucket(key, 0, keyed, format, reducedEmptySubAggs)).                 key = nextKey(key).longValue().             }             assert key == nextBucket.key : "key: " + key + ", nextBucket.key: " + nextBucket.key.         }         lastBucket = iter.next().     }     // finally, adding the empty buckets *after* the actual data (based on the extended_bounds.max requested by the user)     if (bounds != null && lastBucket != null && bounds.getMax() != null && bounds.getMax() + offset > lastBucket.key) {         long key = nextKey(lastBucket.key).longValue().         long max = bounds.getMax() + offset.         while (key <= max) {             reduceContext.consumeBucketsAndMaybeBreak(1).             iter.add(new InternalDateHistogram.Bucket(key, 0, keyed, format, reducedEmptySubAggs)).             key = nextKey(key).longValue().         }     } }
false;public;2;22;;@Override public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) {     List<Bucket> reducedBuckets = reduceBuckets(aggregations, reduceContext).     if (reduceContext.isFinalReduce()) {         if (minDocCount == 0) {             addEmptyBuckets(reducedBuckets, reduceContext).         }         if (InternalOrder.isKeyDesc(order)) {             // we just need to reverse here...             List<Bucket> reverse = new ArrayList<>(reducedBuckets).             Collections.reverse(reverse).             reducedBuckets = reverse.         } else if (InternalOrder.isKeyAsc(order) == false) {             // nothing to do when sorting by key ascending, as data is already sorted since shards return             // sorted buckets and the merge-sort performed by reduceBuckets maintains order.             // otherwise, sorted by compound order or sub-aggregation, we need to fall back to a costly n*log(n) sort             CollectionUtil.introSort(reducedBuckets, order.comparator(null)).         }     }     return new InternalDateHistogram(getName(), reducedBuckets, order, minDocCount, offset, emptyBucketInfo, format, keyed, pipelineAggregators(), getMetaData()). }
false;public;2;17;;@Override public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {     if (keyed) {         builder.startObject(CommonFields.BUCKETS.getPreferredName()).     } else {         builder.startArray(CommonFields.BUCKETS.getPreferredName()).     }     for (Bucket bucket : buckets) {         bucket.toXContent(builder, params).     }     if (keyed) {         builder.endObject().     } else {         builder.endArray().     }     return builder. }
false;public;1;4;;// HistogramFactory method impls @Override public Number getKey(MultiBucketsAggregation.Bucket bucket) {     return ((Bucket) bucket).key. }
false;public;1;4;;@Override public Number nextKey(Number key) {     return emptyBucketInfo.rounding.nextRoundingValue(key.longValue() - offset) + offset. }
false;public;1;11;;@Override public InternalAggregation createAggregation(List<MultiBucketsAggregation.Bucket> buckets) {     // convert buckets to the right type     List<Bucket> buckets2 = new ArrayList<>(buckets.size()).     for (Object b : buckets) {         buckets2.add((Bucket) b).     }     buckets2 = Collections.unmodifiableList(buckets2).     return new InternalDateHistogram(name, buckets2, order, minDocCount, offset, emptyBucketInfo, format, keyed, pipelineAggregators(), getMetaData()). }
false;public;3;4;;@Override public Bucket createBucket(Number key, long docCount, InternalAggregations aggregations) {     return new Bucket(key.longValue(), docCount, keyed, format, aggregations). }
false;protected;1;11;;@Override protected boolean doEquals(Object obj) {     InternalDateHistogram that = (InternalDateHistogram) obj.     return Objects.equals(buckets, that.buckets) && Objects.equals(order, that.order) && Objects.equals(format, that.format) && Objects.equals(keyed, that.keyed) && Objects.equals(minDocCount, that.minDocCount) && Objects.equals(offset, that.offset) && Objects.equals(emptyBucketInfo, that.emptyBucketInfo). }
false;protected;0;4;;@Override protected int doHashCode() {     return Objects.hash(buckets, order, format, keyed, minDocCount, offset, emptyBucketInfo). }
