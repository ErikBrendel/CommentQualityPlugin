commented;modifiers;parameterAmount;loc;comment;code
false;public,static;3;11;;public static Version parseVersion(@Nullable String version, Version defaultVersion, Logger logger) {     if (version == null) {         return defaultVersion.     }     try {         return Version.parse(version).     } catch (ParseException e) {         logger.warn(() -> new ParameterizedMessage("no version match {}, default to {}", version, defaultVersion), e).         return defaultVersion.     } }
true;public,static;1;3;/**  * Reads the segments infos, failing if it fails to load  */ ;/**  * Reads the segments infos, failing if it fails to load  */ public static SegmentInfos readSegmentInfos(Directory directory) throws IOException {     return SegmentInfos.readLatestCommit(directory). }
true;public,static;1;8;/**  * Returns an iterable that allows to iterate over all files in this segments info  */ ;/**  * Returns an iterable that allows to iterate over all files in this segments info  */ public static Iterable<String> files(SegmentInfos infos) throws IOException {     final List<Collection<String>> list = new ArrayList<>().     list.add(Collections.singleton(infos.getSegmentsFileName())).     for (SegmentCommitInfo info : infos) {         list.add(info.files()).     }     return Iterables.flatten(list). }
true;public,static;1;7;/**  * Returns the number of documents in the index referenced by this {@link SegmentInfos}  */ ;/**  * Returns the number of documents in the index referenced by this {@link SegmentInfos}  */ public static int getNumDocs(SegmentInfos info) {     int numDocs = 0.     for (SegmentCommitInfo si : info) {         numDocs += si.info.maxDoc() - si.getDelCount() - si.getSoftDelCount().     }     return numDocs. }
true;public,static;1;6;/**  * Reads the segments infos from the given commit, failing if it fails to load  */ ;/**  * Reads the segments infos from the given commit, failing if it fails to load  */ public static SegmentInfos readSegmentInfos(IndexCommit commit) throws IOException {     // Using commit.getSegmentsFileName() does NOT work here, have to     // manually create the segment filename     String filename = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS, "", commit.getGeneration()).     return SegmentInfos.readCommit(commit.getDirectory(), filename). }
true;private,static;2;3;/**  * Reads the segments infos from the given segments file name, failing if it fails to load  */ ;/**  * Reads the segments infos from the given segments file name, failing if it fails to load  */ private static SegmentInfos readSegmentInfos(String segmentsFileName, Directory directory) throws IOException {     return SegmentInfos.readCommit(directory, segmentsFileName). }
true;public,static;2;35;/**  * This method removes all files from the given directory that are not referenced by the given segments file.  * This method will open an IndexWriter and relies on index file deleter to remove all unreferenced files. Segment files  * that are newer than the given segments file are removed forcefully to prevent problems with IndexWriter opening a potentially  * broken commit point / leftover.  * <b>Note:</b> this method will fail if there is another IndexWriter open on the given directory. This method will also acquire  * a write lock from the directory while pruning unused files. This method expects an existing index in the given directory that has  * the given segments file.  */ ;/**  * This method removes all files from the given directory that are not referenced by the given segments file.  * This method will open an IndexWriter and relies on index file deleter to remove all unreferenced files. Segment files  * that are newer than the given segments file are removed forcefully to prevent problems with IndexWriter opening a potentially  * broken commit point / leftover.  * <b>Note:</b> this method will fail if there is another IndexWriter open on the given directory. This method will also acquire  * a write lock from the directory while pruning unused files. This method expects an existing index in the given directory that has  * the given segments file.  */ public static SegmentInfos pruneUnreferencedFiles(String segmentsFileName, Directory directory) throws IOException {     final SegmentInfos si = readSegmentInfos(segmentsFileName, directory).     try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {         int foundSegmentFiles = 0.         for (final String file : directory.listAll()) {             /*                  * we could also use a deletion policy here but in the case of snapshot and restore                  * sometimes we restore an index and override files that were referenced by a "future"                  * commit. If such a commit is opened by the IW it would likely throw a corrupted index exception                  * since checksums don's match anymore. that's why we prune the name here directly.                  * We also want the caller to know if we were not able to remove a segments_N file.                  */             if (file.startsWith(IndexFileNames.SEGMENTS) || file.equals(IndexFileNames.OLD_SEGMENTS_GEN)) {                 foundSegmentFiles++.                 if (file.equals(si.getSegmentsFileName()) == false) {                     // remove all segment_N files except of the one we wanna keep                     directory.deleteFile(file).                 }             }         }         assert SegmentInfos.getLastCommitSegmentsFileName(directory).equals(segmentsFileName).         if (foundSegmentFiles == 0) {             throw new IllegalStateException("no commit found in the directory").         }     }     final IndexCommit cp = getIndexCommit(si, directory).     try (IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(Lucene.STANDARD_ANALYZER).setSoftDeletesField(Lucene.SOFT_DELETES_FIELD).setIndexCommit(cp).setCommitOnClose(false).setMergePolicy(NoMergePolicy.INSTANCE).setOpenMode(IndexWriterConfig.OpenMode.APPEND))) {     // do nothing and close this will kick off IndexFileDeleter which will remove all pending files     }     return si. }
true;public,static;2;3;/**  * Returns an index commit for the given {@link SegmentInfos} in the given directory.  */ ;/**  * Returns an index commit for the given {@link SegmentInfos} in the given directory.  */ public static IndexCommit getIndexCommit(SegmentInfos si, Directory directory) throws IOException {     return new CommitPoint(si, directory). }
true;public,static;1;17;/**  * This method removes all lucene files from the given directory. It will first try to delete all commit points / segments  * files to ensure broken commits or corrupted indices will not be opened in the future. If any of the segment files can't be deleted  * this operation fails.  */ ;/**  * This method removes all lucene files from the given directory. It will first try to delete all commit points / segments  * files to ensure broken commits or corrupted indices will not be opened in the future. If any of the segment files can't be deleted  * this operation fails.  */ public static void cleanLuceneIndex(Directory directory) throws IOException {     try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {         for (final String file : directory.listAll()) {             if (file.startsWith(IndexFileNames.SEGMENTS) || file.equals(IndexFileNames.OLD_SEGMENTS_GEN)) {                 // remove all segment_N files                 directory.deleteFile(file).             }         }     }     try (IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(Lucene.STANDARD_ANALYZER).setSoftDeletesField(Lucene.SOFT_DELETES_FIELD).setMergePolicy(// no merges     NoMergePolicy.INSTANCE).setCommitOnClose(// no commits     false).setOpenMode(// force creation - don't append...     IndexWriterConfig.OpenMode.CREATE))) {     // do nothing and close this will kick of IndexFileDeleter which will remove all pending files     } }
false;protected;1;7;;@Override protected Object doBody(String segmentFileName) throws IOException {     try (IndexInput input = directory.openInput(segmentFileName, IOContext.READ)) {         CodecUtil.checksumEntireFile(input).     }     return null. }
false;public,static;1;12;;public static void checkSegmentInfoIntegrity(final Directory directory) throws IOException {     new SegmentInfos.FindSegmentsFile(directory) {          @Override         protected Object doBody(String segmentFileName) throws IOException {             try (IndexInput input = directory.openInput(segmentFileName, IOContext.READ)) {                 CodecUtil.checksumEntireFile(input).             }             return null.         }     }.run(). }
true;public,static;2;19;/**  * Check whether there is one or more documents matching the provided query.  */ ;/**  * Check whether there is one or more documents matching the provided query.  */ public static boolean exists(IndexSearcher searcher, Query query) throws IOException {     final Weight weight = searcher.createWeight(searcher.rewrite(query), ScoreMode.COMPLETE_NO_SCORES, 1f).     // match than the bulk scorer API     for (LeafReaderContext context : searcher.getIndexReader().leaves()) {         final Scorer scorer = weight.scorer(context).         if (scorer == null) {             continue.         }         final Bits liveDocs = context.reader().getLiveDocs().         final DocIdSetIterator iterator = scorer.iterator().         for (int doc = iterator.nextDoc(). doc != DocIdSetIterator.NO_MORE_DOCS. doc = iterator.nextDoc()) {             if (liveDocs == null || liveDocs.get(doc)) {                 return true.             }         }     }     return false. }
false;public,static;1;8;;public static TotalHits readTotalHits(StreamInput in) throws IOException {     long totalHits = in.readVLong().     TotalHits.Relation totalHitsRelation = TotalHits.Relation.EQUAL_TO.     if (in.getVersion().onOrAfter(org.elasticsearch.Version.V_7_0_0)) {         totalHitsRelation = in.readEnum(TotalHits.Relation.class).     }     return new TotalHits(totalHits, totalHitsRelation). }
false;public,static;1;38;;public static TopDocsAndMaxScore readTopDocs(StreamInput in) throws IOException {     byte type = in.readByte().     if (type == 0) {         TotalHits totalHits = readTotalHits(in).         float maxScore = in.readFloat().         ScoreDoc[] scoreDocs = new ScoreDoc[in.readVInt()].         for (int i = 0. i < scoreDocs.length. i++) {             scoreDocs[i] = new ScoreDoc(in.readVInt(), in.readFloat()).         }         return new TopDocsAndMaxScore(new TopDocs(totalHits, scoreDocs), maxScore).     } else if (type == 1) {         TotalHits totalHits = readTotalHits(in).         float maxScore = in.readFloat().         SortField[] fields = in.readArray(Lucene::readSortField, SortField[]::new).         FieldDoc[] fieldDocs = new FieldDoc[in.readVInt()].         for (int i = 0. i < fieldDocs.length. i++) {             fieldDocs[i] = readFieldDoc(in).         }         return new TopDocsAndMaxScore(new TopFieldDocs(totalHits, fieldDocs, fields), maxScore).     } else if (type == 2) {         TotalHits totalHits = readTotalHits(in).         float maxScore = in.readFloat().         String field = in.readString().         SortField[] fields = in.readArray(Lucene::readSortField, SortField[]::new).         int size = in.readVInt().         Object[] collapseValues = new Object[size].         FieldDoc[] fieldDocs = new FieldDoc[size].         for (int i = 0. i < fieldDocs.length. i++) {             fieldDocs[i] = readFieldDoc(in).             collapseValues[i] = readSortValue(in).         }         return new TopDocsAndMaxScore(new CollapseTopFieldDocs(field, totalHits, fieldDocs, fields, collapseValues), maxScore).     } else {         throw new IllegalStateException("Unknown type " + type).     } }
false;public,static;1;30;;public static FieldDoc readFieldDoc(StreamInput in) throws IOException {     Comparable[] cFields = new Comparable[in.readVInt()].     for (int j = 0. j < cFields.length. j++) {         byte type = in.readByte().         if (type == 0) {             cFields[j] = null.         } else if (type == 1) {             cFields[j] = in.readString().         } else if (type == 2) {             cFields[j] = in.readInt().         } else if (type == 3) {             cFields[j] = in.readLong().         } else if (type == 4) {             cFields[j] = in.readFloat().         } else if (type == 5) {             cFields[j] = in.readDouble().         } else if (type == 6) {             cFields[j] = in.readByte().         } else if (type == 7) {             cFields[j] = in.readShort().         } else if (type == 8) {             cFields[j] = in.readBoolean().         } else if (type == 9) {             cFields[j] = in.readBytesRef().         } else {             throw new IOException("Can't match type [" + type + "]").         }     }     return new FieldDoc(in.readVInt(), in.readFloat(), cFields). }
false;public,static;1;26;;public static Comparable readSortValue(StreamInput in) throws IOException {     byte type = in.readByte().     if (type == 0) {         return null.     } else if (type == 1) {         return in.readString().     } else if (type == 2) {         return in.readInt().     } else if (type == 3) {         return in.readLong().     } else if (type == 4) {         return in.readFloat().     } else if (type == 5) {         return in.readDouble().     } else if (type == 6) {         return in.readByte().     } else if (type == 7) {         return in.readShort().     } else if (type == 8) {         return in.readBoolean().     } else if (type == 9) {         return in.readBytesRef().     } else {         throw new IOException("Can't match type [" + type + "]").     } }
false;public,static;1;3;;public static ScoreDoc readScoreDoc(StreamInput in) throws IOException {     return new ScoreDoc(in.readVInt(), in.readFloat()). }
false;public,static;2;8;;public static void writeTotalHits(StreamOutput out, TotalHits totalHits) throws IOException {     out.writeVLong(totalHits.value).     if (out.getVersion().onOrAfter(org.elasticsearch.Version.V_7_0_0)) {         out.writeEnum(totalHits.relation).     } else if (totalHits.value > 0 && totalHits.relation != TotalHits.Relation.EQUAL_TO) {         throw new IllegalArgumentException("Cannot serialize approximate total hit counts to nodes that are on a version < 7.0.0").     } }
false;public,static;2;41;;public static void writeTopDocs(StreamOutput out, TopDocsAndMaxScore topDocs) throws IOException {     if (topDocs.topDocs instanceof CollapseTopFieldDocs) {         out.writeByte((byte) 2).         CollapseTopFieldDocs collapseDocs = (CollapseTopFieldDocs) topDocs.topDocs.         writeTotalHits(out, topDocs.topDocs.totalHits).         out.writeFloat(topDocs.maxScore).         out.writeString(collapseDocs.field).         out.writeArray(Lucene::writeSortField, collapseDocs.fields).         out.writeVInt(topDocs.topDocs.scoreDocs.length).         for (int i = 0. i < topDocs.topDocs.scoreDocs.length. i++) {             ScoreDoc doc = collapseDocs.scoreDocs[i].             writeFieldDoc(out, (FieldDoc) doc).             writeSortValue(out, collapseDocs.collapseValues[i]).         }     } else if (topDocs.topDocs instanceof TopFieldDocs) {         out.writeByte((byte) 1).         TopFieldDocs topFieldDocs = (TopFieldDocs) topDocs.topDocs.         writeTotalHits(out, topDocs.topDocs.totalHits).         out.writeFloat(topDocs.maxScore).         out.writeArray(Lucene::writeSortField, topFieldDocs.fields).         out.writeVInt(topDocs.topDocs.scoreDocs.length).         for (ScoreDoc doc : topFieldDocs.scoreDocs) {             writeFieldDoc(out, (FieldDoc) doc).         }     } else {         out.writeByte((byte) 0).         writeTotalHits(out, topDocs.topDocs.totalHits).         out.writeFloat(topDocs.maxScore).         out.writeVInt(topDocs.topDocs.scoreDocs.length).         for (ScoreDoc doc : topDocs.topDocs.scoreDocs) {             writeScoreDoc(out, doc).         }     } }
false;private,static;2;10;;private static void writeMissingValue(StreamOutput out, Object missingValue) throws IOException {     if (missingValue == SortField.STRING_FIRST) {         out.writeByte((byte) 1).     } else if (missingValue == SortField.STRING_LAST) {         out.writeByte((byte) 2).     } else {         out.writeByte((byte) 0).         out.writeGenericValue(missingValue).     } }
false;private,static;1;13;;private static Object readMissingValue(StreamInput in) throws IOException {     final byte id = in.readByte().     switch(id) {         case 0:             return in.readGenericValue().         case 1:             return SortField.STRING_FIRST.         case 2:             return SortField.STRING_LAST.         default:             throw new IOException("Unknown missing value id: " + id).     } }
false;public,static;2;37;;public static void writeSortValue(StreamOutput out, Object field) throws IOException {     if (field == null) {         out.writeByte((byte) 0).     } else {         Class type = field.getClass().         if (type == String.class) {             out.writeByte((byte) 1).             out.writeString((String) field).         } else if (type == Integer.class) {             out.writeByte((byte) 2).             out.writeInt((Integer) field).         } else if (type == Long.class) {             out.writeByte((byte) 3).             out.writeLong((Long) field).         } else if (type == Float.class) {             out.writeByte((byte) 4).             out.writeFloat((Float) field).         } else if (type == Double.class) {             out.writeByte((byte) 5).             out.writeDouble((Double) field).         } else if (type == Byte.class) {             out.writeByte((byte) 6).             out.writeByte((Byte) field).         } else if (type == Short.class) {             out.writeByte((byte) 7).             out.writeShort((Short) field).         } else if (type == Boolean.class) {             out.writeByte((byte) 8).             out.writeBoolean((Boolean) field).         } else if (type == BytesRef.class) {             out.writeByte((byte) 9).             out.writeBytesRef((BytesRef) field).         } else {             throw new IOException("Can't handle sort field value of type [" + type + "]").         }     } }
false;public,static;2;8;;public static void writeFieldDoc(StreamOutput out, FieldDoc fieldDoc) throws IOException {     out.writeVInt(fieldDoc.fields.length).     for (Object field : fieldDoc.fields) {         writeSortValue(out, field).     }     out.writeVInt(fieldDoc.doc).     out.writeFloat(fieldDoc.score). }
false;public,static;2;7;;public static void writeScoreDoc(StreamOutput out, ScoreDoc scoreDoc) throws IOException {     if (!scoreDoc.getClass().equals(ScoreDoc.class)) {         throw new IllegalArgumentException("This method can only be used to serialize a ScoreDoc, not a " + scoreDoc.getClass()).     }     out.writeVInt(scoreDoc.doc).     out.writeFloat(scoreDoc.score). }
true;public,static;1;3;// LUCENE 4 UPGRADE: We might want to maintain our own ordinal, instead of Lucene's ordinal ;// LUCENE 4 UPGRADE: We might want to maintain our own ordinal, instead of Lucene's ordinal public static SortField.Type readSortType(StreamInput in) throws IOException {     return SortField.Type.values()[in.readVInt()]. }
false;public,static;1;14;;public static SortField readSortField(StreamInput in) throws IOException {     String field = null.     if (in.readBoolean()) {         field = in.readString().     }     SortField.Type sortType = readSortType(in).     Object missingValue = readMissingValue(in).     boolean reverse = in.readBoolean().     SortField sortField = new SortField(field, sortType, reverse).     if (missingValue != null) {         sortField.setMissingValue(missingValue).     }     return sortField. }
false;public,static;2;3;;public static void writeSortType(StreamOutput out, SortField.Type sortType) throws IOException {     out.writeVInt(sortType.ordinal()). }
false;public,static;2;43;;public static void writeSortField(StreamOutput out, SortField sortField) throws IOException {     if (sortField.getClass() == GEO_DISTANCE_SORT_TYPE_CLASS) {         // for geo sorting, we replace the SortField with a SortField that assumes a double field.         // this works since the SortField is only used for merging top docs         SortField newSortField = new SortField(sortField.getField(), SortField.Type.DOUBLE).         newSortField.setMissingValue(sortField.getMissingValue()).         sortField = newSortField.     } else if (sortField.getClass() == SortedSetSortField.class) {         // for multi-valued sort field, we replace the SortedSetSortField with a simple SortField.         // It works because the sort field is only used to merge results from different shards.         SortField newSortField = new SortField(sortField.getField(), SortField.Type.STRING, sortField.getReverse()).         newSortField.setMissingValue(sortField.getMissingValue()).         sortField = newSortField.     } else if (sortField.getClass() == SortedNumericSortField.class) {         // for multi-valued sort field, we replace the SortedSetSortField with a simple SortField.         // It works because the sort field is only used to merge results from different shards.         SortField newSortField = new SortField(sortField.getField(), ((SortedNumericSortField) sortField).getNumericType(), sortField.getReverse()).         newSortField.setMissingValue(sortField.getMissingValue()).         sortField = newSortField.     }     if (sortField.getClass() != SortField.class) {         throw new IllegalArgumentException("Cannot serialize SortField impl [" + sortField + "]").     }     if (sortField.getField() == null) {         out.writeBoolean(false).     } else {         out.writeBoolean(true).         out.writeString(sortField.getField()).     }     if (sortField.getComparatorSource() != null) {         IndexFieldData.XFieldComparatorSource comparatorSource = (IndexFieldData.XFieldComparatorSource) sortField.getComparatorSource().         writeSortType(out, comparatorSource.reducedType()).         writeMissingValue(out, comparatorSource.missingValue(sortField.getReverse())).     } else {         writeSortType(out, sortField.getType()).         writeMissingValue(out, sortField.getMissingValue()).     }     out.writeBoolean(sortField.getReverse()). }
false;private,static;1;17;;private static Number readExplanationValue(StreamInput in) throws IOException {     if (in.getVersion().onOrAfter(org.elasticsearch.Version.V_7_0_0)) {         final int numberType = in.readByte().         switch(numberType) {             case 0:                 return in.readFloat().             case 1:                 return in.readDouble().             case 2:                 return in.readZLong().             default:                 throw new IOException("Unexpected number type: " + numberType).         }     } else {         return in.readFloat().     } }
false;public,static;1;13;;public static Explanation readExplanation(StreamInput in) throws IOException {     boolean match = in.readBoolean().     String description = in.readString().     final Explanation[] subExplanations = new Explanation[in.readVInt()].     for (int i = 0. i < subExplanations.length. ++i) {         subExplanations[i] = readExplanation(in).     }     if (match) {         return Explanation.match(readExplanationValue(in), description, subExplanations).     } else {         return Explanation.noMatch(description, subExplanations).     } }
false;private,static;2;16;;private static void writeExplanationValue(StreamOutput out, Number value) throws IOException {     if (out.getVersion().onOrAfter(org.elasticsearch.Version.V_7_0_0)) {         if (value instanceof Float) {             out.writeByte((byte) 0).             out.writeFloat(value.floatValue()).         } else if (value instanceof Double) {             out.writeByte((byte) 1).             out.writeDouble(value.doubleValue()).         } else {             out.writeByte((byte) 2).             out.writeZLong(value.longValue()).         }     } else {         out.writeFloat(value.floatValue()).     } }
false;public,static;2;12;;public static void writeExplanation(StreamOutput out, Explanation explanation) throws IOException {     out.writeBoolean(explanation.isMatch()).     out.writeString(explanation.getDescription()).     Explanation[] subExplanations = explanation.getDetails().     out.writeVInt(subExplanations.length).     for (Explanation subExp : subExplanations) {         writeExplanation(out, subExp).     }     if (explanation.isMatch()) {         writeExplanationValue(out, explanation.getValue()).     } }
false;public,static;1;3;;public static boolean indexExists(final Directory directory) throws IOException {     return DirectoryReader.indexExists(directory). }
true;public,static;2;22;/**  * Wait for an index to exist for up to {@code timeLimitMillis}. Returns  * true if the index eventually exists, false if not.  *  * Will retry the directory every second for at least {@code timeLimitMillis}  */ ;/**  * Wait for an index to exist for up to {@code timeLimitMillis}. Returns  * true if the index eventually exists, false if not.  *  * Will retry the directory every second for at least {@code timeLimitMillis}  */ public static boolean waitForIndex(final Directory directory, final long timeLimitMillis) throws IOException {     final long DELAY = 1000.     long waited = 0.     try {         while (true) {             if (waited >= timeLimitMillis) {                 break.             }             if (indexExists(directory)) {                 return true.             }             Thread.sleep(DELAY).             waited += DELAY.         }     } catch (InterruptedException e) {         Thread.currentThread().interrupt().         return false.     }     // one more try after all retries     return indexExists(directory). }
true;public,static;1;3;/**  * Returns {@code true} iff the given exception or  * one of it's causes is an instance of {@link CorruptIndexException},  * {@link IndexFormatTooOldException}, or {@link IndexFormatTooNewException} otherwise {@code false}.  */ ;/**  * Returns {@code true} iff the given exception or  * one of it's causes is an instance of {@link CorruptIndexException},  * {@link IndexFormatTooOldException}, or {@link IndexFormatTooNewException} otherwise {@code false}.  */ public static boolean isCorruptionException(Throwable t) {     return ExceptionsHelper.unwrapCorruption(t) != null. }
true;public,static;2;3;/**  * Parses the version string lenient and returns the default value if the given string is null or empty  */ ;/**  * Parses the version string lenient and returns the default value if the given string is null or empty  */ public static Version parseVersionLenient(String toParse, Version defaultValue) {     return LenientParser.parse(toParse, defaultValue). }
true;public,static;1;13;/**  * Tries to extract a segment reader from the given index reader.  * If no SegmentReader can be extracted an {@link IllegalStateException} is thrown.  */ ;/**  * Tries to extract a segment reader from the given index reader.  * If no SegmentReader can be extracted an {@link IllegalStateException} is thrown.  */ public static SegmentReader segmentReader(LeafReader reader) {     if (reader instanceof SegmentReader) {         return (SegmentReader) reader.     } else if (reader instanceof FilterLeafReader) {         final FilterLeafReader fReader = (FilterLeafReader) reader.         return segmentReader(FilterLeafReader.unwrap(fReader)).     } else if (reader instanceof FilterCodecReader) {         final FilterCodecReader fReader = (FilterCodecReader) reader.         return segmentReader(FilterCodecReader.unwrap(fReader)).     }     // hard fail - we can't get a SegmentReader     throw new IllegalStateException("Can not extract segment reader from given index reader [" + reader + "]"). }
false;public,static;2;10;;public static Version parse(String toParse, Version defaultValue) {     if (Strings.hasLength(toParse)) {         try {             return Version.parseLeniently(toParse).         } catch (ParseException e) {         // pass to default         }     }     return defaultValue. }
false;public;0;4;;@Override public String toString() {     return "DirectoryReader.ReaderCommit(" + segmentsFileName + ")". }
false;public;0;4;;@Override public int getSegmentCount() {     return segmentCount. }
false;public;0;4;;@Override public String getSegmentsFileName() {     return segmentsFileName. }
false;public;0;4;;@Override public Collection<String> getFileNames() {     return files. }
false;public;0;4;;@Override public Directory getDirectory() {     return dir. }
false;public;0;4;;@Override public long getGeneration() {     return generation. }
false;public;0;4;;@Override public boolean isDeleted() {     return false. }
false;public;0;4;;@Override public Map<String, String> getUserData() {     return userData. }
false;public;0;4;;@Override public void delete() {     throw new UnsupportedOperationException("This IndexCommit does not support deletions"). }
false;public;1;33;;@Override public boolean get(int index) {     if (index < 0 || index >= maxDoc) {         throw new IndexOutOfBoundsException(index + " is out of bounds: [" + 0 + "-" + maxDoc + "[").     }     if (index < previous) {         throw new IllegalArgumentException("This Bits instance can only be consumed in order. " + "Got called on [" + index + "] while previously called on [" + previous + "]").     }     if (index == previous) {         // twoPhase.matches() twice         return previousMatched.     }     previous = index.     int doc = iterator.docID().     if (doc < index) {         try {             doc = iterator.advance(index).         } catch (IOException e) {             throw new IllegalStateException("Cannot advance iterator", e).         }     }     if (index == doc) {         try {             return previousMatched = twoPhase == null || twoPhase.matches().         } catch (IOException e) {             throw new IllegalStateException("Cannot validate match", e).         }     }     return previousMatched = false. }
false;public;0;4;;@Override public int length() {     return maxDoc. }
true;public,static;2;59;/**  * Given a {@link ScorerSupplier}, return a {@link Bits} instance that will match  * all documents contained in the set. Note that the returned {@link Bits}  * instance MUST be consumed in order.  */ ;/**  * Given a {@link ScorerSupplier}, return a {@link Bits} instance that will match  * all documents contained in the set. Note that the returned {@link Bits}  * instance MUST be consumed in order.  */ public static Bits asSequentialAccessBits(final int maxDoc, @Nullable ScorerSupplier scorerSupplier) throws IOException {     if (scorerSupplier == null) {         return new Bits.MatchNoBits(maxDoc).     }     // Since we want bits, we need random-access     // this never returns null     final Scorer scorer = scorerSupplier.get(Long.MAX_VALUE).     final TwoPhaseIterator twoPhase = scorer.twoPhaseIterator().     final DocIdSetIterator iterator.     if (twoPhase == null) {         iterator = scorer.iterator().     } else {         iterator = twoPhase.approximation().     }     return new Bits() {          int previous = -1.          boolean previousMatched = false.          @Override         public boolean get(int index) {             if (index < 0 || index >= maxDoc) {                 throw new IndexOutOfBoundsException(index + " is out of bounds: [" + 0 + "-" + maxDoc + "[").             }             if (index < previous) {                 throw new IllegalArgumentException("This Bits instance can only be consumed in order. " + "Got called on [" + index + "] while previously called on [" + previous + "]").             }             if (index == previous) {                 // twoPhase.matches() twice                 return previousMatched.             }             previous = index.             int doc = iterator.docID().             if (doc < index) {                 try {                     doc = iterator.advance(index).                 } catch (IOException e) {                     throw new IllegalStateException("Cannot advance iterator", e).                 }             }             if (index == doc) {                 try {                     return previousMatched = twoPhase == null || twoPhase.matches().                 } catch (IOException e) {                     throw new IllegalStateException("Cannot validate match", e).                 }             }             return previousMatched = false.         }          @Override         public int length() {             return maxDoc.         }     }. }
true;public,static;2;9;/**  * Whether a query sorted by {@code searchSort} can be early-terminated if the index is sorted by {@code indexSort}.  */ ;/**  * Whether a query sorted by {@code searchSort} can be early-terminated if the index is sorted by {@code indexSort}.  */ public static boolean canEarlyTerminate(Sort searchSort, Sort indexSort) {     final SortField[] fields1 = searchSort.getSort().     final SortField[] fields2 = indexSort.getSort().     // early termination is possible if fields1 is a prefix of fields2     if (fields1.length > fields2.length) {         return false.     }     return Arrays.asList(fields1).equals(Arrays.asList(fields2).subList(0, fields1.length)). }
true;public,static;1;3;/**  * Wraps a directory reader to make all documents live except those were rolled back  * or hard-deleted due to non-aborting exceptions during indexing.  * The wrapped reader can be used to query all documents.  *  * @param in the input directory reader  * @return the wrapped reader  */ ;/**  * Wraps a directory reader to make all documents live except those were rolled back  * or hard-deleted due to non-aborting exceptions during indexing.  * The wrapped reader can be used to query all documents.  *  * @param in the input directory reader  * @return the wrapped reader  */ public static DirectoryReader wrapAllDocsLive(DirectoryReader in) throws IOException {     return new DirectoryReaderWithAllLiveDocs(in). }
false;public;0;4;;@Override public Bits getLiveDocs() {     return liveDocs. }
false;public;0;4;;@Override public int numDocs() {     return numDocs. }
false;public;0;4;;@Override public CacheHelper getCoreCacheHelper() {     return in.getCoreCacheHelper(). }
false;public;0;4;;@Override public CacheHelper getReaderCacheHelper() {     // Modifying liveDocs     return null. }
false;public;1;15;;@Override public LeafReader wrap(LeafReader leaf) {     final SegmentReader segmentReader = segmentReader(leaf).     final Bits hardLiveDocs = segmentReader.getHardLiveDocs().     if (hardLiveDocs == null) {         return new LeafReaderWithLiveDocs(leaf, null, leaf.maxDoc()).     }     // Once soft-deletes is enabled, we no longer hard-update or hard-delete documents directly.     // Two scenarios that we have hard-deletes: (1) from old segments where soft-deletes was disabled,     // (2) when IndexWriter hits non-aborted exceptions. These two cases, IW flushes SegmentInfos     // before exposing the hard-deletes, thus we can use the hard-delete count of SegmentInfos.     final int numDocs = segmentReader.maxDoc() - segmentReader.getSegmentInfo().getDelCount().     assert numDocs == popCount(hardLiveDocs) : numDocs + " != " + popCount(hardLiveDocs).     return new LeafReaderWithLiveDocs(segmentReader, hardLiveDocs, numDocs). }
false;protected;1;4;;@Override protected DirectoryReader doWrapDirectoryReader(DirectoryReader in) throws IOException {     return wrapAllDocsLive(in). }
false;public;0;4;;@Override public CacheHelper getReaderCacheHelper() {     // Modifying liveDocs     return null. }
false;private,static;1;10;;private static int popCount(Bits bits) {     assert bits != null.     int onBits = 0.     for (int i = 0. i < bits.length(). i++) {         if (bits.get(i)) {             onBits++.         }     }     return onBits. }
true;public,static;0;3;/**  * Returns a numeric docvalues which can be used to soft-delete documents.  */ ;/**  * Returns a numeric docvalues which can be used to soft-delete documents.  */ public static NumericDocValuesField newSoftDeletesField() {     return new NumericDocValuesField(SOFT_DELETES_FIELD, 1). }
false;public;1;3;;public Terms terms(String field) {     return null. }
false;public;1;3;;public NumericDocValues getNumericDocValues(String field) {     return null. }
false;public;1;3;;public BinaryDocValues getBinaryDocValues(String field) {     return null. }
false;public;1;3;;public SortedDocValues getSortedDocValues(String field) {     return null. }
false;public;1;3;;public SortedNumericDocValues getSortedNumericDocValues(String field) {     return null. }
false;public;1;3;;public SortedSetDocValues getSortedSetDocValues(String field) {     return null. }
false;public;1;3;;public NumericDocValues getNormValues(String field) {     return null. }
false;public;0;3;;public FieldInfos getFieldInfos() {     return new FieldInfos(new FieldInfo[0]). }
false;public;0;3;;public Bits getLiveDocs() {     return this.liveDocs. }
false;public;1;3;;public PointValues getPointValues(String fieldName) {     return null. }
false;public;0;2;;public void checkIntegrity() { }
false;public;1;3;;public Fields getTermVectors(int docID) {     return null. }
false;public;0;3;;public int numDocs() {     return 0. }
false;public;0;3;;public int maxDoc() {     return maxDoc. }
false;public;2;2;;public void document(int docID, StoredFieldVisitor visitor) { }
false;protected;0;2;;protected void doClose() { }
false;public;0;3;;public LeafMetaData getMetaData() {     return new LeafMetaData(Version.LATEST.major, Version.LATEST, null). }
false;public;0;3;;public CacheHelper getCoreCacheHelper() {     return null. }
false;public;0;3;;public CacheHelper getReaderCacheHelper() {     return null. }
true;public,static;1;78;/**  * Returns an empty leaf reader with the given max docs. The reader will be fully deleted.  */ ;/**  * Returns an empty leaf reader with the given max docs. The reader will be fully deleted.  */ public static LeafReader emptyReader(final int maxDoc) {     return new LeafReader() {          final Bits liveDocs = new Bits.MatchNoBits(maxDoc).          public Terms terms(String field) {             return null.         }          public NumericDocValues getNumericDocValues(String field) {             return null.         }          public BinaryDocValues getBinaryDocValues(String field) {             return null.         }          public SortedDocValues getSortedDocValues(String field) {             return null.         }          public SortedNumericDocValues getSortedNumericDocValues(String field) {             return null.         }          public SortedSetDocValues getSortedSetDocValues(String field) {             return null.         }          public NumericDocValues getNormValues(String field) {             return null.         }          public FieldInfos getFieldInfos() {             return new FieldInfos(new FieldInfo[0]).         }          public Bits getLiveDocs() {             return this.liveDocs.         }          public PointValues getPointValues(String fieldName) {             return null.         }          public void checkIntegrity() {         }          public Fields getTermVectors(int docID) {             return null.         }          public int numDocs() {             return 0.         }          public int maxDoc() {             return maxDoc.         }          public void document(int docID, StoredFieldVisitor visitor) {         }          protected void doClose() {         }          public LeafMetaData getMetaData() {             return new LeafMetaData(Version.LATEST.major, Version.LATEST, null).         }          public CacheHelper getCoreCacheHelper() {             return null.         }          public CacheHelper getReaderCacheHelper() {             return null.         }     }. }
true;public,static;4;25;/**  * Scans sequence numbers (i.e., {@link SeqNoFieldMapper#NAME}) between {@code fromSeqNo}(inclusive) and {@code toSeqNo}(inclusive)  * in the provided directory reader. This method invokes the callback {@code onNewSeqNo} whenever a sequence number value is found.  *  * @param directoryReader the directory reader to scan  * @param fromSeqNo       the lower bound of a range of seq_no to scan (inclusive)  * @param toSeqNo         the upper bound of a range of seq_no to scan (inclusive)  * @param onNewSeqNo      the callback to be called whenever a new valid sequence number is found  */ ;/**  * Scans sequence numbers (i.e., {@link SeqNoFieldMapper#NAME}) between {@code fromSeqNo}(inclusive) and {@code toSeqNo}(inclusive)  * in the provided directory reader. This method invokes the callback {@code onNewSeqNo} whenever a sequence number value is found.  *  * @param directoryReader the directory reader to scan  * @param fromSeqNo       the lower bound of a range of seq_no to scan (inclusive)  * @param toSeqNo         the upper bound of a range of seq_no to scan (inclusive)  * @param onNewSeqNo      the callback to be called whenever a new valid sequence number is found  */ public static void scanSeqNosInReader(DirectoryReader directoryReader, long fromSeqNo, long toSeqNo, LongConsumer onNewSeqNo) throws IOException {     final DirectoryReader reader = Lucene.wrapAllDocsLive(directoryReader).     final IndexSearcher searcher = new IndexSearcher(reader).     searcher.setQueryCache(null).     final Query query = LongPoint.newRangeQuery(SeqNoFieldMapper.NAME, fromSeqNo, toSeqNo).     final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1.0f).     for (LeafReaderContext leaf : reader.leaves()) {         final Scorer scorer = weight.scorer(leaf).         if (scorer == null) {             continue.         }         final DocIdSetIterator docIdSetIterator = scorer.iterator().         final NumericDocValues seqNoDocValues = leaf.reader().getNumericDocValues(SeqNoFieldMapper.NAME).         int docId.         while ((docId = docIdSetIterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {             if (seqNoDocValues == null || seqNoDocValues.advanceExact(docId) == false) {                 throw new IllegalStateException("seq_no doc_values not found for doc_id=" + docId).             }             final long seqNo = seqNoDocValues.longValue().             assert fromSeqNo <= seqNo && seqNo <= toSeqNo : "from_seq_no=" + fromSeqNo + " seq_no=" + seqNo + " to_seq_no=" + toSeqNo.             onNewSeqNo.accept(seqNo).         }     } }
