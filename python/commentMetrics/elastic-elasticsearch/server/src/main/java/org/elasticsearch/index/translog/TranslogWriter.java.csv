commented;modifiers;parameterAmount;loc;comment;code
false;public,static;12;33;;public static TranslogWriter create(ShardId shardId, String translogUUID, long fileGeneration, Path file, ChannelFactory channelFactory, ByteSizeValue bufferSize, final long initialMinTranslogGen, long initialGlobalCheckpoint, final LongSupplier globalCheckpointSupplier, final LongSupplier minTranslogGenerationSupplier, final long primaryTerm, TragicExceptionHolder tragedy) throws IOException {     final FileChannel channel = channelFactory.open(file).     try {         final TranslogHeader header = new TranslogHeader(translogUUID, primaryTerm).         header.write(channel).         final Checkpoint checkpoint = Checkpoint.emptyTranslogCheckpoint(header.sizeInBytes(), fileGeneration, initialGlobalCheckpoint, initialMinTranslogGen).         writeCheckpoint(channelFactory, file.getParent(), checkpoint).         final LongSupplier writerGlobalCheckpointSupplier.         if (Assertions.ENABLED) {             writerGlobalCheckpointSupplier = () -> {                 long gcp = globalCheckpointSupplier.getAsLong().                 assert gcp >= initialGlobalCheckpoint : "global checkpoint [" + gcp + "] lower than initial gcp [" + initialGlobalCheckpoint + "]".                 return gcp.             }.         } else {             writerGlobalCheckpointSupplier = globalCheckpointSupplier.         }         return new TranslogWriter(channelFactory, shardId, checkpoint, channel, file, bufferSize, writerGlobalCheckpointSupplier, minTranslogGenerationSupplier, header, tragedy).     } catch (Exception exception) {         // if we fail to bake the file-generation into the checkpoint we stick with the file and once we recover and that         // file exists we remove it. We only apply this logic to the checkpoint.generation+1 any other file with a higher generation         // is an error condition         IOUtils.closeWhileHandlingException(channel).         throw exception.     } }
false;private,synchronized;1;8;;private synchronized void closeWithTragicEvent(final Exception ex) {     tragedy.setTragicException(ex).     try {         close().     } catch (final IOException | RuntimeException e) {         ex.addSuppressed(e).     } }
true;public,synchronized;2;27;/**  * Add the given bytes to the translog with the specified sequence number. returns the location the bytes were written to.  *  * @param data  the bytes to write  * @param seqNo the sequence number associated with the operation  * @return the location the bytes were written to  * @throws IOException if writing to the translog resulted in an I/O exception  */ ;/**  * add the given bytes to the translog and return the location they were written at  */ /**  * Add the given bytes to the translog with the specified sequence number. returns the location the bytes were written to.  *  * @param data  the bytes to write  * @param seqNo the sequence number associated with the operation  * @return the location the bytes were written to  * @throws IOException if writing to the translog resulted in an I/O exception  */ public synchronized Translog.Location add(final BytesReference data, final long seqNo) throws IOException {     ensureOpen().     final long offset = totalOffset.     try {         data.writeTo(outputStream).     } catch (final Exception ex) {         closeWithTragicEvent(ex).         throw ex.     }     totalOffset += data.length().     if (minSeqNo == SequenceNumbers.NO_OPS_PERFORMED) {         assert operationCounter == 0.     }     if (maxSeqNo == SequenceNumbers.NO_OPS_PERFORMED) {         assert operationCounter == 0.     }     minSeqNo = SequenceNumbers.min(minSeqNo, seqNo).     maxSeqNo = SequenceNumbers.max(maxSeqNo, seqNo).     operationCounter++.     assert assertNoSeqNumberConflict(seqNo, data).     return new Translog.Location(generation, offset, data.length()). }
false;private,synchronized;2;39;;private synchronized boolean assertNoSeqNumberConflict(long seqNo, BytesReference data) throws IOException {     if (seqNo == SequenceNumbers.UNASSIGNED_SEQ_NO) {     // nothing to do     } else if (seenSequenceNumbers.containsKey(seqNo)) {         final Tuple<BytesReference, Exception> previous = seenSequenceNumbers.get(seqNo).         if (previous.v1().equals(data) == false) {             Translog.Operation newOp = Translog.readOperation(new BufferedChecksumStreamInput(data.streamInput(), "assertion")).             Translog.Operation prvOp = Translog.readOperation(new BufferedChecksumStreamInput(previous.v1().streamInput(), "assertion")).             // TODO: We haven't had timestamp for Index operations in Lucene yet, we need to loosen this check without timestamp.             final boolean sameOp.             if (newOp instanceof Translog.Index && prvOp instanceof Translog.Index) {                 final Translog.Index o1 = (Translog.Index) prvOp.                 final Translog.Index o2 = (Translog.Index) newOp.                 sameOp = Objects.equals(o1.id(), o2.id()) && Objects.equals(o1.type(), o2.type()) && Objects.equals(o1.source(), o2.source()) && Objects.equals(o1.routing(), o2.routing()) && o1.primaryTerm() == o2.primaryTerm() && o1.seqNo() == o2.seqNo() && o1.version() == o2.version().             } else if (newOp instanceof Translog.Delete && prvOp instanceof Translog.Delete) {                 final Translog.Delete o1 = (Translog.Delete) newOp.                 final Translog.Delete o2 = (Translog.Delete) prvOp.                 sameOp = Objects.equals(o1.id(), o2.id()) && Objects.equals(o1.type(), o2.type()) && o1.primaryTerm() == o2.primaryTerm() && o1.seqNo() == o2.seqNo() && o1.version() == o2.version().             } else {                 sameOp = false.             }             if (sameOp == false) {                 throw new AssertionError("seqNo [" + seqNo + "] was processed twice in generation [" + generation + "], with different data. " + "prvOp [" + prvOp + "], newOp [" + newOp + "]", previous.v2()).             }         }     } else {         seenSequenceNumbers.put(seqNo, new Tuple<>(new BytesArray(data.toBytesRef(), true), new RuntimeException("stack capture previous op"))).     }     return true. }
false;synchronized;2;19;;synchronized boolean assertNoSeqAbove(long belowTerm, long aboveSeqNo) {     seenSequenceNumbers.entrySet().stream().filter(e -> e.getKey().longValue() > aboveSeqNo).forEach(e -> {         final Translog.Operation op.         try {             op = Translog.readOperation(new BufferedChecksumStreamInput(e.getValue().v1().streamInput(), "assertion")).         } catch (IOException ex) {             throw new RuntimeException(ex).         }         long seqNo = op.seqNo().         long primaryTerm = op.primaryTerm().         if (primaryTerm < belowTerm) {             throw new AssertionError("current should not have any operations with seq#:primaryTerm [" + seqNo + ":" + primaryTerm + "] > " + aboveSeqNo + ":" + belowTerm).         }     }).     return true. }
true;public;0;3;/**  * write all buffered ops to disk and fsync file.  *  * Note: any exception during the sync process will be interpreted as a tragic exception and the writer will be closed before  * raising the exception.  */ ;/**  * write all buffered ops to disk and fsync file.  *  * Note: any exception during the sync process will be interpreted as a tragic exception and the writer will be closed before  * raising the exception.  */ public void sync() throws IOException {     syncUpTo(Long.MAX_VALUE). }
true;public;0;5;/**  * Returns <code>true</code> if there are buffered operations that have not been flushed and fsynced to disk or if the latest global  * checkpoint has not yet been fsynced  */ ;/**  * Returns <code>true</code> if there are buffered operations that have not been flushed and fsynced to disk or if the latest global  * checkpoint has not yet been fsynced  */ public boolean syncNeeded() {     return totalOffset != lastSyncedCheckpoint.offset || globalCheckpointSupplier.getAsLong() != lastSyncedCheckpoint.globalCheckpoint || minTranslogGenerationSupplier.getAsLong() != lastSyncedCheckpoint.minTranslogGeneration. }
false;public;0;4;;@Override public int totalOperations() {     return operationCounter. }
false;synchronized;0;6;;@Override synchronized Checkpoint getCheckpoint() {     return new Checkpoint(totalOffset, operationCounter, generation, minSeqNo, maxSeqNo, globalCheckpointSupplier.getAsLong(), minTranslogGenerationSupplier.getAsLong(), SequenceNumbers.UNASSIGNED_SEQ_NO). }
false;public;0;4;;@Override public long sizeInBytes() {     return totalOffset. }
true;public;0;23;/**  * Closes this writer and transfers its underlying file channel to a new immutable {@link TranslogReader}  * @return a new {@link TranslogReader}  * @throws IOException if any of the file operations resulted in an I/O exception  */ ;/**  * Closes this writer and transfers its underlying file channel to a new immutable {@link TranslogReader}  * @return a new {@link TranslogReader}  * @throws IOException if any of the file operations resulted in an I/O exception  */ public TranslogReader closeIntoReader() throws IOException {     // we do this to for correctness and preventing future issues.     synchronized (syncLock) {         synchronized (this) {             try {                 // sync before we close..                 sync().             } catch (final Exception ex) {                 closeWithTragicEvent(ex).                 throw ex.             }             if (closed.compareAndSet(false, true)) {                 return new TranslogReader(getLastSyncedCheckpoint(), channel, path, header).             } else {                 throw new AlreadyClosedException("translog [" + getGeneration() + "] is already closed (path [" + path + "]", tragedy.get()).             }         }     } }
false;public;0;16;;@Override public TranslogSnapshot newSnapshot() {     // syncUpTo() , where the sync lock is acquired first, following by the synchronize(this)     synchronized (syncLock) {         synchronized (this) {             ensureOpen().             try {                 sync().             } catch (IOException e) {                 throw new TranslogException(shardId, "exception while syncing before creating a snapshot", e).             }             return super.newSnapshot().         }     } }
false;private;0;3;;private long getWrittenOffset() throws IOException {     return channel.position(). }
true;public;1;35;/**  * Syncs the translog up to at least the given offset unless already synced  *  * @return <code>true</code> if this call caused an actual sync operation  */ ;/**  * Syncs the translog up to at least the given offset unless already synced  *  * @return <code>true</code> if this call caused an actual sync operation  */ public boolean syncUpTo(long offset) throws IOException {     if (lastSyncedCheckpoint.offset < offset && syncNeeded()) {         synchronized (syncLock) {             // only one sync/checkpoint should happen concurrently but we wait             if (lastSyncedCheckpoint.offset < offset && syncNeeded()) {                 // double checked locking - we don't want to fsync unless we have to and now that we have                 // the lock we should check again since if this code is busy we might have fsynced enough already                 final Checkpoint checkpointToSync.                 synchronized (this) {                     ensureOpen().                     try {                         outputStream.flush().                         checkpointToSync = getCheckpoint().                     } catch (final Exception ex) {                         closeWithTragicEvent(ex).                         throw ex.                     }                 }                 // we can continue writing to the buffer etc.                 try {                     channel.force(false).                     writeCheckpoint(channelFactory, path.getParent(), checkpointToSync).                 } catch (final Exception ex) {                     closeWithTragicEvent(ex).                     throw ex.                 }                 assert lastSyncedCheckpoint.offset <= checkpointToSync.offset : "illegal state: " + lastSyncedCheckpoint.offset + " <= " + checkpointToSync.offset.                 // write protected by syncLock                 lastSyncedCheckpoint = checkpointToSync.                 return true.             }         }     }     return false. }
false;protected;2;22;;@Override protected void readBytes(ByteBuffer targetBuffer, long position) throws IOException {     try {         if (position + targetBuffer.remaining() > getWrittenOffset()) {             synchronized (this) {                 // if we don't fail in this call unless absolutely necessary.                 if (position + targetBuffer.remaining() > getWrittenOffset()) {                     outputStream.flush().                 }             }         }     } catch (final Exception ex) {         closeWithTragicEvent(ex).         throw ex.     }     // we don't have to have a lock here because we only write ahead to the file, so all writes has been complete     // for the requested location.     Channels.readFromFileChannelWithEofException(channel, position, targetBuffer). }
false;private,static;3;6;;private static void writeCheckpoint(final ChannelFactory channelFactory, final Path translogFile, final Checkpoint checkpoint) throws IOException {     Checkpoint.write(channelFactory, translogFile.resolve(Translog.CHECKPOINT_FILE_NAME), checkpoint, StandardOpenOption.WRITE). }
true;;0;3;/**  * The last synced checkpoint for this translog.  *  * @return the last synced checkpoint  */ ;/**  * The last synced checkpoint for this translog.  *  * @return the last synced checkpoint  */ Checkpoint getLastSyncedCheckpoint() {     return lastSyncedCheckpoint. }
false;protected,final;0;5;;protected final void ensureOpen() {     if (isClosed()) {         throw new AlreadyClosedException("translog [" + getGeneration() + "] is already closed", tragedy.get()).     } }
false;public,final;0;6;;@Override public final void close() throws IOException {     if (closed.compareAndSet(false, true)) {         channel.close().     } }
false;protected,final;0;3;;protected final boolean isClosed() {     return closed.get(). }
false;public,synchronized;0;12;;@Override public synchronized void flush() throws IOException {     if (count > 0) {         try {             ensureOpen().             super.flush().         } catch (final Exception ex) {             closeWithTragicEvent(ex).             throw ex.         }     } }
false;public;0;6;;@Override public void close() throws IOException {     // closing it will close the FileChannel     throw new IllegalStateException("never close this stream"). }
