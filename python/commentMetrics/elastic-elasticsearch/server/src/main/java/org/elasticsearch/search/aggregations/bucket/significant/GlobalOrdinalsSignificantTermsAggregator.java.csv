commented;modifiers;parameterAmount;loc;comment;code
false;public;2;5;;@Override public void collect(int doc, long bucket) throws IOException {     super.collect(doc, bucket).     numCollectedDocs++. }
false;public;2;10;;@Override public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, final LeafBucketCollector sub) throws IOException {     return new LeafBucketCollectorBase(super.getLeafCollector(ctx, sub), null) {          @Override         public void collect(int doc, long bucket) throws IOException {             super.collect(doc, bucket).             numCollectedDocs++.         }     }. }
false;public;1;75;;@Override public SignificantStringTerms buildAggregation(long owningBucketOrdinal) throws IOException {     assert owningBucketOrdinal == 0.     if (valueCount == 0) {         // no context in this reader         return buildEmptyAggregation().     }     final int size.     if (bucketCountThresholds.getMinDocCount() == 0) {         // if minDocCount == 0 then we can end up with more buckets then maxBucketOrd() returns         size = (int) Math.min(valueCount, bucketCountThresholds.getShardSize()).     } else {         size = (int) Math.min(maxBucketOrd(), bucketCountThresholds.getShardSize()).     }     long supersetSize = termsAggFactory.getSupersetNumDocs().     long subsetSize = numCollectedDocs.     BucketSignificancePriorityQueue<SignificantStringTerms.Bucket> ordered = new BucketSignificancePriorityQueue<>(size).     SignificantStringTerms.Bucket spare = null.     final boolean needsFullScan = bucketOrds == null || bucketCountThresholds.getMinDocCount() == 0.     final long maxId = needsFullScan ? valueCount : bucketOrds.size().     for (long ord = 0. ord < maxId. ord++) {         final long globalOrd.         final long bucketOrd.         if (needsFullScan) {             bucketOrd = bucketOrds == null ? ord : bucketOrds.find(ord).             globalOrd = ord.         } else {             assert bucketOrds != null.             bucketOrd = ord.             globalOrd = bucketOrds.get(ord).         }         if (includeExclude != null && !acceptedGlobalOrdinals.get(globalOrd)) {             continue.         }         final int bucketDocCount = bucketOrd < 0 ? 0 : bucketDocCount(bucketOrd).         if (bucketCountThresholds.getMinDocCount() > 0 && bucketDocCount == 0) {             continue.         }         if (bucketDocCount < bucketCountThresholds.getShardMinDocCount()) {             continue.         }         if (spare == null) {             spare = new SignificantStringTerms.Bucket(new BytesRef(), 0, 0, 0, 0, null, format).         }         spare.bucketOrd = bucketOrd.         copy(lookupGlobalOrd.apply(globalOrd), spare.termBytes).         spare.subsetDf = bucketDocCount.         spare.subsetSize = subsetSize.         spare.supersetDf = termsAggFactory.getBackgroundFrequency(spare.termBytes).         spare.supersetSize = supersetSize.         // During shard-local down-selection we use subset/superset stats         // that are for this shard only         // Back at the central reducer these properties will be updated with         // global stats         spare.updateScore(significanceHeuristic).         spare = ordered.insertWithOverflow(spare).         if (spare == null) {             consumeBucketsAndMaybeBreak(1).         }     }     final SignificantStringTerms.Bucket[] list = new SignificantStringTerms.Bucket[ordered.size()].     for (int i = ordered.size() - 1. i >= 0. i--) {         final SignificantStringTerms.Bucket bucket = ordered.pop().         // the terms are owned by the BytesRefHash, we need to pull a copy since the BytesRef hash data may be recycled at some point         bucket.termBytes = BytesRef.deepCopyOf(bucket.termBytes).         bucket.aggregations = bucketAggregations(bucket.bucketOrd).         list[i] = bucket.     }     return new SignificantStringTerms(name, bucketCountThresholds.getRequiredSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators(), metaData(), format, subsetSize, supersetSize, significanceHeuristic, Arrays.asList(list)). }
false;public;0;9;;@Override public SignificantStringTerms buildEmptyAggregation() {     // We need to account for the significance of a miss in our global stats - provide corpus size as context     ContextIndexSearcher searcher = context.searcher().     IndexReader topReader = searcher.getIndexReader().     int supersetSize = topReader.numDocs().     return new SignificantStringTerms(name, bucketCountThresholds.getRequiredSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators(), metaData(), format, numCollectedDocs, supersetSize, significanceHeuristic, emptyList()). }
false;protected;0;5;;@Override protected void doClose() {     super.doClose().     Releasables.close(termsAggFactory). }
