commented;modifiers;parameterAmount;loc;comment;code
false;public;0;4;;@Override public void close() throws IOException {     onClose.close(). }
false;public;0;4;;@Override public int totalOperations() {     return totalHits. }
false;public;0;4;;@Override public int skippedOperations() {     return skippedOperations. }
false;public;0;17;;@Override public Translog.Operation next() throws IOException {     Translog.Operation op = null.     for (int idx = nextDocIndex(). idx != -1. idx = nextDocIndex()) {         op = readDocAsOp(idx).         if (op != null) {             break.         }     }     if (requiredFullRange) {         rangeCheck(op).     }     if (op != null) {         lastSeenSeqNo = op.seqNo().     }     return op. }
false;private;1;14;;private void rangeCheck(Translog.Operation op) {     if (op == null) {         if (lastSeenSeqNo < toSeqNo) {             throw new MissingHistoryOperationsException("Not all operations between from_seqno [" + fromSeqNo + "] " + "and to_seqno [" + toSeqNo + "] found. prematurely terminated last_seen_seqno [" + lastSeenSeqNo + "]").         }     } else {         final long expectedSeqNo = lastSeenSeqNo + 1.         if (op.seqNo() != expectedSeqNo) {             throw new MissingHistoryOperationsException("Not all operations between from_seqno [" + fromSeqNo + "] " + "and to_seqno [" + toSeqNo + "] found. expected seqno [" + expectedSeqNo + "]. found [" + op + "]").         }     } }
false;private;0;15;;private int nextDocIndex() throws IOException {     // we have processed all docs in the current search - fetch the next batch     if (docIndex == scoreDocs.length && docIndex > 0) {         final ScoreDoc prev = scoreDocs[scoreDocs.length - 1].         scoreDocs = searchOperations(prev).scoreDocs.         fillParallelArray(scoreDocs, parallelArray).         docIndex = 0.     }     if (docIndex < scoreDocs.length) {         int idx = docIndex.         docIndex++.         return idx.     }     return -1. }
false;private;2;37;;private void fillParallelArray(ScoreDoc[] scoreDocs, ParallelArray parallelArray) throws IOException {     if (scoreDocs.length > 0) {         for (int i = 0. i < scoreDocs.length. i++) {             scoreDocs[i].shardIndex = i.         }         // for better loading performance we sort the array by docID and         // then visit all leaves in order.         ArrayUtil.introSort(scoreDocs, Comparator.comparingInt(i -> i.doc)).         int docBase = -1.         int maxDoc = 0.         List<LeafReaderContext> leaves = indexSearcher.getIndexReader().leaves().         int readerIndex = 0.         CombinedDocValues combinedDocValues = null.         LeafReaderContext leaf = null.         for (int i = 0. i < scoreDocs.length. i++) {             ScoreDoc scoreDoc = scoreDocs[i].             if (scoreDoc.doc >= docBase + maxDoc) {                 do {                     leaf = leaves.get(readerIndex++).                     docBase = leaf.docBase.                     maxDoc = leaf.reader().maxDoc().                 } while (scoreDoc.doc >= docBase + maxDoc).                 combinedDocValues = new CombinedDocValues(leaf.reader()).             }             final int segmentDocID = scoreDoc.doc - docBase.             final int index = scoreDoc.shardIndex.             parallelArray.leafReaderContexts[index] = leaf.             parallelArray.seqNo[index] = combinedDocValues.docSeqNo(segmentDocID).             parallelArray.primaryTerm[index] = combinedDocValues.docPrimaryTerm(segmentDocID).             parallelArray.version[index] = combinedDocValues.docVersion(segmentDocID).             parallelArray.isTombStone[index] = combinedDocValues.isTombstone(segmentDocID).             parallelArray.hasRecoverySource[index] = combinedDocValues.hasRecoverySource(segmentDocID).         }         // now sort back based on the shardIndex. we use this to store the previous index         ArrayUtil.introSort(scoreDocs, Comparator.comparingInt(i -> i.shardIndex)).     } }
false;private;1;8;;private TopDocs searchOperations(ScoreDoc after) throws IOException {     final Query rangeQuery = LongPoint.newRangeQuery(SeqNoFieldMapper.NAME, Math.max(fromSeqNo, lastSeenSeqNo), toSeqNo).     final Sort sortedBySeqNoThenByTerm = new Sort(new SortField(SeqNoFieldMapper.NAME, SortField.Type.LONG), new SortField(SeqNoFieldMapper.PRIMARY_TERM_NAME, SortField.Type.LONG, true)).     return indexSearcher.searchAfter(after, rangeQuery, searchBatchSize, sortedBySeqNoThenByTerm). }
false;private;1;58;;private Translog.Operation readDocAsOp(int docIndex) throws IOException {     final LeafReaderContext leaf = parallelArray.leafReaderContexts[docIndex].     final int segmentDocID = scoreDocs[docIndex].doc - leaf.docBase.     final long primaryTerm = parallelArray.primaryTerm[docIndex].     // We don't have to read the nested child documents - those docs don't have primary terms.     if (primaryTerm == -1) {         skippedOperations++.         return null.     }     final long seqNo = parallelArray.seqNo[docIndex].     // Only pick the first seen seq#     if (seqNo == lastSeenSeqNo) {         skippedOperations++.         return null.     }     final long version = parallelArray.version[docIndex].     final String sourceField = parallelArray.hasRecoverySource[docIndex] ? SourceFieldMapper.RECOVERY_SOURCE_NAME : SourceFieldMapper.NAME.     final FieldsVisitor fields = new FieldsVisitor(true, sourceField).     leaf.reader().document(segmentDocID, fields).     fields.postProcess(mapperService).     final Translog.Operation op.     final boolean isTombstone = parallelArray.isTombStone[docIndex].     if (isTombstone && fields.uid() == null) {         op = new Translog.NoOp(seqNo, primaryTerm, fields.source().utf8ToString()).         assert version == 1L : "Noop tombstone should have version 1L. actual version [" + version + "]".         assert assertDocSoftDeleted(leaf.reader(), segmentDocID) : "Noop but soft_deletes field is not set [" + op + "]".     } else {         final String id = fields.uid().id().         final String type = fields.uid().type().         final Term uid = new Term(IdFieldMapper.NAME, Uid.encodeId(id)).         if (isTombstone) {             op = new Translog.Delete(type, id, uid, seqNo, primaryTerm, version).             assert assertDocSoftDeleted(leaf.reader(), segmentDocID) : "Delete op but soft_deletes field is not set [" + op + "]".         } else {             final BytesReference source = fields.source().             if (source == null) {                 // check for the existence source once we make peer-recovery to send ops after the local checkpoint.                 if (requiredFullRange) {                     throw new IllegalStateException("source not found for seqno=" + seqNo + " from_seqno=" + fromSeqNo + " to_seqno=" + toSeqNo).                 } else {                     skippedOperations++.                     return null.                 }             }             // TODO: pass the latest timestamp from engine.             final long autoGeneratedIdTimestamp = -1.             op = new Translog.Index(type, id, seqNo, primaryTerm, version, source.toBytesRef().bytes, fields.routing(), autoGeneratedIdTimestamp).         }     }     assert fromSeqNo <= op.seqNo() && op.seqNo() <= toSeqNo && lastSeenSeqNo < op.seqNo() : "Unexpected operation. " + "last_seen_seqno [" + lastSeenSeqNo + "], from_seqno [" + fromSeqNo + "], to_seqno [" + toSeqNo + "], op [" + op + "]".     return op. }
false;private;2;7;;private boolean assertDocSoftDeleted(LeafReader leafReader, int segmentDocId) throws IOException {     final NumericDocValues ndv = leafReader.getNumericDocValues(Lucene.SOFT_DELETES_FIELD).     if (ndv == null || ndv.advanceExact(segmentDocId) == false) {         throw new IllegalStateException("DocValues for field [" + Lucene.SOFT_DELETES_FIELD + "] is not found").     }     return ndv.longValue() == 1. }
false;;1;7;;long docVersion(int segmentDocId) throws IOException {     assert versionDV.docID() < segmentDocId.     if (versionDV.advanceExact(segmentDocId) == false) {         throw new IllegalStateException("DocValues for field [" + VersionFieldMapper.NAME + "] is not found").     }     return versionDV.longValue(). }
false;;1;7;;long docSeqNo(int segmentDocId) throws IOException {     assert seqNoDV.docID() < segmentDocId.     if (seqNoDV.advanceExact(segmentDocId) == false) {         throw new IllegalStateException("DocValues for field [" + SeqNoFieldMapper.NAME + "] is not found").     }     return seqNoDV.longValue(). }
false;;1;11;;long docPrimaryTerm(int segmentDocId) throws IOException {     if (primaryTermDV == null) {         return -1L.     }     assert primaryTermDV.docID() < segmentDocId.     // Use -1 for docs which don't have primary term. The caller considers those docs as nested docs.     if (primaryTermDV.advanceExact(segmentDocId) == false) {         return -1.     }     return primaryTermDV.longValue(). }
false;;1;7;;boolean isTombstone(int segmentDocId) throws IOException {     if (tombstoneDV == null) {         return false.     }     assert tombstoneDV.docID() < segmentDocId.     return tombstoneDV.advanceExact(segmentDocId) && tombstoneDV.longValue() > 0. }
false;;1;7;;boolean hasRecoverySource(int segmentDocId) throws IOException {     if (recoverySource == null) {         return false.     }     assert recoverySource.docID() < segmentDocId.     return recoverySource.advanceExact(segmentDocId). }
