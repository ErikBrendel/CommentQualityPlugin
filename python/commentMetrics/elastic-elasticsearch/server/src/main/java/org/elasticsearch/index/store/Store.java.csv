# id;timestamp;commentText;codeText;commentWords;codeWords
Store -> MetadataSnapshot -> public String getHistoryUUID();1524684173;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1525334055;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1526449283;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1526900724;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1528211342;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1535723122;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1535965276;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1536611444;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1536828374;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1537806831;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1538067637;returns the history uuid the store points at, or null if not existant.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,not,existant;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1539615817;returns the history uuid the store points at, or null if nonexistent.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,nonexistent;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1542697754;returns the history uuid the store points at, or null if nonexistent.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,nonexistent;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1543832502;returns the history uuid the store points at, or null if nonexistent.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,nonexistent;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1543942400;returns the history uuid the store points at, or null if nonexistent.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,nonexistent;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1549395161;returns the history uuid the store points at, or null if nonexistent.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,nonexistent;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1549935387;returns the history uuid the store points at, or null if nonexistent.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,nonexistent;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getHistoryUUID();1550526771;returns the history uuid the store points at, or null if nonexistent.;public String getHistoryUUID() {_            return commitUserData.get(Engine.HISTORY_UUID_KEY)__        };returns,the,history,uuid,the,store,points,at,or,null,if,nonexistent;public,string,get,history,uuid,return,commit,user,data,get,engine
Store -> public void bootstrapNewHistory() throws IOException;1524684173;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1525334055;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1526449283;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1526900724;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1528211342;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1535723122;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1535965276;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1536611444;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1536828374;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            bootstrapNewHistory(maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1537806831;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            bootstrapNewHistory(maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1538067637;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            bootstrapNewHistory(maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1539615817;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            bootstrapNewHistory(maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1542697754;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            bootstrapNewHistory(maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1543832502;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            bootstrapNewHistory(maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1543942400;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            bootstrapNewHistory(maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1549395161;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final long localCheckpoint = Long.parseLong(userData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY))__            bootstrapNewHistory(localCheckpoint, maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,long,local,checkpoint,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,local,checkpoint,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1549935387;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final long localCheckpoint = Long.parseLong(userData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY))__            bootstrapNewHistory(localCheckpoint, maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,long,local,checkpoint,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,local,checkpoint,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory() throws IOException;1550526771;Marks an existing lucene index with a new history uuid._This is used to make sure no existing shard will recovery from this index using ops based recovery.;public void bootstrapNewHistory() throws IOException {_        metadataLock.writeLock().lock()__        try {_            Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData()__            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO))__            final long localCheckpoint = Long.parseLong(userData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY))__            bootstrapNewHistory(localCheckpoint, maxSeqNo)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery;public,void,bootstrap,new,history,throws,ioexception,metadata,lock,write,lock,lock,try,map,string,string,user,data,read,last,committed,segments,info,get,user,data,final,long,max,seq,no,long,parse,long,user,data,get,sequence,numbers,final,long,local,checkpoint,long,parse,long,user,data,get,sequence,numbers,bootstrap,new,history,local,checkpoint,max,seq,no,finally,metadata,lock,write,lock,unlock
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1524684173;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments <tt>.si</tt> files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the <tt>.si</tt> file content as it's hash</li>_</ul>_<p>_The <tt>.si</tt> file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the <tt>segments_N</tt> files as well as generational files like_deletes (<tt>_x_y.del</tt>) or field-info (<tt>_x_y.fnm</tt>) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file <tt>segments_N</tt> files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the <tt>segments_N</tt> file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the <tt>segments.gen</tt> file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                assert FIELD_INFOS_FILE_EXTENSION.equals(extension) == false || IndexFileNames.stripExtension(IndexFileNames.stripSegmentName(meta.name())).isEmpty() : "FieldInfos are generational but updateable DV are not supported in elasticsearch"__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,tt,si,tt,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,tt,si,tt,file,content,as,it,s,hash,li,ul,p,the,tt,si,tt,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,tt,tt,files,as,well,as,generational,files,like,deletes,tt,del,tt,or,field,info,tt,fnm,tt,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,tt,tt,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,tt,tt,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,tt,segments,gen,tt,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,assert,equals,extension,false,index,file,names,strip,extension,index,file,names,strip,segment,name,meta,name,is,empty,field,infos,are,generational,but,updateable,dv,are,not,supported,in,elasticsearch,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1525334055;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                assert FIELD_INFOS_FILE_EXTENSION.equals(extension) == false || IndexFileNames.stripExtension(IndexFileNames.stripSegmentName(meta.name())).isEmpty() : "FieldInfos are generational but updateable DV are not supported in elasticsearch"__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,assert,equals,extension,false,index,file,names,strip,extension,index,file,names,strip,segment,name,meta,name,is,empty,field,infos,are,generational,but,updateable,dv,are,not,supported,in,elasticsearch,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1526449283;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                assert FIELD_INFOS_FILE_EXTENSION.equals(extension) == false || IndexFileNames.stripExtension(IndexFileNames.stripSegmentName(meta.name())).isEmpty() : "FieldInfos are generational but updateable DV are not supported in elasticsearch"__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,assert,equals,extension,false,index,file,names,strip,extension,index,file,names,strip,segment,name,meta,name,is,empty,field,infos,are,generational,but,updateable,dv,are,not,supported,in,elasticsearch,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1526900724;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                assert FIELD_INFOS_FILE_EXTENSION.equals(extension) == false || IndexFileNames.stripExtension(IndexFileNames.stripSegmentName(meta.name())).isEmpty() : "FieldInfos are generational but updateable DV are not supported in elasticsearch"__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,assert,equals,extension,false,index,file,names,strip,extension,index,file,names,strip,segment,name,meta,name,is,empty,field,infos,are,generational,but,updateable,dv,are,not,supported,in,elasticsearch,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1528211342;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                assert FIELD_INFOS_FILE_EXTENSION.equals(extension) == false || IndexFileNames.stripExtension(IndexFileNames.stripSegmentName(meta.name())).isEmpty() : "FieldInfos are generational but updateable DV are not supported in elasticsearch"__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,assert,equals,extension,false,index,file,names,strip,extension,index,file,names,strip,segment,name,meta,name,is,empty,field,infos,are,generational,but,updateable,dv,are,not,supported,in,elasticsearch,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1535723122;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1535965276;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1536611444;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1536828374;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1537806831;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1538067637;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1539615817;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files like_deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1542697754;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files_like deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) ||_                        DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId)__                    if (perSegStoreFiles == null) {_                        perSegStoreFiles = new ArrayList<>()__                        perSegment.put(segmentId, perSegStoreFiles)__                    }_                    perSegStoreFiles.add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical),_                Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" +_                      this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,list,store,file,meta,data,per,seg,store,files,per,segment,get,segment,id,if,per,seg,store,files,null,per,seg,store,files,new,array,list,per,segment,put,segment,id,per,seg,store,files,per,seg,store,files,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1543832502;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files_like deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) ||_                        DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    perSegment.computeIfAbsent(segmentId, k -> new ArrayList<>()).add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical),_                Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" +_                      this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,per,segment,compute,if,absent,segment,id,k,new,array,list,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1543942400;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files_like deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) ||_                        DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    perSegment.computeIfAbsent(segmentId, k -> new ArrayList<>()).add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical),_                Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" +_                      this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,per,segment,compute,if,absent,segment,id,k,new,array,list,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1549395161;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files_like deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) ||_                        DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    perSegment.computeIfAbsent(segmentId, k -> new ArrayList<>()).add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical),_                Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" +_                      this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,per,segment,compute,if,absent,segment,id,k,new,array,list,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1549935387;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files_like deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) ||_                        DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    perSegment.computeIfAbsent(segmentId, k -> new ArrayList<>()).add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical),_                Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" +_                      this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,per,segment,compute,if,absent,segment,id,k,new,array,list,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> MetadataSnapshot -> public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot);1550526771;Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the_recovery target and this snapshot as the source. The returned diff will hold a list of files that are:_<ul>_<li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>_<li>different: they exist in both snapshots but their they are not identical</li>_<li>missing: files that exist in the source but not in the target</li>_</ul>_This method groups file into per-segment files and per-commit files. A file is treated as_identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated_as identical iff:_<ul>_<li>all files in this segment have the same checksum</li>_<li>all files in this segment have the same length</li>_<li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code .si} file content as it's hash</li>_</ul>_<p>_The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be_unique segment identifiers in there hardening this method further._<p>_The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files_like deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated_as identical iff:_<ul>_<li>all files belonging to this commit have the same checksum</li>_<li>all files belonging to this commit have the same length</li>_<li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function,_The metadata transfers the {@code segments_N} file content as it's hash</li>_</ul>_<p>_NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.;public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {_            final List<StoreFileMetaData> identical = new ArrayList<>()__            final List<StoreFileMetaData> different = new ArrayList<>()__            final List<StoreFileMetaData> missing = new ArrayList<>()__            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>()__            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>()___            for (StoreFileMetaData meta : this) {_                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { _                    continue_ _                }_                final String segmentId = IndexFileNames.parseSegmentName(meta.name())__                final String extension = IndexFileNames.getExtension(meta.name())__                if (IndexFileNames.SEGMENTS.equals(segmentId) ||_                        DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {_                    _                    perCommitStoreFiles.add(meta)__                } else {_                    perSegment.computeIfAbsent(segmentId, k -> new ArrayList<>()).add(meta)__                }_            }_            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>()__            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {_                identicalFiles.clear()__                boolean consistent = true__                for (StoreFileMetaData meta : segmentFiles) {_                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name())__                    if (storeFileMetaData == null) {_                        consistent = false__                        missing.add(meta)__                    } else if (storeFileMetaData.isSame(meta) == false) {_                        consistent = false__                        different.add(meta)__                    } else {_                        identicalFiles.add(meta)__                    }_                }_                if (consistent) {_                    identical.addAll(identicalFiles)__                } else {_                    _                    different.addAll(identicalFiles)__                }_            }_            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical),_                Collections.unmodifiableList(different), Collections.unmodifiableList(missing))__            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)_                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" +_                      this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]"__            return recoveryDiff__        };returns,a,diff,between,the,two,snapshots,that,can,be,used,for,recovery,the,given,snapshot,is,treated,as,the,recovery,target,and,this,snapshot,as,the,source,the,returned,diff,will,hold,a,list,of,files,that,are,ul,li,identical,they,exist,in,both,snapshots,and,they,can,be,considered,the,same,ie,they,don,t,need,to,be,recovered,li,li,different,they,exist,in,both,snapshots,but,their,they,are,not,identical,li,li,missing,files,that,exist,in,the,source,but,not,in,the,target,li,ul,this,method,groups,file,into,per,segment,files,and,per,commit,files,a,file,is,treated,as,identical,if,and,on,if,all,files,in,it,s,group,are,identical,on,a,per,segment,level,files,for,a,segment,are,treated,as,identical,iff,ul,li,all,files,in,this,segment,have,the,same,checksum,li,li,all,files,in,this,segment,have,the,same,length,li,li,the,segments,code,si,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,si,file,content,as,it,s,hash,li,ul,p,the,code,si,file,contains,a,lot,of,diagnostics,including,a,timestamp,etc,in,the,future,there,might,be,unique,segment,identifiers,in,there,hardening,this,method,further,p,the,per,commit,files,handles,very,similar,a,commit,is,composed,of,the,code,files,as,well,as,generational,files,like,deletes,code,del,or,field,info,code,fnm,files,on,a,per,commit,level,files,for,a,commit,are,treated,as,identical,iff,ul,li,all,files,belonging,to,this,commit,have,the,same,checksum,li,li,all,files,belonging,to,this,commit,have,the,same,length,li,li,the,segments,file,code,files,hashes,are,byte,identical,note,this,is,a,using,a,perfect,hash,function,the,metadata,transfers,the,code,file,content,as,it,s,hash,li,ul,p,note,this,diff,will,not,contain,the,code,segments,gen,file,this,file,is,omitted,on,recovery;public,recovery,diff,recovery,diff,metadata,snapshot,recovery,target,snapshot,final,list,store,file,meta,data,identical,new,array,list,final,list,store,file,meta,data,different,new,array,list,final,list,store,file,meta,data,missing,new,array,list,final,map,string,list,store,file,meta,data,per,segment,new,hash,map,final,list,store,file,meta,data,per,commit,store,files,new,array,list,for,store,file,meta,data,meta,this,if,index,file,names,equals,meta,name,continue,final,string,segment,id,index,file,names,parse,segment,name,meta,name,final,string,extension,index,file,names,get,extension,meta,name,if,index,file,names,segments,equals,segment,id,equals,extension,equals,extension,per,commit,store,files,add,meta,else,per,segment,compute,if,absent,segment,id,k,new,array,list,add,meta,final,array,list,store,file,meta,data,identical,files,new,array,list,for,list,store,file,meta,data,segment,files,iterables,concat,per,segment,values,collections,singleton,per,commit,store,files,identical,files,clear,boolean,consistent,true,for,store,file,meta,data,meta,segment,files,store,file,meta,data,store,file,meta,data,recovery,target,snapshot,get,meta,name,if,store,file,meta,data,null,consistent,false,missing,add,meta,else,if,store,file,meta,data,is,same,meta,false,consistent,false,different,add,meta,else,identical,files,add,meta,if,consistent,identical,add,all,identical,files,else,different,add,all,identical,files,recovery,diff,recovery,diff,new,recovery,diff,collections,unmodifiable,list,identical,collections,unmodifiable,list,different,collections,unmodifiable,list,missing,assert,recovery,diff,size,this,metadata,size,metadata,contains,key,index,file,names,1,0,some,files,are,missing,recovery,diff,size,recovery,diff,size,metadata,size,this,metadata,size,contains,segments,gen,metadata,contains,key,index,file,names,return,recovery,diff
Store -> RecoveryDiff -> public int size();1524684173;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1525334055;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1526449283;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1526900724;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1528211342;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1535723122;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1535965276;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1536611444;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1536828374;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1537806831;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1538067637;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1539615817;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1542697754;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1543832502;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1543942400;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1549395161;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1549935387;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> RecoveryDiff -> public int size();1550526771;Returns the sum of the files in this diff.;public int size() {_            return identical.size() + different.size() + missing.size()__        };returns,the,sum,of,the,files,in,this,diff;public,int,size,return,identical,size,different,size,missing,size
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1524684173;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1525334055;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1526449283;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1526900724;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1528211342;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1535723122;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1535965276;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1536611444;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1536828374;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1537806831;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1538067637;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1539615817;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1542697754;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1543832502;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1543942400;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1549395161;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1549935387;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public SegmentInfos readLastCommittedSegmentsInfo() throws IOException;1550526771;Returns the last committed segments info for this store__@throws IOException if the index is corrupted or the segments file is not present;public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {_        failIfCorrupted()__        try {_            return readSegmentsInfo(null, directory())__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        }_    };returns,the,last,committed,segments,info,for,this,store,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;public,segment,infos,read,last,committed,segments,info,throws,ioexception,fail,if,corrupted,try,return,read,segments,info,null,directory,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex
Store -> public int refCount();1524684173;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1525334055;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1526449283;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1526900724;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1528211342;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1535723122;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1535965276;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1536611444;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1536828374;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1537806831;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1538067637;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1539615817;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1542697754;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1543832502;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1543942400;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1549395161;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1549935387;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public int refCount();1550526771;Returns the current reference count.;public int refCount() {_        return refCounter.refCount()__    };returns,the,current,reference,count;public,int,ref,count,return,ref,counter,ref,count
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1524684173;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1525334055;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1526449283;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1526900724;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1528211342;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1535723122;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1535965276;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1536611444;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1536828374;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1537806831;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1538067637;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1539615817;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1542697754;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1543832502;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1543942400;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1549395161;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1549935387;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException;1550526771;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed__@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should_only be used if there is no started shard using this store._@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {_        ensureOpen()__        failIfCorrupted()__        assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available"__        _        _        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock()__        lock.lock()__        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {} ) {_            return new MetadataSnapshot(commit, directory, logger)__        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {_            markStoreCorrupted(ex)__            throw ex__        } finally {_            lock.unlock()__        }_    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,param,lock,directory,if,code,true,code,the,index,writer,lock,will,be,obtained,before,reading,the,snapshot,this,should,only,be,used,if,there,is,no,started,shard,using,this,store,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,boolean,lock,directory,throws,ioexception,ensure,open,fail,if,corrupted,assert,lock,directory,commit,null,true,iw,lock,should,not,be,obtained,if,there,is,a,commit,point,available,java,util,concurrent,locks,lock,lock,lock,directory,metadata,lock,write,lock,metadata,lock,read,lock,lock,lock,try,closeable,ignored,lock,directory,directory,obtain,lock,index,writer,return,new,metadata,snapshot,commit,directory,logger,catch,corrupt,index,exception,index,format,too,old,exception,index,format,too,new,exception,ex,mark,store,corrupted,ex,throw,ex,finally,lock,unlock
Store -> MetadataSnapshot -> public long getNumDocs();1524684173;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1525334055;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1526449283;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1526900724;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1528211342;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1535723122;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1535965276;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1536611444;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1536828374;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1537806831;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1538067637;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1539615817;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1542697754;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1543832502;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1543942400;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1549395161;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1549935387;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public long getNumDocs();1550526771;Returns the number of documents in this store snapshot;public long getNumDocs() {_            return numDocs__        };returns,the,number,of,documents,in,this,store,snapshot;public,long,get,num,docs,return,num,docs
Store -> MetadataSnapshot -> public int size();1524684173;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1525334055;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1526449283;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1526900724;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1528211342;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1535723122;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1535965276;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1536611444;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1536828374;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1537806831;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1538067637;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1539615817;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1542697754;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1543832502;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1543942400;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1549395161;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1549935387;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> MetadataSnapshot -> public int size();1550526771;Returns the number of files in this snapshot;public int size() {_            return metadata.size()__        };returns,the,number,of,files,in,this,snapshot;public,int,size,return,metadata,size
Store -> public void exorciseIndex(CheckIndex.Status status) throws IOException;1524684173;Repairs the index using the previous returned status from {@link #checkIndex(PrintStream)}.;public void exorciseIndex(CheckIndex.Status status) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.exorciseIndex(status)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };repairs,the,index,using,the,previous,returned,status,from,link,check,index,print,stream;public,void,exorcise,index,check,index,status,status,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,exorcise,index,status,finally,metadata,lock,write,lock,unlock
Store -> public void exorciseIndex(CheckIndex.Status status) throws IOException;1525334055;Repairs the index using the previous returned status from {@link #checkIndex(PrintStream)}.;public void exorciseIndex(CheckIndex.Status status) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.exorciseIndex(status)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };repairs,the,index,using,the,previous,returned,status,from,link,check,index,print,stream;public,void,exorcise,index,check,index,status,status,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,exorcise,index,status,finally,metadata,lock,write,lock,unlock
Store -> public void exorciseIndex(CheckIndex.Status status) throws IOException;1526449283;Repairs the index using the previous returned status from {@link #checkIndex(PrintStream)}.;public void exorciseIndex(CheckIndex.Status status) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.exorciseIndex(status)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };repairs,the,index,using,the,previous,returned,status,from,link,check,index,print,stream;public,void,exorcise,index,check,index,status,status,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,exorcise,index,status,finally,metadata,lock,write,lock,unlock
Store -> public void exorciseIndex(CheckIndex.Status status) throws IOException;1526900724;Repairs the index using the previous returned status from {@link #checkIndex(PrintStream)}.;public void exorciseIndex(CheckIndex.Status status) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.exorciseIndex(status)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };repairs,the,index,using,the,previous,returned,status,from,link,check,index,print,stream;public,void,exorcise,index,check,index,status,status,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,exorcise,index,status,finally,metadata,lock,write,lock,unlock
Store -> public void exorciseIndex(CheckIndex.Status status) throws IOException;1528211342;Repairs the index using the previous returned status from {@link #checkIndex(PrintStream)}.;public void exorciseIndex(CheckIndex.Status status) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.exorciseIndex(status)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };repairs,the,index,using,the,previous,returned,status,from,link,check,index,print,stream;public,void,exorcise,index,check,index,status,status,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,exorcise,index,status,finally,metadata,lock,write,lock,unlock
Store -> public void exorciseIndex(CheckIndex.Status status) throws IOException;1535723122;Repairs the index using the previous returned status from {@link #checkIndex(PrintStream)}.;public void exorciseIndex(CheckIndex.Status status) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.exorciseIndex(status)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };repairs,the,index,using,the,previous,returned,status,from,link,check,index,print,stream;public,void,exorcise,index,check,index,status,status,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,exorcise,index,status,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1524684173;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1525334055;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1526449283;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1526900724;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1528211342;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1535723122;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1535965276;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1536611444;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1536828374;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1537806831;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(InternalEngine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,internal,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1538067637;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1539615817;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1542697754;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty() throws IOException;1543832502;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,create,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                         Logger logger) throws IOException, ShardLockObtainFailedException;1542697754;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                        Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                         Logger logger) throws IOException, ShardLockObtainFailedException;1543832502;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                        Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                         Logger logger) throws IOException, ShardLockObtainFailedException;1543942400;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                        Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                         Logger logger) throws IOException, ShardLockObtainFailedException;1549395161;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                        Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                         Logger logger) throws IOException, ShardLockObtainFailedException;1549935387;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                        Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                         Logger logger) throws IOException, ShardLockObtainFailedException;1550526771;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                        Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static boolean isAutogenerated(String name);1524684173;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1525334055;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1526449283;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1526900724;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1528211342;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1535723122;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1535965276;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1536611444;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1536828374;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1537806831;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1538067637;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1539615817;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1542697754;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1543832502;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1543942400;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1549395161;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1549935387;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public static boolean isAutogenerated(String name);1550526771;Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup._This includes write lock and checksum files;public static boolean isAutogenerated(String name) {_        return IndexWriter.WRITE_LOCK_NAME.equals(name)__    };returns,true,if,the,file,is,auto,generated,by,the,store,and,shouldn,t,be,deleted,during,cleanup,this,includes,write,lock,and,checksum,files;public,static,boolean,is,autogenerated,string,name,return,index,writer,equals,name
Store -> public void bootstrapNewHistory(long maxSeqNo) throws IOException;1536828374;Marks an existing lucene index with a new history uuid and sets the given maxSeqNo as the local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recovery from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,max,seq,no,as,the,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory(long maxSeqNo) throws IOException;1537806831;Marks an existing lucene index with a new history uuid and sets the given maxSeqNo as the local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recovery from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,max,seq,no,as,the,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory(long maxSeqNo) throws IOException;1538067637;Marks an existing lucene index with a new history uuid and sets the given maxSeqNo as the local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recovery from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,max,seq,no,as,the,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory(long maxSeqNo) throws IOException;1539615817;Marks an existing lucene index with a new history uuid and sets the given maxSeqNo as the local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recovery from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,max,seq,no,as,the,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory(long maxSeqNo) throws IOException;1542697754;Marks an existing lucene index with a new history uuid and sets the given maxSeqNo as the local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recovery from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,max,seq,no,as,the,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory(long maxSeqNo) throws IOException;1543832502;Marks an existing lucene index with a new history uuid and sets the given maxSeqNo as the local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recovery from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,max,seq,no,as,the,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory(long maxSeqNo) throws IOException;1543942400;Marks an existing lucene index with a new history uuid and sets the given maxSeqNo as the local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recovery from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,max,seq,no,as,the,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recovery,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,max,seq,no,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> MetadataSnapshot -> public String getSyncId();1524684173;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1525334055;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1526449283;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1526900724;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1528211342;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1535723122;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1535965276;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1536611444;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1536828374;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1537806831;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1538067637;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1539615817;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1542697754;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1543832502;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1543942400;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1549395161;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1549935387;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> MetadataSnapshot -> public String getSyncId();1550526771;Returns the sync id of the commit point that this MetadataSnapshot represents.__@return sync id if exists, else null;public String getSyncId() {_            return commitUserData.get(Engine.SYNC_COMMIT_ID)__        };returns,the,sync,id,of,the,commit,point,that,this,metadata,snapshot,represents,return,sync,id,if,exists,else,null;public,string,get,sync,id,return,commit,user,data,get,engine
Store -> StoreDirectory -> long estimateSize() throws IOException;1528211342;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1535723122;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1535965276;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1536611444;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1536828374;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1537806831;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1538067637;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1539615817;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1542697754;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1543832502;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1543942400;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1549395161;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1549935387;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> StoreDirectory -> long estimateSize() throws IOException;1550526771;Estimate the cumulative size of all files in this directory in bytes.;long estimateSize() throws IOException {_            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes()__        };estimate,the,cumulative,size,of,all,files,in,this,directory,in,bytes;long,estimate,size,throws,ioexception,return,byte,size,caching,directory,get,delegate,estimate,size,in,bytes
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1524684173;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1525334055;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1526449283;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1526900724;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1528211342;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1535723122;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1535965276;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1536611444;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1536828374;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1537806831;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1538067637;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1539615817;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1542697754;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1543832502;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1543942400;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1549395161;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1549935387;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void associateIndexWithNewTranslog(final String translogUUID) throws IOException;1550526771;Force bakes the given translog generation as recovery information in the lucene index. This is_used when recovering from a snapshot or peer file based recovery where a new empty translog is_created and the existing lucene index needs should be changed to use it.;public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {_                throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]")__            }_            final Map<String, String> map = new HashMap<>()__            map.put(Translog.TRANSLOG_GENERATION_KEY, "1")__            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID)__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };force,bakes,the,given,translog,generation,as,recovery,information,in,the,lucene,index,this,is,used,when,recovering,from,a,snapshot,or,peer,file,based,recovery,where,a,new,empty,translog,is,created,and,the,existing,lucene,index,needs,should,be,changed,to,use,it;public,void,associate,index,with,new,translog,final,string,translog,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,if,translog,uuid,equals,get,user,data,writer,get,translog,throw,new,illegal,argument,exception,a,new,translog,uuid,can,t,be,equal,to,existing,one,got,translog,uuid,final,map,string,string,map,new,hash,map,map,put,translog,1,map,put,translog,translog,uuid,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty(Version luceneVersion) throws IOException;1543942400;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty(Version luceneVersion) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newEmptyIndexWriter(directory, luceneVersion)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,version,lucene,version,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,empty,index,writer,directory,lucene,version,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty(Version luceneVersion) throws IOException;1549395161;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty(Version luceneVersion) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newEmptyIndexWriter(directory, luceneVersion)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,version,lucene,version,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,empty,index,writer,directory,lucene,version,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty(Version luceneVersion) throws IOException;1549935387;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty(Version luceneVersion) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newEmptyIndexWriter(directory, luceneVersion)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,version,lucene,version,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,empty,index,writer,directory,lucene,version,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void createEmpty(Version luceneVersion) throws IOException;1550526771;creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.;public void createEmpty(Version luceneVersion) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newEmptyIndexWriter(directory, luceneVersion)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED))__            map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1")__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };creates,an,empty,lucene,index,and,a,corresponding,empty,translog,any,existing,data,will,be,deleted;public,void,create,empty,version,lucene,version,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,empty,index,writer,directory,lucene,version,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,sequence,numbers,long,to,string,sequence,numbers,map,put,engine,1,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1524684173;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1525334055;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1526449283;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1526900724;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1528211342;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1535723122;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1535965276;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1536611444;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1536828374;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1537806831;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1538067637;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1539615817;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1542697754;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1543832502;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1543942400;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1549395161;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1549935387;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public MetadataSnapshot(StreamInput in) throws IOException;1550526771;Read from a stream.;public MetadataSnapshot(StreamInput in) throws IOException {_            final int size = in.readVInt()__            Map<String, StoreFileMetaData> metadata = new HashMap<>()__            for (int i = 0_ i < size_ i++) {_                StoreFileMetaData meta = new StoreFileMetaData(in)__                metadata.put(meta.name(), meta)__            }_            Map<String, String> commitUserData = new HashMap<>()__            int num = in.readVInt()__            for (int i = num_ i > 0_ i--) {_                commitUserData.put(in.readString(), in.readString())__            }__            this.metadata = unmodifiableMap(metadata)__            this.commitUserData = unmodifiableMap(commitUserData)__            this.numDocs = in.readLong()__            assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles()__        };read,from,a,stream;public,metadata,snapshot,stream,input,in,throws,ioexception,final,int,size,in,read,vint,map,string,store,file,meta,data,metadata,new,hash,map,for,int,i,0,i,size,i,store,file,meta,data,meta,new,store,file,meta,data,in,metadata,put,meta,name,meta,map,string,string,commit,user,data,new,hash,map,int,num,in,read,vint,for,int,i,num,i,0,i,commit,user,data,put,in,read,string,in,read,string,this,metadata,unmodifiable,map,metadata,this,commit,user,data,unmodifiable,map,commit,user,data,this,num,docs,in,read,long,assert,metadata,is,empty,num,segment,files,1,num,segment,files,num,segment,files
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1524684173;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1525334055;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1526449283;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1526900724;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1528211342;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1535723122;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1535965276;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1536611444;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1536828374;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1537806831;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1538067637;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1539615817;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1542697754;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1543832502;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1543942400;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1549395161;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1549935387;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> MetadataSnapshot -> public StoreFileMetaData getSegmentsFile();1550526771;Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.;public StoreFileMetaData getSegmentsFile() {_            for (StoreFileMetaData file : this) {_                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {_                    return file__                }_            }_            assert metadata.isEmpty()__            return null__        };returns,the,segments,file,that,this,metadata,snapshot,represents,or,null,if,the,snapshot,is,empty;public,store,file,meta,data,get,segments,file,for,store,file,meta,data,file,this,if,file,name,starts,with,index,file,names,segments,return,file,assert,metadata,is,empty,return,null
Store -> public static String digestToString(long digest);1524684173;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1525334055;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1526449283;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1526900724;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1528211342;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1535723122;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1535965276;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1536611444;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1536828374;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1537806831;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1538067637;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1539615817;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1542697754;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1543832502;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1543942400;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1549395161;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1549935387;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> public static String digestToString(long digest);1550526771;Produces a string representation of the given digest value.;public static String digestToString(long digest) {_        return Long.toString(digest, Character.MAX_RADIX)__    };produces,a,string,representation,of,the,given,digest,value;public,static,string,digest,to,string,long,digest,return,long,to,string,digest,character
Store -> @Override     public final void incRef();1524684173;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1525334055;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1526449283;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1526900724;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1528211342;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1535723122;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1535965276;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1536611444;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1536828374;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1537806831;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1538067637;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1539615817;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1542697754;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1543832502;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1543942400;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1549395161;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1549935387;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> @Override     public final void incRef();1550526771;Increments the refCount of this Store instance.  RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@throws AlreadyClosedException iff the reference counter can not be incremented._@see #decRef_@see #tryIncRef();@Override_    public final void incRef() {_        refCounter.incRef()__    };increments,the,ref,count,of,this,store,instance,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,throws,already,closed,exception,iff,the,reference,counter,can,not,be,incremented,see,dec,ref,see,try,inc,ref;override,public,final,void,inc,ref,ref,counter,inc,ref
Store -> public void bootstrapNewHistory(long localCheckpoint, long maxSeqNo) throws IOException;1549395161;Marks an existing lucene index with a new history uuid and sets the given local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recover from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long localCheckpoint, long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(localCheckpoint))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recover,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,local,checkpoint,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,local,checkpoint,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory(long localCheckpoint, long maxSeqNo) throws IOException;1549935387;Marks an existing lucene index with a new history uuid and sets the given local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recover from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long localCheckpoint, long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(localCheckpoint))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recover,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,local,checkpoint,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,local,checkpoint,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void bootstrapNewHistory(long localCheckpoint, long maxSeqNo) throws IOException;1550526771;Marks an existing lucene index with a new history uuid and sets the given local checkpoint_as well as the maximum sequence number._This is used to make sure no existing shard will recover from this index using ops based recovery._@see SequenceNumbers#LOCAL_CHECKPOINT_KEY_@see SequenceNumbers#MAX_SEQ_NO;public void bootstrapNewHistory(long localCheckpoint, long maxSeqNo) throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            final Map<String, String> map = new HashMap<>()__            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())__            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(localCheckpoint))__            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo))__            updateCommitData(writer, map)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };marks,an,existing,lucene,index,with,a,new,history,uuid,and,sets,the,given,local,checkpoint,as,well,as,the,maximum,sequence,number,this,is,used,to,make,sure,no,existing,shard,will,recover,from,this,index,using,ops,based,recovery,see,sequence,numbers,see,sequence,numbers;public,void,bootstrap,new,history,long,local,checkpoint,long,max,seq,no,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,final,map,string,string,map,new,hash,map,map,put,engine,uuids,random,base64uuid,map,put,sequence,numbers,long,to,string,local,checkpoint,map,put,sequence,numbers,long,to,string,max,seq,no,update,commit,data,writer,map,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1524684173;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1525334055;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1526449283;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1526900724;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1528211342;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1535723122;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1535965276;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1536611444;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1536828374;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1537806831;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1538067637;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1539615817;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    continue_ _                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1542697754;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    _                    _                    continue__                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1543832502;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    _                    _                    continue__                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1543942400;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    _                    _                    continue__                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1549395161;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    _                    _                    continue__                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1549935387;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    _                    _                    continue__                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException;1550526771;This method deletes every file in this store that is not contained in the given source meta data or is a_legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it_to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.__@param reason         the reason for this cleanup operation logged for each deleted file_@param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around._@throws IOException           if an IOException occurs_@throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.;public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {_        metadataLock.writeLock().lock()__        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (String existingFile : directory.listAll()) {_                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {_                    _                    _                    continue__                }_                try {_                    directory.deleteFile(reason, existingFile)__                    _                } catch (IOException ex) {_                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)_                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)_                            || existingFile.startsWith(CORRUPTED)) {_                        _                        _                        _                        throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex)__                    }_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex)__                    _                }_            }_            directory.syncMetaData()__            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null)__            verifyAfterCleanup(sourceMetaData, metadataOrEmpty)__        } finally {_            metadataLock.writeLock().unlock()__        }_    };this,method,deletes,every,file,in,this,store,that,is,not,contained,in,the,given,source,meta,data,or,is,a,legacy,checksum,file,after,the,delete,it,pulls,the,latest,metadata,snapshot,from,the,store,and,compares,it,to,the,given,snapshot,if,the,snapshots,are,inconsistent,an,illegal,state,exception,is,thrown,param,reason,the,reason,for,this,cleanup,operation,logged,for,each,deleted,file,param,source,meta,data,the,metadata,used,for,cleanup,all,files,in,this,metadata,should,be,kept,around,throws,ioexception,if,an,ioexception,occurs,throws,illegal,state,exception,if,the,latest,snapshot,in,this,store,differs,from,the,given,one,after,the,cleanup;public,void,cleanup,and,verify,string,reason,metadata,snapshot,source,meta,data,throws,ioexception,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,string,existing,file,directory,list,all,if,store,is,autogenerated,existing,file,source,meta,data,contains,existing,file,continue,try,directory,delete,file,reason,existing,file,catch,ioexception,ex,if,existing,file,starts,with,index,file,names,segments,existing,file,equals,index,file,names,existing,file,starts,with,corrupted,throw,new,illegal,state,exception,can,t,delete,existing,file,cleanup,failed,ex,logger,debug,new,parameterized,message,failed,to,delete,file,existing,file,ex,directory,sync,meta,data,final,store,metadata,snapshot,metadata,or,empty,get,metadata,null,verify,after,cleanup,source,meta,data,metadata,or,empty,finally,metadata,lock,write,lock,unlock
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1524684173;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1525334055;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1526449283;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1526900724;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1528211342;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1535723122;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1535965276;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1536611444;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1536828374;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1537806831;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1538067637;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1539615817;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1542697754;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1543832502;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1543942400;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1549395161;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1549935387;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException;1550526771;Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.__@param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null_@return {@link SequenceNumbers.CommitInfo} containing information about the last commit_@throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk;public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {_        final Map<String, String> userData = commit.getUserData()__        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet())__    };loads,the,maximum,sequence,number,and,local,checkpoint,from,the,given,lucene,commit,point,or,the,latest,if,not,provided,param,commit,the,commit,point,to,load,seqno,stats,or,the,last,commit,in,the,store,if,the,parameter,is,null,return,link,sequence,numbers,commit,info,containing,information,about,the,last,commit,throws,ioexception,if,an,i,o,exception,occurred,reading,the,latest,lucene,commit,point,from,disk;public,static,sequence,numbers,commit,info,load,seq,no,info,final,index,commit,commit,throws,ioexception,final,map,string,string,user,data,commit,get,user,data,return,sequence,numbers,load,seq,no,info,from,lucene,commit,user,data,entry,set
Store -> public void removeCorruptionMarker() throws IOException;1524684173;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1525334055;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1526449283;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1526900724;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1528211342;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1535723122;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1535965276;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1536611444;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1536828374;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1537806831;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1538067637;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1539615817;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1542697754;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1543832502;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1543942400;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1549395161;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1549935387;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> public void removeCorruptionMarker() throws IOException;1550526771;Deletes all corruption markers from this store.;public void removeCorruptionMarker() throws IOException {_        ensureOpen()__        final Directory directory = directory()__        IOException firstException = null__        final String[] files = directory.listAll()__        for (String file : files) {_            if (file.startsWith(CORRUPTED)) {_                try {_                    directory.deleteFile(file)__                } catch (IOException ex) {_                    if (firstException == null) {_                        firstException = ex__                    } else {_                        firstException.addSuppressed(ex)__                    }_                }_            }_        }_        if (firstException != null) {_            throw firstException__        }_    };deletes,all,corruption,markers,from,this,store;public,void,remove,corruption,marker,throws,ioexception,ensure,open,final,directory,directory,directory,ioexception,first,exception,null,final,string,files,directory,list,all,for,string,file,files,if,file,starts,with,corrupted,try,directory,delete,file,file,catch,ioexception,ex,if,first,exception,null,first,exception,ex,else,first,exception,add,suppressed,ex,if,first,exception,null,throw,first,exception
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1524684173;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1525334055;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1526449283;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1526900724;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1528211342;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1535723122;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1535965276;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1536611444;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1536828374;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1537806831;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1538067637;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1539615817;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1542697754;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1543832502;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1543942400;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1549395161;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1549935387;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> MetadataSnapshot -> public boolean contains(String existingFile);1550526771;Returns true iff this metadata contains the given file.;public boolean contains(String existingFile) {_            return metadata.containsKey(existingFile)__        };returns,true,iff,this,metadata,contains,the,given,file;public,boolean,contains,string,existing,file,return,metadata,contains,key,existing,file
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1524684173;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1525334055;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1526449283;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1526900724;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1528211342;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1535723122;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1535965276;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1536611444;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1536828374;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1537806831;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1538067637;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1539615817;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1542697754;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1543832502;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1543942400;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1549395161;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1549935387;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException;1550526771;Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>__@throws IOException if the index is corrupted or the segments file is not present;private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {_        assert commit == null || commit.getDirectory() == directory__        try {_            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit)__        } catch (EOFException eof) {_            _            throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof)__        } catch (IOException exception) {_            throw exception_ _        } catch (Exception ex) {_            throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex)__        }__    };returns,the,segments,info,for,the,given,commit,or,for,the,latest,commit,if,the,given,commit,is,code,null,code,throws,ioexception,if,the,index,is,corrupted,or,the,segments,file,is,not,present;private,static,segment,infos,read,segments,info,index,commit,commit,directory,directory,throws,ioexception,assert,commit,null,commit,get,directory,directory,try,return,commit,null,lucene,read,segment,infos,directory,lucene,read,segment,infos,commit,catch,eofexception,eof,throw,new,corrupt,index,exception,read,past,eof,while,reading,segment,infos,commit,commit,eof,catch,ioexception,exception,throw,exception,catch,exception,ex,throw,new,corrupt,index,exception,hit,unexpected,exception,while,reading,segment,infos,commit,commit,ex
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1524684173;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1525334055;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1526449283;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1526900724;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1528211342;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1535723122;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1535965276;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1536611444;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1536828374;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1537806831;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1538067637;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException;1539615817;Returns <code>true</code> iff the given location contains an index an the index_can be successfully opened. This includes reading the segment infos and possible_corruption markers.;public static boolean canOpenIndex(Logger logger, Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker) throws IOException {_        try {_            tryOpenIndex(indexLocation, shardId, shardLocker, logger)__        } catch (Exception ex) {_            logger.trace(() -> new ParameterizedMessage("Can't open index for path [{}]", indexLocation), ex)__            return false__        }_        return true__    };returns,code,true,code,iff,the,given,location,contains,an,index,an,the,index,can,be,successfully,opened,this,includes,reading,the,segment,infos,and,possible,corruption,markers;public,static,boolean,can,open,index,logger,logger,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,throws,ioexception,try,try,open,index,index,location,shard,id,shard,locker,logger,catch,exception,ex,logger,trace,new,parameterized,message,can,t,open,index,for,path,index,location,ex,return,false,return,true
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1524684173;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1525334055;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1526449283;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1526900724;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1528211342;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1535723122;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1535965276;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1536611444;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1536828374;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1537806831;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1538067637;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1539615817;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1542697754;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1543832502;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1543942400;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1549395161;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1549935387;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> OnClose -> null -> @Override             public void accept(ShardLock Lock);1550526771;This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held._This method is only called once after all resources for a store are released.;@Override_            public void accept(ShardLock Lock) {_            };this,method,is,called,while,the,provided,link,org,elasticsearch,env,shard,lock,is,held,this,method,is,only,called,once,after,all,resources,for,a,store,are,released;override,public,void,accept,shard,lock,lock
Store -> @Override     public final boolean tryIncRef();1524684173;Tries to increment the refCount of this Store instance. This method will return <tt>true</tt> iff the refCount was_incremented successfully otherwise <tt>false</tt>. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,tt,true,tt,iff,the,ref,count,was,incremented,successfully,otherwise,tt,false,tt,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1525334055;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1526449283;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1526900724;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1528211342;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1535723122;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1535965276;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1536611444;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1536828374;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1537806831;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1538067637;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1539615817;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1542697754;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1543832502;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1543942400;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1549395161;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1549935387;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> @Override     public final boolean tryIncRef();1550526771;Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was_incremented successfully otherwise {@code false}. RefCounts are used to determine when a_Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a_corresponding {@link #decRef}, in a finally clause_ otherwise the store may never be closed.  Note that_{@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link_#decRef} has been called for all outstanding references._<p>_Note: Close can safely be called multiple times.__@see #decRef()_@see #incRef();@Override_    public final boolean tryIncRef() {_        return refCounter.tryIncRef()__    };tries,to,increment,the,ref,count,of,this,store,instance,this,method,will,return,code,true,iff,the,ref,count,was,incremented,successfully,otherwise,code,false,ref,counts,are,used,to,determine,when,a,store,can,be,closed,safely,i,e,as,soon,as,there,are,no,more,references,be,sure,to,always,call,a,corresponding,link,dec,ref,in,a,finally,clause,otherwise,the,store,may,never,be,closed,note,that,link,close,simply,calls,dec,ref,which,means,that,the,store,will,not,really,be,closed,until,link,dec,ref,has,been,called,for,all,outstanding,references,p,note,close,can,safely,be,called,multiple,times,see,dec,ref,see,inc,ref;override,public,final,boolean,try,inc,ref,return,ref,counter,try,inc,ref
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1524684173;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1525334055;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1526449283;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1526900724;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1528211342;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1535723122;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1535965276;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1536611444;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1536828374;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1537806831;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1538067637;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException;1539615817;Tries to open an index for the given location. This includes reading the_segment infos and possible corruption markers. If the index can not_be opened, an exception is thrown;public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            SegmentInfos segInfo = Lucene.readSegmentInfos(dir)__            logger.trace("{} loaded segment info [{}]", shardId, segInfo)__        }_    };tries,to,open,an,index,for,the,given,location,this,includes,reading,the,segment,infos,and,possible,corruption,markers,if,the,index,can,not,be,opened,an,exception,is,thrown;public,static,void,try,open,index,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,shard,lock,obtain,failed,exception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,segment,infos,seg,info,lucene,read,segment,infos,dir,logger,trace,loaded,segment,info,shard,id,seg,info
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1524684173;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1525334055;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1526449283;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1526900724;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1528211342;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1535723122;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1535965276;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1536611444;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1536828374;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1537806831;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1538067637;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1539615817;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1542697754;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1543832502;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1543942400;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1549395161;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1549935387;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> MetadataSnapshot -> public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException;1550526771;Computes a strong hash value for small files. Note that this method should only be used for files &lt_ 1MB;public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {_            final int len = (int) Math.min(1024 * 1024, size)_ _            fileHash.grow(len)__            fileHash.setLength(len)__            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len)__            assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len)__            assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len)__        };computes,a,strong,hash,value,for,small,files,note,that,this,method,should,only,be,used,for,files,lt,1mb;public,static,void,hash,file,bytes,ref,builder,file,hash,input,stream,in,long,size,throws,ioexception,final,int,len,int,math,min,1024,1024,size,file,hash,grow,len,file,hash,set,length,len,final,int,read,bytes,streams,read,fully,in,file,hash,bytes,0,len,assert,read,bytes,len,integer,to,string,read,bytes,integer,to,string,len,assert,file,hash,length,len,integer,to,string,file,hash,length,integer,to,string,len
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1524684173;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1525334055;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1526449283;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1526900724;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1528211342;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1535723122;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1535965276;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1536611444;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1536828374;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1537806831;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1538067637;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException;1539615817;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1524684173;Marks this store as corrupted. This method writes a <tt>corrupted_${uuid}</tt> file containing the given exception_message. If a store contains a <tt>corrupted_${uuid}</tt> file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,tt,uuid,tt,file,containing,the,given,exception,message,if,a,store,contains,a,tt,uuid,tt,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1525334055;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1526449283;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1526900724;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1528211342;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1535723122;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1535965276;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1536611444;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1536828374;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1537806831;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1538067637;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1539615817;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1542697754;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1543832502;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1543942400;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1549395161;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1549935387;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> public void markStoreCorrupted(IOException exception) throws IOException;1550526771;Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception_message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.;public void markStoreCorrupted(IOException exception) throws IOException {_        ensureOpen()__        if (!isMarkedCorrupted()) {_            String uuid = CORRUPTED + UUIDs.randomBase64UUID()__            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {_                CodecUtil.writeHeader(output, CODEC, VERSION)__                BytesStreamOutput out = new BytesStreamOutput()__                out.writeException(exception)__                BytesReference bytes = out.bytes()__                output.writeVInt(bytes.length())__                BytesRef ref = bytes.toBytesRef()__                output.writeBytes(ref.bytes, ref.offset, ref.length)__                CodecUtil.writeFooter(output)__            } catch (IOException ex) {_                logger.warn("Can't mark store as corrupted", ex)__            }_            directory().sync(Collections.singleton(uuid))__        }_    };marks,this,store,as,corrupted,this,method,writes,a,code,uuid,file,containing,the,given,exception,message,if,a,store,contains,a,code,uuid,file,link,is,marked,corrupted,will,return,code,true,code;public,void,mark,store,corrupted,ioexception,exception,throws,ioexception,ensure,open,if,is,marked,corrupted,string,uuid,corrupted,uuids,random,base64uuid,try,index,output,output,this,directory,create,output,uuid,iocontext,default,codec,util,write,header,output,codec,version,bytes,stream,output,out,new,bytes,stream,output,out,write,exception,exception,bytes,reference,bytes,out,bytes,output,write,vint,bytes,length,bytes,ref,ref,bytes,to,bytes,ref,output,write,bytes,ref,bytes,ref,offset,ref,length,codec,util,write,footer,output,catch,ioexception,ex,logger,warn,can,t,mark,store,as,corrupted,ex,directory,sync,collections,singleton,uuid
Store -> MetadataSnapshot -> public String getTranslogUUID();1524684173;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1525334055;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1526449283;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1526900724;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1528211342;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1535723122;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1535965276;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1536611444;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1536828374;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1537806831;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1538067637;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1539615817;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> MetadataSnapshot -> public String getTranslogUUID();1542697754;returns the translog uuid the store points at;public String getTranslogUUID() {_            return commitUserData.get(Translog.TRANSLOG_UUID_KEY)__        };returns,the,translog,uuid,the,store,points,at;public,string,get,translog,uuid,return,commit,user,data,get,translog
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1524684173;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1525334055;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1526449283;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1526900724;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1528211342;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1535723122;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1535965276;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1536611444;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1536828374;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1537806831;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1538067637;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1539615817;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1542697754;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[tempFileMap.size()])__        ArrayUtil.timSort(entries, new Comparator<Map.Entry<String, String>>() {_            @Override_            public int compare(Map.Entry<String, String> o1, Map.Entry<String, String> o2) {_                String left = o1.getValue()__                String right = o2.getValue()__                if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                    if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return -1__                    } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                        return 1__                    }_                }_                return left.compareTo(right)__            }_        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,temp,file,map,size,array,util,tim,sort,entries,new,comparator,map,entry,string,string,override,public,int,compare,map,entry,string,string,o1,map,entry,string,string,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1543832502;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[0])__        ArrayUtil.timSort(entries, (o1, o2) -> {_            String left = o1.getValue()__            String right = o2.getValue()__            if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return -1__                } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return 1__                }_            }_            return left.compareTo(right)__        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,0,array,util,tim,sort,entries,o1,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1543942400;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[0])__        ArrayUtil.timSort(entries, (o1, o2) -> {_            String left = o1.getValue()__            String right = o2.getValue()__            if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return -1__                } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return 1__                }_            }_            return left.compareTo(right)__        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,0,array,util,tim,sort,entries,o1,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1549395161;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[0])__        ArrayUtil.timSort(entries, (o1, o2) -> {_            String left = o1.getValue()__            String right = o2.getValue()__            if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return -1__                } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return 1__                }_            }_            return left.compareTo(right)__        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,0,array,util,tim,sort,entries,o1,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1549935387;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[0])__        ArrayUtil.timSort(entries, (o1, o2) -> {_            String left = o1.getValue()__            String right = o2.getValue()__            if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return -1__                } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return 1__                }_            }_            return left.compareTo(right)__        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,0,array,util,tim,sort,entries,o1,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException;1550526771;Renames all the given files from the key of the map to the_value of the map. All successfully renamed files are removed from the map in-place.;public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {_        _        _        final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[0])__        ArrayUtil.timSort(entries, (o1, o2) -> {_            String left = o1.getValue()__            String right = o2.getValue()__            if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {_                if (left.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return -1__                } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {_                    return 1__                }_            }_            return left.compareTo(right)__        })__        metadataLock.writeLock().lock()__        _        _        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {_            for (Map.Entry<String, String> entry : entries) {_                String tempFile = entry.getKey()__                String origFile = entry.getValue()__                _                try {_                    directory.deleteFile(origFile)__                } catch (FileNotFoundException | NoSuchFileException e) {_                } catch (Exception ex) {_                    logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex)__                }_                _                directory.rename(tempFile, origFile)__                final String remove = tempFileMap.remove(tempFile)__                assert remove != null__            }_            directory.syncMetaData()__        } finally {_            metadataLock.writeLock().unlock()__        }__    };renames,all,the,given,files,from,the,key,of,the,map,to,the,value,of,the,map,all,successfully,renamed,files,are,removed,from,the,map,in,place;public,void,rename,temp,files,safe,map,string,string,temp,file,map,throws,ioexception,final,map,entry,string,string,entries,temp,file,map,entry,set,to,array,new,map,entry,0,array,util,tim,sort,entries,o1,o2,string,left,o1,get,value,string,right,o2,get,value,if,left,starts,with,index,file,names,segments,right,starts,with,index,file,names,segments,if,left,starts,with,index,file,names,segments,false,return,1,else,if,right,starts,with,index,file,names,segments,false,return,1,return,left,compare,to,right,metadata,lock,write,lock,lock,try,lock,write,lock,directory,obtain,lock,index,writer,for,map,entry,string,string,entry,entries,string,temp,file,entry,get,key,string,orig,file,entry,get,value,try,directory,delete,file,orig,file,catch,file,not,found,exception,no,such,file,exception,e,catch,exception,ex,logger,debug,new,parameterized,message,failed,to,delete,file,orig,file,ex,directory,rename,temp,file,orig,file,final,string,remove,temp,file,map,remove,temp,file,assert,remove,null,directory,sync,meta,data,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1524684173;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1525334055;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1526449283;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1526900724;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1528211342;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1535723122;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1535965276;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1536611444;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1536828374;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1537806831;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1538067637;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1539615817;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1542697754;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1543832502;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1543942400;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1549395161;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1549935387;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void ensureIndexHasHistoryUUID() throws IOException;1550526771;Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.;public void ensureIndexHasHistoryUUID() throws IOException {_        metadataLock.writeLock().lock()__        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {_            final Map<String, String> userData = getUserData(writer)__            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {_                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()))__            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,that,the,lucene,index,contains,a,history,uuid,marker,if,not,a,new,one,is,generated,and,committed;public,void,ensure,index,has,history,uuid,throws,ioexception,metadata,lock,write,lock,lock,try,index,writer,writer,new,appending,index,writer,directory,null,final,map,string,string,user,data,get,user,data,writer,if,user,data,contains,key,engine,false,update,commit,data,writer,collections,singleton,map,engine,uuids,random,base64uuid,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1524684173;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1525334055;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1526449283;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1526900724;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1528211342;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1535723122;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1535965276;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1536611444;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1536828374;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1537806831;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1538067637;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1539615817;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1542697754;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1543832502;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,index,writer,index,writer,config,open,mode,append,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1543942400;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newAppendingIndexWriter(directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,appending,index,writer,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1549395161;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final String translogUUID = existingCommits.get(existingCommits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(existingCommits.get(existingCommits.size() - 1)) == false) {_                try (IndexWriter writer = newAppendingIndexWriter(directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,string,translog,uuid,existing,commits,get,existing,commits,size,1,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,existing,commits,get,existing,commits,size,1,false,try,index,writer,writer,new,appending,index,writer,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1549935387;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final IndexCommit lastIndexCommitCommit = existingCommits.get(existingCommits.size() - 1)__            final String translogUUID = lastIndexCommitCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(lastIndexCommitCommit) == false) {_                _                final Map<String, String> userData = new HashMap<>(startingIndexCommit.getUserData())__                userData.put(Engine.RETENTION_LEASES, lastIndexCommitCommit.getUserData().getOrDefault(Engine.RETENTION_LEASES, ""))__                try (IndexWriter writer = newAppendingIndexWriter(directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(userData.entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,index,commit,last,index,commit,commit,existing,commits,get,existing,commits,size,1,final,string,translog,uuid,last,index,commit,commit,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,last,index,commit,commit,false,final,map,string,string,user,data,new,hash,map,starting,index,commit,get,user,data,user,data,put,engine,last,index,commit,commit,get,user,data,get,or,default,engine,try,index,writer,writer,new,appending,index,writer,directory,starting,index,commit,writer,set,live,commit,data,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,                                   final org.elasticsearch.Version indexVersionCreated) throws IOException;1550526771;Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe_at the recovering time but they can suddenly become safe in the future._The following issues can happen if unsafe commits are kept oninit._<p>_1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)_and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)_is added without flushing, the global checkpoint is advanced to 2_ and the replica recovers again, it will use_the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the_commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica._<p>_2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit_c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2)._The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new_commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery_translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1_while the local checkpoint of c2 is 2._<p>_3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced_(v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,_the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.;public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,_                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {_        metadataLock.writeLock().lock()__        try {_            final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory)__            if (existingCommits.isEmpty()) {_                throw new IllegalArgumentException("No index found to trim")__            }_            final IndexCommit lastIndexCommitCommit = existingCommits.get(existingCommits.size() - 1)__            final String translogUUID = lastIndexCommitCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)__            final IndexCommit startingIndexCommit__            _            _            _            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {_                final List<IndexCommit> recoverableCommits = new ArrayList<>()__                for (IndexCommit commit : existingCommits) {_                    if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {_                        recoverableCommits.add(commit)__                    }_                }_                assert recoverableCommits.isEmpty() == false : "No commit point with translog found_ " +_                    "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"__                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint)__            } else {_                _                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint)__            }__            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {_                throw new IllegalStateException("starting commit translog uuid ["_                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid ["_                    + translogUUID + "]")__            }_            if (startingIndexCommit.equals(lastIndexCommitCommit) == false) {_                try (IndexWriter writer = newAppendingIndexWriter(directory, startingIndexCommit)) {_                    _                    _                    _                    _                    _                    __                    _                    _                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet())__                    writer.commit()__                }_            }_        } finally {_            metadataLock.writeLock().unlock()__        }_    };keeping,existing,unsafe,commits,when,opening,an,engine,can,be,problematic,because,these,commits,are,not,safe,at,the,recovering,time,but,they,can,suddenly,become,safe,in,the,future,the,following,issues,can,happen,if,unsafe,commits,are,kept,oninit,p,1,replica,can,use,unsafe,commit,in,peer,recovery,this,happens,when,a,replica,with,a,safe,commit,c1,1,and,an,unsafe,commit,c2,2,recovers,from,a,primary,with,c1,1,if,a,new,document,seqno,2,is,added,without,flushing,the,global,checkpoint,is,advanced,to,2,and,the,replica,recovers,again,it,will,use,the,unsafe,commit,c2,2,at,most,gcp,2,as,the,starting,commit,for,sequenced,based,recovery,even,the,commit,c2,contains,a,stale,operation,and,the,document,with,seqno,2,will,not,be,replicated,to,the,replica,p,2,min,translog,gen,for,recovery,can,go,backwards,in,peer,recovery,this,happens,when,are,replica,with,a,safe,commit,c1,1,1,and,an,unsafe,commit,c2,2,2,the,replica,recovers,from,a,primary,and,keeps,c2,as,the,last,commit,then,sets,to,2,flushing,a,new,commit,on,the,replica,will,cause,exception,as,the,new,last,commit,c3,will,have,1,the,recovery,translog,generation,of,a,commit,is,calculated,based,on,the,current,local,checkpoint,the,local,checkpoint,of,c3,is,1,while,the,local,checkpoint,of,c2,is,2,p,3,commit,without,translog,can,be,used,in,recovery,an,old,index,which,was,created,before,multiple,commits,is,introduced,v6,2,may,not,have,a,safe,commit,if,that,index,has,a,snapshotted,commit,without,translog,and,an,unsafe,commit,the,policy,can,consider,the,snapshotted,commit,as,a,safe,commit,for,recovery,even,the,commit,does,not,have,translog;public,void,trim,unsafe,commits,final,long,last,synced,global,checkpoint,final,long,min,retained,translog,gen,final,org,elasticsearch,version,index,version,created,throws,ioexception,metadata,lock,write,lock,lock,try,final,list,index,commit,existing,commits,directory,reader,list,commits,directory,if,existing,commits,is,empty,throw,new,illegal,argument,exception,no,index,found,to,trim,final,index,commit,last,index,commit,commit,existing,commits,get,existing,commits,size,1,final,string,translog,uuid,last,index,commit,commit,get,user,data,get,translog,final,index,commit,starting,index,commit,if,index,version,created,before,org,elasticsearch,version,final,list,index,commit,recoverable,commits,new,array,list,for,index,commit,commit,existing,commits,if,min,retained,translog,gen,long,parse,long,commit,get,user,data,get,translog,recoverable,commits,add,commit,assert,recoverable,commits,is,empty,false,no,commit,point,with,translog,found,commits,existing,commits,min,retained,translog,gen,min,retained,translog,gen,starting,index,commit,combined,deletion,policy,find,safe,commit,point,recoverable,commits,last,synced,global,checkpoint,else,starting,index,commit,combined,deletion,policy,find,safe,commit,point,existing,commits,last,synced,global,checkpoint,if,translog,uuid,equals,starting,index,commit,get,user,data,get,translog,false,throw,new,illegal,state,exception,starting,commit,translog,uuid,starting,index,commit,get,user,data,get,translog,is,not,equal,to,last,commit,s,translog,uuid,translog,uuid,if,starting,index,commit,equals,last,index,commit,commit,false,try,index,writer,writer,new,appending,index,writer,directory,starting,index,commit,writer,set,live,commit,data,starting,index,commit,get,user,data,entry,set,writer,commit,finally,metadata,lock,write,lock,unlock
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,                                                 final IOContext context) throws IOException;1542697754;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,_                                                final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,                                                 final IOContext context) throws IOException;1543832502;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,_                                                final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,                                                 final IOContext context) throws IOException;1543942400;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,_                                                final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,                                                 final IOContext context) throws IOException;1549395161;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,_                                                final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,                                                 final IOContext context) throws IOException;1549935387;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,_                                                final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,                                                 final IOContext context) throws IOException;1550526771;The returned IndexOutput validates the files checksum._<p>_Note: Checksums are calculated by default since version 4.8.0. This method only adds the_verification against the checksum in the given metadata and does not add any significant overhead.;public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,_                                                final IOContext context) throws IOException {_        IndexOutput output = directory().createOutput(fileName, context)__        boolean success = false__        try {_            assert metadata.writtenBy() != null__            output = new LuceneVerifyingIndexOutput(metadata, output)__            success = true__        } finally {_            if (success == false) {_                IOUtils.closeWhileHandlingException(output)__            }_        }_        return output__    };the,returned,index,output,validates,the,files,checksum,p,note,checksums,are,calculated,by,default,since,version,4,8,0,this,method,only,adds,the,verification,against,the,checksum,in,the,given,metadata,and,does,not,add,any,significant,overhead;public,index,output,create,verifying,output,string,file,name,final,store,file,meta,data,metadata,final,iocontext,context,throws,ioexception,index,output,output,directory,create,output,file,name,context,boolean,success,false,try,assert,metadata,written,by,null,output,new,lucene,verifying,index,output,metadata,output,success,true,finally,if,success,false,ioutils,close,while,handling,exception,output,return,output
Store -> @Override     public final void decRef();1524684173;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1525334055;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1526449283;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1526900724;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1528211342;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1535723122;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1535965276;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1536611444;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1536828374;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1537806831;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1538067637;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1539615817;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1542697754;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1543832502;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1543942400;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1549395161;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1549935387;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> @Override     public final void decRef();1550526771;Decreases the refCount of this Store instance. If the refCount drops to 0, then this_store is closed.__@see #incRef;@Override_    public final void decRef() {_        refCounter.decRef()__    };decreases,the,ref,count,of,this,store,instance,if,the,ref,count,drops,to,0,then,this,store,is,closed,see,inc,ref;override,public,final,void,dec,ref,ref,counter,dec,ref
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1524684173;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1525334055;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1526449283;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1526900724;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1528211342;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1535723122;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1535965276;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1536611444;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1536828374;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1537806831;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1538067637;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1539615817;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1542697754;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1543832502;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1543942400;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1549395161;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1549935387;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public CheckIndex.Status checkIndex(PrintStream out) throws IOException;1550526771;Checks and returns the status of the existing index in this store.__@param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)};public CheckIndex.Status checkIndex(PrintStream out) throws IOException {_        metadataLock.writeLock().lock()__        try (CheckIndex checkIndex = new CheckIndex(directory)) {_            checkIndex.setInfoStream(out)__            return checkIndex.checkIndex()__        } finally {_            metadataLock.writeLock().unlock()__        }_    };checks,and,returns,the,status,of,the,existing,index,in,this,store,param,out,where,info,stream,messages,should,go,see,link,check,index,set,info,stream,print,stream;public,check,index,status,check,index,print,stream,out,throws,ioexception,metadata,lock,write,lock,lock,try,check,index,check,index,new,check,index,directory,check,index,set,info,stream,out,return,check,index,check,index,finally,metadata,lock,write,lock,unlock
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1524684173;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1525334055;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1526449283;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1526900724;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1528211342;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1535723122;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1535965276;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1536611444;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1536828374;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1537806831;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1538067637;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1539615817;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1542697754;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1543832502;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1543942400;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1549395161;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1549935387;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException;1550526771;Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>_the latest commit point is used.__Note that this method requires the caller verify it has the right to access the store and_no concurrent file changes are happening. If in doubt, you probably want to use one of the following:__{@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking_{@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard_{@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed_@param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the_directory_@throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an_unexpected exception when opening the index reading the segments file._@throws IndexFormatTooOldException if the lucene index is too old to be opened._@throws IndexFormatTooNewException if the lucene index is too new to be opened._@throws FileNotFoundException      if one or more files referenced by a commit are not present._@throws NoSuchFileException        if one or more files referenced by a commit are not present._@throws IndexNotFoundException     if the commit point can't be found in this store;public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {_        return getMetadata(commit, false)__    };returns,a,new,metadata,snapshot,for,the,given,commit,if,the,given,commit,is,code,null,code,the,latest,commit,point,is,used,note,that,this,method,requires,the,caller,verify,it,has,the,right,to,access,the,store,and,no,concurrent,file,changes,are,happening,if,in,doubt,you,probably,want,to,use,one,of,the,following,link,read,metadata,snapshot,path,shard,id,node,environment,shard,locker,logger,to,read,a,meta,data,while,locking,link,index,shard,snapshot,store,metadata,to,safely,read,from,an,existing,shard,link,index,shard,acquire,last,index,commit,boolean,to,get,an,link,index,commit,which,is,safe,to,use,but,has,to,be,freed,param,commit,the,index,commit,to,read,the,snapshot,from,or,code,null,code,if,the,latest,snapshot,should,be,read,from,the,directory,throws,corrupt,index,exception,if,the,lucene,index,is,corrupted,this,can,be,caused,by,a,checksum,mismatch,or,an,unexpected,exception,when,opening,the,index,reading,the,segments,file,throws,index,format,too,old,exception,if,the,lucene,index,is,too,old,to,be,opened,throws,index,format,too,new,exception,if,the,lucene,index,is,too,new,to,be,opened,throws,file,not,found,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,no,such,file,exception,if,one,or,more,files,referenced,by,a,commit,are,not,present,throws,index,not,found,exception,if,the,commit,point,can,t,be,found,in,this,store;public,metadata,snapshot,get,metadata,index,commit,commit,throws,ioexception,return,get,metadata,commit,false
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1524684173;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1525334055;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1526449283;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1526900724;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1528211342;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1535723122;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1535965276;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1536611444;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1536828374;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1537806831;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1538067637;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1539615817;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1542697754;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1543832502;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1543942400;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1549395161;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1549935387;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
Store -> public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,                                                         Logger logger) throws IOException;1550526771;Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.__@throws IOException if the index we try to read is corrupted;public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,_                                                        Logger logger) throws IOException {_        try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5))__             Directory dir = new SimpleFSDirectory(indexLocation)) {_            failIfCorrupted(dir, shardId)__            return new MetadataSnapshot(null, dir, logger)__        } catch (IndexNotFoundException ex) {_            _        } catch (FileNotFoundException | NoSuchFileException ex) {_            logger.info("Failed to open / find files while reading metadata snapshot")__        } catch (ShardLockObtainFailedException ex) {_            logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex)__        }_        return MetadataSnapshot.EMPTY__    };reads,a,metadata,snapshot,from,the,given,index,locations,or,returns,an,empty,snapshot,if,it,can,t,be,read,throws,ioexception,if,the,index,we,try,to,read,is,corrupted;public,static,metadata,snapshot,read,metadata,snapshot,path,index,location,shard,id,shard,id,node,environment,shard,locker,shard,locker,logger,logger,throws,ioexception,try,shard,lock,lock,shard,locker,lock,shard,id,time,unit,seconds,to,millis,5,directory,dir,new,simple,fsdirectory,index,location,fail,if,corrupted,dir,shard,id,return,new,metadata,snapshot,null,dir,logger,catch,index,not,found,exception,ex,catch,file,not,found,exception,no,such,file,exception,ex,logger,info,failed,to,open,find,files,while,reading,metadata,snapshot,catch,shard,lock,obtain,failed,exception,ex,logger,info,new,parameterized,message,failed,to,obtain,shard,lock,shard,id,ex,return,metadata,snapshot,empty
