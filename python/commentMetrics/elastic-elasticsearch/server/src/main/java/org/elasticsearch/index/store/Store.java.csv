commented;modifiers;parameterAmount;loc;comment;code
false;protected;0;5;;@Override protected void closeInternal() {     // close us once we are done     Store.this.closeInternal(). }
false;public;0;4;;public Directory directory() {     ensureOpen().     return directory. }
true;public;0;9;/**  * Returns the last committed segments info for this store  *  * @throws IOException if the index is corrupted or the segments file is not present  */ ;/**  * Returns the last committed segments info for this store  *  * @throws IOException if the index is corrupted or the segments file is not present  */ public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {     failIfCorrupted().     try {         return readSegmentsInfo(null, directory()).     } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {         markStoreCorrupted(ex).         throw ex.     } }
true;private,static;2;14;/**  * Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>  *  * @throws IOException if the index is corrupted or the segments file is not present  */ ;/**  * Returns the segments info for the given commit or for the latest commit if the given commit is <code>null</code>  *  * @throws IOException if the index is corrupted or the segments file is not present  */ private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {     assert commit == null || commit.getDirectory() == directory.     try {         return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit).     } catch (EOFException eof) {         // TODO this should be caught by lucene - EOF is almost certainly an index corruption         throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof).     } catch (IOException exception) {         // IOExceptions like too many open files are not necessarily a corruption - just bubble it up         throw exception.     } catch (Exception ex) {         throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex).     } }
true;public,static;1;4;/**  * Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.  *  * @param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null  * @return {@link SequenceNumbers.CommitInfo} containing information about the last commit  * @throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk  */ ;/**  * Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.  *  * @param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null  * @return {@link SequenceNumbers.CommitInfo} containing information about the last commit  * @throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk  */ public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {     final Map<String, String> userData = commit.getUserData().     return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet()). }
false;final;0;5;;final void ensureOpen() {     if (this.refCounter.refCount() <= 0) {         throw new AlreadyClosedException("store is already closed").     } }
true;public;1;3;/**  * Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>  * the latest commit point is used.  *  * Note that this method requires the caller verify it has the right to access the store and  * no concurrent file changes are happening. If in doubt, you probably want to use one of the following:  *  * {@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking  * {@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard  * {@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed  * @param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the  *               directory  * @throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an  *                                    unexpected exception when opening the index reading the segments file.  * @throws IndexFormatTooOldException if the lucene index is too old to be opened.  * @throws IndexFormatTooNewException if the lucene index is too new to be opened.  * @throws FileNotFoundException      if one or more files referenced by a commit are not present.  * @throws NoSuchFileException        if one or more files referenced by a commit are not present.  * @throws IndexNotFoundException     if the commit point can't be found in this store  */ ;/**  * Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>  * the latest commit point is used.  *  * Note that this method requires the caller verify it has the right to access the store and  * no concurrent file changes are happening. If in doubt, you probably want to use one of the following:  *  * {@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking  * {@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard  * {@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed  * @param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the  *               directory  * @throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an  *                                    unexpected exception when opening the index reading the segments file.  * @throws IndexFormatTooOldException if the lucene index is too old to be opened.  * @throws IndexFormatTooNewException if the lucene index is too new to be opened.  * @throws FileNotFoundException      if one or more files referenced by a commit are not present.  * @throws NoSuchFileException        if one or more files referenced by a commit are not present.  * @throws IndexNotFoundException     if the commit point can't be found in this store  */ public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {     return getMetadata(commit, false). }
true;public;2;17;/**  * Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>  * the latest commit point is used.  *  * Note that this method requires the caller verify it has the right to access the store and  * no concurrent file changes are happening. If in doubt, you probably want to use one of the following:  *  * {@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking  * {@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard  * {@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed  *  * @param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the  *               directory  * @param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should  *                      only be used if there is no started shard using this store.  * @throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an  *                                    unexpected exception when opening the index reading the segments file.  * @throws IndexFormatTooOldException if the lucene index is too old to be opened.  * @throws IndexFormatTooNewException if the lucene index is too new to be opened.  * @throws FileNotFoundException      if one or more files referenced by a commit are not present.  * @throws NoSuchFileException        if one or more files referenced by a commit are not present.  * @throws IndexNotFoundException     if the commit point can't be found in this store  */ ;/**  * Returns a new MetadataSnapshot for the given commit. If the given commit is <code>null</code>  * the latest commit point is used.  *  * Note that this method requires the caller verify it has the right to access the store and  * no concurrent file changes are happening. If in doubt, you probably want to use one of the following:  *  * {@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking  * {@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard  * {@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed  *  * @param commit the index commit to read the snapshot from or <code>null</code> if the latest snapshot should be read from the  *               directory  * @param lockDirectory if <code>true</code> the index writer lock will be obtained before reading the snapshot. This should  *                      only be used if there is no started shard using this store.  * @throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an  *                                    unexpected exception when opening the index reading the segments file.  * @throws IndexFormatTooOldException if the lucene index is too old to be opened.  * @throws IndexFormatTooNewException if the lucene index is too new to be opened.  * @throws FileNotFoundException      if one or more files referenced by a commit are not present.  * @throws NoSuchFileException        if one or more files referenced by a commit are not present.  * @throws IndexNotFoundException     if the commit point can't be found in this store  */ public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {     ensureOpen().     failIfCorrupted().     assert lockDirectory ? commit == null : true : "IW lock should not be obtained if there is a commit point available".     // if we lock the directory we also acquire the write lock since that makes sure that nobody else tries to lock the IW     // on this store at the same time.     java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock().     lock.lock().     try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -> {     }) {         return new MetadataSnapshot(commit, directory, logger).     } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {         markStoreCorrupted(ex).         throw ex.     } finally {         lock.unlock().     } }
true;public;1;41;/**  * Renames all the given files from the key of the map to the  * value of the map. All successfully renamed files are removed from the map in-place.  */ ;/**  * Renames all the given files from the key of the map to the  * value of the map. All successfully renamed files are removed from the map in-place.  */ public void renameTempFilesSafe(Map<String, String> tempFileMap) throws IOException {     // this works just like a lucene commit - we rename all temp files and once we successfully     // renamed all the segments we rename the commit to ensure we don't leave half baked commits behind.     final Map.Entry<String, String>[] entries = tempFileMap.entrySet().toArray(new Map.Entry[0]).     ArrayUtil.timSort(entries, (o1, o2) -> {         String left = o1.getValue().         String right = o2.getValue().         if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {             if (left.startsWith(IndexFileNames.SEGMENTS) == false) {                 return -1.             } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {                 return 1.             }         }         return left.compareTo(right).     }).     metadataLock.writeLock().lock().     // get exceptions if files are still open.     try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {         for (Map.Entry<String, String> entry : entries) {             String tempFile = entry.getKey().             String origFile = entry.getValue().             // first, go and delete the existing ones             try {                 directory.deleteFile(origFile).             } catch (FileNotFoundException | NoSuchFileException e) {             } catch (Exception ex) {                 logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", origFile), ex).             }             // now, rename the files... and fail it it won't work             directory.rename(tempFile, origFile).             final String remove = tempFileMap.remove(tempFile).             assert remove != null.         }         directory.syncMetaData().     } finally {         metadataLock.writeLock().unlock().     } }
true;public;1;9;/**  * Checks and returns the status of the existing index in this store.  *  * @param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)}  */ ;/**  * Checks and returns the status of the existing index in this store.  *  * @param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)}  */ public CheckIndex.Status checkIndex(PrintStream out) throws IOException {     metadataLock.writeLock().lock().     try (CheckIndex checkIndex = new CheckIndex(directory)) {         checkIndex.setInfoStream(out).         return checkIndex.checkIndex().     } finally {         metadataLock.writeLock().unlock().     } }
false;public;0;4;;public StoreStats stats() throws IOException {     ensureOpen().     return new StoreStats(directory.estimateSize()). }
true;public,final;0;4;/**  * Increments the refCount of this Store instance.  RefCounts are used to determine when a  * Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a  * corresponding {@link #decRef}, in a finally clause. otherwise the store may never be closed.  Note that  * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link  * #decRef} has been called for all outstanding references.  * <p>  * Note: Close can safely be called multiple times.  *  * @throws AlreadyClosedException iff the reference counter can not be incremented.  * @see #decRef  * @see #tryIncRef()  */ ;/**  * Increments the refCount of this Store instance.  RefCounts are used to determine when a  * Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a  * corresponding {@link #decRef}, in a finally clause. otherwise the store may never be closed.  Note that  * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link  * #decRef} has been called for all outstanding references.  * <p>  * Note: Close can safely be called multiple times.  *  * @throws AlreadyClosedException iff the reference counter can not be incremented.  * @see #decRef  * @see #tryIncRef()  */ @Override public final void incRef() {     refCounter.incRef(). }
true;public,final;0;4;/**  * Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was  * incremented successfully otherwise {@code false}. RefCounts are used to determine when a  * Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a  * corresponding {@link #decRef}, in a finally clause. otherwise the store may never be closed.  Note that  * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link  * #decRef} has been called for all outstanding references.  * <p>  * Note: Close can safely be called multiple times.  *  * @see #decRef()  * @see #incRef()  */ ;/**  * Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was  * incremented successfully otherwise {@code false}. RefCounts are used to determine when a  * Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a  * corresponding {@link #decRef}, in a finally clause. otherwise the store may never be closed.  Note that  * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link  * #decRef} has been called for all outstanding references.  * <p>  * Note: Close can safely be called multiple times.  *  * @see #decRef()  * @see #incRef()  */ @Override public final boolean tryIncRef() {     return refCounter.tryIncRef(). }
true;public,final;0;4;/**  * Decreases the refCount of this Store instance. If the refCount drops to 0, then this  * store is closed.  *  * @see #incRef  */ ;/**  * Decreases the refCount of this Store instance. If the refCount drops to 0, then this  * store is closed.  *  * @see #incRef  */ @Override public final void decRef() {     refCounter.decRef(). }
false;public;0;9;;@Override public void close() {     if (isClosed.compareAndSet(false, true)) {         // only do this once!         decRef().         logger.debug("store reference count on close: {}", refCounter.refCount()).     } }
false;private;0;13;;private void closeInternal() {     try {         try {             // this closes the distributorDirectory as well             directory.innerClose().         } finally {             onClose.accept(shardLock).         }     } catch (IOException e) {         logger.debug("failed to close directory", e).     } finally {         IOUtils.closeWhileHandlingException(shardLock).     } }
true;public,static;4;15;/**  * Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.  *  * @throws IOException if the index we try to read is corrupted  */ ;/**  * Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can't be read.  *  * @throws IOException if the index we try to read is corrupted  */ public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException {     try (ShardLock lock = shardLocker.lock(shardId, "read metadata snapshot", TimeUnit.SECONDS.toMillis(5)).         Directory dir = new SimpleFSDirectory(indexLocation)) {         failIfCorrupted(dir, shardId).         return new MetadataSnapshot(null, dir, logger).     } catch (IndexNotFoundException ex) {     // that's fine - happens all the time no need to log     } catch (FileNotFoundException | NoSuchFileException ex) {         logger.info("Failed to open / find files while reading metadata snapshot").     } catch (ShardLockObtainFailedException ex) {         logger.info(() -> new ParameterizedMessage("{}: failed to obtain shard lock", shardId), ex).     }     return MetadataSnapshot.EMPTY. }
true;public,static;4;9;/**  * Tries to open an index for the given location. This includes reading the  * segment infos and possible corruption markers. If the index can not  * be opened, an exception is thrown  */ ;/**  * Tries to open an index for the given location. This includes reading the  * segment infos and possible corruption markers. If the index can not  * be opened, an exception is thrown  */ public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker, Logger logger) throws IOException, ShardLockObtainFailedException {     try (ShardLock lock = shardLocker.lock(shardId, "open index", TimeUnit.SECONDS.toMillis(5)).         Directory dir = new SimpleFSDirectory(indexLocation)) {         failIfCorrupted(dir, shardId).         SegmentInfos segInfo = Lucene.readSegmentInfos(dir).         logger.trace("{} loaded segment info [{}]", shardId, segInfo).     } }
true;public;3;15;/**  * The returned IndexOutput validates the files checksum.  * <p>  * Note: Checksums are calculated by default since version 4.8.0. This method only adds the  * verification against the checksum in the given metadata and does not add any significant overhead.  */ ;/**  * The returned IndexOutput validates the files checksum.  * <p>  * Note: Checksums are calculated by default since version 4.8.0. This method only adds the  * verification against the checksum in the given metadata and does not add any significant overhead.  */ public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata, final IOContext context) throws IOException {     IndexOutput output = directory().createOutput(fileName, context).     boolean success = false.     try {         assert metadata.writtenBy() != null.         output = new LuceneVerifyingIndexOutput(metadata, output).         success = true.     } finally {         if (success == false) {             IOUtils.closeWhileHandlingException(output).         }     }     return output. }
false;public,static;1;5;;public static void verify(IndexOutput output) throws IOException {     if (output instanceof VerifyingIndexOutput) {         ((VerifyingIndexOutput) output).verify().     } }
false;public;3;4;;public IndexInput openVerifyingInput(String filename, IOContext context, StoreFileMetaData metadata) throws IOException {     assert metadata.writtenBy() != null.     return new VerifyingIndexInput(directory().openInput(filename, context)). }
false;public,static;1;5;;public static void verify(IndexInput input) throws IOException {     if (input instanceof VerifyingIndexInput) {         ((VerifyingIndexInput) input).verify().     } }
false;public;1;3;;public boolean checkIntegrityNoException(StoreFileMetaData md) {     return checkIntegrityNoException(md, directory()). }
false;public,static;2;8;;public static boolean checkIntegrityNoException(StoreFileMetaData md, Directory directory) {     try {         checkIntegrity(md, directory).         return true.     } catch (IOException e) {         return false.     } }
false;public,static;2;15;;public static void checkIntegrity(final StoreFileMetaData md, final Directory directory) throws IOException {     try (IndexInput input = directory.openInput(md.name(), IOContext.READONCE)) {         if (input.length() != md.length()) {             // first check the length no matter how old this file is             throw new CorruptIndexException("expected length=" + md.length() + " != actual length: " + input.length() + " : file truncated?", input).         }         // throw exception if the file is corrupt         String checksum = Store.digestToString(CodecUtil.checksumEntireFile(input)).         // throw exception if metadata is inconsistent         if (!checksum.equals(md.checksum())) {             throw new CorruptIndexException("inconsistent metadata: lucene checksum=" + checksum + ", metadata checksum=" + md.checksum(), input).         }     } }
false;public;0;13;;public boolean isMarkedCorrupted() throws IOException {     ensureOpen().     /* marking a store as corrupted is basically adding a _corrupted to all          * the files. This prevent          */     final String[] files = directory().listAll().     for (String file : files) {         if (file.startsWith(CORRUPTED)) {             return true.         }     }     return false. }
true;public;0;22;/**  * Deletes all corruption markers from this store.  */ ;/**  * Deletes all corruption markers from this store.  */ public void removeCorruptionMarker() throws IOException {     ensureOpen().     final Directory directory = directory().     IOException firstException = null.     final String[] files = directory.listAll().     for (String file : files) {         if (file.startsWith(CORRUPTED)) {             try {                 directory.deleteFile(file).             } catch (IOException ex) {                 if (firstException == null) {                     firstException = ex.                 } else {                     firstException.addSuppressed(ex).                 }             }         }     }     if (firstException != null) {         throw firstException.     } }
false;public;0;4;;public void failIfCorrupted() throws IOException {     ensureOpen().     failIfCorrupted(directory, shardId). }
false;private,static;2;40;;private static void failIfCorrupted(Directory directory, ShardId shardId) throws IOException {     final String[] files = directory.listAll().     List<CorruptIndexException> ex = new ArrayList<>().     for (String file : files) {         if (file.startsWith(CORRUPTED)) {             try (ChecksumIndexInput input = directory.openChecksumInput(file, IOContext.READONCE)) {                 int version = CodecUtil.checkHeader(input, CODEC, VERSION_START, VERSION).                 if (version == VERSION_WRITE_THROWABLE) {                     final int size = input.readVInt().                     final byte[] buffer = new byte[size].                     input.readBytes(buffer, 0, buffer.length).                     StreamInput in = StreamInput.wrap(buffer).                     Exception t = in.readException().                     if (t instanceof CorruptIndexException) {                         ex.add((CorruptIndexException) t).                     } else {                         ex.add(new CorruptIndexException(t.getMessage(), "preexisting_corruption", t)).                     }                 } else {                     assert version == VERSION_START || version == VERSION_STACK_TRACE.                     String msg = input.readString().                     StringBuilder builder = new StringBuilder(shardId.toString()).                     builder.append(" Preexisting corrupted index [").                     builder.append(file).append("] caused by: ").                     builder.append(msg).                     if (version == VERSION_STACK_TRACE) {                         builder.append(System.lineSeparator()).                         builder.append(input.readString()).                     }                     ex.add(new CorruptIndexException(builder.toString(), "preexisting_corruption")).                 }                 CodecUtil.checkFooter(input).             }         }     }     if (ex.isEmpty() == false) {         ExceptionsHelper.rethrowAndSuppress(ex).     } }
true;public;2;32;/**  * This method deletes every file in this store that is not contained in the given source meta data or is a  * legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it  * to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.  *  * @param reason         the reason for this cleanup operation logged for each deleted file  * @param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around.  * @throws IOException           if an IOException occurs  * @throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.  */ ;/**  * This method deletes every file in this store that is not contained in the given source meta data or is a  * legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it  * to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.  *  * @param reason         the reason for this cleanup operation logged for each deleted file  * @param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around.  * @throws IOException           if an IOException occurs  * @throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.  */ public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {     metadataLock.writeLock().lock().     try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {         for (String existingFile : directory.listAll()) {             if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {                 // checksum)                 continue.             }             try {                 directory.deleteFile(reason, existingFile).             // FNF should not happen since we hold a write lock?             } catch (IOException ex) {                 if (existingFile.startsWith(IndexFileNames.SEGMENTS) || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN) || existingFile.startsWith(CORRUPTED)) {                     // point around?                     throw new IllegalStateException("Can't delete " + existingFile + " - cleanup failed", ex).                 }                 logger.debug(() -> new ParameterizedMessage("failed to delete file [{}]", existingFile), ex).             // ignore, we don't really care, will get deleted later on             }         }         directory.syncMetaData().         final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null).         verifyAfterCleanup(sourceMetaData, metadataOrEmpty).     } finally {         metadataLock.writeLock().unlock().     } }
true;final;2;26;// pkg private for testing ;// pkg private for testing final void verifyAfterCleanup(MetadataSnapshot sourceMetaData, MetadataSnapshot targetMetaData) {     final RecoveryDiff recoveryDiff = targetMetaData.recoveryDiff(sourceMetaData).     if (recoveryDiff.identical.size() != recoveryDiff.size()) {         if (recoveryDiff.missing.isEmpty()) {             for (StoreFileMetaData meta : recoveryDiff.different) {                 StoreFileMetaData local = targetMetaData.get(meta.name()).                 StoreFileMetaData remote = sourceMetaData.get(meta.name()).                 // all is fine this file is just part of a commit or a segment that is different                 if (local.isSame(remote) == false) {                     logger.debug("Files are different on the recovery target: {} ", recoveryDiff).                     throw new IllegalStateException("local version: " + local + " is different from remote version after recovery: " + remote, null).                 }             }         } else {             logger.debug("Files are missing on the recovery target: {} ", recoveryDiff).             throw new IllegalStateException("Files are missing on the recovery target: [different=" + recoveryDiff.different + ", missing=" + recoveryDiff.missing + ']', null).         }     } }
true;public;0;3;/**  * Returns the current reference count.  */ ;/**  * Returns the current reference count.  */ public int refCount() {     return refCounter.refCount(). }
true;;0;3;/**  * Estimate the cumulative size of all files in this directory in bytes.  */ ;/**  * Estimate the cumulative size of all files in this directory in bytes.  */ long estimateSize() throws IOException {     return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes(). }
false;public;0;4;;@Override public void close() {     assert false : "Nobody should close this directory except of the Store itself". }
false;public;2;4;;public void deleteFile(String msg, String name) throws IOException {     deletesLogger.trace("{}: delete file {}", msg, name).     super.deleteFile(name). }
false;public;1;4;;@Override public void deleteFile(String name) throws IOException {     deleteFile("StoreDirectory.deleteFile", name). }
false;private;0;3;;private void innerClose() throws IOException {     super.close(). }
false;public;0;4;;@Override public String toString() {     return "store(" + in.toString() + ")". }
false;public;1;13;;@Override public void writeTo(StreamOutput out) throws IOException {     out.writeVInt(this.metadata.size()).     for (StoreFileMetaData meta : this) {         meta.writeTo(out).     }     out.writeVInt(commitUserData.size()).     for (Map.Entry<String, String> entry : commitUserData.entrySet()) {         out.writeString(entry.getKey()).         out.writeString(entry.getValue()).     }     out.writeLong(numDocs). }
true;public;0;3;/**  * Returns the number of documents in this store snapshot  */ ;/**  * Returns the number of documents in this store snapshot  */ public long getNumDocs() {     return numDocs. }
false;static;3;52;;static LoadedMetadata loadMetadata(IndexCommit commit, Directory directory, Logger logger) throws IOException {     long numDocs.     Map<String, StoreFileMetaData> builder = new HashMap<>().     Map<String, String> commitUserDataBuilder = new HashMap<>().     try {         final SegmentInfos segmentCommitInfos = Store.readSegmentsInfo(commit, directory).         numDocs = Lucene.getNumDocs(segmentCommitInfos).         commitUserDataBuilder.putAll(segmentCommitInfos.getUserData()).         // we don't know which version was used to write so we take the max version.         Version maxVersion = segmentCommitInfos.getMinSegmentLuceneVersion().         for (SegmentCommitInfo info : segmentCommitInfos) {             final Version version = info.info.getVersion().             if (version == null) {                 // version is written since 3.1+: we should have already hit IndexFormatTooOld.                 throw new IllegalArgumentException("expected valid version value: " + info.info.toString()).             }             if (version.onOrAfter(maxVersion)) {                 maxVersion = version.             }             for (String file : info.files()) {                 checksumFromLuceneFile(directory, file, builder, logger, version, SEGMENT_INFO_EXTENSION.equals(IndexFileNames.getExtension(file))).             }         }         if (maxVersion == null) {             maxVersion = org.elasticsearch.Version.CURRENT.minimumIndexCompatibilityVersion().luceneVersion.         }         final String segmentsFile = segmentCommitInfos.getSegmentsFileName().         checksumFromLuceneFile(directory, segmentsFile, builder, logger, maxVersion, true).     } catch (CorruptIndexException | IndexNotFoundException | IndexFormatTooOldException | IndexFormatTooNewException ex) {         // we either know the index is corrupted or it's just not there         throw ex.     } catch (Exception ex) {         try {             // Lucene checks the checksum after it tries to lookup the codec etc.             // in that case we might get only IAE or similar exceptions while we are really corrupt...             // TODO we should check the checksum in lucene if we hit an exception             logger.warn(() -> new ParameterizedMessage("failed to build store metadata. checking segment info integrity " + "(with commit [{}])", commit == null ? "no" : "yes"), ex).             Lucene.checkSegmentInfoIntegrity(directory).         } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException cex) {             cex.addSuppressed(ex).             throw cex.         } catch (Exception inner) {             inner.addSuppressed(ex).             throw inner.         }         throw ex.     }     return new LoadedMetadata(unmodifiableMap(builder), unmodifiableMap(commitUserDataBuilder), numDocs). }
false;private,static;6;29;;private static void checksumFromLuceneFile(Directory directory, String file, Map<String, StoreFileMetaData> builder, Logger logger, Version version, boolean readFileAsHash) throws IOException {     final String checksum.     final BytesRefBuilder fileHash = new BytesRefBuilder().     try (IndexInput in = directory.openInput(file, IOContext.READONCE)) {         final long length.         try {             length = in.length().             if (length < CodecUtil.footerLength()) {                 // truncated files trigger IAE if we seek negative... these files are really corrupted though                 throw new CorruptIndexException("Can't retrieve checksum from file: " + file + " file length must be >= " + CodecUtil.footerLength() + " but was: " + in.length(), in).             }             if (readFileAsHash) {                 // additional safety we checksum the entire file we read the hash for...                 final VerifyingIndexInput verifyingIndexInput = new VerifyingIndexInput(in).                 hashFile(fileHash, new InputStreamIndexInput(verifyingIndexInput, length), length).                 checksum = digestToString(verifyingIndexInput.verify()).             } else {                 checksum = digestToString(CodecUtil.retrieveChecksum(in)).             }         } catch (Exception ex) {             logger.debug(() -> new ParameterizedMessage("Can retrieve checksum from file [{}]", file), ex).             throw ex.         }         builder.put(file, new StoreFileMetaData(file, length, checksum, version, fileHash.get())).     } }
true;public,static;3;8;/**  * Computes a strong hash value for small files. Note that this method should only be used for files &lt. 1MB  */ ;/**  * Computes a strong hash value for small files. Note that this method should only be used for files &lt. 1MB  */ public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {     // for safety we limit this to 1MB     final int len = (int) Math.min(1024 * 1024, size).     fileHash.grow(len).     fileHash.setLength(len).     final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len).     assert readBytes == len : Integer.toString(readBytes) + " != " + Integer.toString(len).     assert fileHash.length() == len : Integer.toString(fileHash.length()) + " != " + Integer.toString(len). }
false;public;0;4;;@Override public Iterator<StoreFileMetaData> iterator() {     return metadata.values().iterator(). }
false;public;1;3;;public StoreFileMetaData get(String name) {     return metadata.get(name). }
false;public;0;3;;public Map<String, StoreFileMetaData> asMap() {     return metadata. }
true;public;1;51;/**  * Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the  * recovery target and this snapshot as the source. The returned diff will hold a list of files that are:  * <ul>  * <li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>  * <li>different: they exist in both snapshots but their they are not identical</li>  * <li>missing: files that exist in the source but not in the target</li>  * </ul>  * This method groups file into per-segment files and per-commit files. A file is treated as  * identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated  * as identical iff:  * <ul>  * <li>all files in this segment have the same checksum</li>  * <li>all files in this segment have the same length</li>  * <li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function,  * The metadata transfers the {@code .si} file content as it's hash</li>  * </ul>  * <p>  * The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be  * unique segment identifiers in there hardening this method further.  * <p>  * The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files  * like deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated  * as identical iff:  * <ul>  * <li>all files belonging to this commit have the same checksum</li>  * <li>all files belonging to this commit have the same length</li>  * <li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function,  * The metadata transfers the {@code segments_N} file content as it's hash</li>  * </ul>  * <p>  * NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.  */ ;/**  * Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the  * recovery target and this snapshot as the source. The returned diff will hold a list of files that are:  * <ul>  * <li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>  * <li>different: they exist in both snapshots but their they are not identical</li>  * <li>missing: files that exist in the source but not in the target</li>  * </ul>  * This method groups file into per-segment files and per-commit files. A file is treated as  * identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated  * as identical iff:  * <ul>  * <li>all files in this segment have the same checksum</li>  * <li>all files in this segment have the same length</li>  * <li>the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function,  * The metadata transfers the {@code .si} file content as it's hash</li>  * </ul>  * <p>  * The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be  * unique segment identifiers in there hardening this method further.  * <p>  * The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files  * like deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated  * as identical iff:  * <ul>  * <li>all files belonging to this commit have the same checksum</li>  * <li>all files belonging to this commit have the same length</li>  * <li>the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function,  * The metadata transfers the {@code segments_N} file content as it's hash</li>  * </ul>  * <p>  * NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.  */ public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {     final List<StoreFileMetaData> identical = new ArrayList<>().     final List<StoreFileMetaData> different = new ArrayList<>().     final List<StoreFileMetaData> missing = new ArrayList<>().     final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>().     final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>().     for (StoreFileMetaData meta : this) {         if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) {             // we don't need that file at all             continue.         }         final String segmentId = IndexFileNames.parseSegmentName(meta.name()).         final String extension = IndexFileNames.getExtension(meta.name()).         if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {             // only treat del files as per-commit files fnm files are generational but only for upgradable DV             perCommitStoreFiles.add(meta).         } else {             perSegment.computeIfAbsent(segmentId, k -> new ArrayList<>()).add(meta).         }     }     final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>().     for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {         identicalFiles.clear().         boolean consistent = true.         for (StoreFileMetaData meta : segmentFiles) {             StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name()).             if (storeFileMetaData == null) {                 consistent = false.                 missing.add(meta).             } else if (storeFileMetaData.isSame(meta) == false) {                 consistent = false.                 different.add(meta).             } else {                 identicalFiles.add(meta).             }         }         if (consistent) {             identical.addAll(identicalFiles).         } else {             // make sure all files are added - this can happen if only the deletes are different             different.addAll(identicalFiles).         }     }     RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing)).     assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0) : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]".     return recoveryDiff. }
true;public;0;3;/**  * Returns the number of files in this snapshot  */ ;/**  * Returns the number of files in this snapshot  */ public int size() {     return metadata.size(). }
false;public;0;3;;public Map<String, String> getCommitUserData() {     return commitUserData. }
true;public;0;3;/**  * returns the history uuid the store points at, or null if nonexistent.  */ ;/**  * returns the history uuid the store points at, or null if nonexistent.  */ public String getHistoryUUID() {     return commitUserData.get(Engine.HISTORY_UUID_KEY). }
true;public;1;3;/**  * Returns true iff this metadata contains the given file.  */ ;/**  * Returns true iff this metadata contains the given file.  */ public boolean contains(String existingFile) {     return metadata.containsKey(existingFile). }
true;public;0;9;/**  * Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.  */ ;/**  * Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.  */ public StoreFileMetaData getSegmentsFile() {     for (StoreFileMetaData file : this) {         if (file.name().startsWith(IndexFileNames.SEGMENTS)) {             return file.         }     }     assert metadata.isEmpty().     return null. }
false;private;0;9;;private int numSegmentFiles() {     // only for asserts     int count = 0.     for (StoreFileMetaData file : this) {         if (file.name().startsWith(IndexFileNames.SEGMENTS)) {             count++.         }     }     return count. }
true;public;0;3;/**  * Returns the sync id of the commit point that this MetadataSnapshot represents.  *  * @return sync id if exists, else null  */ ;/**  * Returns the sync id of the commit point that this MetadataSnapshot represents.  *  * @return sync id if exists, else null  */ public String getSyncId() {     return commitUserData.get(Engine.SYNC_COMMIT_ID). }
true;public;0;3;/**  * Returns the sum of the files in this diff.  */ ;/**  * Returns the sum of the files in this diff.  */ public int size() {     return identical.size() + different.size() + missing.size(). }
false;public;0;8;;@Override public String toString() {     return "RecoveryDiff{" + "identical=" + identical + ", different=" + different + ", missing=" + missing + '}'. }
true;public,static;1;3;/**  * Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup.  * This includes write lock and checksum files  */ ;/**  * Returns true if the file is auto-generated by the store and shouldn't be deleted during cleanup.  * This includes write lock and checksum files  */ public static boolean isAutogenerated(String name) {     return IndexWriter.WRITE_LOCK_NAME.equals(name). }
true;public,static;1;3;/**  * Produces a string representation of the given digest value.  */ ;/**  * Produces a string representation of the given digest value.  */ public static String digestToString(long digest) {     return Long.toString(digest, Character.MAX_RADIX). }
false;public;0;14;;@Override public void verify() throws IOException {     String footerDigest = null.     if (metadata.checksum().equals(actualChecksum) && writtenBytes == metadata.length()) {         ByteArrayIndexInput indexInput = new ByteArrayIndexInput("checksum", this.footerChecksum).         footerDigest = digestToString(indexInput.readLong()).         if (metadata.checksum().equals(footerDigest)) {             return.         }     }     throw new CorruptIndexException("verification failed (hardware problem?) : expected=" + metadata.checksum() + " actual=" + actualChecksum + " footer=" + footerDigest + " writtenLength=" + writtenBytes + " expectedLength=" + metadata.length() + " (resource=" + metadata.toString() + ")", "VerifyingIndexOutput(" + metadata.name() + ")"). }
false;public;1;21;;@Override public void writeByte(byte b) throws IOException {     final long writtenBytes = this.writtenBytes++.     if (writtenBytes >= checksumPosition) {         // we are writing parts of the checksum....         if (writtenBytes == checksumPosition) {             readAndCompareChecksum().         }         final int index = Math.toIntExact(writtenBytes - checksumPosition).         if (index < footerChecksum.length) {             footerChecksum[index] = b.             if (index == footerChecksum.length - 1) {                 // we have recorded the entire checksum                 verify().             }         } else {             // fail if we write more than expected             verify().             throw new AssertionError("write past EOF expected length: " + metadata.length() + " writtenBytes: " + writtenBytes).         }     }     out.writeByte(b). }
false;private;0;8;;private void readAndCompareChecksum() throws IOException {     actualChecksum = digestToString(getChecksum()).     if (!metadata.checksum().equals(actualChecksum)) {         throw new CorruptIndexException("checksum failed (hardware problem?) : expected=" + metadata.checksum() + " actual=" + actualChecksum + " (resource=" + metadata.toString() + ")", "VerifyingIndexOutput(" + metadata.name() + ")").     } }
false;public;3;11;;@Override public void writeBytes(byte[] b, int offset, int length) throws IOException {     if (writtenBytes + length > checksumPosition) {         for (int i = 0. i < length. i++) {             // don't optimze writing the last block of bytes             writeByte(b[offset + i]).         }     } else {         out.writeBytes(b, offset, length).         writtenBytes += length.     } }
false;public;0;15;;@Override public byte readByte() throws IOException {     long pos = input.getFilePointer().     final byte b = input.readByte().     pos++.     if (pos > verifiedPosition) {         if (pos <= checksumPosition) {             digest.update(b).         } else {             checksum[(int) (pos - checksumPosition - 1)] = b.         }         verifiedPosition = pos.     }     return b. }
false;public;3;27;;@Override public void readBytes(byte[] b, int offset, int len) throws IOException {     long pos = input.getFilePointer().     input.readBytes(b, offset, len).     if (pos + len > verifiedPosition) {         // Conversion to int is safe here because (verifiedPosition - pos) can be at most len, which is integer         int alreadyVerified = (int) Math.max(0, verifiedPosition - pos).         if (pos < checksumPosition) {             if (pos + len < checksumPosition) {                 digest.update(b, offset + alreadyVerified, len - alreadyVerified).             } else {                 int checksumOffset = (int) (checksumPosition - pos).                 if (checksumOffset - alreadyVerified > 0) {                     digest.update(b, offset + alreadyVerified, checksumOffset - alreadyVerified).                 }                 System.arraycopy(b, offset + checksumOffset, checksum, 0, len - checksumOffset).             }         } else {             // (pos - checksumPosition) cannot be bigger than 8 unless we are reading after the end of file             assert pos - checksumPosition < 8.             System.arraycopy(b, offset, checksum, (int) (pos - checksumPosition), len).         }         verifiedPosition = pos + len.     } }
false;public;0;4;;@Override public long getChecksum() {     return digest.getValue(). }
false;public;1;17;;@Override public void seek(long pos) throws IOException {     if (pos < verifiedPosition) {         // going within verified region - just seek there         input.seek(pos).     } else {         if (verifiedPosition > getFilePointer()) {             // portion of the skip region is verified and portion is not             // skipping the verified portion             input.seek(verifiedPosition).             // and checking unverified             skipBytes(pos - verifiedPosition).         } else {             skipBytes(pos - getFilePointer()).         }     } }
false;public;0;4;;@Override public void close() throws IOException {     input.close(). }
false;public;0;4;;@Override public long getFilePointer() {     return input.getFilePointer(). }
false;public;0;4;;@Override public long length() {     return input.length(). }
false;public;0;4;;@Override public IndexInput clone() {     throw new UnsupportedOperationException(). }
false;public;3;4;;@Override public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {     throw new UnsupportedOperationException(). }
false;public;0;3;;public long getStoredChecksum() {     return new ByteArrayDataInput(checksum).readLong(). }
false;public;0;8;;public long verify() throws CorruptIndexException {     long storedChecksum = getStoredChecksum().     if (getChecksum() == storedChecksum) {         return storedChecksum.     }     throw new CorruptIndexException("verification failed : calculated=" + Store.digestToString(getChecksum()) + " stored=" + Store.digestToString(storedChecksum), this). }
false;public;1;11;;public void deleteQuiet(String... files) {     ensureOpen().     StoreDirectory directory = this.directory.     for (String file : files) {         try {             directory.deleteFile("Store.deleteQuiet", file).         } catch (Exception ex) {         // ignore :(         }     } }
true;public;1;19;/**  * Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception  * message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.  */ ;/**  * Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception  * message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return <code>true</code>.  */ public void markStoreCorrupted(IOException exception) throws IOException {     ensureOpen().     if (!isMarkedCorrupted()) {         String uuid = CORRUPTED + UUIDs.randomBase64UUID().         try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {             CodecUtil.writeHeader(output, CODEC, VERSION).             BytesStreamOutput out = new BytesStreamOutput().             out.writeException(exception).             BytesReference bytes = out.bytes().             output.writeVInt(bytes.length()).             BytesRef ref = bytes.toBytesRef().             output.writeBytes(ref.bytes, ref.offset, ref.length).             CodecUtil.writeFooter(output).         } catch (IOException ex) {             logger.warn("Can't mark store as corrupted", ex).         }         directory().sync(Collections.singleton(uuid)).     } }
true;public;1;3;/**  * This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held.  * This method is only called once after all resources for a store are released.  */ ;/**  * This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held.  * This method is only called once after all resources for a store are released.  */ @Override public void accept(ShardLock Lock) { }
true;public;1;13;/**  * creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.  */ ;/**  * creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.  */ public void createEmpty(Version luceneVersion) throws IOException {     metadataLock.writeLock().lock().     try (IndexWriter writer = newEmptyIndexWriter(directory, luceneVersion)) {         final Map<String, String> map = new HashMap<>().         map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()).         map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED)).         map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED)).         map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, "-1").         updateCommitData(writer, map).     } finally {         metadataLock.writeLock().unlock().     } }
true;public;0;11;/**  * Marks an existing lucene index with a new history uuid.  * This is used to make sure no existing shard will recovery from this index using ops based recovery.  */ ;/**  * Marks an existing lucene index with a new history uuid.  * This is used to make sure no existing shard will recovery from this index using ops based recovery.  */ public void bootstrapNewHistory() throws IOException {     metadataLock.writeLock().lock().     try {         Map<String, String> userData = readLastCommittedSegmentsInfo().getUserData().         final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO)).         final long localCheckpoint = Long.parseLong(userData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY)).         bootstrapNewHistory(localCheckpoint, maxSeqNo).     } finally {         metadataLock.writeLock().unlock().     } }
true;public;2;12;/**  * Marks an existing lucene index with a new history uuid and sets the given local checkpoint  * as well as the maximum sequence number.  * This is used to make sure no existing shard will recover from this index using ops based recovery.  * @see SequenceNumbers#LOCAL_CHECKPOINT_KEY  * @see SequenceNumbers#MAX_SEQ_NO  */ ;/**  * Marks an existing lucene index with a new history uuid and sets the given local checkpoint  * as well as the maximum sequence number.  * This is used to make sure no existing shard will recover from this index using ops based recovery.  * @see SequenceNumbers#LOCAL_CHECKPOINT_KEY  * @see SequenceNumbers#MAX_SEQ_NO  */ public void bootstrapNewHistory(long localCheckpoint, long maxSeqNo) throws IOException {     metadataLock.writeLock().lock().     try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {         final Map<String, String> map = new HashMap<>().         map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()).         map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(localCheckpoint)).         map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo)).         updateCommitData(writer, map).     } finally {         metadataLock.writeLock().unlock().     } }
true;public;1;14;/**  * Force bakes the given translog generation as recovery information in the lucene index. This is  * used when recovering from a snapshot or peer file based recovery where a new empty translog is  * created and the existing lucene index needs should be changed to use it.  */ ;/**  * Force bakes the given translog generation as recovery information in the lucene index. This is  * used when recovering from a snapshot or peer file based recovery where a new empty translog is  * created and the existing lucene index needs should be changed to use it.  */ public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {     metadataLock.writeLock().lock().     try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {         if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {             throw new IllegalArgumentException("a new translog uuid can't be equal to existing one. got [" + translogUUID + "]").         }         final Map<String, String> map = new HashMap<>().         map.put(Translog.TRANSLOG_GENERATION_KEY, "1").         map.put(Translog.TRANSLOG_UUID_KEY, translogUUID).         updateCommitData(writer, map).     } finally {         metadataLock.writeLock().unlock().     } }
true;public;0;11;/**  * Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.  */ ;/**  * Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.  */ public void ensureIndexHasHistoryUUID() throws IOException {     metadataLock.writeLock().lock().     try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {         final Map<String, String> userData = getUserData(writer).         if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {             updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID())).         }     } finally {         metadataLock.writeLock().unlock().     } }
true;public;3;53;/**  * Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe  * at the recovering time but they can suddenly become safe in the future.  * The following issues can happen if unsafe commits are kept oninit.  * <p>  * 1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)  * and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)  * is added without flushing, the global checkpoint is advanced to 2. and the replica recovers again, it will use  * the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the  * commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica.  * <p>  * 2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit  * c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2).  * The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new  * commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery  * translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1  * while the local checkpoint of c2 is 2.  * <p>  * 3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced  * (v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,  * the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.  */ ;/**  * Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe  * at the recovering time but they can suddenly become safe in the future.  * The following issues can happen if unsafe commits are kept oninit.  * <p>  * 1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)  * and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)  * is added without flushing, the global checkpoint is advanced to 2. and the replica recovers again, it will use  * the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the  * commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica.  * <p>  * 2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit  * c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2).  * The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new  * commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery  * translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1  * while the local checkpoint of c2 is 2.  * <p>  * 3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced  * (v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,  * the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.  */ public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen, final org.elasticsearch.Version indexVersionCreated) throws IOException {     metadataLock.writeLock().lock().     try {         final List<IndexCommit> existingCommits = DirectoryReader.listCommits(directory).         if (existingCommits.isEmpty()) {             throw new IllegalArgumentException("No index found to trim").         }         final IndexCommit lastIndexCommitCommit = existingCommits.get(existingCommits.size() - 1).         final String translogUUID = lastIndexCommitCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY).         final IndexCommit startingIndexCommit.         // To avoid this issue, we only select index commits whose translog are fully retained.         if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {             final List<IndexCommit> recoverableCommits = new ArrayList<>().             for (IndexCommit commit : existingCommits) {                 if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {                     recoverableCommits.add(commit).                 }             }             assert recoverableCommits.isEmpty() == false : "No commit point with translog found. " + "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]".             startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint).         } else {             // TODO: Asserts the starting commit is a safe commit once peer-recovery sets global checkpoint.             startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint).         }         if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {             throw new IllegalStateException("starting commit translog uuid [" + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + "] is not equal to last commit's translog uuid [" + translogUUID + "]").         }         if (startingIndexCommit.equals(lastIndexCommitCommit) == false) {             try (IndexWriter writer = newAppendingIndexWriter(directory, startingIndexCommit)) {                 // this achieves two things:                 // - by committing a new commit based on the starting commit, it make sure the starting commit will be opened                 // - deletes any other commit (by lucene standard deletion policy)                 //                  // note that we can't just use IndexCommit.delete() as we really want to make sure that those files won't be used                 // even if a virus scanner causes the files not to be used.                 // The new commit will use segment files from the starting commit but userData from the last commit by default.                 // Thus, we need to manually set the userData from the starting commit to the new commit.                 writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet()).                 writer.commit().             }         }     } finally {         metadataLock.writeLock().unlock().     } }
false;private,static;2;6;;private static void updateCommitData(IndexWriter writer, Map<String, String> keysToUpdate) throws IOException {     final Map<String, String> userData = getUserData(writer).     userData.putAll(keysToUpdate).     writer.setLiveCommitData(userData.entrySet()).     writer.commit(). }
false;private,static;1;5;;private static Map<String, String> getUserData(IndexWriter writer) {     final Map<String, String> userData = new HashMap<>().     writer.getLiveCommitData().forEach(e -> userData.put(e.getKey(), e.getValue())).     return userData. }
false;private,static;2;6;;private static IndexWriter newAppendingIndexWriter(final Directory dir, final IndexCommit commit) throws IOException {     IndexWriterConfig iwc = newIndexWriterConfig().setIndexCommit(commit).setOpenMode(IndexWriterConfig.OpenMode.APPEND).     return new IndexWriter(dir, iwc). }
false;private,static;2;6;;private static IndexWriter newEmptyIndexWriter(final Directory dir, final Version luceneVersion) throws IOException {     IndexWriterConfig iwc = newIndexWriterConfig().setOpenMode(IndexWriterConfig.OpenMode.CREATE).setIndexCreatedVersionMajor(luceneVersion.major).     return new IndexWriter(dir, iwc). }
false;private,static;0;9;;private static IndexWriterConfig newIndexWriterConfig() {     return new IndexWriterConfig(null).setSoftDeletesField(Lucene.SOFT_DELETES_FIELD).setCommitOnClose(false).setMergePolicy(NoMergePolicy.INSTANCE). }
