commented;modifiers;parameterAmount;loc;comment;code
false;private,static;2;13;;private static Map<String, Processor.Factory> processorFactories(List<IngestPlugin> ingestPlugins, Processor.Parameters parameters) {     Map<String, Processor.Factory> processorFactories = new HashMap<>().     for (IngestPlugin ingestPlugin : ingestPlugins) {         Map<String, Processor.Factory> newProcessors = ingestPlugin.getProcessors(parameters).         for (Map.Entry<String, Processor.Factory> entry : newProcessors.entrySet()) {             if (processorFactories.put(entry.getKey(), entry.getValue()) != null) {                 throw new IllegalArgumentException("Ingest processor [" + entry.getKey() + "] is already registered").             }         }     }     return Collections.unmodifiableMap(processorFactories). }
false;public;0;3;;public ClusterService getClusterService() {     return clusterService. }
false;public;0;3;;public ScriptService getScriptService() {     return scriptService. }
false;protected;1;4;;@Override protected AcknowledgedResponse newResponse(boolean acknowledged) {     return new AcknowledgedResponse(acknowledged). }
false;public;1;4;;@Override public ClusterState execute(ClusterState currentState) {     return innerDelete(request, currentState). }
true;public;2;15;/**  * Deletes the pipeline specified by id in the request.  */ ;/**  * Deletes the pipeline specified by id in the request.  */ public void delete(DeletePipelineRequest request, ActionListener<AcknowledgedResponse> listener) {     clusterService.submitStateUpdateTask("delete-pipeline-" + request.getId(), new AckedClusterStateUpdateTask<AcknowledgedResponse>(request, listener) {          @Override         protected AcknowledgedResponse newResponse(boolean acknowledged) {             return new AcknowledgedResponse(acknowledged).         }          @Override         public ClusterState execute(ClusterState currentState) {             return innerDelete(request, currentState).         }     }). }
false;static;2;27;;static ClusterState innerDelete(DeletePipelineRequest request, ClusterState currentState) {     IngestMetadata currentIngestMetadata = currentState.metaData().custom(IngestMetadata.TYPE).     if (currentIngestMetadata == null) {         return currentState.     }     Map<String, PipelineConfiguration> pipelines = currentIngestMetadata.getPipelines().     Set<String> toRemove = new HashSet<>().     for (String pipelineKey : pipelines.keySet()) {         if (Regex.simpleMatch(request.getId(), pipelineKey)) {             toRemove.add(pipelineKey).         }     }     if (toRemove.isEmpty() && Regex.isMatchAllPattern(request.getId()) == false) {         throw new ResourceNotFoundException("pipeline [{}] is missing", request.getId()).     } else if (toRemove.isEmpty()) {         return currentState.     }     final Map<String, PipelineConfiguration> pipelinesCopy = new HashMap<>(pipelines).     for (String key : toRemove) {         pipelinesCopy.remove(key).     }     ClusterState.Builder newState = ClusterState.builder(currentState).     newState.metaData(MetaData.builder(currentState.getMetaData()).putCustom(IngestMetadata.TYPE, new IngestMetadata(pipelinesCopy)).build()).     return newState.build(). }
true;public,static;2;4;// know how to serialize themselves. ;/**  * @return pipeline configuration specified by id. If multiple ids or wildcards are specified multiple pipelines  * may be returned  */ // Returning PipelineConfiguration instead of Pipeline, because Pipeline and Processor interface don't // know how to serialize themselves. public static List<PipelineConfiguration> getPipelines(ClusterState clusterState, String... ids) {     IngestMetadata ingestMetadata = clusterState.getMetaData().custom(IngestMetadata.TYPE).     return innerGetPipelines(ingestMetadata, ids). }
false;static;2;27;;static List<PipelineConfiguration> innerGetPipelines(IngestMetadata ingestMetadata, String... ids) {     if (ingestMetadata == null) {         return Collections.emptyList().     }     // if we didn't ask for _any_ ID, then we get them all (this is the same as if they ask for '*')     if (ids.length == 0) {         return new ArrayList<>(ingestMetadata.getPipelines().values()).     }     List<PipelineConfiguration> result = new ArrayList<>(ids.length).     for (String id : ids) {         if (Regex.isSimpleMatchPattern(id)) {             for (Map.Entry<String, PipelineConfiguration> entry : ingestMetadata.getPipelines().entrySet()) {                 if (Regex.simpleMatch(id, entry.getKey())) {                     result.add(entry.getValue()).                 }             }         } else {             PipelineConfiguration pipeline = ingestMetadata.getPipelines().get(id).             if (pipeline != null) {                 result.add(pipeline).             }         }     }     return result. }
false;protected;1;4;;@Override protected AcknowledgedResponse newResponse(boolean acknowledged) {     return new AcknowledgedResponse(acknowledged). }
false;public;1;4;;@Override public ClusterState execute(ClusterState currentState) {     return innerPut(request, currentState). }
true;public;3;18;/**  * Stores the specified pipeline definition in the request.  */ ;/**  * Stores the specified pipeline definition in the request.  */ public void putPipeline(Map<DiscoveryNode, IngestInfo> ingestInfos, PutPipelineRequest request, ActionListener<AcknowledgedResponse> listener) throws Exception {     // validates the pipeline and processor configuration before submitting a cluster update task:     validatePipeline(ingestInfos, request).     clusterService.submitStateUpdateTask("put-pipeline-" + request.getId(), new AckedClusterStateUpdateTask<AcknowledgedResponse>(request, listener) {          @Override         protected AcknowledgedResponse newResponse(boolean acknowledged) {             return new AcknowledgedResponse(acknowledged).         }          @Override         public ClusterState execute(ClusterState currentState) {             return innerPut(request, currentState).         }     }). }
true;public;1;3;/**  * Returns the pipeline by the specified id  */ ;/**  * Returns the pipeline by the specified id  */ public Pipeline getPipeline(String id) {     return pipelines.get(id). }
false;public;0;3;;public Map<String, Processor.Factory> getProcessorFactories() {     return processorFactories. }
false;public;0;8;;public IngestInfo info() {     Map<String, Processor.Factory> processorFactories = getProcessorFactories().     List<ProcessorInfo> processorInfoList = new ArrayList<>(processorFactories.size()).     for (Map.Entry<String, Processor.Factory> entry : processorFactories.entrySet()) {         processorInfoList.add(new ProcessorInfo(entry.getKey())).     }     return new IngestInfo(processorInfoList). }
false;;0;3;;Map<String, Pipeline> pipelines() {     return pipelines. }
false;public;1;41;;@Override public void applyClusterState(final ClusterChangedEvent event) {     ClusterState state = event.state().     Map<String, Pipeline> originalPipelines = pipelines.     try {         innerUpdatePipelines(event.previousState(), state).     } catch (ElasticsearchParseException e) {         logger.warn("failed to update ingest pipelines", e).     }     // pipelines changed, so add the old metrics to the new metrics     if (originalPipelines != pipelines) {         pipelines.forEach((id, pipeline) -> {             Pipeline originalPipeline = originalPipelines.get(id).             if (originalPipeline != null) {                 pipeline.getMetrics().add(originalPipeline.getMetrics()).                 List<Tuple<Processor, IngestMetric>> oldPerProcessMetrics = new ArrayList<>().                 List<Tuple<Processor, IngestMetric>> newPerProcessMetrics = new ArrayList<>().                 getProcessorMetrics(originalPipeline.getCompoundProcessor(), oldPerProcessMetrics).                 getProcessorMetrics(pipeline.getCompoundProcessor(), newPerProcessMetrics).                 // consistent id's per processor and/or semantic equals for each processor will be needed.                 if (newPerProcessMetrics.size() == oldPerProcessMetrics.size()) {                     Iterator<Tuple<Processor, IngestMetric>> oldMetricsIterator = oldPerProcessMetrics.iterator().                     for (Tuple<Processor, IngestMetric> compositeMetric : newPerProcessMetrics) {                         String type = compositeMetric.v1().getType().                         IngestMetric metric = compositeMetric.v2().                         if (oldMetricsIterator.hasNext()) {                             Tuple<Processor, IngestMetric> oldCompositeMetric = oldMetricsIterator.next().                             String oldType = oldCompositeMetric.v1().getType().                             IngestMetric oldMetric = oldCompositeMetric.v2().                             if (type.equals(oldType)) {                                 metric.add(oldMetric).                             }                         }                     }                 }             }         }).     } }
true;private,static;2;18;/**  * Recursive method to obtain all of the non-failure processors for given compoundProcessor. Since conditionals are implemented as  * wrappers to the actual processor, always prefer the actual processor's metric over the conditional processor's metric.  * @param compoundProcessor The compound processor to start walking the non-failure processors  * @param processorMetrics The list of {@link Processor} {@link IngestMetric} tuples.  * @return the processorMetrics for all non-failure processor that belong to the original compoundProcessor  */ ;/**  * Recursive method to obtain all of the non-failure processors for given compoundProcessor. Since conditionals are implemented as  * wrappers to the actual processor, always prefer the actual processor's metric over the conditional processor's metric.  * @param compoundProcessor The compound processor to start walking the non-failure processors  * @param processorMetrics The list of {@link Processor} {@link IngestMetric} tuples.  * @return the processorMetrics for all non-failure processor that belong to the original compoundProcessor  */ private static List<Tuple<Processor, IngestMetric>> getProcessorMetrics(CompoundProcessor compoundProcessor, List<Tuple<Processor, IngestMetric>> processorMetrics) {     // only surface the top level non-failure processors, on-failure processor times will be included in the top level non-failure     for (Tuple<Processor, IngestMetric> processorWithMetric : compoundProcessor.getProcessorsWithMetrics()) {         Processor processor = processorWithMetric.v1().         IngestMetric metric = processorWithMetric.v2().         if (processor instanceof CompoundProcessor) {             getProcessorMetrics((CompoundProcessor) processor, processorMetrics).         } else {             // Prefer the conditional's metric since it only includes metrics when the conditional evaluated to true.             if (processor instanceof ConditionalProcessor) {                 metric = ((ConditionalProcessor) processor).getMetric().             }             processorMetrics.add(new Tuple<>(processor, metric)).         }     }     return processorMetrics. }
false;public;1;4;;@Override public IngestDocument execute(IngestDocument ingestDocument) {     throw new IllegalStateException(errorMessage). }
false;public;0;4;;@Override public String getType() {     return type. }
false;private,static;2;18;;private static Pipeline substitutePipeline(String id, ElasticsearchParseException e) {     String tag = e.getHeaderKeys().contains("processor_tag") ? e.getHeader("processor_tag").get(0) : null.     String type = e.getHeaderKeys().contains("processor_type") ? e.getHeader("processor_type").get(0) : "unknown".     String errorMessage = "pipeline with id [" + id + "] could not be loaded, caused by [" + e.getDetailedMessage() + "]".     Processor failureProcessor = new AbstractProcessor(tag) {          @Override         public IngestDocument execute(IngestDocument ingestDocument) {             throw new IllegalStateException(errorMessage).         }          @Override         public String getType() {             return type.         }     }.     String description = "this is a place holder pipeline, because pipeline with id [" + id + "] could not be loaded".     return new Pipeline(id, description, null, new CompoundProcessor(failureProcessor)). }
false;static;2;16;;static ClusterState innerPut(PutPipelineRequest request, ClusterState currentState) {     IngestMetadata currentIngestMetadata = currentState.metaData().custom(IngestMetadata.TYPE).     Map<String, PipelineConfiguration> pipelines.     if (currentIngestMetadata != null) {         pipelines = new HashMap<>(currentIngestMetadata.getPipelines()).     } else {         pipelines = new HashMap<>().     }     pipelines.put(request.getId(), new PipelineConfiguration(request.getId(), request.getSource(), request.getXContentType())).     ClusterState.Builder newState = ClusterState.builder(currentState).     newState.metaData(MetaData.builder(currentState.getMetaData()).putCustom(IngestMetadata.TYPE, new IngestMetadata(pipelines)).build()).     return newState.build(). }
false;;2;21;;void validatePipeline(Map<DiscoveryNode, IngestInfo> ingestInfos, PutPipelineRequest request) throws Exception {     if (ingestInfos.isEmpty()) {         throw new IllegalStateException("Ingest info is empty").     }     Map<String, Object> pipelineConfig = XContentHelper.convertToMap(request.getSource(), false, request.getXContentType()).v2().     Pipeline pipeline = Pipeline.create(request.getId(), pipelineConfig, processorFactories, scriptService).     List<Exception> exceptions = new ArrayList<>().     for (Processor processor : pipeline.flattenAllProcessors()) {         for (Map.Entry<DiscoveryNode, IngestInfo> entry : ingestInfos.entrySet()) {             String type = processor.getType().             if (entry.getValue().containsProcessor(type) == false && ConditionalProcessor.TYPE.equals(type) == false) {                 String message = "Processor type [" + processor.getType() + "] is not installed on node [" + entry.getKey() + "]".                 exceptions.add(ConfigurationUtils.newConfigurationException(processor.getType(), processor.getTag(), null, message)).             }         }     }     ExceptionsHelper.rethrowAndSuppress(exceptions). }
false;public;1;4;;@Override public void onFailure(Exception e) {     completionHandler.accept(e). }
false;protected;0;25;;@Override protected void doRun() {     for (DocWriteRequest<?> actionRequest : actionRequests) {         IndexRequest indexRequest = TransportBulkAction.getIndexWriteRequest(actionRequest).         if (indexRequest == null) {             continue.         }         String pipelineId = indexRequest.getPipeline().         if (NOOP_PIPELINE_NAME.equals(pipelineId) == false) {             try {                 Pipeline pipeline = pipelines.get(pipelineId).                 if (pipeline == null) {                     throw new IllegalArgumentException("pipeline with id [" + pipelineId + "] does not exist").                 }                 innerExecute(indexRequest, pipeline, itemDroppedHandler).                 // this shouldn't be needed here but we do it for consistency with index api                 // which requires it to prevent double execution                 indexRequest.setPipeline(NOOP_PIPELINE_NAME).             } catch (Exception e) {                 itemFailureHandler.accept(indexRequest, e).             }         }     }     completionHandler.accept(null). }
false;public;4;38;;public void executeBulkRequest(Iterable<DocWriteRequest<?>> actionRequests, BiConsumer<IndexRequest, Exception> itemFailureHandler, Consumer<Exception> completionHandler, Consumer<IndexRequest> itemDroppedHandler) {     threadPool.executor(ThreadPool.Names.WRITE).execute(new AbstractRunnable() {          @Override         public void onFailure(Exception e) {             completionHandler.accept(e).         }          @Override         protected void doRun() {             for (DocWriteRequest<?> actionRequest : actionRequests) {                 IndexRequest indexRequest = TransportBulkAction.getIndexWriteRequest(actionRequest).                 if (indexRequest == null) {                     continue.                 }                 String pipelineId = indexRequest.getPipeline().                 if (NOOP_PIPELINE_NAME.equals(pipelineId) == false) {                     try {                         Pipeline pipeline = pipelines.get(pipelineId).                         if (pipeline == null) {                             throw new IllegalArgumentException("pipeline with id [" + pipelineId + "] does not exist").                         }                         innerExecute(indexRequest, pipeline, itemDroppedHandler).                         // this shouldn't be needed here but we do it for consistency with index api                         // which requires it to prevent double execution                         indexRequest.setPipeline(NOOP_PIPELINE_NAME).                     } catch (Exception e) {                         itemFailureHandler.accept(indexRequest, e).                     }                 }             }             completionHandler.accept(null).         }     }). }
false;public;0;16;;public IngestStats stats() {     IngestStats.Builder statsBuilder = new IngestStats.Builder().     statsBuilder.addTotalMetrics(totalMetrics).     pipelines.forEach((id, pipeline) -> {         CompoundProcessor rootProcessor = pipeline.getCompoundProcessor().         statsBuilder.addPipelineMetrics(id, pipeline.getMetrics()).         List<Tuple<Processor, IngestMetric>> processorMetrics = new ArrayList<>().         getProcessorMetrics(rootProcessor, processorMetrics).         processorMetrics.forEach(t -> {             Processor processor = t.v1().             IngestMetric processorMetric = t.v2().             statsBuilder.addProcessorMetrics(id, getProcessorName(processor), processorMetric).         }).     }).     return statsBuilder.build(). }
true;static;1;20;// package private for testing ;// package private for testing static String getProcessorName(Processor processor) {     // conditionals are implemented as wrappers around the real processor, so get the real processor for the correct type for the name     if (processor instanceof ConditionalProcessor) {         processor = ((ConditionalProcessor) processor).getProcessor().     }     StringBuilder sb = new StringBuilder(5).     sb.append(processor.getType()).     if (processor instanceof PipelineProcessor) {         String pipelineName = ((PipelineProcessor) processor).getPipelineName().         sb.append(":").         sb.append(pipelineName).     }     String tag = processor.getTag().     if (tag != null && !tag.isEmpty()) {         sb.append(":").         sb.append(tag).     }     return sb.toString(). }
false;private;3;42;;private void innerExecute(IndexRequest indexRequest, Pipeline pipeline, Consumer<IndexRequest> itemDroppedHandler) throws Exception {     if (pipeline.getProcessors().isEmpty()) {         return.     }     long startTimeInNanos = System.nanoTime().     // (e.g. the pipeline may have been removed while we're ingesting a document     try {         totalMetrics.preIngest().         String index = indexRequest.index().         String type = indexRequest.type().         String id = indexRequest.id().         String routing = indexRequest.routing().         Long version = indexRequest.version().         VersionType versionType = indexRequest.versionType().         Map<String, Object> sourceAsMap = indexRequest.sourceAsMap().         IngestDocument ingestDocument = new IngestDocument(index, type, id, routing, version, versionType, sourceAsMap).         if (pipeline.execute(ingestDocument) == null) {             itemDroppedHandler.accept(indexRequest).         } else {             Map<IngestDocument.MetaData, Object> metadataMap = ingestDocument.extractMetadata().             // it's fine to set all metadata fields all the time, as ingest document holds their starting values             // before ingestion, which might also get modified during ingestion.             indexRequest.index((String) metadataMap.get(IngestDocument.MetaData.INDEX)).             indexRequest.type((String) metadataMap.get(IngestDocument.MetaData.TYPE)).             indexRequest.id((String) metadataMap.get(IngestDocument.MetaData.ID)).             indexRequest.routing((String) metadataMap.get(IngestDocument.MetaData.ROUTING)).             indexRequest.version(((Number) metadataMap.get(IngestDocument.MetaData.VERSION)).longValue()).             if (metadataMap.get(IngestDocument.MetaData.VERSION_TYPE) != null) {                 indexRequest.versionType(VersionType.fromString((String) metadataMap.get(IngestDocument.MetaData.VERSION_TYPE))).             }             indexRequest.source(ingestDocument.getSourceAndMetadata()).         }     } catch (Exception e) {         totalMetrics.ingestFailed().         throw e.     } finally {         long ingestTimeInMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTimeInNanos).         totalMetrics.postIngest(ingestTimeInMillis).     } }
false;private;2;32;;private void innerUpdatePipelines(ClusterState previousState, ClusterState state) {     if (state.blocks().hasGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK)) {         return.     }     IngestMetadata ingestMetadata = state.getMetaData().custom(IngestMetadata.TYPE).     IngestMetadata previousIngestMetadata = previousState.getMetaData().custom(IngestMetadata.TYPE).     if (Objects.equals(ingestMetadata, previousIngestMetadata)) {         return.     }     Map<String, Pipeline> pipelines = new HashMap<>().     List<ElasticsearchParseException> exceptions = new ArrayList<>().     for (PipelineConfiguration pipeline : ingestMetadata.getPipelines().values()) {         try {             pipelines.put(pipeline.getId(), Pipeline.create(pipeline.getId(), pipeline.getConfigAsMap(), processorFactories, scriptService)).         } catch (ElasticsearchParseException e) {             pipelines.put(pipeline.getId(), substitutePipeline(pipeline.getId(), e)).             exceptions.add(e).         } catch (Exception e) {             ElasticsearchParseException parseException = new ElasticsearchParseException("Error updating pipeline with id [" + pipeline.getId() + "]", e).             pipelines.put(pipeline.getId(), substitutePipeline(pipeline.getId(), parseException)).             exceptions.add(parseException).         }     }     this.pipelines = Collections.unmodifiableMap(pipelines).     ExceptionsHelper.rethrowAndSuppress(exceptions). }
