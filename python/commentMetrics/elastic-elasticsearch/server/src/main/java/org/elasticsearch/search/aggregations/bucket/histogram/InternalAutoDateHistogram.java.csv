# id;timestamp;commentText;codeText;commentWords;codeWords
InternalAutoDateHistogram -> public InternalAutoDateHistogram(StreamInput in) throws IOException;1531729807;Stream from a stream.;public InternalAutoDateHistogram(StreamInput in) throws IOException {_        super(in)__        bucketInfo = new BucketInfo(in)__        format = in.readNamedWriteable(DocValueFormat.class)__        buckets = in.readList(stream -> new Bucket(stream, format))__        this.targetBuckets = in.readVInt()__    };stream,from,a,stream;public,internal,auto,date,histogram,stream,input,in,throws,ioexception,super,in,bucket,info,new,bucket,info,in,format,in,read,named,writeable,doc,value,format,class,buckets,in,read,list,stream,new,bucket,stream,format,this,target,buckets,in,read,vint
InternalAutoDateHistogram -> public InternalAutoDateHistogram(StreamInput in) throws IOException;1533063033;Stream from a stream.;public InternalAutoDateHistogram(StreamInput in) throws IOException {_        super(in)__        bucketInfo = new BucketInfo(in)__        format = in.readNamedWriteable(DocValueFormat.class)__        buckets = in.readList(stream -> new Bucket(stream, format))__        this.targetBuckets = in.readVInt()__    };stream,from,a,stream;public,internal,auto,date,histogram,stream,input,in,throws,ioexception,super,in,bucket,info,new,bucket,info,in,format,in,read,named,writeable,doc,value,format,class,buckets,in,read,list,stream,new,bucket,stream,format,this,target,buckets,in,read,vint
InternalAutoDateHistogram -> public InternalAutoDateHistogram(StreamInput in) throws IOException;1534539448;Stream from a stream.;public InternalAutoDateHistogram(StreamInput in) throws IOException {_        super(in)__        bucketInfo = new BucketInfo(in)__        format = in.readNamedWriteable(DocValueFormat.class)__        buckets = in.readList(stream -> new Bucket(stream, format))__        this.targetBuckets = in.readVInt()__    };stream,from,a,stream;public,internal,auto,date,histogram,stream,input,in,throws,ioexception,super,in,bucket,info,new,bucket,info,in,format,in,read,named,writeable,doc,value,format,class,buckets,in,read,list,stream,new,bucket,stream,format,this,target,buckets,in,read,vint
InternalAutoDateHistogram -> public InternalAutoDateHistogram(StreamInput in) throws IOException;1536177418;Stream from a stream.;public InternalAutoDateHistogram(StreamInput in) throws IOException {_        super(in)__        bucketInfo = new BucketInfo(in)__        format = in.readNamedWriteable(DocValueFormat.class)__        buckets = in.readList(stream -> new Bucket(stream, format))__        this.targetBuckets = in.readVInt()__    };stream,from,a,stream;public,internal,auto,date,histogram,stream,input,in,throws,ioexception,super,in,bucket,info,new,bucket,info,in,format,in,read,named,writeable,doc,value,format,class,buckets,in,read,list,stream,new,bucket,stream,format,this,target,buckets,in,read,vint
InternalAutoDateHistogram -> public InternalAutoDateHistogram(StreamInput in) throws IOException;1548236405;Stream from a stream.;public InternalAutoDateHistogram(StreamInput in) throws IOException {_        super(in)__        bucketInfo = new BucketInfo(in)__        format = in.readNamedWriteable(DocValueFormat.class)__        buckets = in.readList(stream -> new Bucket(stream, format))__        this.targetBuckets = in.readVInt()__    };stream,from,a,stream;public,internal,auto,date,histogram,stream,input,in,throws,ioexception,super,in,bucket,info,new,bucket,info,in,format,in,read,named,writeable,doc,value,format,class,buckets,in,read,list,stream,new,bucket,stream,format,this,target,buckets,in,read,vint
InternalAutoDateHistogram -> private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext);1531729807;This method works almost exactly the same as_InternalDateHistogram#reduceBuckets(List, ReduceContext), the different_here is that we need to round all the keys we see using the highest level_rounding returned across all the shards so the resolution of the buckets_is the same and they can be reduced together.;private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext) {__        _        _        int reduceRoundingIdx = 0__        for (InternalAggregation aggregation : aggregations) {_            int aggRoundingIdx = ((InternalAutoDateHistogram) aggregation).bucketInfo.roundingIdx__            if (aggRoundingIdx > reduceRoundingIdx) {_                reduceRoundingIdx = aggRoundingIdx__            }_        }_        _        RoundingInfo reduceRoundingInfo = bucketInfo.roundingInfos[reduceRoundingIdx]__        Rounding reduceRounding = reduceRoundingInfo.rounding___        final PriorityQueue<IteratorAndCurrent> pq = new PriorityQueue<IteratorAndCurrent>(aggregations.size()) {_            @Override_            protected boolean lessThan(IteratorAndCurrent a, IteratorAndCurrent b) {_                return a.current.key < b.current.key__            }_        }__        for (InternalAggregation aggregation : aggregations) {_            InternalAutoDateHistogram histogram = (InternalAutoDateHistogram) aggregation__            if (histogram.buckets.isEmpty() == false) {_                pq.add(new IteratorAndCurrent(histogram.buckets.iterator()))__            }_        }__        List<Bucket> reducedBuckets = new ArrayList<>()__        if (pq.size() > 0) {_            _            List<Bucket> currentBuckets = new ArrayList<>()__            double key = reduceRounding.round(pq.top().current.key)___            do {_                final IteratorAndCurrent top = pq.top()___                if (reduceRounding.round(top.current.key) != key) {_                    _                    final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                    reduceContext.consumeBucketsAndMaybeBreak(1)__                    reducedBuckets.add(reduced)__                    currentBuckets.clear()__                    key = reduceRounding.round(top.current.key)__                }__                currentBuckets.add(top.current)___                if (top.iterator.hasNext()) {_                    final Bucket next = top.iterator.next()__                    assert next.key > top.current.key : "shards must return data sorted by key"__                    top.current = next__                    pq.updateTop()__                } else {_                    pq.pop()__                }_            } while (pq.size() > 0)___            if (currentBuckets.isEmpty() == false) {_                final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                reduceContext.consumeBucketsAndMaybeBreak(1)__                reducedBuckets.add(reduced)__            }_        }__        return mergeBucketsIfNeeded(reducedBuckets, reduceRoundingIdx, reduceRoundingInfo, reduceContext)__    };this,method,works,almost,exactly,the,same,as,internal,date,histogram,reduce,buckets,list,reduce,context,the,different,here,is,that,we,need,to,round,all,the,keys,we,see,using,the,highest,level,rounding,returned,across,all,the,shards,so,the,resolution,of,the,buckets,is,the,same,and,they,can,be,reduced,together;private,bucket,reduce,result,reduce,buckets,list,internal,aggregation,aggregations,reduce,context,reduce,context,int,reduce,rounding,idx,0,for,internal,aggregation,aggregation,aggregations,int,agg,rounding,idx,internal,auto,date,histogram,aggregation,bucket,info,rounding,idx,if,agg,rounding,idx,reduce,rounding,idx,reduce,rounding,idx,agg,rounding,idx,rounding,info,reduce,rounding,info,bucket,info,rounding,infos,reduce,rounding,idx,rounding,reduce,rounding,reduce,rounding,info,rounding,final,priority,queue,iterator,and,current,pq,new,priority,queue,iterator,and,current,aggregations,size,override,protected,boolean,less,than,iterator,and,current,a,iterator,and,current,b,return,a,current,key,b,current,key,for,internal,aggregation,aggregation,aggregations,internal,auto,date,histogram,histogram,internal,auto,date,histogram,aggregation,if,histogram,buckets,is,empty,false,pq,add,new,iterator,and,current,histogram,buckets,iterator,list,bucket,reduced,buckets,new,array,list,if,pq,size,0,list,bucket,current,buckets,new,array,list,double,key,reduce,rounding,round,pq,top,current,key,do,final,iterator,and,current,top,pq,top,if,reduce,rounding,round,top,current,key,key,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,current,buckets,clear,key,reduce,rounding,round,top,current,key,current,buckets,add,top,current,if,top,iterator,has,next,final,bucket,next,top,iterator,next,assert,next,key,top,current,key,shards,must,return,data,sorted,by,key,top,current,next,pq,update,top,else,pq,pop,while,pq,size,0,if,current,buckets,is,empty,false,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,return,merge,buckets,if,needed,reduced,buckets,reduce,rounding,idx,reduce,rounding,info,reduce,context
InternalAutoDateHistogram -> private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext);1533063033;This method works almost exactly the same as_InternalDateHistogram#reduceBuckets(List, ReduceContext), the different_here is that we need to round all the keys we see using the highest level_rounding returned across all the shards so the resolution of the buckets_is the same and they can be reduced together.;private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext) {__        _        _        int reduceRoundingIdx = 0__        for (InternalAggregation aggregation : aggregations) {_            int aggRoundingIdx = ((InternalAutoDateHistogram) aggregation).bucketInfo.roundingIdx__            if (aggRoundingIdx > reduceRoundingIdx) {_                reduceRoundingIdx = aggRoundingIdx__            }_        }_        _        RoundingInfo reduceRoundingInfo = bucketInfo.roundingInfos[reduceRoundingIdx]__        Rounding reduceRounding = reduceRoundingInfo.rounding___        final PriorityQueue<IteratorAndCurrent> pq = new PriorityQueue<IteratorAndCurrent>(aggregations.size()) {_            @Override_            protected boolean lessThan(IteratorAndCurrent a, IteratorAndCurrent b) {_                return a.current.key < b.current.key__            }_        }__        for (InternalAggregation aggregation : aggregations) {_            InternalAutoDateHistogram histogram = (InternalAutoDateHistogram) aggregation__            if (histogram.buckets.isEmpty() == false) {_                pq.add(new IteratorAndCurrent(histogram.buckets.iterator()))__            }_        }__        List<Bucket> reducedBuckets = new ArrayList<>()__        if (pq.size() > 0) {_            _            List<Bucket> currentBuckets = new ArrayList<>()__            double key = reduceRounding.round(pq.top().current.key)___            do {_                final IteratorAndCurrent top = pq.top()___                if (reduceRounding.round(top.current.key) != key) {_                    _                    final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                    reduceContext.consumeBucketsAndMaybeBreak(1)__                    reducedBuckets.add(reduced)__                    currentBuckets.clear()__                    key = reduceRounding.round(top.current.key)__                }__                currentBuckets.add(top.current)___                if (top.iterator.hasNext()) {_                    final Bucket next = top.iterator.next()__                    assert next.key > top.current.key : "shards must return data sorted by key"__                    top.current = next__                    pq.updateTop()__                } else {_                    pq.pop()__                }_            } while (pq.size() > 0)___            if (currentBuckets.isEmpty() == false) {_                final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                reduceContext.consumeBucketsAndMaybeBreak(1)__                reducedBuckets.add(reduced)__            }_        }__        return mergeBucketsIfNeeded(reducedBuckets, reduceRoundingIdx, reduceRoundingInfo, reduceContext)__    };this,method,works,almost,exactly,the,same,as,internal,date,histogram,reduce,buckets,list,reduce,context,the,different,here,is,that,we,need,to,round,all,the,keys,we,see,using,the,highest,level,rounding,returned,across,all,the,shards,so,the,resolution,of,the,buckets,is,the,same,and,they,can,be,reduced,together;private,bucket,reduce,result,reduce,buckets,list,internal,aggregation,aggregations,reduce,context,reduce,context,int,reduce,rounding,idx,0,for,internal,aggregation,aggregation,aggregations,int,agg,rounding,idx,internal,auto,date,histogram,aggregation,bucket,info,rounding,idx,if,agg,rounding,idx,reduce,rounding,idx,reduce,rounding,idx,agg,rounding,idx,rounding,info,reduce,rounding,info,bucket,info,rounding,infos,reduce,rounding,idx,rounding,reduce,rounding,reduce,rounding,info,rounding,final,priority,queue,iterator,and,current,pq,new,priority,queue,iterator,and,current,aggregations,size,override,protected,boolean,less,than,iterator,and,current,a,iterator,and,current,b,return,a,current,key,b,current,key,for,internal,aggregation,aggregation,aggregations,internal,auto,date,histogram,histogram,internal,auto,date,histogram,aggregation,if,histogram,buckets,is,empty,false,pq,add,new,iterator,and,current,histogram,buckets,iterator,list,bucket,reduced,buckets,new,array,list,if,pq,size,0,list,bucket,current,buckets,new,array,list,double,key,reduce,rounding,round,pq,top,current,key,do,final,iterator,and,current,top,pq,top,if,reduce,rounding,round,top,current,key,key,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,current,buckets,clear,key,reduce,rounding,round,top,current,key,current,buckets,add,top,current,if,top,iterator,has,next,final,bucket,next,top,iterator,next,assert,next,key,top,current,key,shards,must,return,data,sorted,by,key,top,current,next,pq,update,top,else,pq,pop,while,pq,size,0,if,current,buckets,is,empty,false,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,return,merge,buckets,if,needed,reduced,buckets,reduce,rounding,idx,reduce,rounding,info,reduce,context
InternalAutoDateHistogram -> private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext);1534539448;This method works almost exactly the same as_InternalDateHistogram#reduceBuckets(List, ReduceContext), the different_here is that we need to round all the keys we see using the highest level_rounding returned across all the shards so the resolution of the buckets_is the same and they can be reduced together.;private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext) {__        _        _        int reduceRoundingIdx = 0__        for (InternalAggregation aggregation : aggregations) {_            int aggRoundingIdx = ((InternalAutoDateHistogram) aggregation).bucketInfo.roundingIdx__            if (aggRoundingIdx > reduceRoundingIdx) {_                reduceRoundingIdx = aggRoundingIdx__            }_        }_        _        RoundingInfo reduceRoundingInfo = bucketInfo.roundingInfos[reduceRoundingIdx]__        Rounding reduceRounding = reduceRoundingInfo.rounding___        final PriorityQueue<IteratorAndCurrent> pq = new PriorityQueue<IteratorAndCurrent>(aggregations.size()) {_            @Override_            protected boolean lessThan(IteratorAndCurrent a, IteratorAndCurrent b) {_                return a.current.key < b.current.key__            }_        }__        for (InternalAggregation aggregation : aggregations) {_            InternalAutoDateHistogram histogram = (InternalAutoDateHistogram) aggregation__            if (histogram.buckets.isEmpty() == false) {_                pq.add(new IteratorAndCurrent(histogram.buckets.iterator()))__            }_        }__        List<Bucket> reducedBuckets = new ArrayList<>()__        if (pq.size() > 0) {_            _            List<Bucket> currentBuckets = new ArrayList<>()__            double key = reduceRounding.round(pq.top().current.key)___            do {_                final IteratorAndCurrent top = pq.top()___                if (reduceRounding.round(top.current.key) != key) {_                    _                    final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                    reduceContext.consumeBucketsAndMaybeBreak(1)__                    reducedBuckets.add(reduced)__                    currentBuckets.clear()__                    key = reduceRounding.round(top.current.key)__                }__                currentBuckets.add(top.current)___                if (top.iterator.hasNext()) {_                    final Bucket next = top.iterator.next()__                    assert next.key > top.current.key : "shards must return data sorted by key"__                    top.current = next__                    pq.updateTop()__                } else {_                    pq.pop()__                }_            } while (pq.size() > 0)___            if (currentBuckets.isEmpty() == false) {_                final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                reduceContext.consumeBucketsAndMaybeBreak(1)__                reducedBuckets.add(reduced)__            }_        }__        return mergeBucketsIfNeeded(reducedBuckets, reduceRoundingIdx, reduceRoundingInfo, reduceContext)__    };this,method,works,almost,exactly,the,same,as,internal,date,histogram,reduce,buckets,list,reduce,context,the,different,here,is,that,we,need,to,round,all,the,keys,we,see,using,the,highest,level,rounding,returned,across,all,the,shards,so,the,resolution,of,the,buckets,is,the,same,and,they,can,be,reduced,together;private,bucket,reduce,result,reduce,buckets,list,internal,aggregation,aggregations,reduce,context,reduce,context,int,reduce,rounding,idx,0,for,internal,aggregation,aggregation,aggregations,int,agg,rounding,idx,internal,auto,date,histogram,aggregation,bucket,info,rounding,idx,if,agg,rounding,idx,reduce,rounding,idx,reduce,rounding,idx,agg,rounding,idx,rounding,info,reduce,rounding,info,bucket,info,rounding,infos,reduce,rounding,idx,rounding,reduce,rounding,reduce,rounding,info,rounding,final,priority,queue,iterator,and,current,pq,new,priority,queue,iterator,and,current,aggregations,size,override,protected,boolean,less,than,iterator,and,current,a,iterator,and,current,b,return,a,current,key,b,current,key,for,internal,aggregation,aggregation,aggregations,internal,auto,date,histogram,histogram,internal,auto,date,histogram,aggregation,if,histogram,buckets,is,empty,false,pq,add,new,iterator,and,current,histogram,buckets,iterator,list,bucket,reduced,buckets,new,array,list,if,pq,size,0,list,bucket,current,buckets,new,array,list,double,key,reduce,rounding,round,pq,top,current,key,do,final,iterator,and,current,top,pq,top,if,reduce,rounding,round,top,current,key,key,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,current,buckets,clear,key,reduce,rounding,round,top,current,key,current,buckets,add,top,current,if,top,iterator,has,next,final,bucket,next,top,iterator,next,assert,next,key,top,current,key,shards,must,return,data,sorted,by,key,top,current,next,pq,update,top,else,pq,pop,while,pq,size,0,if,current,buckets,is,empty,false,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,return,merge,buckets,if,needed,reduced,buckets,reduce,rounding,idx,reduce,rounding,info,reduce,context
InternalAutoDateHistogram -> private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext);1536177418;This method works almost exactly the same as_InternalDateHistogram#reduceBuckets(List, ReduceContext), the different_here is that we need to round all the keys we see using the highest level_rounding returned across all the shards so the resolution of the buckets_is the same and they can be reduced together.;private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext) {__        _        _        int reduceRoundingIdx = 0__        for (InternalAggregation aggregation : aggregations) {_            int aggRoundingIdx = ((InternalAutoDateHistogram) aggregation).bucketInfo.roundingIdx__            if (aggRoundingIdx > reduceRoundingIdx) {_                reduceRoundingIdx = aggRoundingIdx__            }_        }_        _        RoundingInfo reduceRoundingInfo = bucketInfo.roundingInfos[reduceRoundingIdx]__        Rounding reduceRounding = reduceRoundingInfo.rounding___        final PriorityQueue<IteratorAndCurrent> pq = new PriorityQueue<IteratorAndCurrent>(aggregations.size()) {_            @Override_            protected boolean lessThan(IteratorAndCurrent a, IteratorAndCurrent b) {_                return a.current.key < b.current.key__            }_        }__        for (InternalAggregation aggregation : aggregations) {_            InternalAutoDateHistogram histogram = (InternalAutoDateHistogram) aggregation__            if (histogram.buckets.isEmpty() == false) {_                pq.add(new IteratorAndCurrent(histogram.buckets.iterator()))__            }_        }__        List<Bucket> reducedBuckets = new ArrayList<>()__        if (pq.size() > 0) {_            _            List<Bucket> currentBuckets = new ArrayList<>()__            double key = reduceRounding.round(pq.top().current.key)___            do {_                final IteratorAndCurrent top = pq.top()___                if (reduceRounding.round(top.current.key) != key) {_                    _                    final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                    reduceContext.consumeBucketsAndMaybeBreak(1)__                    reducedBuckets.add(reduced)__                    currentBuckets.clear()__                    key = reduceRounding.round(top.current.key)__                }__                currentBuckets.add(top.current)___                if (top.iterator.hasNext()) {_                    final Bucket next = top.iterator.next()__                    assert next.key > top.current.key : "shards must return data sorted by key"__                    top.current = next__                    pq.updateTop()__                } else {_                    pq.pop()__                }_            } while (pq.size() > 0)___            if (currentBuckets.isEmpty() == false) {_                final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                reduceContext.consumeBucketsAndMaybeBreak(1)__                reducedBuckets.add(reduced)__            }_        }__        return mergeBucketsIfNeeded(reducedBuckets, reduceRoundingIdx, reduceRoundingInfo, reduceContext)__    };this,method,works,almost,exactly,the,same,as,internal,date,histogram,reduce,buckets,list,reduce,context,the,different,here,is,that,we,need,to,round,all,the,keys,we,see,using,the,highest,level,rounding,returned,across,all,the,shards,so,the,resolution,of,the,buckets,is,the,same,and,they,can,be,reduced,together;private,bucket,reduce,result,reduce,buckets,list,internal,aggregation,aggregations,reduce,context,reduce,context,int,reduce,rounding,idx,0,for,internal,aggregation,aggregation,aggregations,int,agg,rounding,idx,internal,auto,date,histogram,aggregation,bucket,info,rounding,idx,if,agg,rounding,idx,reduce,rounding,idx,reduce,rounding,idx,agg,rounding,idx,rounding,info,reduce,rounding,info,bucket,info,rounding,infos,reduce,rounding,idx,rounding,reduce,rounding,reduce,rounding,info,rounding,final,priority,queue,iterator,and,current,pq,new,priority,queue,iterator,and,current,aggregations,size,override,protected,boolean,less,than,iterator,and,current,a,iterator,and,current,b,return,a,current,key,b,current,key,for,internal,aggregation,aggregation,aggregations,internal,auto,date,histogram,histogram,internal,auto,date,histogram,aggregation,if,histogram,buckets,is,empty,false,pq,add,new,iterator,and,current,histogram,buckets,iterator,list,bucket,reduced,buckets,new,array,list,if,pq,size,0,list,bucket,current,buckets,new,array,list,double,key,reduce,rounding,round,pq,top,current,key,do,final,iterator,and,current,top,pq,top,if,reduce,rounding,round,top,current,key,key,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,current,buckets,clear,key,reduce,rounding,round,top,current,key,current,buckets,add,top,current,if,top,iterator,has,next,final,bucket,next,top,iterator,next,assert,next,key,top,current,key,shards,must,return,data,sorted,by,key,top,current,next,pq,update,top,else,pq,pop,while,pq,size,0,if,current,buckets,is,empty,false,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,return,merge,buckets,if,needed,reduced,buckets,reduce,rounding,idx,reduce,rounding,info,reduce,context
InternalAutoDateHistogram -> private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext);1548236405;This method works almost exactly the same as_InternalDateHistogram#reduceBuckets(List, ReduceContext), the different_here is that we need to round all the keys we see using the highest level_rounding returned across all the shards so the resolution of the buckets_is the same and they can be reduced together.;private BucketReduceResult reduceBuckets(List<InternalAggregation> aggregations, ReduceContext reduceContext) {__        _        _        int reduceRoundingIdx = 0__        for (InternalAggregation aggregation : aggregations) {_            int aggRoundingIdx = ((InternalAutoDateHistogram) aggregation).bucketInfo.roundingIdx__            if (aggRoundingIdx > reduceRoundingIdx) {_                reduceRoundingIdx = aggRoundingIdx__            }_        }_        _        RoundingInfo reduceRoundingInfo = bucketInfo.roundingInfos[reduceRoundingIdx]__        Rounding reduceRounding = reduceRoundingInfo.rounding___        final PriorityQueue<IteratorAndCurrent> pq = new PriorityQueue<IteratorAndCurrent>(aggregations.size()) {_            @Override_            protected boolean lessThan(IteratorAndCurrent a, IteratorAndCurrent b) {_                return a.current.key < b.current.key__            }_        }__        for (InternalAggregation aggregation : aggregations) {_            InternalAutoDateHistogram histogram = (InternalAutoDateHistogram) aggregation__            if (histogram.buckets.isEmpty() == false) {_                pq.add(new IteratorAndCurrent(histogram.buckets.iterator()))__            }_        }__        List<Bucket> reducedBuckets = new ArrayList<>()__        if (pq.size() > 0) {_            _            List<Bucket> currentBuckets = new ArrayList<>()__            double key = reduceRounding.round(pq.top().current.key)___            do {_                final IteratorAndCurrent top = pq.top()___                if (reduceRounding.round(top.current.key) != key) {_                    _                    final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                    reduceContext.consumeBucketsAndMaybeBreak(1)__                    reducedBuckets.add(reduced)__                    currentBuckets.clear()__                    key = reduceRounding.round(top.current.key)__                }__                currentBuckets.add(top.current)___                if (top.iterator.hasNext()) {_                    final Bucket next = top.iterator.next()__                    assert next.key > top.current.key : "shards must return data sorted by key"__                    top.current = next__                    pq.updateTop()__                } else {_                    pq.pop()__                }_            } while (pq.size() > 0)___            if (currentBuckets.isEmpty() == false) {_                final Bucket reduced = currentBuckets.get(0).reduce(currentBuckets, reduceRounding, reduceContext)__                reduceContext.consumeBucketsAndMaybeBreak(1)__                reducedBuckets.add(reduced)__            }_        }__        return mergeBucketsIfNeeded(reducedBuckets, reduceRoundingIdx, reduceRoundingInfo, reduceContext)__    };this,method,works,almost,exactly,the,same,as,internal,date,histogram,reduce,buckets,list,reduce,context,the,different,here,is,that,we,need,to,round,all,the,keys,we,see,using,the,highest,level,rounding,returned,across,all,the,shards,so,the,resolution,of,the,buckets,is,the,same,and,they,can,be,reduced,together;private,bucket,reduce,result,reduce,buckets,list,internal,aggregation,aggregations,reduce,context,reduce,context,int,reduce,rounding,idx,0,for,internal,aggregation,aggregation,aggregations,int,agg,rounding,idx,internal,auto,date,histogram,aggregation,bucket,info,rounding,idx,if,agg,rounding,idx,reduce,rounding,idx,reduce,rounding,idx,agg,rounding,idx,rounding,info,reduce,rounding,info,bucket,info,rounding,infos,reduce,rounding,idx,rounding,reduce,rounding,reduce,rounding,info,rounding,final,priority,queue,iterator,and,current,pq,new,priority,queue,iterator,and,current,aggregations,size,override,protected,boolean,less,than,iterator,and,current,a,iterator,and,current,b,return,a,current,key,b,current,key,for,internal,aggregation,aggregation,aggregations,internal,auto,date,histogram,histogram,internal,auto,date,histogram,aggregation,if,histogram,buckets,is,empty,false,pq,add,new,iterator,and,current,histogram,buckets,iterator,list,bucket,reduced,buckets,new,array,list,if,pq,size,0,list,bucket,current,buckets,new,array,list,double,key,reduce,rounding,round,pq,top,current,key,do,final,iterator,and,current,top,pq,top,if,reduce,rounding,round,top,current,key,key,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,current,buckets,clear,key,reduce,rounding,round,top,current,key,current,buckets,add,top,current,if,top,iterator,has,next,final,bucket,next,top,iterator,next,assert,next,key,top,current,key,shards,must,return,data,sorted,by,key,top,current,next,pq,update,top,else,pq,pop,while,pq,size,0,if,current,buckets,is,empty,false,final,bucket,reduced,current,buckets,get,0,reduce,current,buckets,reduce,rounding,reduce,context,reduce,context,consume,buckets,and,maybe,break,1,reduced,buckets,add,reduced,return,merge,buckets,if,needed,reduced,buckets,reduce,rounding,idx,reduce,rounding,info,reduce,context
InternalAutoDateHistogram -> Bucket -> public Bucket(StreamInput in, DocValueFormat format) throws IOException;1531729807;Read from a stream.;public Bucket(StreamInput in, DocValueFormat format) throws IOException {_            this.format = format__            key = in.readLong()__            docCount = in.readVLong()__            aggregations = InternalAggregations.readAggregations(in)__        };read,from,a,stream;public,bucket,stream,input,in,doc,value,format,format,throws,ioexception,this,format,format,key,in,read,long,doc,count,in,read,vlong,aggregations,internal,aggregations,read,aggregations,in
InternalAutoDateHistogram -> Bucket -> public Bucket(StreamInput in, DocValueFormat format) throws IOException;1533063033;Read from a stream.;public Bucket(StreamInput in, DocValueFormat format) throws IOException {_            this.format = format__            key = in.readLong()__            docCount = in.readVLong()__            aggregations = InternalAggregations.readAggregations(in)__        };read,from,a,stream;public,bucket,stream,input,in,doc,value,format,format,throws,ioexception,this,format,format,key,in,read,long,doc,count,in,read,vlong,aggregations,internal,aggregations,read,aggregations,in
InternalAutoDateHistogram -> Bucket -> public Bucket(StreamInput in, DocValueFormat format) throws IOException;1534539448;Read from a stream.;public Bucket(StreamInput in, DocValueFormat format) throws IOException {_            this.format = format__            key = in.readLong()__            docCount = in.readVLong()__            aggregations = InternalAggregations.readAggregations(in)__        };read,from,a,stream;public,bucket,stream,input,in,doc,value,format,format,throws,ioexception,this,format,format,key,in,read,long,doc,count,in,read,vlong,aggregations,internal,aggregations,read,aggregations,in
InternalAutoDateHistogram -> Bucket -> public Bucket(StreamInput in, DocValueFormat format) throws IOException;1536177418;Read from a stream.;public Bucket(StreamInput in, DocValueFormat format) throws IOException {_            this.format = format__            key = in.readLong()__            docCount = in.readVLong()__            aggregations = InternalAggregations.readAggregations(in)__        };read,from,a,stream;public,bucket,stream,input,in,doc,value,format,format,throws,ioexception,this,format,format,key,in,read,long,doc,count,in,read,vlong,aggregations,internal,aggregations,read,aggregations,in
InternalAutoDateHistogram -> Bucket -> public Bucket(StreamInput in, DocValueFormat format) throws IOException;1548236405;Read from a stream.;public Bucket(StreamInput in, DocValueFormat format) throws IOException {_            this.format = format__            key = in.readLong()__            docCount = in.readVLong()__            aggregations = InternalAggregations.readAggregations(in)__        };read,from,a,stream;public,bucket,stream,input,in,doc,value,format,format,throws,ioexception,this,format,format,key,in,read,long,doc,count,in,read,vlong,aggregations,internal,aggregations,read,aggregations,in
