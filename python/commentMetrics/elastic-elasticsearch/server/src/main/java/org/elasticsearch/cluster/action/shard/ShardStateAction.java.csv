commented;modifiers;parameterAmount;loc;comment;code
false;public;1;4;;@Override public void handleResponse(TransportResponse.Empty response) {     listener.onResponse(null). }
false;public;1;12;;@Override public void handleException(TransportException exp) {     if (isMasterChannelException(exp)) {         waitForNewMasterAndRetry(actionName, observer, request, listener, changePredicate).     } else {         logger.warn(new ParameterizedMessage("unexpected failure while sending request [{}]" + " to [{}] for shard entry [{}]", actionName, masterNode, request), exp).         listener.onFailure(exp instanceof RemoteTransportException ? (Exception) (exp.getCause() instanceof Exception ? exp.getCause() : new ElasticsearchException(exp.getCause())) : exp).     } }
false;private;4;33;;private void sendShardAction(final String actionName, final ClusterState currentState, final TransportRequest request, final ActionListener<Void> listener) {     ClusterStateObserver observer = new ClusterStateObserver(currentState, clusterService, null, logger, threadPool.getThreadContext()).     DiscoveryNode masterNode = currentState.nodes().getMasterNode().     Predicate<ClusterState> changePredicate = MasterNodeChangePredicate.build(currentState).     if (masterNode == null) {         logger.warn("no master known for action [{}] for shard entry [{}]", actionName, request).         waitForNewMasterAndRetry(actionName, observer, request, listener, changePredicate).     } else {         logger.debug("sending [{}] to [{}] for shard entry [{}]", actionName, masterNode.getId(), request).         transportService.sendRequest(masterNode, actionName, request, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {              @Override             public void handleResponse(TransportResponse.Empty response) {                 listener.onResponse(null).             }              @Override             public void handleException(TransportException exp) {                 if (isMasterChannelException(exp)) {                     waitForNewMasterAndRetry(actionName, observer, request, listener, changePredicate).                 } else {                     logger.warn(new ParameterizedMessage("unexpected failure while sending request [{}]" + " to [{}] for shard entry [{}]", actionName, masterNode, request), exp).                     listener.onFailure(exp instanceof RemoteTransportException ? (Exception) (exp.getCause() instanceof Exception ? exp.getCause() : new ElasticsearchException(exp.getCause())) : exp).                 }             }         }).     } }
false;private,static;1;3;;private static boolean isMasterChannelException(TransportException exp) {     return ExceptionsHelper.unwrap(exp, MASTER_CHANNEL_EXCEPTIONS) != null. }
true;public;7;7;/**  * Send a shard failed request to the master node to update the cluster state with the failure of a shard on another node. This means  * that the shard should be failed because a write made it into the primary but was not replicated to this shard copy. If the shard  * does not exist anymore but still has an entry in the in-sync set, remove its allocation id from the in-sync set.  *  * @param shardId            shard id of the shard to fail  * @param allocationId       allocation id of the shard to fail  * @param primaryTerm        the primary term associated with the primary shard that is failing the shard. Must be strictly positive.  * @param markAsStale        whether or not to mark a failing shard as stale (eg. removing from in-sync set) when failing the shard.  * @param message            the reason for the failure  * @param failure            the underlying cause of the failure  * @param listener           callback upon completion of the request  */ ;/**  * Send a shard failed request to the master node to update the cluster state with the failure of a shard on another node. This means  * that the shard should be failed because a write made it into the primary but was not replicated to this shard copy. If the shard  * does not exist anymore but still has an entry in the in-sync set, remove its allocation id from the in-sync set.  *  * @param shardId            shard id of the shard to fail  * @param allocationId       allocation id of the shard to fail  * @param primaryTerm        the primary term associated with the primary shard that is failing the shard. Must be strictly positive.  * @param markAsStale        whether or not to mark a failing shard as stale (eg. removing from in-sync set) when failing the shard.  * @param message            the reason for the failure  * @param failure            the underlying cause of the failure  * @param listener           callback upon completion of the request  */ public void remoteShardFailed(final ShardId shardId, String allocationId, long primaryTerm, boolean markAsStale, final String message, @Nullable final Exception failure, ActionListener<Void> listener) {     assert primaryTerm > 0L : "primary term should be strictly positive".     remoteFailedShardsDeduplicator.executeOnce(new FailedShardEntry(shardId, allocationId, primaryTerm, message, failure, markAsStale), listener, (req, reqListener) -> sendShardAction(SHARD_FAILED_ACTION_NAME, clusterService.state(), req, reqListener)). }
false;;0;3;;int remoteShardFailedCacheSize() {     return remoteFailedShardsDeduplicator.size(). }
true;public;4;4;/**  * Send a shard failed request to the master node to update the cluster state when a shard on the local node failed.  */ ;/**  * Send a shard failed request to the master node to update the cluster state when a shard on the local node failed.  */ public void localShardFailed(final ShardRouting shardRouting, final String message, @Nullable final Exception failure, ActionListener<Void> listener) {     localShardFailed(shardRouting, message, failure, listener, clusterService.state()). }
true;public;5;6;/**  * Send a shard failed request to the master node to update the cluster state when a shard on the local node failed.  */ ;/**  * Send a shard failed request to the master node to update the cluster state when a shard on the local node failed.  */ public void localShardFailed(final ShardRouting shardRouting, final String message, @Nullable final Exception failure, ActionListener<Void> listener, final ClusterState currentState) {     FailedShardEntry shardEntry = new FailedShardEntry(shardRouting.shardId(), shardRouting.allocationId().getId(), 0L, message, failure, true).     sendShardAction(SHARD_FAILED_ACTION_NAME, currentState, shardEntry, listener). }
false;public;1;7;;@Override public void onNewClusterState(ClusterState state) {     if (logger.isTraceEnabled()) {         logger.trace("new cluster state [{}] after waiting for master election for shard entry [{}]", state, request).     }     sendShardAction(actionName, state, request, listener). }
false;public;0;5;;@Override public void onClusterServiceClose() {     logger.warn("node closed while execution action [{}] for shard entry [{}]", actionName, request).     listener.onFailure(new NodeClosedException(clusterService.localNode())). }
false;public;1;5;;@Override public void onTimeout(TimeValue timeout) {     // we wait indefinitely for a new master     assert false. }
true;protected;5;25;// visible for testing ;// visible for testing protected void waitForNewMasterAndRetry(String actionName, ClusterStateObserver observer, TransportRequest request, ActionListener<Void> listener, Predicate<ClusterState> changePredicate) {     observer.waitForNextChange(new ClusterStateObserver.Listener() {          @Override         public void onNewClusterState(ClusterState state) {             if (logger.isTraceEnabled()) {                 logger.trace("new cluster state [{}] after waiting for master election for shard entry [{}]", state, request).             }             sendShardAction(actionName, state, request, listener).         }          @Override         public void onClusterServiceClose() {             logger.warn("node closed while execution action [{}] for shard entry [{}]", actionName, request).             listener.onFailure(new NodeClosedException(clusterService.localNode())).         }          @Override         public void onTimeout(TimeValue timeout) {             // we wait indefinitely for a new master             assert false.         }     }, changePredicate). }
false;public;2;13;;@Override public void onFailure(String source, Exception e) {     logger.error(() -> new ParameterizedMessage("{} unexpected failure while failing shard [{}]", request.shardId, request), e).     try {         channel.sendResponse(e).     } catch (Exception channelException) {         channelException.addSuppressed(e).         logger.warn(() -> new ParameterizedMessage("{} failed to send failure [{}] while failing shard [{}]", request.shardId, e, request), channelException).     } }
false;public;1;11;;@Override public void onNoLongerMaster(String source) {     logger.error("{} no longer master while failing shard [{}]", request.shardId, request).     try {         channel.sendResponse(new NotMasterException(source)).     } catch (Exception channelException) {         logger.warn(() -> new ParameterizedMessage("{} failed to send no longer master while failing shard [{}]", request.shardId, request), channelException).     } }
false;public;3;10;;@Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {     try {         channel.sendResponse(TransportResponse.Empty.INSTANCE).     } catch (Exception channelException) {         logger.warn(() -> new ParameterizedMessage("{} failed to send response while failing shard [{}]", request.shardId, request), channelException).     } }
false;public;3;49;;@Override public void messageReceived(FailedShardEntry request, TransportChannel channel, Task task) throws Exception {     logger.debug(() -> new ParameterizedMessage("{} received shard failed for {}", request.shardId, request), request.failure).     clusterService.submitStateUpdateTask("shard-failed", request, ClusterStateTaskConfig.build(Priority.HIGH), shardFailedClusterStateTaskExecutor, new ClusterStateTaskListener() {          @Override         public void onFailure(String source, Exception e) {             logger.error(() -> new ParameterizedMessage("{} unexpected failure while failing shard [{}]", request.shardId, request), e).             try {                 channel.sendResponse(e).             } catch (Exception channelException) {                 channelException.addSuppressed(e).                 logger.warn(() -> new ParameterizedMessage("{} failed to send failure [{}] while failing shard [{}]", request.shardId, e, request), channelException).             }         }          @Override         public void onNoLongerMaster(String source) {             logger.error("{} no longer master while failing shard [{}]", request.shardId, request).             try {                 channel.sendResponse(new NotMasterException(source)).             } catch (Exception channelException) {                 logger.warn(() -> new ParameterizedMessage("{} failed to send no longer master while failing shard [{}]", request.shardId, request), channelException).             }         }          @Override         public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {             try {                 channel.sendResponse(TransportResponse.Empty.INSTANCE).             } catch (Exception channelException) {                 logger.warn(() -> new ParameterizedMessage("{} failed to send response while failing shard [{}]", request.shardId, request), channelException).             }         }     }). }
false;public;2;76;;@Override public ClusterTasksResult<FailedShardEntry> execute(ClusterState currentState, List<FailedShardEntry> tasks) throws Exception {     ClusterTasksResult.Builder<FailedShardEntry> batchResultBuilder = ClusterTasksResult.builder().     List<FailedShardEntry> tasksToBeApplied = new ArrayList<>().     List<FailedShard> failedShardsToBeApplied = new ArrayList<>().     List<StaleShard> staleShardsToBeApplied = new ArrayList<>().     for (FailedShardEntry task : tasks) {         IndexMetaData indexMetaData = currentState.metaData().index(task.shardId.getIndex()).         if (indexMetaData == null) {             // tasks that correspond to non-existent indices are marked as successful             logger.debug("{} ignoring shard failed task [{}] (unknown index {})", task.shardId, task, task.shardId.getIndex()).             batchResultBuilder.success(task).         } else {             // primary unnecessarily fail currently active shards.             if (task.primaryTerm > 0) {                 long currentPrimaryTerm = indexMetaData.primaryTerm(task.shardId.id()).                 if (currentPrimaryTerm != task.primaryTerm) {                     assert currentPrimaryTerm > task.primaryTerm : "received a primary term with a higher term than in the " + "current cluster state (received [" + task.primaryTerm + "] but current is [" + currentPrimaryTerm + "])".                     logger.debug("{} failing shard failed task [{}] (primary term {} does not match current term {})", task.shardId, task, task.primaryTerm, indexMetaData.primaryTerm(task.shardId.id())).                     batchResultBuilder.failure(task, new NoLongerPrimaryShardException(task.shardId, "primary term [" + task.primaryTerm + "] did not match current primary term [" + currentPrimaryTerm + "]")).                     continue.                 }             }             ShardRouting matched = currentState.getRoutingTable().getByAllocationId(task.shardId, task.allocationId).             if (matched == null) {                 Set<String> inSyncAllocationIds = indexMetaData.inSyncAllocationIds(task.shardId.id()).                 // the check "primaryTerm > 0").                 if (task.primaryTerm > 0 && inSyncAllocationIds.contains(task.allocationId)) {                     logger.debug("{} marking shard {} as stale (shard failed task: [{}])", task.shardId, task.allocationId, task).                     tasksToBeApplied.add(task).                     staleShardsToBeApplied.add(new StaleShard(task.shardId, task.allocationId)).                 } else {                     // tasks that correspond to non-existent shards are marked as successful                     logger.debug("{} ignoring shard failed task [{}] (shard does not exist anymore)", task.shardId, task).                     batchResultBuilder.success(task).                 }             } else {                 // failing a shard also possibly marks it as stale (see IndexMetaDataUpdater)                 logger.debug("{} failing shard {} (shard failed task: [{}])", task.shardId, matched, task).                 tasksToBeApplied.add(task).                 failedShardsToBeApplied.add(new FailedShard(matched, task.message, task.failure, task.markAsStale)).             }         }     }     assert tasksToBeApplied.size() == failedShardsToBeApplied.size() + staleShardsToBeApplied.size().     ClusterState maybeUpdatedState = currentState.     try {         maybeUpdatedState = applyFailedShards(currentState, failedShardsToBeApplied, staleShardsToBeApplied).         batchResultBuilder.successes(tasksToBeApplied).     } catch (Exception e) {         logger.warn(() -> new ParameterizedMessage("failed to apply failed shards {}", failedShardsToBeApplied), e).         // failures are communicated back to the requester         // cluster state will not be updated in this case         batchResultBuilder.failures(tasksToBeApplied, e).     }     return batchResultBuilder.build(maybeUpdatedState). }
true;;3;3;// visible for testing ;// visible for testing ClusterState applyFailedShards(ClusterState currentState, List<FailedShard> failedShards, List<StaleShard> staleShards) {     return allocationService.applyFailedShards(currentState, failedShards, staleShards). }
false;public;1;11;;@Override public void clusterStatePublished(ClusterChangedEvent clusterChangedEvent) {     int numberOfUnassignedShards = clusterChangedEvent.state().getRoutingNodes().unassigned().size().     if (numberOfUnassignedShards > 0) {         String reason = String.format(Locale.ROOT, "[%d] unassigned shards after failing shards", numberOfUnassignedShards).         if (logger.isTraceEnabled()) {             logger.trace("{}, scheduling a reroute", reason).         }         routingService.reroute(reason).     } }
false;public;0;3;;public ShardId getShardId() {     return shardId. }
false;public;0;3;;public String getAllocationId() {     return allocationId. }
false;public;1;12;;@Override public void writeTo(StreamOutput out) throws IOException {     super.writeTo(out).     shardId.writeTo(out).     out.writeString(allocationId).     out.writeVLong(primaryTerm).     out.writeString(message).     out.writeException(failure).     if (out.getVersion().onOrAfter(Version.V_6_3_0)) {         out.writeBoolean(markAsStale).     } }
false;public;0;13;;@Override public String toString() {     List<String> components = new ArrayList<>(6).     components.add("shard id [" + shardId + "]").     components.add("allocation id [" + allocationId + "]").     components.add("primary term [" + primaryTerm + "]").     components.add("message [" + message + "]").     if (failure != null) {         components.add("failure [" + ExceptionsHelper.detailedMessage(failure) + "]").     }     components.add("markAsStale [" + markAsStale + "]").     return String.join(", ", components). }
false;public;1;11;;@Override public boolean equals(Object o) {     if (this == o)         return true.     if (o == null || getClass() != o.getClass())         return false.     FailedShardEntry that = (FailedShardEntry) o.     // Exclude message and exception from equals and hashCode     return Objects.equals(this.shardId, that.shardId) && Objects.equals(this.allocationId, that.allocationId) && primaryTerm == that.primaryTerm && markAsStale == that.markAsStale. }
false;public;0;4;;@Override public int hashCode() {     return Objects.hash(shardId, allocationId, primaryTerm, markAsStale). }
false;public;4;6;;public void shardStarted(final ShardRouting shardRouting, final long primaryTerm, final String message, final ActionListener<Void> listener) {     shardStarted(shardRouting, primaryTerm, message, listener, clusterService.state()). }
false;public;5;8;;public void shardStarted(final ShardRouting shardRouting, final long primaryTerm, final String message, final ActionListener<Void> listener, final ClusterState currentState) {     StartedShardEntry entry = new StartedShardEntry(shardRouting.shardId(), shardRouting.allocationId().getId(), primaryTerm, message).     sendShardAction(SHARD_STARTED_ACTION_NAME, currentState, entry, listener). }
false;public;3;11;;@Override public void messageReceived(StartedShardEntry request, TransportChannel channel, Task task) throws Exception {     logger.debug("{} received shard started for [{}]", request.shardId, request).     clusterService.submitStateUpdateTask("shard-started " + request, request, ClusterStateTaskConfig.build(Priority.URGENT), shardStartedClusterStateTaskExecutor, shardStartedClusterStateTaskExecutor).     channel.sendResponse(TransportResponse.Empty.INSTANCE). }
false;public;2;63;;@Override public ClusterTasksResult<StartedShardEntry> execute(ClusterState currentState, List<StartedShardEntry> tasks) throws Exception {     ClusterTasksResult.Builder<StartedShardEntry> builder = ClusterTasksResult.builder().     List<StartedShardEntry> tasksToBeApplied = new ArrayList<>().     List<ShardRouting> shardRoutingsToBeApplied = new ArrayList<>(tasks.size()).     // to prevent duplicates     Set<ShardRouting> seenShardRoutings = new HashSet<>().     for (StartedShardEntry task : tasks) {         final ShardRouting matched = currentState.getRoutingTable().getByAllocationId(task.shardId, task.allocationId).         if (matched == null) {             // tasks that correspond to non-existent shards are marked as successful. The reason is that we resend shard started             // events on every cluster state publishing that does not contain the shard as started yet. This means that old stale             // requests might still be in flight even after the shard has already been started or failed on the master. We just             // ignore these requests for now.             logger.debug("{} ignoring shard started task [{}] (shard does not exist anymore)", task.shardId, task).             builder.success(task).         } else {             if (matched.primary() && task.primaryTerm > 0) {                 final IndexMetaData indexMetaData = currentState.metaData().index(task.shardId.getIndex()).                 assert indexMetaData != null.                 final long currentPrimaryTerm = indexMetaData.primaryTerm(task.shardId.id()).                 if (currentPrimaryTerm != task.primaryTerm) {                     assert currentPrimaryTerm > task.primaryTerm : "received a primary term with a higher term than in the " + "current cluster state (received [" + task.primaryTerm + "] but current is [" + currentPrimaryTerm + "])".                     logger.debug("{} ignoring shard started task [{}] (primary term {} does not match current term {})", task.shardId, task, task.primaryTerm, currentPrimaryTerm).                     builder.success(task).                     continue.                 }             }             if (matched.initializing() == false) {                 assert matched.active() : "expected active shard routing for task " + task + " but found " + matched.                 // same as above, this might have been a stale in-flight request, so we just ignore.                 logger.debug("{} ignoring shard started task [{}] (shard exists but is not initializing: {})", task.shardId, task, matched).                 builder.success(task).             } else {                 // remove duplicate actions as allocation service expects a clean list without duplicates                 if (seenShardRoutings.contains(matched)) {                     logger.trace("{} ignoring shard started task [{}] (already scheduled to start {})", task.shardId, task, matched).                     tasksToBeApplied.add(task).                 } else {                     logger.debug("{} starting shard {} (shard started task: [{}])", task.shardId, matched, task).                     tasksToBeApplied.add(task).                     shardRoutingsToBeApplied.add(matched).                     seenShardRoutings.add(matched).                 }             }         }     }     assert tasksToBeApplied.size() >= shardRoutingsToBeApplied.size().     ClusterState maybeUpdatedState = currentState.     try {         maybeUpdatedState = allocationService.applyStartedShards(currentState, shardRoutingsToBeApplied).         builder.successes(tasksToBeApplied).     } catch (Exception e) {         logger.warn(() -> new ParameterizedMessage("failed to apply started shards {}", shardRoutingsToBeApplied), e).         builder.failures(tasksToBeApplied, e).     }     return builder.build(maybeUpdatedState). }
false;public;2;4;;@Override public void onFailure(String source, Exception e) {     logger.error(() -> new ParameterizedMessage("unexpected failure during [{}]", source), e). }
false;public;1;15;;@Override public void writeTo(StreamOutput out) throws IOException {     super.writeTo(out).     shardId.writeTo(out).     out.writeString(allocationId).     if (out.getVersion().before(Version.V_6_3_0)) {         out.writeVLong(0L).     } else if (out.getVersion().onOrAfter(Version.V_6_7_0)) {         out.writeVLong(primaryTerm).     }     out.writeString(message).     if (out.getVersion().before(Version.V_6_3_0)) {         out.writeException(null).     } }
false;public;0;5;;@Override public String toString() {     return String.format(Locale.ROOT, "StartedShardEntry{shardId [%s], allocationId [%s], primary term [%d], message [%s]}", shardId, allocationId, primaryTerm, message). }
