commented;modifiers;parameterAmount;loc;comment;code
true;private;1;60;/**  * recover all translog files found on disk  */ ;/**  * recover all translog files found on disk  */ private ArrayList<TranslogReader> recoverFromFiles(Checkpoint checkpoint) throws IOException {     boolean success = false.     ArrayList<TranslogReader> foundTranslogs = new ArrayList<>().     try (ReleasableLock lock = writeLock.acquire()) {         logger.debug("open uncommitted translog checkpoint {}", checkpoint).         final long minGenerationToRecoverFrom.         if (checkpoint.minTranslogGeneration < 0) {             final Version indexVersionCreated = indexSettings().getIndexVersionCreated().             assert indexVersionCreated.before(Version.V_6_0_0_beta1) : "no minTranslogGeneration in checkpoint, but index was created with version [" + indexVersionCreated + "]".             minGenerationToRecoverFrom = deletionPolicy.getMinTranslogGenerationForRecovery().         } else {             minGenerationToRecoverFrom = checkpoint.minTranslogGeneration.         }         final String checkpointTranslogFile = getFilename(checkpoint.generation).         // we open files in reverse order in order to validate tranlsog uuid before we start traversing the translog based on         // the generation id we found in the lucene commit. This gives for better error messages if the wrong         // translog was found.         foundTranslogs.add(openReader(location.resolve(checkpointTranslogFile), checkpoint)).         for (long i = checkpoint.generation - 1. i >= minGenerationToRecoverFrom. i--) {             Path committedTranslogFile = location.resolve(getFilename(i)).             if (Files.exists(committedTranslogFile) == false) {                 throw new IllegalStateException("translog file doesn't exist with generation: " + i + " recovering from: " + minGenerationToRecoverFrom + " checkpoint: " + checkpoint.generation + " - translog ids must be consecutive").             }             final TranslogReader reader = openReader(committedTranslogFile, Checkpoint.read(location.resolve(getCommitCheckpointFileName(i)))).             assert reader.getPrimaryTerm() <= primaryTermSupplier.getAsLong() : "Primary terms go backwards. current term [" + primaryTermSupplier.getAsLong() + "]" + "translog path [ " + committedTranslogFile + ", existing term [" + reader.getPrimaryTerm() + "]".             foundTranslogs.add(reader).             logger.debug("recovered local translog from checkpoint {}", checkpoint).         }         Collections.reverse(foundTranslogs).         // when we clean up files, we first update the checkpoint with a new minReferencedTranslog and then delete them.         // if we crash just at the wrong moment, it may be that we leave one unreferenced file behind so we delete it if there         IOUtils.deleteFilesIgnoringExceptions(location.resolve(getFilename(minGenerationToRecoverFrom - 1)), location.resolve(getCommitCheckpointFileName(minGenerationToRecoverFrom - 1))).         Path commitCheckpoint = location.resolve(getCommitCheckpointFileName(checkpoint.generation)).         if (Files.exists(commitCheckpoint)) {             Checkpoint checkpointFromDisk = Checkpoint.read(commitCheckpoint).             if (checkpoint.equals(checkpointFromDisk) == false) {                 throw new IllegalStateException("Checkpoint file " + commitCheckpoint.getFileName() + " already exists but has corrupted content expected: " + checkpoint + " but got: " + checkpointFromDisk).             }         } else {             copyCheckpointTo(commitCheckpoint).         }         success = true.     } finally {         if (success == false) {             IOUtils.closeWhileHandlingException(foundTranslogs).         }     }     return foundTranslogs. }
false;private;1;24;;private void copyCheckpointTo(Path targetPath) throws IOException {     // a temp file to copy checkpoint to - note it must be in on the same FS otherwise atomic move won't work     final Path tempFile = Files.createTempFile(location, TRANSLOG_FILE_PREFIX, CHECKPOINT_SUFFIX).     boolean tempFileRenamed = false.     try {         // we first copy this into the temp-file and then fsync it followed by an atomic move into the target file         // that way if we hit a disk-full here we are still in an consistent state.         Files.copy(location.resolve(CHECKPOINT_FILE_NAME), tempFile, StandardCopyOption.REPLACE_EXISTING).         IOUtils.fsync(tempFile, false).         Files.move(tempFile, targetPath, StandardCopyOption.ATOMIC_MOVE).         tempFileRenamed = true.         // we only fsync the directory the tempFile was already fsynced         IOUtils.fsync(targetPath.getParent(), true).     } finally {         if (tempFileRenamed == false) {             try {                 Files.delete(tempFile).             } catch (IOException ex) {                 logger.warn(() -> new ParameterizedMessage("failed to delete temp file {}", tempFile), ex).             }         }     } }
false;;2;12;;TranslogReader openReader(Path path, Checkpoint checkpoint) throws IOException {     FileChannel channel = FileChannel.open(path, StandardOpenOption.READ).     try {         assert Translog.parseIdFromFileName(path) == checkpoint.generation : "expected generation: " + Translog.parseIdFromFileName(path) + " but got: " + checkpoint.generation.         TranslogReader reader = TranslogReader.open(channel, path, checkpoint, translogUUID).         channel = null.         return reader.     } finally {         IOUtils.close(channel).     } }
true;public,static;1;13;/**  * Extracts the translog generation from a file name.  *  * @throws IllegalArgumentException if the path doesn't match the expected pattern.  */ ;/**  * Extracts the translog generation from a file name.  *  * @throws IllegalArgumentException if the path doesn't match the expected pattern.  */ public static long parseIdFromFileName(Path translogFile) {     final String fileName = translogFile.getFileName().toString().     final Matcher matcher = PARSE_STRICT_ID_PATTERN.matcher(fileName).     if (matcher.matches()) {         try {             return Long.parseLong(matcher.group(1)).         } catch (NumberFormatException e) {             throw new IllegalStateException("number formatting issue in a file that passed PARSE_STRICT_ID_PATTERN: " + fileName + "]", e).         }     }     throw new IllegalArgumentException("can't parse id from file: " + fileName). }
true;public;0;3;/**  * Returns {@code true} if this {@code Translog} is still open.  */ ;/**  * Returns {@code true} if this {@code Translog} is still open.  */ public boolean isOpen() {     return closed.get() == false. }
false;private,static;0;17;;private static boolean calledFromOutsideOrViaTragedyClose() {     List<StackTraceElement> frames = Stream.of(Thread.currentThread().getStackTrace()).skip(// skip getStackTrace, current method and close method frames     3).limit(// limit depth of analysis to 10 frames, it should be enough to catch closing with, e.g. IOUtils     10).filter(f -> {         try {             return Translog.class.isAssignableFrom(Class.forName(f.getClassName())).         } catch (Exception ignored) {             return false.         }     }).collect(Collectors.toList()).     // the list of inner callers should be either empty or should contain closeOnTragicEvent method     return frames.isEmpty() || frames.stream().anyMatch(f -> f.getMethodName().equals("closeOnTragicEvent")). }
false;public;0;16;;@Override public void close() throws IOException {     assert calledFromOutsideOrViaTragedyClose() : "Translog.close method is called from inside Translog, but not via closeOnTragicEvent method".     if (closed.compareAndSet(false, true)) {         try (ReleasableLock lock = writeLock.acquire()) {             try {                 current.sync().             } finally {                 closeFilesIfNoPendingRetentionLocks().             }         } finally {             logger.debug("translog closed").         }     } }
true;public;0;3;/**  * Returns all translog locations as absolute paths.  * These paths don't contain actual translog files they are  * directories holding the transaction logs.  */ ;/**  * Returns all translog locations as absolute paths.  * These paths don't contain actual translog files they are  * directories holding the transaction logs.  */ public Path location() {     return location. }
true;public;0;5;/**  * Returns the generation of the current transaction log.  */ ;/**  * Returns the generation of the current transaction log.  */ public long currentFileGeneration() {     try (ReleasableLock ignored = readLock.acquire()) {         return current.getGeneration().     } }
true;public;0;11;/**  * Returns the minimum file generation referenced by the translog  */ ;/**  * Returns the minimum file generation referenced by the translog  */ public long getMinFileGeneration() {     try (ReleasableLock ignored = readLock.acquire()) {         if (readers.isEmpty()) {             return current.getGeneration().         } else {             assert readers.stream().map(TranslogReader::getGeneration).min(Long::compareTo).get().equals(readers.get(0).getGeneration()) : "the first translog isn't the one with the minimum generation:" + readers.             return readers.get(0).getGeneration().         }     } }
true;public;0;3;/**  * Returns the number of operations in the translog files  */ ;/**  * Returns the number of operations in the translog files  */ public int totalOperations() {     return totalOperationsByMinGen(-1). }
true;public;0;3;/**  * Returns the size in bytes of the v files  */ ;/**  * Returns the size in bytes of the v files  */ public long sizeInBytes() {     return sizeInBytesByMinGen(-1). }
false;;0;8;;long earliestLastModifiedAge() {     try (ReleasableLock ignored = readLock.acquire()) {         ensureOpen().         return findEarliestLastModifiedAge(System.currentTimeMillis(), readers, current).     } catch (IOException e) {         throw new TranslogException(shardId, "Unable to get the earliest last modified time for the transaction log").     } }
true;static;3;7;/**  * Returns the age of the oldest entry in the translog files in seconds  */ ;/**  * Returns the age of the oldest entry in the translog files in seconds  */ static long findEarliestLastModifiedAge(long currentTime, Iterable<TranslogReader> readers, TranslogWriter writer) throws IOException {     long earliestTime = currentTime.     for (BaseTranslogReader r : readers) {         earliestTime = Math.min(r.getLastModifiedTime(), earliestTime).     }     return Math.max(0, currentTime - Math.min(earliestTime, writer.getLastModifiedTime())). }
true;public;1;9;/**  * Returns the number of operations in the translog files at least the given generation  */ ;/**  * Returns the number of operations in the translog files at least the given generation  */ public int totalOperationsByMinGen(long minGeneration) {     try (ReleasableLock ignored = readLock.acquire()) {         ensureOpen().         return Stream.concat(readers.stream(), Stream.of(current)).filter(r -> r.getGeneration() >= minGeneration).mapToInt(BaseTranslogReader::totalOperations).sum().     } }
true;public;1;6;/**  * Returns the number of operations in the transaction files that contain operations with seq# above the given number.  */ ;/**  * Returns the number of operations in the transaction files that contain operations with seq# above the given number.  */ public int estimateTotalOperationsFromMinSeq(long minSeqNo) {     try (ReleasableLock ignored = readLock.acquire()) {         ensureOpen().         return readersAboveMinSeqNo(minSeqNo).mapToInt(BaseTranslogReader::totalOperations).sum().     } }
true;public;1;9;/**  * Returns the size in bytes of the translog files at least the given generation  */ ;/**  * Returns the size in bytes of the translog files at least the given generation  */ public long sizeInBytesByMinGen(long minGeneration) {     try (ReleasableLock ignored = readLock.acquire()) {         ensureOpen().         return Stream.concat(readers.stream(), Stream.of(current)).filter(r -> r.getGeneration() >= minGeneration).mapToLong(BaseTranslogReader::sizeInBytes).sum().     } }
true;;1;6;/**  * Creates a new translog for the specified generation.  *  * @param fileGeneration the translog generation  * @return a writer for the new translog  * @throws IOException if creating the translog failed  */ ;/**  * Creates a new translog for the specified generation.  *  * @param fileGeneration the translog generation  * @return a writer for the new translog  * @throws IOException if creating the translog failed  */ TranslogWriter createWriter(long fileGeneration) throws IOException {     final TranslogWriter writer = createWriter(fileGeneration, getMinFileGeneration(), globalCheckpointSupplier.getAsLong()).     assert writer.sizeInBytes() == DEFAULT_HEADER_SIZE_IN_BYTES : "Mismatch translog header size. " + "empty translog size [" + writer.sizeInBytes() + ", header size [" + DEFAULT_HEADER_SIZE_IN_BYTES + "]".     return writer. }
true;;3;17;/**  * creates a new writer  *  * @param fileGeneration          the generation of the write to be written  * @param initialMinTranslogGen   the minimum translog generation to be written in the first checkpoint. This is  *                                needed to solve and initialization problem while constructing an empty translog.  *                                With no readers and no current, a call to  {@link #getMinFileGeneration()} would not work.  * @param initialGlobalCheckpoint the global checkpoint to be written in the first checkpoint.  */ ;/**  * creates a new writer  *  * @param fileGeneration          the generation of the write to be written  * @param initialMinTranslogGen   the minimum translog generation to be written in the first checkpoint. This is  *                                needed to solve and initialization problem while constructing an empty translog.  *                                With no readers and no current, a call to  {@link #getMinFileGeneration()} would not work.  * @param initialGlobalCheckpoint the global checkpoint to be written in the first checkpoint.  */ TranslogWriter createWriter(long fileGeneration, long initialMinTranslogGen, long initialGlobalCheckpoint) throws IOException {     final TranslogWriter newFile.     try {         newFile = TranslogWriter.create(shardId, translogUUID, fileGeneration, location.resolve(getFilename(fileGeneration)), getChannelFactory(), config.getBufferSize(), initialMinTranslogGen, initialGlobalCheckpoint, globalCheckpointSupplier, this::getMinFileGeneration, primaryTermSupplier.getAsLong(), tragedy).     } catch (final IOException e) {         throw new TranslogException(shardId, "failed to create new translog file", e).     }     return newFile. }
true;public;1;33;/**  * Adds an operation to the transaction log.  *  * @param operation the operation to add  * @return the location of the operation in the translog  * @throws IOException if adding the operation to the translog resulted in an I/O exception  */ ;/**  * Adds an operation to the transaction log.  *  * @param operation the operation to add  * @return the location of the operation in the translog  * @throws IOException if adding the operation to the translog resulted in an I/O exception  */ public Location add(final Operation operation) throws IOException {     final ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(bigArrays).     try {         final long start = out.position().         out.skip(Integer.BYTES).         writeOperationNoSize(new BufferedChecksumStreamOutput(out), operation).         final long end = out.position().         final int operationSize = (int) (end - Integer.BYTES - start).         out.seek(start).         out.writeInt(operationSize).         out.seek(end).         final ReleasablePagedBytesReference bytes = out.bytes().         try (ReleasableLock ignored = readLock.acquire()) {             ensureOpen().             if (operation.primaryTerm() > current.getPrimaryTerm()) {                 assert false : "Operation term is newer than the current term. " + "current term[" + current.getPrimaryTerm() + "], operation term[" + operation + "]".                 throw new IllegalArgumentException("Operation term is newer than the current term. " + "current term[" + current.getPrimaryTerm() + "], operation term[" + operation + "]").             }             return current.add(bytes, operation.seqNo()).         }     } catch (final AlreadyClosedException | IOException ex) {         closeOnTragicEvent(ex).         throw ex.     } catch (final Exception ex) {         closeOnTragicEvent(ex).         throw new TranslogException(shardId, "Failed to write operation [" + operation + "]", ex).     } finally {         Releasables.close(out).     } }
true;public;0;5;/**  * Tests whether or not the translog generation should be rolled to a new generation. This test  * is based on the size of the current generation compared to the configured generation  * threshold size.  *  * @return {@code true} if the current generation should be rolled to a new generation  */ ;/**  * Tests whether or not the translog generation should be rolled to a new generation. This test  * is based on the size of the current generation compared to the configured generation  * threshold size.  *  * @return {@code true} if the current generation should be rolled to a new generation  */ public boolean shouldRollGeneration() {     final long size = this.current.sizeInBytes().     final long threshold = this.indexSettings.getGenerationThresholdSize().getBytes().     return size > threshold. }
true;public;0;10;/**  * The a {@linkplain Location} that will sort after the {@linkplain Location} returned by the last write but before any locations which  * can be returned by the next write.  */ ;/**  * The a {@linkplain Location} that will sort after the {@linkplain Location} returned by the last write but before any locations which  * can be returned by the next write.  */ public Location getLastWriteLocation() {     try (ReleasableLock lock = readLock.acquire()) {         /*              * We use position = current - 1 and size = Integer.MAX_VALUE here instead of position current and size = 0 for two reasons:              * 1. Translog.Location's compareTo doesn't actually pay attention to size even though it's equals method does.              * 2. It feels more right to return a *position* that is before the next write's position rather than rely on the size.              */         return new Location(current.generation, current.sizeInBytes() - 1, Integer.MAX_VALUE).     } }
true;public;0;5;/**  * The last synced checkpoint for this translog.  *  * @return the last synced checkpoint  */ ;/**  * The last synced checkpoint for this translog.  *  * @return the last synced checkpoint  */ public long getLastSyncedGlobalCheckpoint() {     try (ReleasableLock ignored = readLock.acquire()) {         return current.getLastSyncedCheckpoint().globalCheckpoint.     } }
true;public;0;5;/**  * Snapshots the current transaction log allowing to safely iterate over the snapshot.  * Snapshots are fixed in time and will not be updated with future operations.  */ ;/**  * Snapshots the current transaction log allowing to safely iterate over the snapshot.  * Snapshots are fixed in time and will not be updated with future operations.  */ public Snapshot newSnapshot() throws IOException {     try (ReleasableLock ignored = readLock.acquire()) {         return newSnapshotFromGen(new TranslogGeneration(translogUUID, getMinFileGeneration()), Long.MAX_VALUE).     } }
false;public;2;19;;public Snapshot newSnapshotFromGen(TranslogGeneration fromGeneration, long upToSeqNo) throws IOException {     try (ReleasableLock ignored = readLock.acquire()) {         ensureOpen().         final long fromFileGen = fromGeneration.translogFileGeneration.         if (fromFileGen < getMinFileGeneration()) {             throw new IllegalArgumentException("requested snapshot generation [" + fromFileGen + "] is not available. " + "Min referenced generation is [" + getMinFileGeneration() + "]").         }         TranslogSnapshot[] snapshots = Stream.concat(readers.stream(), Stream.of(current)).filter(reader -> reader.getGeneration() >= fromFileGen && reader.getCheckpoint().minSeqNo <= upToSeqNo).map(BaseTranslogReader::newSnapshot).toArray(TranslogSnapshot[]::new).         final Snapshot snapshot = newMultiSnapshot(snapshots).         if (upToSeqNo == Long.MAX_VALUE) {             return snapshot.         } else {             return new SeqNoFilterSnapshot(snapshot, Long.MIN_VALUE, upToSeqNo).         }     } }
true;public;1;25;/**  * Reads and returns the operation from the given location if the generation it references is still available. Otherwise  * this method will return <code>null</code>.  */ ;/**  * Reads and returns the operation from the given location if the generation it references is still available. Otherwise  * this method will return <code>null</code>.  */ public Operation readOperation(Location location) throws IOException {     try (ReleasableLock ignored = readLock.acquire()) {         ensureOpen().         if (location.generation < getMinFileGeneration()) {             return null.         }         if (current.generation == location.generation) {             // if they are still in RAM and we are reading onto that position             return current.read(location).         } else {             // read backwards - it's likely we need to read on that is recent             for (int i = readers.size() - 1. i >= 0. i--) {                 TranslogReader translogReader = readers.get(i).                 if (translogReader.generation == location.generation) {                     return translogReader.read(location).                 }             }         }     } catch (final Exception ex) {         closeOnTragicEvent(ex).         throw ex.     }     return null. }
false;public;1;8;;public Snapshot newSnapshotFromMinSeqNo(long minSeqNo) throws IOException {     try (ReleasableLock ignored = readLock.acquire()) {         ensureOpen().         TranslogSnapshot[] snapshots = readersAboveMinSeqNo(minSeqNo).map(BaseTranslogReader::newSnapshot).toArray(TranslogSnapshot[]::new).         return newMultiSnapshot(snapshots).     } }
false;private;1;20;;private Snapshot newMultiSnapshot(TranslogSnapshot[] snapshots) throws IOException {     final Closeable onClose.     if (snapshots.length == 0) {         onClose = () -> {         }.     } else {         assert Arrays.stream(snapshots).map(BaseTranslogReader::getGeneration).min(Long::compareTo).get() == snapshots[0].generation : "first reader generation of " + snapshots + " is not the smallest".         onClose = acquireTranslogGenFromDeletionPolicy(snapshots[0].generation).     }     boolean success = false.     try {         Snapshot result = new MultiSnapshot(snapshots, onClose).         success = true.         return result.     } finally {         if (success == false) {             onClose.close().         }     } }
false;private;1;10;;private Stream<? extends BaseTranslogReader> readersAboveMinSeqNo(long minSeqNo) {     assert readLock.isHeldByCurrentThread() || writeLock.isHeldByCurrentThread() : "callers of readersAboveMinSeqNo must hold a lock: readLock [" + readLock.isHeldByCurrentThread() + "], writeLock [" + readLock.isHeldByCurrentThread() + "]".     return Stream.concat(readers.stream(), Stream.of(current)).filter(reader -> {         final long maxSeqNo = reader.getCheckpoint().maxSeqNo.         return maxSeqNo == SequenceNumbers.UNASSIGNED_SEQ_NO || maxSeqNo >= minSeqNo.     }). }
true;public;0;7;/**  * Acquires a lock on the translog files, preventing them from being trimmed  */ ;/**  * Acquires a lock on the translog files, preventing them from being trimmed  */ public Closeable acquireRetentionLock() {     try (ReleasableLock lock = readLock.acquire()) {         ensureOpen().         final long viewGen = getMinFileGeneration().         return acquireTranslogGenFromDeletionPolicy(viewGen).     } }
false;private;1;11;;private Closeable acquireTranslogGenFromDeletionPolicy(long viewGen) {     Releasable toClose = deletionPolicy.acquireTranslogGen(viewGen).     return () -> {         try {             toClose.close().         } finally {             trimUnreferencedReaders().             closeFilesIfNoPendingRetentionLocks().         }     }. }
true;public;0;10;/**  * Sync's the translog.  */ ;/**  * Sync's the translog.  */ public void sync() throws IOException {     try (ReleasableLock lock = readLock.acquire()) {         if (closed.get() == false) {             current.sync().         }     } catch (final Exception ex) {         closeOnTragicEvent(ex).         throw ex.     } }
true;public;0;5;/**  *  Returns <code>true</code> if an fsync is required to ensure durability of the translogs operations or it's metadata.  */ ;/**  *  Returns <code>true</code> if an fsync is required to ensure durability of the translogs operations or it's metadata.  */ public boolean syncNeeded() {     try (ReleasableLock lock = readLock.acquire()) {         return current.syncNeeded().     } }
true;public,static;1;3;/**  * package private for testing  */ ;/**  * package private for testing  */ public static String getFilename(long generation) {     return TRANSLOG_FILE_PREFIX + generation + TRANSLOG_FILE_SUFFIX. }
false;static;1;3;;static String getCommitCheckpointFileName(long generation) {     return TRANSLOG_FILE_PREFIX + generation + CHECKPOINT_SUFFIX. }
true;public;2;32;/**  * Trims translog for terms of files below <code>belowTerm</code> and seq# above <code>aboveSeqNo</code>.  * Effectively it moves max visible seq# {@link Checkpoint#trimmedAboveSeqNo} therefore {@link TranslogSnapshot} skips those operations.  */ ;/**  * Trims translog for terms of files below <code>belowTerm</code> and seq# above <code>aboveSeqNo</code>.  * Effectively it moves max visible seq# {@link Checkpoint#trimmedAboveSeqNo} therefore {@link TranslogSnapshot} skips those operations.  */ public void trimOperations(long belowTerm, long aboveSeqNo) throws IOException {     assert aboveSeqNo >= SequenceNumbers.NO_OPS_PERFORMED : "aboveSeqNo has to a valid sequence number".     try (ReleasableLock lock = writeLock.acquire()) {         ensureOpen().         if (current.getPrimaryTerm() < belowTerm) {             throw new IllegalArgumentException("Trimming the translog can only be done for terms lower than the current one. " + "Trim requested for term [ " + belowTerm + " ] , current is [ " + current.getPrimaryTerm() + " ]").         }         // we assume that the current translog generation doesn't have trimmable ops. Verify that.         assert current.assertNoSeqAbove(belowTerm, aboveSeqNo).         // update all existed ones (if it is necessary) as checkpoint and reader are immutable         final List<TranslogReader> newReaders = new ArrayList<>(readers.size()).         try {             for (TranslogReader reader : readers) {                 final TranslogReader newReader = reader.getPrimaryTerm() < belowTerm ? reader.closeIntoTrimmedReader(aboveSeqNo, getChannelFactory()) : reader.                 newReaders.add(newReader).             }         } catch (IOException e) {             IOUtils.closeWhileHandlingException(newReaders).             tragedy.setTragicException(e).             closeOnTragicEvent(e).             throw e.         }         this.readers.clear().         this.readers.addAll(newReaders).     } }
true;public;1;12;/**  * Ensures that the given location has be synced / written to the underlying storage.  *  * @return Returns <code>true</code> iff this call caused an actual sync operation otherwise <code>false</code>  */ ;/**  * Ensures that the given location has be synced / written to the underlying storage.  *  * @return Returns <code>true</code> iff this call caused an actual sync operation otherwise <code>false</code>  */ public boolean ensureSynced(Location location) throws IOException {     try (ReleasableLock lock = readLock.acquire()) {         if (location.generation == current.getGeneration()) {             // if we have a new one it's already synced             ensureOpen().             return current.syncUpTo(location.translogLocation + location.size).         }     } catch (final Exception ex) {         closeOnTragicEvent(ex).         throw ex.     }     return false. }
true;public;1;10;/**  * Ensures that all locations in the given stream have been synced / written to the underlying storage.  * This method allows for internal optimization to minimize the amount of fsync operations if multiple  * locations must be synced.  *  * @return Returns <code>true</code> iff this call caused an actual sync operation otherwise <code>false</code>  */ ;/**  * Ensures that all locations in the given stream have been synced / written to the underlying storage.  * This method allows for internal optimization to minimize the amount of fsync operations if multiple  * locations must be synced.  *  * @return Returns <code>true</code> iff this call caused an actual sync operation otherwise <code>false</code>  */ public boolean ensureSynced(Stream<Location> locations) throws IOException {     final Optional<Location> max = locations.max(Location::compareTo).     // locations implicitly     if (max.isPresent()) {         return ensureSynced(max.get()).     } else {         return false.     } }
true;protected;1;18;/**  * Closes the translog if the current translog writer experienced a tragic exception.  *  * Note that in case this thread closes the translog it must not already be holding a read lock on the translog as it will acquire a  * write lock in the course of closing the translog  *  * @param ex if an exception occurs closing the translog, it will be suppressed into the provided exception  */ ;/**  * Closes the translog if the current translog writer experienced a tragic exception.  *  * Note that in case this thread closes the translog it must not already be holding a read lock on the translog as it will acquire a  * write lock in the course of closing the translog  *  * @param ex if an exception occurs closing the translog, it will be suppressed into the provided exception  */ protected void closeOnTragicEvent(final Exception ex) {     // we can not hold a read lock here because closing will attempt to obtain a write lock and that would result in self-deadlock     assert readLock.isHeldByCurrentThread() == false : Thread.currentThread().getName().     if (tragedy.get() != null) {         try {             close().         } catch (final AlreadyClosedException inner) {         /*                  * Don't do anything in this case. The AlreadyClosedException comes from TranslogWriter and we should not add it as                  * suppressed because it will contain the provided exception as its cause. See also                  * https://github.com/elastic/elasticsearch/issues/15941.                  */         } catch (final Exception inner) {             assert ex != inner.getCause().             ex.addSuppressed(inner).         }     } }
true;public;0;8;/**  * return stats  */ ;/**  * return stats  */ public TranslogStats stats() {     // acquire lock to make the two numbers roughly consistent (no file change half way)     try (ReleasableLock lock = readLock.acquire()) {         final long uncommittedGen = deletionPolicy.getTranslogGenerationOfLastCommit().         return new TranslogStats(totalOperations(), sizeInBytes(), totalOperationsByMinGen(uncommittedGen), sizeInBytesByMinGen(uncommittedGen), earliestLastModifiedAge()).     } }
false;public;0;3;;public TranslogConfig getConfig() {     return config. }
true;public;0;3;// public for testing ;// public for testing public TranslogDeletionPolicy getDeletionPolicy() {     return deletionPolicy. }
false;public;0;3;;public String toString() {     return "[generation: " + generation + ", location: " + translogLocation + ", size: " + size + "]". }
false;public;1;7;;@Override public int compareTo(Location o) {     if (generation == o.generation) {         return Long.compare(translogLocation, o.translogLocation).     }     return Long.compare(generation, o.generation). }
false;public;1;20;;@Override public boolean equals(Object o) {     if (this == o) {         return true.     }     if (o == null || getClass() != o.getClass()) {         return false.     }     Location location = (Location) o.     if (generation != location.generation) {         return false.     }     if (translogLocation != location.translogLocation) {         return false.     }     return size == location.size. }
false;public;0;7;;@Override public int hashCode() {     int result = Long.hashCode(generation).     result = 31 * result + Long.hashCode(translogLocation).     result = 31 * result + size.     return result. }
true;;0;1;/**  * The total estimated number of operations in the snapshot.  */ ;/**  * The total estimated number of operations in the snapshot.  */ int totalOperations().
true;default;0;3;/**  * The number of operations have been skipped (overridden or trimmed) in the snapshot so far.  */ ;/**  * The number of operations have been skipped (overridden or trimmed) in the snapshot so far.  */ default int skippedOperations() {     return 0. }
true;default;0;3;/**  * The number of operations have been overridden (eg. superseded) in the snapshot so far.  * If two operations have the same sequence number, the operation with a lower term will be overridden by the operation  * with a higher term. Unlike {@link #totalOperations()}, this value is updated each time after {@link #next()}) is called.  */ ;/**  * The number of operations have been overridden (eg. superseded) in the snapshot so far.  * If two operations have the same sequence number, the operation with a lower term will be overridden by the operation  * with a higher term. Unlike {@link #totalOperations()}, this value is updated each time after {@link #next()}) is called.  */ default int overriddenOperations() {     return 0. }
true;;0;1;/**  * Returns the next operation in the snapshot or <code>null</code> if we reached the end.  */ ;/**  * Returns the next operation in the snapshot or <code>null</code> if we reached the end.  */ Translog.Operation next() throws IOException.
false;public;0;4;;@Override public int totalOperations() {     return delegate.totalOperations(). }
false;public;0;4;;@Override public int skippedOperations() {     return filteredOpsCount + delegate.skippedOperations(). }
false;public;0;4;;@Override public int overriddenOperations() {     return delegate.overriddenOperations(). }
false;public;0;12;;@Override public Operation next() throws IOException {     Translog.Operation op.     while ((op = delegate.next()) != null) {         if (fromSeqNo <= op.seqNo() && op.seqNo() <= toSeqNo) {             return op.         } else {             filteredOpsCount++.         }     }     return null. }
false;public;0;4;;@Override public void close() throws IOException {     delegate.close(). }
false;public;0;3;;public byte id() {     return this.id. }
false;public,static;1;14;;public static Type fromId(byte id) {     switch(id) {         case 1:             return CREATE.         case 2:             return INDEX.         case 3:             return DELETE.         case 4:             return NO_OP.         default:             throw new IllegalArgumentException("no type mapped for [" + id + "]").     } }
false;;0;1;;Type opType().
false;;0;1;;long estimateSize().
false;;0;1;;Source getSource().
false;;0;1;;long seqNo().
false;;0;1;;long primaryTerm().
true;static;1;15;/**  * Reads the type and the operation from the given stream. The operation must be written with  * {@link Operation#writeOperation(StreamOutput, Operation)}  */ ;/**  * Reads the type and the operation from the given stream. The operation must be written with  * {@link Operation#writeOperation(StreamOutput, Operation)}  */ static Operation readOperation(final StreamInput input) throws IOException {     final Translog.Operation.Type type = Translog.Operation.Type.fromId(input.readByte()).     switch(type) {         case CREATE:         // the de-serialization logic in Index was identical to that of Create when create was deprecated         case INDEX:             return new Index(input).         case DELETE:             return new Delete(input).         case NO_OP:             return new NoOp(input).         default:             throw new AssertionError("no case for [" + type + "]").     } }
true;static;2;18;/**  * Writes the type and translog operation to the given stream  */ ;/**  * Writes the type and translog operation to the given stream  */ static void writeOperation(final StreamOutput output, final Operation operation) throws IOException {     output.writeByte(operation.opType().id()).     switch(operation.opType()) {         case CREATE:         // the serialization logic in Index was identical to that of Create when create was deprecated         case INDEX:             ((Index) operation).write(output).             break.         case DELETE:             ((Delete) operation).write(output).             break.         case NO_OP:             ((NoOp) operation).write(output).             break.         default:             throw new AssertionError("no case for [" + operation.opType() + "]").     } }
false;public;0;4;;@Override public Type opType() {     return Type.INDEX. }
false;public;0;4;;@Override public long estimateSize() {     return ((id.length() + type.length()) * 2) + source.length() + 12. }
false;public;0;3;;public String type() {     return this.type. }
false;public;0;3;;public String id() {     return this.id. }
false;public;0;3;;public String routing() {     return this.routing. }
false;public;0;3;;public BytesReference source() {     return this.source. }
false;public;0;4;;@Override public long seqNo() {     return seqNo. }
false;public;0;4;;@Override public long primaryTerm() {     return primaryTerm. }
false;public;0;3;;public long version() {     return this.version. }
false;public;0;4;;@Override public Source getSource() {     return new Source(source, routing). }
false;private;1;18;;private void write(final StreamOutput out) throws IOException {     final int format = out.getVersion().onOrAfter(Version.V_7_0_0) ? SERIALIZATION_FORMAT : FORMAT_6_0.     out.writeVInt(format).     out.writeString(id).     out.writeString(type).     out.writeBytesReference(source).     out.writeOptionalString(routing).     if (format < FORMAT_NO_PARENT) {         // _parent         out.writeOptionalString(null).     }     out.writeLong(version).     if (format < FORMAT_NO_VERSION_TYPE) {         out.writeByte(VersionType.EXTERNAL.getValue()).     }     out.writeLong(autoGeneratedIdTimestamp).     out.writeLong(seqNo).     out.writeLong(primaryTerm). }
false;public;1;26;;@Override public boolean equals(Object o) {     if (this == o) {         return true.     }     if (o == null || getClass() != o.getClass()) {         return false.     }     Index index = (Index) o.     if (version != index.version || seqNo != index.seqNo || primaryTerm != index.primaryTerm || id.equals(index.id) == false || type.equals(index.type) == false || autoGeneratedIdTimestamp != index.autoGeneratedIdTimestamp || source.equals(index.source) == false) {         return false.     }     if (routing != null ? !routing.equals(index.routing) : index.routing != null) {         return false.     }     return true. }
false;public;0;12;;@Override public int hashCode() {     int result = id.hashCode().     result = 31 * result + type.hashCode().     result = 31 * result + Long.hashCode(seqNo).     result = 31 * result + Long.hashCode(primaryTerm).     result = 31 * result + Long.hashCode(version).     result = 31 * result + source.hashCode().     result = 31 * result + (routing != null ? routing.hashCode() : 0).     result = 31 * result + Long.hashCode(autoGeneratedIdTimestamp).     return result. }
false;public;0;11;;@Override public String toString() {     return "Index{" + "id='" + id + '\'' + ", type='" + type + '\'' + ", seqNo=" + seqNo + ", primaryTerm=" + primaryTerm + ", version=" + version + ", autoGeneratedIdTimestamp=" + autoGeneratedIdTimestamp + '}'. }
false;public;0;3;;public long getAutoGeneratedIdTimestamp() {     return autoGeneratedIdTimestamp. }
false;public;0;4;;@Override public Type opType() {     return Type.DELETE. }
false;public;0;4;;@Override public long estimateSize() {     return ((uid.field().length() + uid.text().length()) * 2) + 20. }
false;public;0;3;;public String type() {     return type. }
false;public;0;3;;public String id() {     return id. }
false;public;0;3;;public Term uid() {     return this.uid. }
false;public;0;4;;@Override public long seqNo() {     return seqNo. }
false;public;0;4;;@Override public long primaryTerm() {     return primaryTerm. }
false;public;0;3;;public long version() {     return this.version. }
false;public;0;4;;@Override public Source getSource() {     throw new IllegalStateException("trying to read doc source from delete operation"). }
false;private;1;14;;private void write(final StreamOutput out) throws IOException {     final int format = out.getVersion().onOrAfter(Version.V_7_0_0) ? SERIALIZATION_FORMAT : FORMAT_6_0.     out.writeVInt(format).     out.writeString(type).     out.writeString(id).     out.writeString(uid.field()).     out.writeBytesRef(uid.bytes()).     out.writeLong(version).     if (format < FORMAT_NO_VERSION_TYPE) {         out.writeByte(VersionType.EXTERNAL.getValue()).     }     out.writeLong(seqNo).     out.writeLong(primaryTerm). }
false;public;1;16;;@Override public boolean equals(Object o) {     if (this == o) {         return true.     }     if (o == null || getClass() != o.getClass()) {         return false.     }     Delete delete = (Delete) o.     return version == delete.version && seqNo == delete.seqNo && primaryTerm == delete.primaryTerm && uid.equals(delete.uid). }
false;public;0;8;;@Override public int hashCode() {     int result = uid.hashCode().     result = 31 * result + Long.hashCode(seqNo).     result = 31 * result + Long.hashCode(primaryTerm).     result = 31 * result + Long.hashCode(version).     return result. }
false;public;0;9;;@Override public String toString() {     return "Delete{" + "uid=" + uid + ", seqNo=" + seqNo + ", primaryTerm=" + primaryTerm + ", version=" + version + '}'. }
false;public;0;4;;@Override public long seqNo() {     return seqNo. }
false;public;0;4;;@Override public long primaryTerm() {     return primaryTerm. }
false;public;0;3;;public String reason() {     return reason. }
false;private;1;5;;private void write(final StreamOutput out) throws IOException {     out.writeLong(seqNo).     out.writeLong(primaryTerm).     out.writeString(reason). }
false;public;0;4;;@Override public Type opType() {     return Type.NO_OP. }
false;public;0;4;;@Override public long estimateSize() {     return 2 * reason.length() + 2 * Long.BYTES. }
false;public;0;4;;@Override public Source getSource() {     throw new UnsupportedOperationException("source does not exist for a no-op"). }
false;public;1;11;;@Override public boolean equals(Object obj) {     if (this == obj) {         return true.     }     if (obj == null || getClass() != obj.getClass()) {         return false.     }     final NoOp that = (NoOp) obj.     return seqNo == that.seqNo && primaryTerm == that.primaryTerm && reason.equals(that.reason). }
false;public;0;4;;@Override public int hashCode() {     return 31 * 31 * 31 + 31 * 31 * Long.hashCode(seqNo) + 31 * Long.hashCode(primaryTerm) + reason().hashCode(). }
false;public;0;8;;@Override public String toString() {     return "NoOp{" + "seqNo=" + seqNo + ", primaryTerm=" + primaryTerm + ", reason='" + reason + '\'' + '}'. }
false;static;1;9;;static void verifyChecksum(BufferedChecksumStreamInput in) throws IOException {     // This absolutely must come first, or else reading the checksum becomes part of the checksum     long expectedChecksum = in.getChecksum().     long readChecksum = Integer.toUnsignedLong(in.readInt()).     if (readChecksum != expectedChecksum) {         throw new TranslogCorruptedException(in.getSource(), "checksum verification failed - expected: 0x" + Long.toHexString(expectedChecksum) + ", got: 0x" + Long.toHexString(readChecksum)).     } }
true;public,static;2;9;/**  * Reads a list of operations written with {@link #writeOperations(StreamOutput, List)}  */ ;/**  * Reads a list of operations written with {@link #writeOperations(StreamOutput, List)}  */ public static List<Operation> readOperations(StreamInput input, String source) throws IOException {     ArrayList<Operation> operations = new ArrayList<>().     int numOps = input.readInt().     final BufferedChecksumStreamInput checksumStreamInput = new BufferedChecksumStreamInput(input, source).     for (int i = 0. i < numOps. i++) {         operations.add(readOperation(checksumStreamInput)).     }     return operations. }
false;static;1;25;;static Translog.Operation readOperation(BufferedChecksumStreamInput in) throws IOException {     final Translog.Operation operation.     try {         final int opSize = in.readInt().         if (opSize < 4) {             // 4byte for the checksum             throw new TranslogCorruptedException(in.getSource(), "operation size must be at least 4 but was: " + opSize).         }         // size is not part of the checksum!         in.resetDigest().         if (in.markSupported()) {             // if we can we validate the checksum first             // we are sometimes called when mark is not supported this is the case when             // we are sending translogs across the network with LZ4 compression enabled - currently there is no way s             // to prevent this unfortunately.             in.mark(opSize).             in.skip(opSize - 4).             verifyChecksum(in).             in.reset().         }         operation = Translog.Operation.readOperation(in).         verifyChecksum(in).     } catch (EOFException e) {         throw new TruncatedTranslogException(in.getSource(), "reached premature end of file, translog is truncated", e).     }     return operation. }
true;public,static;2;23;/**  * Writes all operations in the given iterable to the given output stream including the size of the array  * use {@link #readOperations(StreamInput, String)} to read it back.  */ ;/**  * Writes all operations in the given iterable to the given output stream including the size of the array  * use {@link #readOperations(StreamInput, String)} to read it back.  */ public static void writeOperations(StreamOutput outStream, List<Operation> toWrite) throws IOException {     final ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(BigArrays.NON_RECYCLING_INSTANCE).     try {         outStream.writeInt(toWrite.size()).         final BufferedChecksumStreamOutput checksumStreamOutput = new BufferedChecksumStreamOutput(out).         for (Operation op : toWrite) {             out.reset().             final long start = out.position().             out.skip(Integer.BYTES).             writeOperationNoSize(checksumStreamOutput, op).             long end = out.position().             int operationSize = (int) (out.position() - Integer.BYTES - start).             out.seek(start).             out.writeInt(operationSize).             out.seek(end).             ReleasablePagedBytesReference bytes = out.bytes().             bytes.writeTo(outStream).         }     } finally {         Releasables.close(out).     } }
false;public,static;2;9;;public static void writeOperationNoSize(BufferedChecksumStreamOutput out, Translog.Operation op) throws IOException {     // This BufferedChecksumStreamOutput remains unclosed on purpose,     // because closing it closes the underlying stream, which we don't     // want to do here.     out.resetDigest().     Translog.Operation.writeOperation(out, op).     long checksum = out.getChecksum().     out.writeInt((int) checksum). }
true;public;1;17;/**  * Gets the minimum generation that could contain any sequence number after the specified sequence number, or the current generation if  * there is no generation that could any such sequence number.  *  * @param seqNo the sequence number  * @return the minimum generation for the sequence number  */ ;/**  * Gets the minimum generation that could contain any sequence number after the specified sequence number, or the current generation if  * there is no generation that could any such sequence number.  *  * @param seqNo the sequence number  * @return the minimum generation for the sequence number  */ public TranslogGeneration getMinGenerationForSeqNo(final long seqNo) {     try (ReleasableLock ignored = readLock.acquire()) {         /*              * When flushing, the engine will ask the translog for the minimum generation that could contain any sequence number after the              * local checkpoint. Immediately after flushing, there will be no such generation, so this minimum generation in this case will              * be the current translog generation as we do not need any prior generations to have a complete history up to the current local              * checkpoint.              */         long minTranslogFileGeneration = this.currentFileGeneration().         for (final TranslogReader reader : readers) {             if (seqNo <= reader.getCheckpoint().maxSeqNo) {                 minTranslogFileGeneration = Math.min(minTranslogFileGeneration, reader.getGeneration()).             }         }         return new TranslogGeneration(translogUUID, minTranslogFileGeneration).     } }
true;public;0;17;/**  * Roll the current translog generation into a new generation. This does not commit the  * translog.  *  * @throws IOException if an I/O exception occurred during any file operations  */ ;/**  * Roll the current translog generation into a new generation. This does not commit the  * translog.  *  * @throws IOException if an I/O exception occurred during any file operations  */ public void rollGeneration() throws IOException {     try (Releasable ignored = writeLock.acquire()) {         try {             final TranslogReader reader = current.closeIntoReader().             readers.add(reader).             assert Checkpoint.read(location.resolve(CHECKPOINT_FILE_NAME)).generation == current.getGeneration().             copyCheckpointTo(location.resolve(getCommitCheckpointFileName(current.getGeneration()))).             // create a new translog file. this will sync it and update the checkpoint data.             current = createWriter(current.getGeneration() + 1).             logger.trace("current translog set to [{}]", current.getGeneration()).         } catch (final Exception e) {             tragedy.setTragicException(e).             closeOnTragicEvent(e).             throw e.         }     } }
true;public;0;39;/**  * Trims unreferenced translog generations by asking {@link TranslogDeletionPolicy} for the minimum  * required generation  */ ;/**  * Trims unreferenced translog generations by asking {@link TranslogDeletionPolicy} for the minimum  * required generation  */ public void trimUnreferencedReaders() throws IOException {     try (ReleasableLock ignored = writeLock.acquire()) {         if (closed.get()) {             // we're shutdown potentially on some tragic event, don't delete anything             return.         }         long minReferencedGen = deletionPolicy.minTranslogGenRequired(readers, current).         assert minReferencedGen >= getMinFileGeneration() : "deletion policy requires a minReferenceGen of [" + minReferencedGen + "] but the lowest gen available is [" + getMinFileGeneration() + "]".         assert minReferencedGen <= currentFileGeneration() : "deletion policy requires a minReferenceGen of [" + minReferencedGen + "] which is higher than the current generation [" + currentFileGeneration() + "]".         for (Iterator<TranslogReader> iterator = readers.iterator(). iterator.hasNext(). ) {             TranslogReader reader = iterator.next().             if (reader.getGeneration() >= minReferencedGen) {                 break.             }             iterator.remove().             IOUtils.closeWhileHandlingException(reader).             final Path translogPath = reader.path().             logger.trace("delete translog file [{}], not referenced and not current anymore", translogPath).             // The checkpoint is used when opening the translog to know which files should be recovered from.             // We now update the checkpoint to ignore the file we are going to remove.             // Note that there is a provision in recoverFromFiles to allow for the case where we synced the checkpoint             // but crashed before we could delete the file.             current.sync().             deleteReaderFiles(reader).         }         assert readers.isEmpty() == false || current.generation == minReferencedGen : "all readers were cleaned but the minReferenceGen [" + minReferencedGen + "] is not the current writer's gen [" + current.generation + "]".     } catch (final Exception ex) {         closeOnTragicEvent(ex).         throw ex.     } }
true;;1;4;/**  * deletes all files associated with a reader. package-private to be able to simulate node failures at this point  */ ;/**  * deletes all files associated with a reader. package-private to be able to simulate node failures at this point  */ void deleteReaderFiles(TranslogReader reader) {     IOUtils.deleteFilesIgnoringExceptions(reader.path(), reader.path().resolveSibling(getCommitCheckpointFileName(reader.getGeneration()))). }
false;;0;10;;void closeFilesIfNoPendingRetentionLocks() throws IOException {     try (ReleasableLock ignored = writeLock.acquire()) {         if (closed.get() && deletionPolicy.pendingTranslogRefCount() == 0) {             logger.trace("closing files. translog is closed and there are no pending retention locks").             ArrayList<Closeable> toClose = new ArrayList<>(readers).             toClose.add(current).             IOUtils.close(toClose).         }     } }
true;public;0;5;/**  * Returns the current generation of this translog. This corresponds to the latest uncommitted translog generation  */ ;/**  * Returns the current generation of this translog. This corresponds to the latest uncommitted translog generation  */ public TranslogGeneration getGeneration() {     try (ReleasableLock lock = writeLock.acquire()) {         return new TranslogGeneration(translogUUID, currentFileGeneration()).     } }
true;public;1;12;/**  * Returns <code>true</code> iff the given generation is the current generation of this translog  */ ;/**  * Returns <code>true</code> iff the given generation is the current generation of this translog  */ public boolean isCurrent(TranslogGeneration generation) {     try (ReleasableLock lock = writeLock.acquire()) {         if (generation != null) {             if (generation.translogUUID.equals(translogUUID) == false) {                 throw new IllegalArgumentException("commit belongs to a different translog: " + generation.translogUUID + " vs. " + translogUUID).             }             return generation.translogFileGeneration == currentFileGeneration().         }     }     return false. }
false;;0;3;;long getFirstOperationPosition() {     // for testing     return current.getFirstOperationOffset(). }
false;private;0;5;;private void ensureOpen() {     if (closed.get()) {         throw new AlreadyClosedException("translog is already closed", tragedy.get()).     } }
false;;0;3;;ChannelFactory getChannelFactory() {     return FileChannel::open. }
true;public;0;3;/**  * If this {@code Translog} was closed as a side-effect of a tragic exception,  * e.g. disk full while flushing a new segment, this returns the root cause exception.  * Otherwise (no tragic exception has occurred) it returns null.  */ ;/**  * If this {@code Translog} was closed as a side-effect of a tragic exception,  * e.g. disk full while flushing a new segment, this returns the root cause exception.  * Otherwise (no tragic exception has occurred) it returns null.  */ public Exception getTragicException() {     return tragedy.get(). }
true;static;1;3;/**  * Reads and returns the current checkpoint  */ ;/**  * Reads and returns the current checkpoint  */ static Checkpoint readCheckpoint(final Path location) throws IOException {     return Checkpoint.read(location.resolve(CHECKPOINT_FILE_NAME)). }
true;public,static;2;4;/**  * Reads the sequence numbers global checkpoint from the translog checkpoint.  * This ensures that the translogUUID from this translog matches with the provided translogUUID.  *  * @param location the location of the translog  * @return the global checkpoint  * @throws IOException                if an I/O exception occurred reading the checkpoint  * @throws TranslogCorruptedException if the translog is corrupted or mismatched with the given uuid  */ ;/**  * Reads the sequence numbers global checkpoint from the translog checkpoint.  * This ensures that the translogUUID from this translog matches with the provided translogUUID.  *  * @param location the location of the translog  * @return the global checkpoint  * @throws IOException                if an I/O exception occurred reading the checkpoint  * @throws TranslogCorruptedException if the translog is corrupted or mismatched with the given uuid  */ public static long readGlobalCheckpoint(final Path location, final String expectedTranslogUUID) throws IOException {     final Checkpoint checkpoint = readCheckpoint(location, expectedTranslogUUID).     return checkpoint.globalCheckpoint. }
false;private,static;2;13;;private static Checkpoint readCheckpoint(Path location, String expectedTranslogUUID) throws IOException {     final Checkpoint checkpoint = readCheckpoint(location).     // We need to open at least one translog header to validate the translogUUID.     final Path translogFile = location.resolve(getFilename(checkpoint.generation)).     try (FileChannel channel = FileChannel.open(translogFile, StandardOpenOption.READ)) {         TranslogHeader.read(expectedTranslogUUID, translogFile, channel).     } catch (TranslogCorruptedException ex) {         // just bubble up.         throw ex.     } catch (Exception ex) {         throw new TranslogCorruptedException(location.toString(), ex).     }     return checkpoint. }
true;public,static;2;4;/**  * Returns the minimum translog generation retained by the translog at the given location.  * This ensures that the translogUUID from this translog matches with the provided translogUUID.  *  * @param location the location of the translog  * @return the minimum translog generation  * @throws IOException                if an I/O exception occurred reading the checkpoint  * @throws TranslogCorruptedException if the translog is corrupted or mismatched with the given uuid  */ ;/**  * Returns the minimum translog generation retained by the translog at the given location.  * This ensures that the translogUUID from this translog matches with the provided translogUUID.  *  * @param location the location of the translog  * @return the minimum translog generation  * @throws IOException                if an I/O exception occurred reading the checkpoint  * @throws TranslogCorruptedException if the translog is corrupted or mismatched with the given uuid  */ public static long readMinTranslogGeneration(final Path location, final String expectedTranslogUUID) throws IOException {     final Checkpoint checkpoint = readCheckpoint(location, expectedTranslogUUID).     return checkpoint.minTranslogGeneration. }
true;public;0;3;/**  * Returns the translog uuid used to associate a lucene index with a translog.  */ ;/**  * Returns the translog uuid used to associate a lucene index with a translog.  */ public String getTranslogUUID() {     return translogUUID. }
true;public;0;9;/**  * Returns the max seq_no of translog operations found in this translog. Since this value is calculated based on the current  * existing readers, this value is not necessary to be the max seq_no of all operations have been stored in this translog.  */ ;/**  * Returns the max seq_no of translog operations found in this translog. Since this value is calculated based on the current  * existing readers, this value is not necessary to be the max seq_no of all operations have been stored in this translog.  */ public long getMaxSeqNo() {     try (ReleasableLock ignored = readLock.acquire()) {         ensureOpen().         final OptionalLong maxSeqNo = Stream.concat(readers.stream(), Stream.of(current)).mapToLong(reader -> reader.getCheckpoint().maxSeqNo).max().         assert maxSeqNo.isPresent() : "must have at least one translog generation".         return maxSeqNo.getAsLong().     } }
false;;0;3;;TranslogWriter getCurrent() {     return current. }
false;;0;3;;List<TranslogReader> getReaders() {     return readers. }
false;public,static;4;5;;public static String createEmptyTranslog(final Path location, final long initialGlobalCheckpoint, final ShardId shardId, final long primaryTerm) throws IOException {     final ChannelFactory channelFactory = FileChannel::open.     return createEmptyTranslog(location, initialGlobalCheckpoint, shardId, channelFactory, primaryTerm). }
false;static;5;18;;static String createEmptyTranslog(Path location, long initialGlobalCheckpoint, ShardId shardId, ChannelFactory channelFactory, long primaryTerm) throws IOException {     IOUtils.rm(location).     Files.createDirectories(location).     final Checkpoint checkpoint = Checkpoint.emptyTranslogCheckpoint(0, 1, initialGlobalCheckpoint, 1).     final Path checkpointFile = location.resolve(CHECKPOINT_FILE_NAME).     Checkpoint.write(channelFactory, checkpointFile, checkpoint, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW).     IOUtils.fsync(checkpointFile, false).     final String translogUUID = UUIDs.randomBase64UUID().     TranslogWriter writer = TranslogWriter.create(shardId, translogUUID, 1, location.resolve(getFilename(1)), channelFactory, new ByteSizeValue(10), 1, initialGlobalCheckpoint, () -> {         throw new UnsupportedOperationException().     }, () -> {         throw new UnsupportedOperationException().     }, primaryTerm, new TragicExceptionHolder()).     writer.close().     return translogUUID. }
