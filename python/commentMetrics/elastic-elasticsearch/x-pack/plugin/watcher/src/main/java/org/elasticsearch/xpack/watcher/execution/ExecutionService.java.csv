commented;modifiers;parameterAmount;loc;comment;code
false;public;0;3;;public void unPause() {     paused.set(false). }
true;public;0;4;/**  * Pause the execution of the watcher executor, and empty the state.  * Pausing means, that no new watch executions will be done unless this pausing is explicitly unset.  * This is important when watcher is stopped, so that scheduled watches do not accidentally get executed.  * This should not be used when we need to reload watcher based on some cluster state changes, then just calling  * {@link #clearExecutionsAndQueue()} is the way to go  *  * @return the number of tasks that have been removed  */ ;/**  * Pause the execution of the watcher executor, and empty the state.  * Pausing means, that no new watch executions will be done unless this pausing is explicitly unset.  * This is important when watcher is stopped, so that scheduled watches do not accidentally get executed.  * This should not be used when we need to reload watcher based on some cluster state changes, then just calling  * {@link #clearExecutionsAndQueue()} is the way to go  *  * @return the number of tasks that have been removed  */ public int pause() {     paused.set(true).     return clearExecutionsAndQueue(). }
true;public;0;5;/**  * Empty the currently queued tasks and wait for current executions to finish.  *  * @return the number of tasks that have been removed  */ ;/**  * Empty the currently queued tasks and wait for current executions to finish.  *  * @return the number of tasks that have been removed  */ public int clearExecutionsAndQueue() {     int cancelledTaskCount = executor.queue().drainTo(new ArrayList<>()).     this.clearExecutions().     return cancelledTaskCount. }
false;public;0;3;;public TimeValue defaultThrottlePeriod() {     return defaultThrottlePeriod. }
false;public;0;3;;public long executionThreadPoolQueueSize() {     return executor.queue().size(). }
false;public;0;3;;public long executionThreadPoolMaxSize() {     return executor.largestPoolSize(). }
true;;0;3;// for testing only ;// for testing only CurrentExecutions getCurrentExecutions() {     return currentExecutions.get(). }
false;public;0;9;;public List<WatchExecutionSnapshot> currentExecutions() {     List<WatchExecutionSnapshot> currentExecutions = new ArrayList<>().     for (WatchExecution watchExecution : this.currentExecutions.get()) {         currentExecutions.add(watchExecution.createSnapshot()).     }     // Lets show the longest running watch first:     currentExecutions.sort(Comparator.comparing(WatchExecutionSnapshot::executionTime)).     return currentExecutions. }
false;public;0;17;;public List<QueuedWatch> queuedWatches() {     List<Runnable> snapshot = new ArrayList<>().     executor.tasks().forEach(snapshot::add).     if (snapshot.isEmpty()) {         return Collections.emptyList().     }     List<QueuedWatch> queuedWatches = new ArrayList<>(snapshot.size()).     for (Runnable task : snapshot) {         WatchExecutionTask executionTask = (WatchExecutionTask) task.         queuedWatches.add(new QueuedWatch(executionTask.ctx)).     }     // Lets show the execution that pending the longest first:     queuedWatches.sort(Comparator.comparing(QueuedWatch::executionTime)).     return queuedWatches. }
false;;1;18;;void processEventsAsync(Iterable<TriggerEvent> events) throws Exception {     if (paused.get()) {         logger.debug("watcher execution service paused, not processing [{}] events", Iterables.size(events)).         return.     }     Tuple<List<TriggeredWatch>, List<TriggeredExecutionContext>> watchesAndContext = createTriggeredWatchesAndContext(events).     List<TriggeredWatch> triggeredWatches = watchesAndContext.v1().     triggeredWatchStore.putAll(triggeredWatches, ActionListener.wrap(response -> executeTriggeredWatches(response, watchesAndContext), e -> {         Throwable cause = ExceptionsHelper.unwrapCause(e).         if (cause instanceof EsRejectedExecutionException) {             logger.debug("failed to store watch records due to filled up watcher threadpool").         } else {             logger.warn("failed to store watch records", e).         }     })). }
false;;1;11;;void processEventsSync(Iterable<TriggerEvent> events) throws IOException {     if (paused.get()) {         logger.debug("watcher execution service paused, not processing [{}] events", Iterables.size(events)).         return.     }     Tuple<List<TriggeredWatch>, List<TriggeredExecutionContext>> watchesAndContext = createTriggeredWatchesAndContext(events).     List<TriggeredWatch> triggeredWatches = watchesAndContext.v1().     logger.debug("saving watch records [{}]", triggeredWatches.size()).     BulkResponse bulkResponse = triggeredWatchStore.putAll(triggeredWatches).     executeTriggeredWatches(bulkResponse, watchesAndContext). }
true;private;1;18;/**  * Create a tuple of triggered watches and their corresponding contexts, usable for sync and async processing  *  * @param events The iterable list of trigger events to create the two lists from  * @return       Two linked lists that contain the triggered watches and contexts  */ ;/**  * Create a tuple of triggered watches and their corresponding contexts, usable for sync and async processing  *  * @param events The iterable list of trigger events to create the two lists from  * @return       Two linked lists that contain the triggered watches and contexts  */ private Tuple<List<TriggeredWatch>, List<TriggeredExecutionContext>> createTriggeredWatchesAndContext(Iterable<TriggerEvent> events) {     final LinkedList<TriggeredWatch> triggeredWatches = new LinkedList<>().     final LinkedList<TriggeredExecutionContext> contexts = new LinkedList<>().     ZonedDateTime now = clock.instant().atZone(ZoneOffset.UTC).     for (TriggerEvent event : events) {         GetResponse response = getWatch(event.jobName()).         if (response.isExists() == false) {             logger.warn("unable to find watch [{}] in watch index, perhaps it has been deleted", event.jobName()).             continue.         }         TriggeredExecutionContext ctx = new TriggeredExecutionContext(event.jobName(), now, event, defaultThrottlePeriod).         contexts.add(ctx).         triggeredWatches.add(new TriggeredWatch(ctx.id(), event)).     }     return Tuple.tuple(triggeredWatches, contexts). }
true;private;2;11;/**  * Execute triggered watches, which have been successfully indexed into the triggered watches index  *  * @param response            The bulk response containing the response of indexing triggered watches  * @param watchesAndContext   The triggered watches and context objects needed for execution  */ ;/**  * Execute triggered watches, which have been successfully indexed into the triggered watches index  *  * @param response            The bulk response containing the response of indexing triggered watches  * @param watchesAndContext   The triggered watches and context objects needed for execution  */ private void executeTriggeredWatches(final BulkResponse response, final Tuple<List<TriggeredWatch>, List<TriggeredExecutionContext>> watchesAndContext) {     for (int i = 0. i < response.getItems().length. i++) {         BulkItemResponse itemResponse = response.getItems()[i].         if (itemResponse.isFailed()) {             logger.error("could not store triggered watch with id [{}]: [{}]", itemResponse.getId(), itemResponse.getFailureMessage()).         } else {             executeAsync(watchesAndContext.v2().get(i), watchesAndContext.v1().get(i)).         }     } }
false;public;1;65;;public WatchRecord execute(WatchExecutionContext ctx) {     ctx.setNodeId(clusterService.localNode().getId()).     WatchRecord record = null.     final String watchId = ctx.id().watchId().     try {         boolean executionAlreadyExists = currentExecutions.get().put(watchId, new WatchExecution(ctx, Thread.currentThread())).         if (executionAlreadyExists) {             logger.trace("not executing watch [{}] because it is already queued", watchId).             record = ctx.abortBeforeExecution(ExecutionState.NOT_EXECUTED_ALREADY_QUEUED, "Watch is already queued in thread pool").         } else {             try {                 ctx.ensureWatchExists(() -> {                     GetResponse resp = getWatch(watchId).                     if (resp.isExists() == false) {                         throw new ResourceNotFoundException("watch [{}] does not exist", watchId).                     }                     return parser.parseWithSecrets(watchId, true, resp.getSourceAsBytesRef(), ctx.executionTime(), XContentType.JSON, resp.getSeqNo(), resp.getPrimaryTerm()).                 }).             } catch (ResourceNotFoundException e) {                 String message = "unable to find watch for record [" + ctx.id() + "]".                 record = ctx.abortBeforeExecution(ExecutionState.NOT_EXECUTED_WATCH_MISSING, message).             } catch (Exception e) {                 record = ctx.abortFailedExecution(e).             }             if (ctx.watch() != null) {                 if (ctx.shouldBeExecuted()) {                     logger.debug("executing watch [{}]", watchId).                     record = executeInner(ctx).                     if (ctx.recordExecution()) {                         updateWatchStatus(ctx.watch()).                     }                 } else {                     logger.debug("not executing watch [{}]", watchId).                     record = ctx.abortBeforeExecution(ExecutionState.EXECUTION_NOT_NEEDED, "Watch is not active").                 }             }         }     } catch (Exception e) {         record = createWatchRecord(record, ctx, e).         logWatchRecord(ctx, e).     } finally {         if (ctx.knownWatch()) {             if (record != null && ctx.recordExecution()) {                 try {                     if (ctx.overrideRecordOnConflict()) {                         historyStore.forcePut(record).                     } else {                         historyStore.put(record).                     }                 } catch (Exception e) {                     logger.error((Supplier<?>) () -> new ParameterizedMessage("failed to update watch record [{}]", ctx.id()), e).                 // TODO log watch record in logger, when saving in history store failed, otherwise the info is gone!                 }             }             triggeredWatchStore.delete(ctx.id()).         }         currentExecutions.get().remove(watchId).         logger.debug("finished [{}]/[{}]", watchId, ctx.id()).     }     return record. }
true;public;1;26;/**  * Updates and persists the status of the given watch  *  * If the watch is missing (because it might have been deleted by the user during an execution), then this method  * does nothing and just returns without throwing an exception  */ ;/**  * Updates and persists the status of the given watch  *  * If the watch is missing (because it might have been deleted by the user during an execution), then this method  * does nothing and just returns without throwing an exception  */ public void updateWatchStatus(Watch watch) throws IOException {     // at the moment we store the status together with the watch,     // so we just need to update the watch itself     // we do not want to update the status.state field, as it might have been deactivated in-between     Map<String, String> parameters = MapBuilder.<String, String>newMapBuilder().put(Watch.INCLUDE_STATUS_KEY, "true").put(WatchStatus.INCLUDE_STATE, "false").immutableMap().     ToXContent.MapParams params = new ToXContent.MapParams(parameters).     XContentBuilder source = JsonXContent.contentBuilder().startObject().field(WatchField.STATUS.getPreferredName(), watch.status(), params).endObject().     UpdateRequest updateRequest = new UpdateRequest(Watch.INDEX, Watch.DOC_TYPE, watch.id()).     updateRequest.doc(source).     updateRequest.setIfSeqNo(watch.getSourceSeqNo()).     updateRequest.setIfPrimaryTerm(watch.getSourcePrimaryTerm()).     try (ThreadContext.StoredContext ignore = stashWithOrigin(client.threadPool().getThreadContext(), WATCHER_ORIGIN)) {         client.update(updateRequest).actionGet(indexDefaultTimeout).     } catch (DocumentMissingException e) {     // do not rethrow this exception, otherwise the watch history will contain an exception     // even though the execution might have been fine     // TODO should we really just drop this exception on the floor?     } }
false;private;3;12;;private WatchRecord createWatchRecord(WatchRecord existingRecord, WatchExecutionContext ctx, Exception e) {     // it is possible that the watch store update failed, the execution phase is finished     if (ctx.executionPhase().sealed()) {         if (existingRecord == null) {             return new WatchRecord.ExceptionWatchRecord(ctx, e).         } else {             return new WatchRecord.ExceptionWatchRecord(existingRecord, e).         }     } else {         return ctx.abortFailedExecution(e).     } }
false;private;2;8;;private void logWatchRecord(WatchExecutionContext ctx, Exception e) {     // failed watches stack traces are only logged in debug, otherwise they should be checked out in the history     if (logger.isDebugEnabled()) {         logger.debug((Supplier<?>) () -> new ParameterizedMessage("failed to execute watch [{}]", ctx.id().watchId()), e).     } else {         logger.warn("failed to execute watch [{}]", ctx.id().watchId()).     } }
true;private;2;21;/*        The execution of an watch is split into two phases:        1. the trigger part which just makes sure to store the associated watch record in the history        2. the actual processing of the watch        The reason this split is that we don't want to lose the fact watch was triggered. This way, even if the        thread pool that executes the watches is completely busy, we don't lose the fact that the watch was        triggered (it'll have its history record)     */ ;/*        The execution of an watch is split into two phases:        1. the trigger part which just makes sure to store the associated watch record in the history        2. the actual processing of the watch        The reason this split is that we don't want to lose the fact watch was triggered. This way, even if the        thread pool that executes the watches is completely busy, we don't lose the fact that the watch was        triggered (it'll have its history record)     */ private void executeAsync(WatchExecutionContext ctx, final TriggeredWatch triggeredWatch) {     try {         executor.execute(new WatchExecutionTask(ctx, () -> execute(ctx))).     } catch (EsRejectedExecutionException e) {         String message = "failed to run triggered watch [" + triggeredWatch.id() + "] due to thread pool capacity".         WatchRecord record = ctx.abortBeforeExecution(ExecutionState.THREADPOOL_REJECTION, message).         try {             if (ctx.overrideRecordOnConflict()) {                 historyStore.forcePut(record).             } else {                 historyStore.put(record).             }         } catch (Exception exc) {             logger.error((Supplier<?>) () -> new ParameterizedMessage("Error storing watch history record for watch [{}] after thread pool rejection", triggeredWatch.id()), exc).         }         triggeredWatchStore.delete(triggeredWatch.id()).     } }
false;;1;53;;WatchRecord executeInner(WatchExecutionContext ctx) {     ctx.start().     Watch watch = ctx.watch().     // input     ctx.beforeInput().     Input.Result inputResult = ctx.inputResult().     if (inputResult == null) {         inputResult = watch.input().execute(ctx, ctx.payload()).         ctx.onInputResult(inputResult).     }     if (inputResult.status() == Input.Result.Status.FAILURE) {         return ctx.abortFailedExecution("failed to execute watch input").     }     // condition     ctx.beforeCondition().     Condition.Result conditionResult = ctx.conditionResult().     if (conditionResult == null) {         conditionResult = watch.condition().execute(ctx).         ctx.onConditionResult(conditionResult).     }     if (conditionResult.status() == Condition.Result.Status.FAILURE) {         return ctx.abortFailedExecution("failed to execute watch condition").     }     if (conditionResult.met()) {         if (watch.actions().size() > 0 && watch.transform() != null) {             ctx.beforeWatchTransform().             Transform.Result transformResult = watch.transform().execute(ctx, ctx.payload()).             ctx.onWatchTransformResult(transformResult).             if (transformResult.status() == Transform.Result.Status.FAILURE) {                 return ctx.abortFailedExecution("failed to execute watch transform").             }         }         // actions         ctx.beforeActions().         for (ActionWrapper action : watch.actions()) {             long start = System.nanoTime().             ActionWrapperResult actionResult = action.execute(ctx).             long executionTime = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start).             String type = action.action().type().             actionByTypeExecutionTime.putIfAbsent(type, new MeanMetric()).             actionByTypeExecutionTime.get(type).inc(executionTime).             ctx.onActionResult(actionResult).         }     }     WatchRecord record = ctx.finish().     totalExecutionsTime.inc(record.result().executionDurationMs()).     return record. }
false;public;1;22;;public void executeTriggeredWatches(Collection<TriggeredWatch> triggeredWatches) {     assert triggeredWatches != null.     int counter = 0.     for (TriggeredWatch triggeredWatch : triggeredWatches) {         GetResponse response = getWatch(triggeredWatch.id().watchId()).         if (response.isExists() == false) {             String message = "unable to find watch for record [" + triggeredWatch.id().watchId() + "]/[" + triggeredWatch.id() + "], perhaps it has been deleted, ignoring...".             WatchRecord record = new WatchRecord.MessageWatchRecord(triggeredWatch.id(), triggeredWatch.triggerEvent(), ExecutionState.NOT_EXECUTED_WATCH_MISSING, message, clusterService.localNode().getId()).             historyStore.forcePut(record).             triggeredWatchStore.delete(triggeredWatch.id()).         } else {             ZonedDateTime now = clock.instant().atZone(ZoneOffset.UTC).             TriggeredExecutionContext ctx = new TriggeredExecutionContext(triggeredWatch.id().watchId(), now, triggeredWatch.triggerEvent(), defaultThrottlePeriod, true).             executeAsync(ctx, triggeredWatch).             counter++.         }     }     logger.debug("triggered execution of [{}] watches", counter). }
true;private;1;8;/**  * Gets a watch but in a synchronous way, so that no async calls need to be built  * @param id The id of watch  * @return The GetResponse of calling the get API of this watch  */ ;/**  * Gets a watch but in a synchronous way, so that no async calls need to be built  * @param id The id of watch  * @return The GetResponse of calling the get API of this watch  */ private GetResponse getWatch(String id) {     try (ThreadContext.StoredContext ignore = stashWithOrigin(client.threadPool().getThreadContext(), WATCHER_ORIGIN)) {         GetRequest getRequest = new GetRequest(Watch.INDEX, Watch.DOC_TYPE, id).preference(Preference.LOCAL.type()).realtime(true).         PlainActionFuture<GetResponse> future = PlainActionFuture.newFuture().         client.get(getRequest, future).         return future.actionGet().     } }
false;public;0;12;;public Counters executionTimes() {     Counters counters = new Counters().     counters.inc("execution.actions._all.total", totalExecutionsTime.count()).     counters.inc("execution.actions._all.total_time_in_ms", totalExecutionsTime.sum()).     for (Map.Entry<String, MeanMetric> entry : actionByTypeExecutionTime.entrySet()) {         counters.inc("execution.actions." + entry.getKey() + ".total", entry.getValue().count()).         counters.inc("execution.actions." + entry.getKey() + ".total_time_in_ms", entry.getValue().sum()).     }     return counters. }
true;private;0;5;/**  * This clears out the current executions and sets new empty current executions  * This is needed, because when this method is called, watcher keeps running, so sealing executions would be a bad idea  */ ;/**  * This clears out the current executions and sets new empty current executions  * This is needed, because when this method is called, watcher keeps running, so sealing executions would be a bad idea  */ private void clearExecutions() {     final CurrentExecutions currentExecutionsBeforeSetting = currentExecutions.getAndSet(new CurrentExecutions()).     // clear old executions in background, no need to wait     genericExecutor.execute(() -> currentExecutionsBeforeSetting.sealAndAwaitEmpty(maxStopTimeout)). }
false;public;0;4;;@Override public void run() {     runnable.run(). }
false;;0;3;;WatchExecutionSnapshot createSnapshot() {     return context.createSnapshot(executionThread). }
