commented;modifiers;parameterAmount;loc;comment;code
false;private,static;1;36;;private static ConstructingObjectParser<AnomalyRecord, Void> createParser(boolean ignoreUnknownFields) {     // As a record contains fields named after the data fields, the parser for the record should always ignore unknown fields.     // However, it makes sense to offer strict/lenient parsing for other members, e.g. influences, anomaly causes, etc.     ConstructingObjectParser<AnomalyRecord, Void> parser = new ConstructingObjectParser<>(RESULT_TYPE_VALUE, true, a -> new AnomalyRecord((String) a[0], (Date) a[1], (long) a[2])).     parser.declareString(ConstructingObjectParser.constructorArg(), Job.ID).     parser.declareField(ConstructingObjectParser.constructorArg(), p -> TimeUtils.parseTimeField(p, Result.TIMESTAMP.getPreferredName()), Result.TIMESTAMP, ValueType.VALUE).     parser.declareLong(ConstructingObjectParser.constructorArg(), BUCKET_SPAN).     parser.declareString((anomalyRecord, s) -> {     }, Result.RESULT_TYPE).     parser.declareDouble(AnomalyRecord::setProbability, PROBABILITY).     parser.declareDouble(AnomalyRecord::setMultiBucketImpact, MULTI_BUCKET_IMPACT).     parser.declareDouble(AnomalyRecord::setRecordScore, RECORD_SCORE).     parser.declareDouble(AnomalyRecord::setInitialRecordScore, INITIAL_RECORD_SCORE).     parser.declareInt(AnomalyRecord::setDetectorIndex, Detector.DETECTOR_INDEX).     parser.declareBoolean(AnomalyRecord::setInterim, Result.IS_INTERIM).     parser.declareString(AnomalyRecord::setByFieldName, BY_FIELD_NAME).     parser.declareString(AnomalyRecord::setByFieldValue, BY_FIELD_VALUE).     parser.declareString(AnomalyRecord::setCorrelatedByFieldValue, CORRELATED_BY_FIELD_VALUE).     parser.declareString(AnomalyRecord::setPartitionFieldName, PARTITION_FIELD_NAME).     parser.declareString(AnomalyRecord::setPartitionFieldValue, PARTITION_FIELD_VALUE).     parser.declareString(AnomalyRecord::setFunction, FUNCTION).     parser.declareString(AnomalyRecord::setFunctionDescription, FUNCTION_DESCRIPTION).     parser.declareDoubleArray(AnomalyRecord::setTypical, TYPICAL).     parser.declareDoubleArray(AnomalyRecord::setActual, ACTUAL).     parser.declareString(AnomalyRecord::setFieldName, FIELD_NAME).     parser.declareString(AnomalyRecord::setOverFieldName, OVER_FIELD_NAME).     parser.declareString(AnomalyRecord::setOverFieldValue, OVER_FIELD_VALUE).     parser.declareObjectArray(AnomalyRecord::setCauses, ignoreUnknownFields ? AnomalyCause.LENIENT_PARSER : AnomalyCause.STRICT_PARSER, CAUSES).     parser.declareObjectArray(AnomalyRecord::setInfluencers, ignoreUnknownFields ? Influence.LENIENT_PARSER : Influence.STRICT_PARSER, INFLUENCERS).     return parser. }
false;public;1;44;;@Override public void writeTo(StreamOutput out) throws IOException {     out.writeString(jobId).     out.writeInt(detectorIndex).     out.writeDouble(probability).     if (out.getVersion().onOrAfter(Version.V_6_5_0)) {         out.writeOptionalDouble(multiBucketImpact).     }     out.writeOptionalString(byFieldName).     out.writeOptionalString(byFieldValue).     out.writeOptionalString(correlatedByFieldValue).     out.writeOptionalString(partitionFieldName).     out.writeOptionalString(partitionFieldValue).     out.writeOptionalString(function).     out.writeOptionalString(functionDescription).     out.writeOptionalString(fieldName).     out.writeOptionalString(overFieldName).     out.writeOptionalString(overFieldValue).     boolean hasTypical = typical != null.     out.writeBoolean(hasTypical).     if (hasTypical) {         out.writeGenericValue(typical).     }     boolean hasActual = actual != null.     out.writeBoolean(hasActual).     if (hasActual) {         out.writeGenericValue(actual).     }     out.writeBoolean(isInterim).     boolean hasCauses = causes != null.     out.writeBoolean(hasCauses).     if (hasCauses) {         out.writeList(causes).     }     out.writeDouble(recordScore).     out.writeDouble(initialRecordScore).     out.writeLong(timestamp.getTime()).     out.writeLong(bucketSpan).     boolean hasInfluencers = influences != null.     out.writeBoolean(hasInfluencers).     if (hasInfluencers) {         out.writeList(influences).     } }
false;public;2;7;;@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {     builder.startObject().     innerToXContent(builder, params).     builder.endObject().     return builder. }
false;;2;62;;XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException {     builder.field(Job.ID.getPreferredName(), jobId).     builder.field(Result.RESULT_TYPE.getPreferredName(), RESULT_TYPE_VALUE).     builder.field(PROBABILITY.getPreferredName(), probability).     if (multiBucketImpact != null) {         builder.field(MULTI_BUCKET_IMPACT.getPreferredName(), multiBucketImpact).     }     builder.field(RECORD_SCORE.getPreferredName(), recordScore).     builder.field(INITIAL_RECORD_SCORE.getPreferredName(), initialRecordScore).     builder.field(BUCKET_SPAN.getPreferredName(), bucketSpan).     builder.field(Detector.DETECTOR_INDEX.getPreferredName(), detectorIndex).     builder.field(Result.IS_INTERIM.getPreferredName(), isInterim).     builder.timeField(Result.TIMESTAMP.getPreferredName(), Result.TIMESTAMP.getPreferredName() + "_string", timestamp.getTime()).     if (byFieldName != null) {         builder.field(BY_FIELD_NAME.getPreferredName(), byFieldName).     }     if (byFieldValue != null) {         builder.field(BY_FIELD_VALUE.getPreferredName(), byFieldValue).     }     if (correlatedByFieldValue != null) {         builder.field(CORRELATED_BY_FIELD_VALUE.getPreferredName(), correlatedByFieldValue).     }     if (partitionFieldName != null) {         builder.field(PARTITION_FIELD_NAME.getPreferredName(), partitionFieldName).     }     if (partitionFieldValue != null) {         builder.field(PARTITION_FIELD_VALUE.getPreferredName(), partitionFieldValue).     }     if (function != null) {         builder.field(FUNCTION.getPreferredName(), function).     }     if (functionDescription != null) {         builder.field(FUNCTION_DESCRIPTION.getPreferredName(), functionDescription).     }     if (typical != null) {         builder.field(TYPICAL.getPreferredName(), typical).     }     if (actual != null) {         builder.field(ACTUAL.getPreferredName(), actual).     }     if (fieldName != null) {         builder.field(FIELD_NAME.getPreferredName(), fieldName).     }     if (overFieldName != null) {         builder.field(OVER_FIELD_NAME.getPreferredName(), overFieldName).     }     if (overFieldValue != null) {         builder.field(OVER_FIELD_VALUE.getPreferredName(), overFieldValue).     }     if (causes != null) {         builder.field(CAUSES.getPreferredName(), causes).     }     if (influences != null) {         builder.field(INFLUENCERS.getPreferredName(), influences).     }     Map<String, LinkedHashSet<String>> inputFields = inputFieldMap().     for (String fieldName : inputFields.keySet()) {         builder.field(fieldName, inputFields.get(fieldName)).     }     return builder. }
false;private;0;18;;private Map<String, LinkedHashSet<String>> inputFieldMap() {     // LinkedHashSet preserves insertion order when iterating entries     Map<String, LinkedHashSet<String>> result = new HashMap<>().     addInputFieldsToMap(result, byFieldName, byFieldValue).     addInputFieldsToMap(result, overFieldName, overFieldValue).     addInputFieldsToMap(result, partitionFieldName, partitionFieldValue).     if (influences != null) {         for (Influence inf : influences) {             String fieldName = inf.getInfluencerFieldName().             for (String fieldValue : inf.getInfluencerFieldValues()) {                 addInputFieldsToMap(result, fieldName, fieldValue).             }         }     }     return result. }
false;private;3;7;;private void addInputFieldsToMap(Map<String, LinkedHashSet<String>> inputFields, String fieldName, String fieldValue) {     if (!Strings.isNullOrEmpty(fieldName) && fieldValue != null) {         if (ReservedFieldNames.isValidFieldName(fieldName)) {             inputFields.computeIfAbsent(fieldName, k -> new LinkedHashSet<>()).add(fieldValue).         }     } }
false;public;0;3;;public String getJobId() {     return this.jobId. }
true;public;0;8;/**  * Data store ID of this record.  */ ;/**  * Data store ID of this record.  */ public String getId() {     int valuesHash = Objects.hash(byFieldValue, overFieldValue, partitionFieldValue).     int length = (byFieldValue == null ? 0 : byFieldValue.length()) + (overFieldValue == null ? 0 : overFieldValue.length()) + (partitionFieldValue == null ? 0 : partitionFieldValue.length()).     return jobId + "_record_" + timestamp.getTime() + "_" + bucketSpan + "_" + detectorIndex + "_" + valuesHash + "_" + length. }
false;public;0;3;;public int getDetectorIndex() {     return detectorIndex. }
false;public;1;3;;public void setDetectorIndex(int detectorIndex) {     this.detectorIndex = detectorIndex. }
false;public;0;3;;public double getRecordScore() {     return recordScore. }
false;public;1;3;;public void setRecordScore(double recordScore) {     this.recordScore = recordScore. }
false;public;0;3;;public double getInitialRecordScore() {     return initialRecordScore. }
false;public;1;3;;public void setInitialRecordScore(double initialRecordScore) {     this.initialRecordScore = initialRecordScore. }
false;public;0;3;;public Date getTimestamp() {     return timestamp. }
true;public;0;3;/**  * Bucketspan expressed in seconds  */ ;/**  * Bucketspan expressed in seconds  */ public long getBucketSpan() {     return bucketSpan. }
false;public;0;3;;public double getProbability() {     return probability. }
false;public;1;3;;public void setProbability(double value) {     probability = value. }
false;public;0;3;;public double getMultiBucketImpact() {     return multiBucketImpact. }
false;public;1;3;;public void setMultiBucketImpact(double value) {     multiBucketImpact = value. }
false;public;0;3;;public String getByFieldName() {     return byFieldName. }
false;public;1;3;;public void setByFieldName(String value) {     byFieldName = value.intern(). }
false;public;0;3;;public String getByFieldValue() {     return byFieldValue. }
false;public;1;3;;public void setByFieldValue(String value) {     byFieldValue = value.intern(). }
false;public;0;3;;public String getCorrelatedByFieldValue() {     return correlatedByFieldValue. }
false;public;1;3;;public void setCorrelatedByFieldValue(String value) {     correlatedByFieldValue = value.intern(). }
false;public;0;3;;public String getPartitionFieldName() {     return partitionFieldName. }
false;public;1;3;;public void setPartitionFieldName(String field) {     partitionFieldName = field.intern(). }
false;public;0;3;;public String getPartitionFieldValue() {     return partitionFieldValue. }
false;public;1;3;;public void setPartitionFieldValue(String value) {     partitionFieldValue = value.intern(). }
false;public;0;3;;public String getFunction() {     return function. }
false;public;1;3;;public void setFunction(String name) {     function = name.intern(). }
false;public;0;3;;public String getFunctionDescription() {     return functionDescription. }
false;public;1;3;;public void setFunctionDescription(String functionDescription) {     this.functionDescription = functionDescription.intern(). }
false;public;0;3;;public List<Double> getTypical() {     return typical. }
false;public;1;3;;public void setTypical(List<Double> typical) {     this.typical = typical. }
false;public;0;3;;public List<Double> getActual() {     return actual. }
false;public;1;3;;public void setActual(List<Double> actual) {     this.actual = actual. }
false;public;0;3;;public boolean isInterim() {     return isInterim. }
false;public;1;3;;public void setInterim(boolean isInterim) {     this.isInterim = isInterim. }
false;public;0;3;;public String getFieldName() {     return fieldName. }
false;public;1;3;;public void setFieldName(String field) {     fieldName = field.intern(). }
false;public;0;3;;public String getOverFieldName() {     return overFieldName. }
false;public;1;3;;public void setOverFieldName(String name) {     overFieldName = name.intern(). }
false;public;0;3;;public String getOverFieldValue() {     return overFieldValue. }
false;public;1;3;;public void setOverFieldValue(String value) {     overFieldValue = value.intern(). }
false;public;0;3;;public List<AnomalyCause> getCauses() {     return causes. }
false;public;1;3;;public void setCauses(List<AnomalyCause> causes) {     this.causes = causes. }
false;public;1;6;;public void addCause(AnomalyCause cause) {     if (causes == null) {         causes = new ArrayList<>().     }     causes.add(cause). }
false;public;0;3;;public List<Influence> getInfluencers() {     return influences. }
false;public;1;3;;public void setInfluencers(List<Influence> influencers) {     this.influences = influencers. }
false;public;0;8;;@Override public int hashCode() {     return Objects.hash(jobId, detectorIndex, bucketSpan, probability, multiBucketImpact, recordScore, initialRecordScore, typical, actual, function, functionDescription, fieldName, byFieldName, byFieldValue, correlatedByFieldValue, partitionFieldName, partitionFieldValue, overFieldName, overFieldValue, timestamp, isInterim, causes, influences, jobId). }
false;public;1;36;;@Override public boolean equals(Object other) {     if (this == other) {         return true.     }     if (other instanceof AnomalyRecord == false) {         return false.     }     AnomalyRecord that = (AnomalyRecord) other.     return Objects.equals(this.jobId, that.jobId) && this.detectorIndex == that.detectorIndex && this.bucketSpan == that.bucketSpan && this.probability == that.probability && Objects.equals(this.multiBucketImpact, that.multiBucketImpact) && this.recordScore == that.recordScore && this.initialRecordScore == that.initialRecordScore && Objects.deepEquals(this.typical, that.typical) && Objects.deepEquals(this.actual, that.actual) && Objects.equals(this.function, that.function) && Objects.equals(this.functionDescription, that.functionDescription) && Objects.equals(this.fieldName, that.fieldName) && Objects.equals(this.byFieldName, that.byFieldName) && Objects.equals(this.byFieldValue, that.byFieldValue) && Objects.equals(this.correlatedByFieldValue, that.correlatedByFieldValue) && Objects.equals(this.partitionFieldName, that.partitionFieldName) && Objects.equals(this.partitionFieldValue, that.partitionFieldValue) && Objects.equals(this.overFieldName, that.overFieldName) && Objects.equals(this.overFieldValue, that.overFieldValue) && Objects.equals(this.timestamp, that.timestamp) && Objects.equals(this.isInterim, that.isInterim) && Objects.equals(this.causes, that.causes) && Objects.equals(this.influences, that.influences). }
