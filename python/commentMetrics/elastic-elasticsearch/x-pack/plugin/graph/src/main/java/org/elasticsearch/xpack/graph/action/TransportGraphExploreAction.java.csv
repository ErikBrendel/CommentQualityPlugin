commented;modifiers;parameterAmount;loc;comment;code
false;protected;2;4;;@Override protected boolean lessThan(Vertex a, Vertex b) {     return a.getWeight() < b.getWeight(). }
false;protected;3;8;;@Override protected void doExecute(Task task, GraphExploreRequest request, ActionListener<GraphExploreResponse> listener) {     if (licenseState.isGraphAllowed()) {         new AsyncGraphAction(request, listener).start().     } else {         listener.onFailure(LicenseUtils.newComplianceException(XPackField.GRAPH)).     } }
false;private;2;3;;private Vertex getVertex(String field, String term) {     return vertices.get(Vertex.createId(field, term)). }
false;private;4;5;;private Connection addConnection(Vertex from, Vertex to, double weight, long docCount) {     Connection connection = new Connection(from, to, weight, docCount).     connections.put(connection.getId(), connection).     return connection. }
false;private;6;20;;private Vertex addVertex(String field, String term, double score, int depth, long bg, long fg) {     VertexId key = Vertex.createId(field, term).     Vertex vertex = vertices.get(key).     if (vertex == null) {         vertex = new Vertex(field, term, score, depth, bg, fg).         vertices.put(key, vertex).         Map<String, Set<Vertex>> currentWave = hopFindings.get(currentHopNumber).         if (currentWave == null) {             currentWave = new HashMap<>().             hopFindings.put(currentHopNumber, currentWave).         }         Set<Vertex> verticesForField = currentWave.get(field).         if (verticesForField == null) {             verticesForField = new HashSet<>().             currentWave.put(field, verticesForField).         }         verticesForField.add(vertex).     }     return vertex. }
false;private;1;4;;private void removeVertex(Vertex vertex) {     vertices.remove(vertex.getId()).     hopFindings.get(currentHopNumber).get(vertex.getField()).remove(vertex). }
false;public;1;29;;@Override public void onResponse(SearchResponse searchResponse) {     // System.out.println(searchResponse).     addShardFailures(searchResponse.getShardFailures()).     ArrayList<Connection> newConnections = new ArrayList<Connection>().     ArrayList<Vertex> newVertices = new ArrayList<Vertex>().     Sampler sample = searchResponse.getAggregations().get("sample").     // We think of the total scores as the energy-level pouring     // out of all the last hop's connections.     // Each new node encountered is given a score which is     // normalized between zero and one based on     // what percentage of the total scores its own score     // provides     double totalSignalOutput = getExpandTotalSignalStrength(lastHop, currentHop, sample).     // terms as part of this stage     if (totalSignalOutput > 0) {         addAndScoreNewVertices(lastHop, currentHop, sample, totalSignalOutput, newConnections, newVertices).         trimNewAdditions(currentHop, newConnections, newVertices).     }     // Potentially run another round of queries to perform next"hop" - will terminate if no new additions     expand(). }
true;private;6;73;// connections ;// Add new vertices and apportion share of total signal along // connections private void addAndScoreNewVertices(Hop lastHop, Hop currentHop, Sampler sample, double totalSignalOutput, ArrayList<Connection> newConnections, ArrayList<Vertex> newVertices) {     // signals     for (int j = 0. j < lastHop.getNumberVertexRequests(). j++) {         VertexRequest lastVr = lastHop.getVertexRequest(j).         Terms lastWaveTerms = sample.getAggregations().get("field" + j).         if (lastWaveTerms == null) {             // There were no terms from the previous phase that needed pursuing             continue.         }         List<? extends Terms.Bucket> buckets = lastWaveTerms.getBuckets().         for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket lastWaveTerm : buckets) {             Vertex fromVertex = getVertex(lastVr.fieldName(), lastWaveTerm.getKeyAsString()).             for (int k = 0. k < currentHop.getNumberVertexRequests(). k++) {                 VertexRequest vr = currentHop.getVertexRequest(k).                 // As we travel further out into the graph we apply a                 // decay to the signals being propagated down the various channels.                 double decay = 0.95d.                 if (request.useSignificance()) {                     SignificantTerms significantTerms = lastWaveTerm.getAggregations().get("field" + k).                     if (significantTerms != null) {                         for (Bucket bucket : significantTerms.getBuckets()) {                             if ((vr.fieldName().equals(fromVertex.getField())) && (bucket.getKeyAsString().equals(fromVertex.getTerm()))) {                                 // Avoid self-joins                                 continue.                             }                             double signalStrength = bucket.getSignificanceScore() / totalSignalOutput.                             // Decay the signal by the weight attached to the source vertex                             signalStrength = signalStrength * Math.min(decay, fromVertex.getWeight()).                             Vertex toVertex = getVertex(vr.fieldName(), bucket.getKeyAsString()).                             if (toVertex == null) {                                 toVertex = addVertex(vr.fieldName(), bucket.getKeyAsString(), signalStrength, currentHopNumber, bucket.getSupersetDf(), bucket.getSubsetDf()).                                 newVertices.add(toVertex).                             } else {                                 toVertex.setWeight(toVertex.getWeight() + signalStrength).                                 // We cannot (without further querying) determine an accurate number                                 // for the foreground count of the toVertex term - if we sum the values                                 // from each fromVertex term we may actually double-count occurrences so                                 // the best we can do is take the maximum foreground value we have observed                                 toVertex.setFg(Math.max(toVertex.getFg(), bucket.getSubsetDf())).                             }                             newConnections.add(addConnection(fromVertex, toVertex, signalStrength, bucket.getDocCount())).                         }                     }                 } else {                     Terms terms = lastWaveTerm.getAggregations().get("field" + k).                     if (terms != null) {                         for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket bucket : terms.getBuckets()) {                             double signalStrength = bucket.getDocCount() / totalSignalOutput.                             // Decay the signal by the weight attached to the source vertex                             signalStrength = signalStrength * Math.min(decay, fromVertex.getWeight()).                             Vertex toVertex = getVertex(vr.fieldName(), bucket.getKeyAsString()).                             if (toVertex == null) {                                 toVertex = addVertex(vr.fieldName(), bucket.getKeyAsString(), signalStrength, currentHopNumber, 0, 0).                                 newVertices.add(toVertex).                             } else {                                 toVertex.setWeight(toVertex.getWeight() + signalStrength).                             }                             newConnections.add(addConnection(fromVertex, toVertex, signalStrength, bucket.getDocCount())).                         }                     }                 }             }         }     } }
true;private;3;31;// requested for each field. ;// Having let the signals from the last results rattle around the graph // we have adjusted weights for the various vertices we encountered. // Now we review these new additions and remove those with the // weakest weights. // A priority queue is used to trim vertices according to the size settings // requested for each field. private void trimNewAdditions(Hop currentHop, ArrayList<Connection> newConnections, ArrayList<Vertex> newVertices) {     Set<Vertex> evictions = new HashSet<>().     for (int k = 0. k < currentHop.getNumberVertexRequests(). k++) {         // For each of the fields         VertexRequest vr = currentHop.getVertexRequest(k).         if (newVertices.size() <= vr.size()) {             // Nothing to trim             continue.         }         // Get the top vertices for this field         VertexPriorityQueue pq = new VertexPriorityQueue(vr.size()).         for (Vertex vertex : newVertices) {             if (vertex.getField().equals(vr.fieldName())) {                 Vertex eviction = pq.insertWithOverflow(vertex).                 if (eviction != null) {                     evictions.add(eviction).                 }             }         }     }     // Remove weak new nodes and their dangling connections from the main graph     if (evictions.size() > 0) {         for (Connection connection : newConnections) {             if (evictions.contains(connection.getTo())) {                 connections.remove(connection.getId()).                 removeVertex(connection.getTo()).             }         }     } }
true;private;3;47;// Helper method - compute the total signal of all scores in the search results ;// TODO right now we only trim down to the best N vertices. We might also want to offer // clients the option to limit to the best M connections. One scenario where this is required // is if the "from" and "to" nodes are a client-supplied set of includes e.g. a list of // music artists then the client may be wanting to draw only the most-interesting connections // between them. See https://github.com/elastic/x-plugins/issues/518#issuecomment-160186424 // I guess clients could trim the returned connections (which all have weights) but I wonder if // we can do something server-side here // Helper method - compute the total signal of all scores in the search results private double getExpandTotalSignalStrength(Hop lastHop, Hop currentHop, Sampler sample) {     double totalSignalOutput = 0.     for (int j = 0. j < lastHop.getNumberVertexRequests(). j++) {         VertexRequest lastVr = lastHop.getVertexRequest(j).         Terms lastWaveTerms = sample.getAggregations().get("field" + j).         if (lastWaveTerms == null) {             continue.         }         List<? extends Terms.Bucket> buckets = lastWaveTerms.getBuckets().         for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket lastWaveTerm : buckets) {             for (int k = 0. k < currentHop.getNumberVertexRequests(). k++) {                 VertexRequest vr = currentHop.getVertexRequest(k).                 if (request.useSignificance()) {                     // Signal is based on significance score                     SignificantTerms significantTerms = lastWaveTerm.getAggregations().get("field" + k).                     if (significantTerms != null) {                         for (Bucket bucket : significantTerms.getBuckets()) {                             if ((vr.fieldName().equals(lastVr.fieldName())) && (bucket.getKeyAsString().equals(lastWaveTerm.getKeyAsString()))) {                                 // don't count self joins (term A obviously co-occurs with term A)                                 continue.                             } else {                                 totalSignalOutput += bucket.getSignificanceScore().                             }                         }                     }                 } else {                     // Signal is based on popularity (number of                     // documents)                     Terms terms = lastWaveTerm.getAggregations().get("field" + k).                     if (terms != null) {                         for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket bucket : terms.getBuckets()) {                             if ((vr.fieldName().equals(lastVr.fieldName())) && (bucket.getKeyAsString().equals(lastWaveTerm.getKeyAsString()))) {                                 // don't count self joins (term A obviously co-occurs with term A)                                 continue.                             } else {                                 totalSignalOutput += bucket.getDocCount().                             }                         }                     }                 }             }         }     }     return totalSignalOutput. }
false;public;1;4;;@Override public void onFailure(Exception e) {     listener.onFailure(e). }
true;synchronized;0;358;/**  * Step out from some existing vertex terms looking for useful  * connections  */ ;/**  * Step out from some existing vertex terms looking for useful  * connections  */ synchronized void expand() {     if (hasTimedOut()) {         timedOut.set(true).         listener.onResponse(buildResponse()).         return.     }     Map<String, Set<Vertex>> lastHopFindings = hopFindings.get(currentHopNumber).     if ((currentHopNumber >= (request.getHopNumbers() - 1)) || (lastHopFindings == null) || (lastHopFindings.size() == 0)) {         // Either we gathered no leads from the last hop or we have         // reached the final hop         listener.onResponse(buildResponse()).         return.     }     Hop lastHop = request.getHop(currentHopNumber).     currentHopNumber++.     Hop currentHop = request.getHop(currentHopNumber).     final SearchRequest searchRequest = new SearchRequest(request.indices()).types(request.types()).indicesOptions(request.indicesOptions()).     if (request.routing() != null) {         searchRequest.routing(request.routing()).     }     BoolQueryBuilder rootBool = QueryBuilders.boolQuery().     // A single sample pool of docs is built at the root of the aggs tree.     // For quality's sake it might have made more sense to sample top docs     // for each of the terms from the previous hop (e.g. an initial query for "beatles"     // may have separate doc-sample pools for significant root terms "john", "paul", "yoko" etc)     // but I found this dramatically slowed down execution - each pool typically had different docs which     // each had non-overlapping sets of terms that needed frequencies looking up for significant terms.     // A common sample pool reduces the specialization that can be given to each root term but     // ultimately is much faster to run because of the shared vocabulary in a single sample set.     AggregationBuilder sampleAgg = null.     if (request.sampleDiversityField() != null) {         DiversifiedAggregationBuilder diversifiedSampleAgg = AggregationBuilders.diversifiedSampler("sample").shardSize(request.sampleSize()).         diversifiedSampleAgg.field(request.sampleDiversityField()).         diversifiedSampleAgg.maxDocsPerValue(request.maxDocsPerDiversityValue()).         sampleAgg = diversifiedSampleAgg.     } else {         sampleAgg = AggregationBuilders.sampler("sample").shardSize(request.sampleSize()).     }     // Add any user-supplied criteria to the root query as a must clause     rootBool.must(currentHop.guidingQuery()).     // Build a MUST clause that matches one of either     // a:) include clauses supplied by the client or     // b:) vertex terms from the previous hop.     BoolQueryBuilder sourceTermsOrClause = QueryBuilders.boolQuery().     addUserDefinedIncludesToQuery(currentHop, sourceTermsOrClause).     addBigOrClause(lastHopFindings, sourceTermsOrClause).     rootBool.must(sourceTermsOrClause).     // under each is a sig_terms agg to find next candidates (again, one per field)...     for (int fieldNum = 0. fieldNum < lastHop.getNumberVertexRequests(). fieldNum++) {         VertexRequest lastVr = lastHop.getVertexRequest(fieldNum).         Set<Vertex> lastWaveVerticesForField = lastHopFindings.get(lastVr.fieldName()).         if (lastWaveVerticesForField == null) {             continue.         }         String[] terms = new String[lastWaveVerticesForField.size()].         int i = 0.         for (Vertex v : lastWaveVerticesForField) {             terms[i++] = v.getTerm().         }         TermsAggregationBuilder lastWaveTermsAgg = AggregationBuilders.terms("field" + fieldNum).includeExclude(new IncludeExclude(terms, null)).shardMinDocCount(1).field(lastVr.fieldName()).minDocCount(1).executionHint("map").size(terms.length).         sampleAgg.subAggregation(lastWaveTermsAgg).         for (int f = 0. f < currentHop.getNumberVertexRequests(). f++) {             VertexRequest vr = currentHop.getVertexRequest(f).             int size = vr.size().             if (vr.fieldName().equals(lastVr.fieldName())) {                 // We have the potential for self-loops as we are looking at the same field so add 1 to the requested size                 // because we need to eliminate fieldA:termA -> fieldA:termA links that are likely to be in the results.                 size++.             }             if (request.useSignificance()) {                 SignificantTermsAggregationBuilder nextWaveSigTerms = AggregationBuilders.significantTerms("field" + f).field(vr.fieldName()).minDocCount(vr.minDocCount()).shardMinDocCount(vr.shardMinDocCount()).executionHint("map").size(size).                 // number of final results (eg 1) and only one shard. Setting shard_size higher helped.                 if (size < 10) {                     nextWaveSigTerms.shardSize(10).                 }                 if (vr.hasIncludeClauses()) {                     String[] includes = vr.includeValuesAsStringArray().                     nextWaveSigTerms.includeExclude(new IncludeExclude(includes, null)).                 // Originally I thought users would always want the                 // same number of results as listed in the include                 // clause but it may be the only want the most                 // significant e.g. in the lastfm example of                 // plotting a single user's tastes and how that maps                 // into a network showing only the most interesting                 // band connections. So line below commmented out                 // nextWaveSigTerms.size(includes.length).                 } else if (vr.hasExcludeClauses()) {                     nextWaveSigTerms.includeExclude(new IncludeExclude(null, vr.excludesAsArray())).                 }                 lastWaveTermsAgg.subAggregation(nextWaveSigTerms).             } else {                 TermsAggregationBuilder nextWavePopularTerms = AggregationBuilders.terms("field" + f).field(vr.fieldName()).minDocCount(vr.minDocCount()).shardMinDocCount(vr.shardMinDocCount()).executionHint("map").size(size).                 if (vr.hasIncludeClauses()) {                     String[] includes = vr.includeValuesAsStringArray().                     nextWavePopularTerms.includeExclude(new IncludeExclude(includes, null)).                 // nextWavePopularTerms.size(includes.length).                 } else if (vr.hasExcludeClauses()) {                     nextWavePopularTerms.includeExclude(new IncludeExclude(null, vr.excludesAsArray())).                 }                 lastWaveTermsAgg.subAggregation(nextWavePopularTerms).             }         }     }     // Execute the search     SearchSourceBuilder source = new SearchSourceBuilder().query(rootBool).aggregation(sampleAgg).size(0).     if (request.timeout() != null) {         source.timeout(TimeValue.timeValueMillis(timeRemainingMillis())).     }     searchRequest.source(source).     // System.out.println(source).     logger.trace("executing expansion graph search request").     client.search(searchRequest, new ActionListener<SearchResponse>() {          @Override         public void onResponse(SearchResponse searchResponse) {             // System.out.println(searchResponse).             addShardFailures(searchResponse.getShardFailures()).             ArrayList<Connection> newConnections = new ArrayList<Connection>().             ArrayList<Vertex> newVertices = new ArrayList<Vertex>().             Sampler sample = searchResponse.getAggregations().get("sample").             // We think of the total scores as the energy-level pouring             // out of all the last hop's connections.             // Each new node encountered is given a score which is             // normalized between zero and one based on             // what percentage of the total scores its own score             // provides             double totalSignalOutput = getExpandTotalSignalStrength(lastHop, currentHop, sample).             // terms as part of this stage             if (totalSignalOutput > 0) {                 addAndScoreNewVertices(lastHop, currentHop, sample, totalSignalOutput, newConnections, newVertices).                 trimNewAdditions(currentHop, newConnections, newVertices).             }             // Potentially run another round of queries to perform next"hop" - will terminate if no new additions             expand().         }          // Add new vertices and apportion share of total signal along         // connections         private void addAndScoreNewVertices(Hop lastHop, Hop currentHop, Sampler sample, double totalSignalOutput, ArrayList<Connection> newConnections, ArrayList<Vertex> newVertices) {             // signals             for (int j = 0. j < lastHop.getNumberVertexRequests(). j++) {                 VertexRequest lastVr = lastHop.getVertexRequest(j).                 Terms lastWaveTerms = sample.getAggregations().get("field" + j).                 if (lastWaveTerms == null) {                     // There were no terms from the previous phase that needed pursuing                     continue.                 }                 List<? extends Terms.Bucket> buckets = lastWaveTerms.getBuckets().                 for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket lastWaveTerm : buckets) {                     Vertex fromVertex = getVertex(lastVr.fieldName(), lastWaveTerm.getKeyAsString()).                     for (int k = 0. k < currentHop.getNumberVertexRequests(). k++) {                         VertexRequest vr = currentHop.getVertexRequest(k).                         // As we travel further out into the graph we apply a                         // decay to the signals being propagated down the various channels.                         double decay = 0.95d.                         if (request.useSignificance()) {                             SignificantTerms significantTerms = lastWaveTerm.getAggregations().get("field" + k).                             if (significantTerms != null) {                                 for (Bucket bucket : significantTerms.getBuckets()) {                                     if ((vr.fieldName().equals(fromVertex.getField())) && (bucket.getKeyAsString().equals(fromVertex.getTerm()))) {                                         // Avoid self-joins                                         continue.                                     }                                     double signalStrength = bucket.getSignificanceScore() / totalSignalOutput.                                     // Decay the signal by the weight attached to the source vertex                                     signalStrength = signalStrength * Math.min(decay, fromVertex.getWeight()).                                     Vertex toVertex = getVertex(vr.fieldName(), bucket.getKeyAsString()).                                     if (toVertex == null) {                                         toVertex = addVertex(vr.fieldName(), bucket.getKeyAsString(), signalStrength, currentHopNumber, bucket.getSupersetDf(), bucket.getSubsetDf()).                                         newVertices.add(toVertex).                                     } else {                                         toVertex.setWeight(toVertex.getWeight() + signalStrength).                                         // We cannot (without further querying) determine an accurate number                                         // for the foreground count of the toVertex term - if we sum the values                                         // from each fromVertex term we may actually double-count occurrences so                                         // the best we can do is take the maximum foreground value we have observed                                         toVertex.setFg(Math.max(toVertex.getFg(), bucket.getSubsetDf())).                                     }                                     newConnections.add(addConnection(fromVertex, toVertex, signalStrength, bucket.getDocCount())).                                 }                             }                         } else {                             Terms terms = lastWaveTerm.getAggregations().get("field" + k).                             if (terms != null) {                                 for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket bucket : terms.getBuckets()) {                                     double signalStrength = bucket.getDocCount() / totalSignalOutput.                                     // Decay the signal by the weight attached to the source vertex                                     signalStrength = signalStrength * Math.min(decay, fromVertex.getWeight()).                                     Vertex toVertex = getVertex(vr.fieldName(), bucket.getKeyAsString()).                                     if (toVertex == null) {                                         toVertex = addVertex(vr.fieldName(), bucket.getKeyAsString(), signalStrength, currentHopNumber, 0, 0).                                         newVertices.add(toVertex).                                     } else {                                         toVertex.setWeight(toVertex.getWeight() + signalStrength).                                     }                                     newConnections.add(addConnection(fromVertex, toVertex, signalStrength, bucket.getDocCount())).                                 }                             }                         }                     }                 }             }         }          // Having let the signals from the last results rattle around the graph         // we have adjusted weights for the various vertices we encountered.         // Now we review these new additions and remove those with the         // weakest weights.         // A priority queue is used to trim vertices according to the size settings         // requested for each field.         private void trimNewAdditions(Hop currentHop, ArrayList<Connection> newConnections, ArrayList<Vertex> newVertices) {             Set<Vertex> evictions = new HashSet<>().             for (int k = 0. k < currentHop.getNumberVertexRequests(). k++) {                 // For each of the fields                 VertexRequest vr = currentHop.getVertexRequest(k).                 if (newVertices.size() <= vr.size()) {                     // Nothing to trim                     continue.                 }                 // Get the top vertices for this field                 VertexPriorityQueue pq = new VertexPriorityQueue(vr.size()).                 for (Vertex vertex : newVertices) {                     if (vertex.getField().equals(vr.fieldName())) {                         Vertex eviction = pq.insertWithOverflow(vertex).                         if (eviction != null) {                             evictions.add(eviction).                         }                     }                 }             }             // Remove weak new nodes and their dangling connections from the main graph             if (evictions.size() > 0) {                 for (Connection connection : newConnections) {                     if (evictions.contains(connection.getTo())) {                         connections.remove(connection.getId()).                         removeVertex(connection.getTo()).                     }                 }             }         }          // TODO right now we only trim down to the best N vertices. We might also want to offer         // clients the option to limit to the best M connections. One scenario where this is required         // is if the "from" and "to" nodes are a client-supplied set of includes e.g. a list of         // music artists then the client may be wanting to draw only the most-interesting connections         // between them. See https://github.com/elastic/x-plugins/issues/518#issuecomment-160186424         // I guess clients could trim the returned connections (which all have weights) but I wonder if         // we can do something server-side here         // Helper method - compute the total signal of all scores in the search results         private double getExpandTotalSignalStrength(Hop lastHop, Hop currentHop, Sampler sample) {             double totalSignalOutput = 0.             for (int j = 0. j < lastHop.getNumberVertexRequests(). j++) {                 VertexRequest lastVr = lastHop.getVertexRequest(j).                 Terms lastWaveTerms = sample.getAggregations().get("field" + j).                 if (lastWaveTerms == null) {                     continue.                 }                 List<? extends Terms.Bucket> buckets = lastWaveTerms.getBuckets().                 for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket lastWaveTerm : buckets) {                     for (int k = 0. k < currentHop.getNumberVertexRequests(). k++) {                         VertexRequest vr = currentHop.getVertexRequest(k).                         if (request.useSignificance()) {                             // Signal is based on significance score                             SignificantTerms significantTerms = lastWaveTerm.getAggregations().get("field" + k).                             if (significantTerms != null) {                                 for (Bucket bucket : significantTerms.getBuckets()) {                                     if ((vr.fieldName().equals(lastVr.fieldName())) && (bucket.getKeyAsString().equals(lastWaveTerm.getKeyAsString()))) {                                         // don't count self joins (term A obviously co-occurs with term A)                                         continue.                                     } else {                                         totalSignalOutput += bucket.getSignificanceScore().                                     }                                 }                             }                         } else {                             // Signal is based on popularity (number of                             // documents)                             Terms terms = lastWaveTerm.getAggregations().get("field" + k).                             if (terms != null) {                                 for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket bucket : terms.getBuckets()) {                                     if ((vr.fieldName().equals(lastVr.fieldName())) && (bucket.getKeyAsString().equals(lastWaveTerm.getKeyAsString()))) {                                         // don't count self joins (term A obviously co-occurs with term A)                                         continue.                                     } else {                                         totalSignalOutput += bucket.getDocCount().                                     }                                 }                             }                         }                     }                 }             }             return totalSignalOutput.         }          @Override         public void onFailure(Exception e) {             listener.onFailure(e).         }     }). }
false;private;2;8;;private void addUserDefinedIncludesToQuery(Hop hop, BoolQueryBuilder sourceTermsOrClause) {     for (int i = 0. i < hop.getNumberVertexRequests(). i++) {         VertexRequest vr = hop.getVertexRequest(i).         if (vr.hasIncludeClauses()) {             addNormalizedBoosts(sourceTermsOrClause, vr).         }     } }
false;private;2;27;;private void addBigOrClause(Map<String, Set<Vertex>> lastHopFindings, BoolQueryBuilder sourceTermsOrClause) {     int numClauses = sourceTermsOrClause.should().size().     for (Entry<String, Set<Vertex>> entry : lastHopFindings.entrySet()) {         numClauses += entry.getValue().size().     }     if (numClauses < BooleanQuery.getMaxClauseCount()) {         // boosts for interesting terms         for (Entry<String, Set<Vertex>> entry : lastHopFindings.entrySet()) {             for (Vertex vertex : entry.getValue()) {                 sourceTermsOrClause.should(QueryBuilders.constantScoreQuery(QueryBuilders.termQuery(vertex.getField(), vertex.getTerm())).boost((float) vertex.getWeight())).             }         }     } else {         // Too many terms - we need a cheaper form of query to execute this         for (Entry<String, Set<Vertex>> entry : lastHopFindings.entrySet()) {             List<String> perFieldTerms = new ArrayList<>().             for (Vertex vertex : entry.getValue()) {                 perFieldTerms.add(vertex.getTerm()).             }             sourceTermsOrClause.should(QueryBuilders.constantScoreQuery(QueryBuilders.termsQuery(entry.getKey(), perFieldTerms))).         }     } }
false;public;1;34;;@Override public void onResponse(SearchResponse searchResponse) {     addShardFailures(searchResponse.getShardFailures()).     Sampler sample = searchResponse.getAggregations().get("sample").     // Determine the total scores for all interesting terms     double totalSignalStrength = getInitialTotalSignalStrength(rootHop, sample).     // share of the total signal strength     for (int j = 0. j < rootHop.getNumberVertexRequests(). j++) {         VertexRequest vr = rootHop.getVertexRequest(j).         if (request.useSignificance()) {             SignificantTerms significantTerms = sample.getAggregations().get("field" + j).             List<? extends Bucket> buckets = significantTerms.getBuckets().             for (Bucket bucket : buckets) {                 double signalWeight = bucket.getSignificanceScore() / totalSignalStrength.                 addVertex(vr.fieldName(), bucket.getKeyAsString(), signalWeight, currentHopNumber, bucket.getSupersetDf(), bucket.getSubsetDf()).             }         } else {             Terms terms = sample.getAggregations().get("field" + j).             List<? extends Terms.Bucket> buckets = terms.getBuckets().             for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket bucket : buckets) {                 double signalWeight = bucket.getDocCount() / totalSignalStrength.                 addVertex(vr.fieldName(), bucket.getKeyAsString(), signalWeight, currentHopNumber, 0, 0).             }         }     }     // Expand out from these root vertices looking for connections with other terms     expand(). }
true;private;2;21;// Helper method - Provides a total signal strength for all terms connected to the initial query ;// Helper method - Provides a total signal strength for all terms connected to the initial query private double getInitialTotalSignalStrength(Hop rootHop, Sampler sample) {     double totalSignalStrength = 0.     for (int i = 0. i < rootHop.getNumberVertexRequests(). i++) {         if (request.useSignificance()) {             // Signal is based on significance score             SignificantTerms significantTerms = sample.getAggregations().get("field" + i).             List<? extends Bucket> buckets = significantTerms.getBuckets().             for (Bucket bucket : buckets) {                 totalSignalStrength += bucket.getSignificanceScore().             }         } else {             // Signal is based on popularity (number of documents)             Terms terms = sample.getAggregations().get("field" + i).             List<? extends Terms.Bucket> buckets = terms.getBuckets().             for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket bucket : buckets) {                 totalSignalStrength += bucket.getDocCount().             }         }     }     return totalSignalStrength. }
false;public;1;4;;@Override public void onFailure(Exception e) {     listener.onFailure(e). }
true;public,synchronized;0;164;/**  * For a given root query (or a set of "includes" root constraints) find  * the related terms. These will be our start points in the graph  * navigation.  */ ;/**  * For a given root query (or a set of "includes" root constraints) find  * the related terms. These will be our start points in the graph  * navigation.  */ public synchronized void start() {     try {         final SearchRequest searchRequest = new SearchRequest(request.indices()).types(request.types()).indicesOptions(request.indicesOptions()).         if (request.routing() != null) {             searchRequest.routing(request.routing()).         }         BoolQueryBuilder rootBool = QueryBuilders.boolQuery().         AggregationBuilder rootSampleAgg = null.         if (request.sampleDiversityField() != null) {             DiversifiedAggregationBuilder diversifiedRootSampleAgg = AggregationBuilders.diversifiedSampler("sample").shardSize(request.sampleSize()).             diversifiedRootSampleAgg.field(request.sampleDiversityField()).             diversifiedRootSampleAgg.maxDocsPerValue(request.maxDocsPerDiversityValue()).             rootSampleAgg = diversifiedRootSampleAgg.         } else {             rootSampleAgg = AggregationBuilders.sampler("sample").shardSize(request.sampleSize()).         }         Hop rootHop = request.getHop(0).         // Add any user-supplied criteria to the root query as a should clause         rootBool.must(rootHop.guidingQuery()).         // If any of the root terms have an "include" restriction then         // we add a root-level MUST clause that         // mandates that at least one of the potentially many terms of         // interest must be matched (using a should array)         BoolQueryBuilder includesContainer = QueryBuilders.boolQuery().         addUserDefinedIncludesToQuery(rootHop, includesContainer).         if (includesContainer.should().size() > 0) {             rootBool.must(includesContainer).         }         for (int i = 0. i < rootHop.getNumberVertexRequests(). i++) {             VertexRequest vr = rootHop.getVertexRequest(i).             if (request.useSignificance()) {                 SignificantTermsAggregationBuilder sigBuilder = AggregationBuilders.significantTerms("field" + i).                 sigBuilder.field(vr.fieldName()).shardMinDocCount(vr.shardMinDocCount()).minDocCount(vr.minDocCount()).executionHint("map").size(vr.size()).                 if (vr.hasIncludeClauses()) {                     String[] includes = vr.includeValuesAsStringArray().                     sigBuilder.includeExclude(new IncludeExclude(includes, null)).                     sigBuilder.size(includes.length).                 }                 if (vr.hasExcludeClauses()) {                     sigBuilder.includeExclude(new IncludeExclude(null, vr.excludesAsArray())).                 }                 rootSampleAgg.subAggregation(sigBuilder).             } else {                 TermsAggregationBuilder termsBuilder = AggregationBuilders.terms("field" + i).                 // Min doc count etc really only applies when we are                 // thinking about certainty of significance scores -                 // perhaps less necessary when considering popularity                 // termsBuilder.field(vr.fieldName()).shardMinDocCount(shardMinDocCount)                 // .minDocCount(minDocCount).executionHint("map").size(vr.size()).                 termsBuilder.field(vr.fieldName()).executionHint("map").size(vr.size()).                 if (vr.hasIncludeClauses()) {                     String[] includes = vr.includeValuesAsStringArray().                     termsBuilder.includeExclude(new IncludeExclude(includes, null)).                     termsBuilder.size(includes.length).                 }                 if (vr.hasExcludeClauses()) {                     termsBuilder.includeExclude(new IncludeExclude(null, vr.excludesAsArray())).                 }                 rootSampleAgg.subAggregation(termsBuilder).             }         }         // Run the search         SearchSourceBuilder source = new SearchSourceBuilder().query(rootBool).aggregation(rootSampleAgg).size(0).         if (request.timeout() != null) {             source.timeout(request.timeout()).         }         searchRequest.source(source).         // System.out.println(source).         logger.trace("executing initial graph search request").         client.search(searchRequest, new ActionListener<SearchResponse>() {              @Override             public void onResponse(SearchResponse searchResponse) {                 addShardFailures(searchResponse.getShardFailures()).                 Sampler sample = searchResponse.getAggregations().get("sample").                 // Determine the total scores for all interesting terms                 double totalSignalStrength = getInitialTotalSignalStrength(rootHop, sample).                 // share of the total signal strength                 for (int j = 0. j < rootHop.getNumberVertexRequests(). j++) {                     VertexRequest vr = rootHop.getVertexRequest(j).                     if (request.useSignificance()) {                         SignificantTerms significantTerms = sample.getAggregations().get("field" + j).                         List<? extends Bucket> buckets = significantTerms.getBuckets().                         for (Bucket bucket : buckets) {                             double signalWeight = bucket.getSignificanceScore() / totalSignalStrength.                             addVertex(vr.fieldName(), bucket.getKeyAsString(), signalWeight, currentHopNumber, bucket.getSupersetDf(), bucket.getSubsetDf()).                         }                     } else {                         Terms terms = sample.getAggregations().get("field" + j).                         List<? extends Terms.Bucket> buckets = terms.getBuckets().                         for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket bucket : buckets) {                             double signalWeight = bucket.getDocCount() / totalSignalStrength.                             addVertex(vr.fieldName(), bucket.getKeyAsString(), signalWeight, currentHopNumber, 0, 0).                         }                     }                 }                 // Expand out from these root vertices looking for connections with other terms                 expand().             }              // Helper method - Provides a total signal strength for all terms connected to the initial query             private double getInitialTotalSignalStrength(Hop rootHop, Sampler sample) {                 double totalSignalStrength = 0.                 for (int i = 0. i < rootHop.getNumberVertexRequests(). i++) {                     if (request.useSignificance()) {                         // Signal is based on significance score                         SignificantTerms significantTerms = sample.getAggregations().get("field" + i).                         List<? extends Bucket> buckets = significantTerms.getBuckets().                         for (Bucket bucket : buckets) {                             totalSignalStrength += bucket.getSignificanceScore().                         }                     } else {                         // Signal is based on popularity (number of documents)                         Terms terms = sample.getAggregations().get("field" + i).                         List<? extends Terms.Bucket> buckets = terms.getBuckets().                         for (org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket bucket : buckets) {                             totalSignalStrength += bucket.getDocCount().                         }                     }                 }                 return totalSignalStrength.             }              @Override             public void onFailure(Exception e) {                 listener.onFailure(e).             }         }).     } catch (Exception e) {         logger.error("unable to execute the graph query", e).         listener.onFailure(e).     } }
false;private;2;26;;private void addNormalizedBoosts(BoolQueryBuilder includesContainer, VertexRequest vr) {     TermBoost[] termBoosts = vr.includeValues().     if ((includesContainer.should().size() + termBoosts.length) > BooleanQuery.getMaxClauseCount()) {         // Too many terms - we need a cheaper form of query to execute this         List<String> termValues = new ArrayList<>().         for (TermBoost tb : termBoosts) {             termValues.add(tb.getTerm()).         }         includesContainer.should(QueryBuilders.constantScoreQuery(QueryBuilders.termsQuery(vr.fieldName(), termValues))).         return.     }     // We have a sufficiently low number of terms to use the per-term boosts.     // Lucene boosts are >=1 so we baseline the provided boosts to start     // from 1     float minBoost = Float.MAX_VALUE.     for (TermBoost tb : termBoosts) {         minBoost = Math.min(minBoost, tb.getBoost()).     }     for (TermBoost tb : termBoosts) {         float normalizedBoost = tb.getBoost() / minBoost.         includesContainer.should(QueryBuilders.termQuery(vr.fieldName(), tb.getTerm()).boost(normalizedBoost)).     } }
false;;0;3;;boolean hasTimedOut() {     return request.timeout() != null && (timeRemainingMillis() <= 0). }
false;;0;5;;long timeRemainingMillis() {     // configured globally for updating estimated time.     return (startTime + request.timeout().millis()) - threadPool.relativeTimeInMillis(). }
false;;1;8;;void addShardFailures(ShardOperationFailedException[] failures) {     if (!CollectionUtils.isEmpty(failures)) {         ShardOperationFailedException[] duplicates = new ShardOperationFailedException[shardFailures.length + failures.length].         System.arraycopy(shardFailures, 0, duplicates, 0, shardFailures.length).         System.arraycopy(failures, 0, duplicates, shardFailures.length, failures.length).         shardFailures = ExceptionsHelper.groupBy(duplicates).     } }
false;protected;0;4;;protected GraphExploreResponse buildResponse() {     long took = threadPool.relativeTimeInMillis() - startTime.     return new GraphExploreResponse(took, timedOut.get(), shardFailures, vertices, connections, request.returnDetailedInfo()). }
