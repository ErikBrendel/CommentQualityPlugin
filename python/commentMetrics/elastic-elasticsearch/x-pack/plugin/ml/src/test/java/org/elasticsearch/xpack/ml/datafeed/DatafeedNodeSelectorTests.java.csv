commented;modifiers;parameterAmount;loc;comment;code
false;public;0;9;;@Before public void init() {     resolver = new IndexNameExpressionResolver().     nodes = DiscoveryNodes.builder().add(new DiscoveryNode("node_name", "node_id", new TransportAddress(InetAddress.getLoopbackAddress(), 9300), Collections.emptyMap(), Collections.emptySet(), Version.CURRENT)).build().     mlMetadata = new MlMetadata.Builder().build(). }
false;public;0;15;;public void testSelectNode_GivenJobIsOpened() {     Job job = createScheduledJob("job_id").build(new Date()).     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("foo")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id", JobState.OPENED, tasksBuilder).     tasks = tasksBuilder.build().     givenClusterState("foo", 1, 0).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertEquals("node_id", result.getExecutorNode()).     new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated(). }
false;public;0;15;;public void testSelectNode_GivenJobIsOpening() {     Job job = createScheduledJob("job_id").build(new Date()).     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("foo")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id", null, tasksBuilder).     tasks = tasksBuilder.build().     givenClusterState("foo", 1, 0).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertEquals("node_id", result.getExecutorNode()).     new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated(). }
false;public;0;22;;public void testNoJobTask() {     Job job = createScheduledJob("job_id").build(new Date()).     // Using wildcard index name to test for index resolving as well     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("fo*")).     tasks = PersistentTasksCustomMetaData.builder().build().     givenClusterState("foo", 1, 0).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertNull(result.getExecutorNode()).     assertThat(result.getExplanation(), equalTo("cannot start datafeed [datafeed_id], because the job's [job_id] state is " + "[closed] while state [opened] is required")).     ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated()).     assertThat(e.getMessage(), containsString("No node found to start datafeed [datafeed_id], allocation explanation " + "[cannot start datafeed [datafeed_id], because the job's [job_id] state is [closed] while state [opened] is required]")). }
false;public;0;24;;public void testSelectNode_GivenJobFailedOrClosed() {     Job job = createScheduledJob("job_id").build(new Date()).     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("foo")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     JobState jobState = randomFrom(JobState.FAILED, JobState.CLOSED).     addJobTask(job.getId(), "node_id", jobState, tasksBuilder).     tasks = tasksBuilder.build().     givenClusterState("foo", 1, 0).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertNull(result.getExecutorNode()).     assertEquals("cannot start datafeed [datafeed_id], because the job's [job_id] state is [" + jobState + "] while state [opened] is required", result.getExplanation()).     ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated()).     assertThat(e.getMessage(), containsString("No node found to start datafeed [datafeed_id], allocation explanation " + "[cannot start datafeed [datafeed_id], because the job's [job_id] state is [" + jobState + "] while state [opened] is required]")). }
false;public;0;23;;public void testShardUnassigned() {     Job job = createScheduledJob("job_id").build(new Date()).     // Using wildcard index name to test for index resolving as well     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("fo*")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id", JobState.OPENED, tasksBuilder).     tasks = tasksBuilder.build().     List<Tuple<Integer, ShardRoutingState>> states = new ArrayList<>(2).     states.add(new Tuple<>(0, ShardRoutingState.UNASSIGNED)).     givenClusterState("foo", 1, 0, states).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertNull(result.getExecutorNode()).     assertThat(result.getExplanation(), equalTo("cannot start datafeed [datafeed_id] because index [foo] " + "does not have all primary shards active yet.")).     new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated(). }
false;public;0;24;;public void testShardNotAllActive() {     Job job = createScheduledJob("job_id").build(new Date()).     // Using wildcard index name to test for index resolving as well     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("fo*")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id", JobState.OPENED, tasksBuilder).     tasks = tasksBuilder.build().     List<Tuple<Integer, ShardRoutingState>> states = new ArrayList<>(2).     states.add(new Tuple<>(0, ShardRoutingState.STARTED)).     states.add(new Tuple<>(1, ShardRoutingState.INITIALIZING)).     givenClusterState("foo", 2, 0, states).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertNull(result.getExecutorNode()).     assertThat(result.getExplanation(), equalTo("cannot start datafeed [datafeed_id] because index [foo] " + "does not have all primary shards active yet.")).     new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated(). }
false;public;0;22;;public void testIndexDoesntExist() {     Job job = createScheduledJob("job_id").build(new Date()).     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("not_foo")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id", JobState.OPENED, tasksBuilder).     tasks = tasksBuilder.build().     givenClusterState("foo", 1, 0).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertNull(result.getExecutorNode()).     assertThat(result.getExplanation(), equalTo("cannot start datafeed [datafeed_id] because index [not_foo] " + "does not exist, is closed, or is still initializing.")).     ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated()).     assertThat(e.getMessage(), containsString("No node found to start datafeed [datafeed_id], allocation explanation " + "[cannot start datafeed [datafeed_id] because index [not_foo] does not exist, is closed, or is still initializing.]")). }
false;public;0;14;;public void testRemoteIndex() {     Job job = createScheduledJob("job_id").build(new Date()).     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("remote:foo")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id", JobState.OPENED, tasksBuilder).     tasks = tasksBuilder.build().     givenClusterState("foo", 1, 0).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertNotNull(result.getExecutorNode()). }
false;public;0;33;;public void testSelectNode_jobTaskStale() {     Job job = createScheduledJob("job_id").build(new Date()).     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("foo")).     String nodeId = randomBoolean() ? "node_id2" : null.     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), nodeId, JobState.OPENED, tasksBuilder).     // Set to lower allocationId, so job task is stale:     tasksBuilder.updateTaskState(MlTasks.jobTaskId(job.getId()), new JobTaskState(JobState.OPENED, 0, null)).     tasks = tasksBuilder.build().     givenClusterState("foo", 1, 0).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertNull(result.getExecutorNode()).     assertEquals("cannot start datafeed [datafeed_id], because the job's [job_id] state is stale", result.getExplanation()).     ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated()).     assertThat(e.getMessage(), containsString("No node found to start datafeed [datafeed_id], allocation explanation " + "[cannot start datafeed [datafeed_id], because the job's [job_id] state is stale]")).     tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id1", JobState.OPENED, tasksBuilder).     tasks = tasksBuilder.build().     givenClusterState("foo", 1, 0).     result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertEquals("node_id1", result.getExecutorNode()).     new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated(). }
false;public;0;19;;public void testSelectNode_GivenJobOpeningAndIndexDoesNotExist() {     // Here we test that when there are 2 problems, the most critical gets reported first.     // In this case job is Opening (non-critical) and the index does not exist (critical)     Job job = createScheduledJob("job_id").build(new Date()).     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("not_foo")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id", JobState.OPENING, tasksBuilder).     tasks = tasksBuilder.build().     givenClusterState("foo", 1, 0).     ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated()).     assertThat(e.getMessage(), containsString("No node found to start datafeed [datafeed_id], allocation explanation " + "[cannot start datafeed [datafeed_id] because index [not_foo] does not exist, is closed, or is still initializing.]")). }
false;public;0;15;;public void testSelectNode_GivenMlUpgradeMode() {     Job job = createScheduledJob("job_id").build(new Date()).     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("foo")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id", JobState.OPENED, tasksBuilder).     tasks = tasksBuilder.build().     mlMetadata = new MlMetadata.Builder().isUpgradeMode(true).build().     givenClusterState("foo", 1, 0).     PersistentTasksCustomMetaData.Assignment result = new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).selectNode().     assertThat(result, equalTo(MlTasks.AWAITING_UPGRADE)). }
false;public;0;16;;public void testCheckDatafeedTaskCanBeCreated_GivenMlUpgradeMode() {     Job job = createScheduledJob("job_id").build(new Date()).     DatafeedConfig df = createDatafeed("datafeed_id", job.getId(), Collections.singletonList("foo")).     PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder().     addJobTask(job.getId(), "node_id", JobState.OPENED, tasksBuilder).     tasks = tasksBuilder.build().     mlMetadata = new MlMetadata.Builder().isUpgradeMode(true).build().     givenClusterState("foo", 1, 0).     ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> new DatafeedNodeSelector(clusterState, resolver, df.getId(), df.getJobId(), df.getIndices()).checkDatafeedTaskCanBeCreated()).     assertThat(e.getMessage(), equalTo("Could not start datafeed [datafeed_id] as indices are being upgraded")). }
false;private;3;5;;private void givenClusterState(String index, int numberOfShards, int numberOfReplicas) {     List<Tuple<Integer, ShardRoutingState>> states = new ArrayList<>(1).     states.add(new Tuple<>(0, ShardRoutingState.STARTED)).     givenClusterState(index, numberOfShards, numberOfReplicas, states). }
false;private;4;16;;private void givenClusterState(String index, int numberOfShards, int numberOfReplicas, List<Tuple<Integer, ShardRoutingState>> states) {     IndexMetaData indexMetaData = IndexMetaData.builder(index).settings(settings(Version.CURRENT)).numberOfShards(numberOfShards).numberOfReplicas(numberOfReplicas).build().     clusterState = ClusterState.builder(new ClusterName("cluster_name")).metaData(new MetaData.Builder().putCustom(PersistentTasksCustomMetaData.TYPE, tasks).putCustom(MlMetadata.TYPE, mlMetadata).put(indexMetaData, false)).nodes(nodes).routingTable(generateRoutingTable(indexMetaData, states)).build(). }
false;private,static;2;33;;private static RoutingTable generateRoutingTable(IndexMetaData indexMetaData, List<Tuple<Integer, ShardRoutingState>> states) {     IndexRoutingTable.Builder rtBuilder = IndexRoutingTable.builder(indexMetaData.getIndex()).     final String index = indexMetaData.getIndex().getName().     int counter = 0.     for (Tuple<Integer, ShardRoutingState> state : states) {         ShardId shardId = new ShardId(index, "_na_", counter).         IndexShardRoutingTable.Builder shardRTBuilder = new IndexShardRoutingTable.Builder(shardId).         ShardRouting shardRouting.         if (state.v2().equals(ShardRoutingState.STARTED)) {             shardRouting = TestShardRouting.newShardRouting(index, shardId.getId(), "node_" + Integer.toString(state.v1()), null, true, ShardRoutingState.STARTED).         } else if (state.v2().equals(ShardRoutingState.INITIALIZING)) {             shardRouting = TestShardRouting.newShardRouting(index, shardId.getId(), "node_" + Integer.toString(state.v1()), null, true, ShardRoutingState.INITIALIZING).         } else if (state.v2().equals(ShardRoutingState.RELOCATING)) {             shardRouting = TestShardRouting.newShardRouting(index, shardId.getId(), "node_" + Integer.toString(state.v1()), "node_" + Integer.toString((state.v1() + 1) % 3), true, ShardRoutingState.RELOCATING).         } else {             shardRouting = ShardRouting.newUnassigned(shardId, true, RecoverySource.EmptyStoreRecoverySource.INSTANCE, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "")).         }         shardRTBuilder.addShard(shardRouting).         rtBuilder.addIndexShard(shardRTBuilder.build()).         counter += 1.     }     return new RoutingTable.Builder().add(rtBuilder).build(). }
