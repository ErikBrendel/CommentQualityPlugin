commented;modifiers;parameterAmount;loc;comment;code
false;public;1;40;;public void process(AutodetectProcess process) {     Context context = new Context(jobId, persister.bulkPersisterBuilder(jobId)).     // trying to write its output.     try {         readResults(process, context).         try {             if (processKilled == false) {                 context.bulkResultsPersister.executeRequest().             }         } catch (Exception e) {             LOGGER.warn(new ParameterizedMessage("[{}] Error persisting autodetect results", jobId), e).         }         LOGGER.info("[{}] {} buckets parsed from autodetect output", jobId, bucketCount).     } catch (Exception e) {         failed = true.         if (processKilled) {             // Don't log the stack trace in this case.  Log just enough to hint             // that it would have been better to close jobs before shutting down,             // but we now fully expect jobs to move between nodes without doing             // all their graceful close activities.             LOGGER.warn("[{}] some results not processed due to the process being killed", jobId).         } else if (process.isProcessAliveAfterWaiting() == false) {             // Don't log the stack trace to not shadow the root cause.             LOGGER.warn("[{}] some results not processed due to the termination of autodetect", jobId).         } else {             // We should only get here if the iterator throws in which             // case parsing the autodetect output has failed.             LOGGER.error(new ParameterizedMessage("[{}] error parsing autodetect output", jobId), e).         }     } finally {         flushListener.clear().         completionLatch.countDown().     } }
false;private;2;25;;private void readResults(AutodetectProcess process, Context context) {     bucketCount = 0.     try {         Iterator<AutodetectResult> iterator = process.readAutodetectResults().         while (iterator.hasNext()) {             try {                 AutodetectResult result = iterator.next().                 processResult(context, result).                 if (result.getBucket() != null) {                     LOGGER.trace("[{}] Bucket number {} parsed from output", jobId, bucketCount).                 }             } catch (Exception e) {                 if (processKilled) {                     throw e.                 }                 if (process.isProcessAliveAfterWaiting() == false) {                     throw e.                 }                 LOGGER.warn(new ParameterizedMessage("[{}] Error processing autodetect result", jobId), e).             }         }     } finally {         process.consumeAndCloseOutputStream().     } }
false;public;0;4;;public void setProcessKilled() {     processKilled = true.     renormalizer.shutdown(). }
false;;2;99;;void processResult(Context context, AutodetectResult result) {     if (processKilled) {         return.     }     Bucket bucket = result.getBucket().     if (bucket != null) {         if (context.deleteInterimRequired) {             // Delete any existing interim results generated by a Flush command             // which have not been replaced or superseded by new results.             LOGGER.trace("[{}] Deleting interim results", context.jobId).             persister.deleteInterimResults(context.jobId).             context.deleteInterimRequired = false.         }         // persist after deleting interim results in case the new         // results are also interim         context.bulkResultsPersister.persistBucket(bucket).executeRequest().         ++bucketCount.     }     List<AnomalyRecord> records = result.getRecords().     if (records != null && !records.isEmpty()) {         context.bulkResultsPersister.persistRecords(records).     }     List<Influencer> influencers = result.getInfluencers().     if (influencers != null && !influencers.isEmpty()) {         context.bulkResultsPersister.persistInfluencers(influencers).     }     CategoryDefinition categoryDefinition = result.getCategoryDefinition().     if (categoryDefinition != null) {         persister.persistCategoryDefinition(categoryDefinition).     }     ModelPlot modelPlot = result.getModelPlot().     if (modelPlot != null) {         context.bulkResultsPersister.persistModelPlot(modelPlot).     }     Forecast forecast = result.getForecast().     if (forecast != null) {         context.bulkResultsPersister.persistForecast(forecast).     }     ForecastRequestStats forecastRequestStats = result.getForecastRequestStats().     if (forecastRequestStats != null) {         LOGGER.trace("Received Forecast Stats [{}]", forecastRequestStats.getId()).         context.bulkResultsPersister.persistForecastRequestStats(forecastRequestStats).         // otherwise rely on the count-based trigger         switch(forecastRequestStats.getStatus()) {             case OK:             case STARTED:                 break.             case FAILED:             case SCHEDULED:             case FINISHED:             default:                 context.bulkResultsPersister.executeRequest().         }     }     ModelSizeStats modelSizeStats = result.getModelSizeStats().     if (modelSizeStats != null) {         processModelSizeStats(context, modelSizeStats).     }     ModelSnapshot modelSnapshot = result.getModelSnapshot().     if (modelSnapshot != null) {         // We need to refresh in order for the snapshot to be available when we try to update the job with it         IndexResponse indexResponse = persister.persistModelSnapshot(modelSnapshot, WriteRequest.RefreshPolicy.IMMEDIATE).         if (indexResponse.getResult() == DocWriteResponse.Result.CREATED) {             updateModelSnapshotOnJob(modelSnapshot).         }     }     Quantiles quantiles = result.getQuantiles().     if (quantiles != null) {         LOGGER.debug("[{}] Parsed Quantiles with timestamp {}", context.jobId, quantiles.getTimestamp()).         persister.persistQuantiles(quantiles).         context.bulkResultsPersister.executeRequest().         if (processKilled == false && renormalizer.isEnabled()) {             // We need to make all results written up to these quantiles available for renormalization             persister.commitResultWrites(context.jobId).             LOGGER.debug("[{}] Quantiles queued for renormalization", context.jobId).             renormalizer.renormalize(quantiles).         }     }     FlushAcknowledgement flushAcknowledgement = result.getFlushAcknowledgement().     if (flushAcknowledgement != null) {         LOGGER.debug("[{}] Flush acknowledgement parsed from output for ID {}", context.jobId, flushAcknowledgement.getId()).         // Commit previous writes here, effectively continuing         // the flush from the C++ autodetect process right         // through to the data store         context.bulkResultsPersister.executeRequest().         persister.commitResultWrites(context.jobId).         flushListener.acknowledgeFlush(flushAcknowledgement).         // Interim results may have been produced by the flush,         // which need to be         // deleted when the next finalized results come through         context.deleteInterimRequired = true.     } }
false;private;2;10;;private void processModelSizeStats(Context context, ModelSizeStats modelSizeStats) {     LOGGER.trace("[{}] Parsed ModelSizeStats: {} / {} / {} / {} / {} / {}", context.jobId, modelSizeStats.getModelBytes(), modelSizeStats.getTotalByFieldCount(), modelSizeStats.getTotalOverFieldCount(), modelSizeStats.getTotalPartitionFieldCount(), modelSizeStats.getBucketAllocationFailuresCount(), modelSizeStats.getMemoryStatus()).     persister.persistModelSizeStats(modelSizeStats).     notifyModelMemoryStatusChange(context, modelSizeStats).     latestModelSizeStats = modelSizeStats. }
false;private;2;11;;private void notifyModelMemoryStatusChange(Context context, ModelSizeStats modelSizeStats) {     ModelSizeStats.MemoryStatus memoryStatus = modelSizeStats.getMemoryStatus().     if (memoryStatus != latestModelSizeStats.getMemoryStatus()) {         if (memoryStatus == ModelSizeStats.MemoryStatus.SOFT_LIMIT) {             auditor.warning(context.jobId, Messages.getMessage(Messages.JOB_AUDIT_MEMORY_STATUS_SOFT_LIMIT)).         } else if (memoryStatus == ModelSizeStats.MemoryStatus.HARD_LIMIT) {             auditor.error(context.jobId, Messages.getMessage(Messages.JOB_AUDIT_MEMORY_STATUS_HARD_LIMIT, new ByteSizeValue(modelSizeStats.getModelBytes(), ByteSizeUnit.BYTES).toString())).         }     } }
false;public;1;5;;@Override public void onResponse(PutJobAction.Response response) {     updateModelSnapshotSemaphore.release().     LOGGER.debug("[{}] Updated job with model snapshot id [{}]", jobId, modelSnapshot.getSnapshotId()). }
false;public;1;6;;@Override public void onFailure(Exception e) {     updateModelSnapshotSemaphore.release().     LOGGER.error("[" + jobId + "] Failed to update job with new model snapshot id [" + modelSnapshot.getSnapshotId() + "]", e). }
false;protected;1;30;;protected void updateModelSnapshotOnJob(ModelSnapshot modelSnapshot) {     JobUpdate update = new JobUpdate.Builder(jobId).setModelSnapshotId(modelSnapshot.getSnapshotId()).build().     UpdateJobAction.Request updateRequest = UpdateJobAction.Request.internal(jobId, update).     try {         // This blocks the main processing thread in the unlikely event         // there are 2 model snapshots queued up. But it also has the         // advantage of ensuring order         updateModelSnapshotSemaphore.acquire().     } catch (InterruptedException e) {         Thread.currentThread().interrupt().         LOGGER.info("[{}] Interrupted acquiring update model snapshot semaphore", jobId).         return.     }     executeAsyncWithOrigin(client, ML_ORIGIN, UpdateJobAction.INSTANCE, updateRequest, new ActionListener<PutJobAction.Response>() {          @Override         public void onResponse(PutJobAction.Response response) {             updateModelSnapshotSemaphore.release().             LOGGER.debug("[{}] Updated job with model snapshot id [{}]", jobId, modelSnapshot.getSnapshotId()).         }          @Override         public void onFailure(Exception e) {             updateModelSnapshotSemaphore.release().             LOGGER.error("[" + jobId + "] Failed to update job with new model snapshot id [" + modelSnapshot.getSnapshotId() + "]", e).         }     }). }
false;public;0;24;;public void awaitCompletion() throws TimeoutException {     try {         // until the state is persisted, and that can take a while         if (completionLatch.await(MachineLearningField.STATE_PERSIST_RESTORE_TIMEOUT.getMinutes(), TimeUnit.MINUTES) == false) {             throw new TimeoutException("Timed out waiting for results processor to complete for job " + jobId).         }         // Input stream has been completely processed at this point.         // Wait for any updateModelSnapshotOnJob calls to complete.         updateModelSnapshotSemaphore.acquire().         updateModelSnapshotSemaphore.release().         // These lines ensure that the "completion" we're awaiting includes making the results searchable         waitUntilRenormalizerIsIdle().         persister.commitResultWrites(jobId).         persister.commitStateWrites(jobId).     } catch (InterruptedException e) {         Thread.currentThread().interrupt().         LOGGER.info("[{}] Interrupted waiting for results processor to complete", jobId).     } }
true;public;2;4;/**  * Blocks until a flush is acknowledged or the timeout expires, whichever happens first.  *  * @param flushId the id of the flush request to wait for  * @param timeout the timeout  * @return The {@link FlushAcknowledgement} if the flush has completed or the parsing finished. {@code null} if the timeout expired  */ ;/**  * Blocks until a flush is acknowledged or the timeout expires, whichever happens first.  *  * @param flushId the id of the flush request to wait for  * @param timeout the timeout  * @return The {@link FlushAcknowledgement} if the flush has completed or the parsing finished. {@code null} if the timeout expired  */ @Nullable public FlushAcknowledgement waitForFlushAcknowledgement(String flushId, Duration timeout) throws InterruptedException {     return failed ? null : flushListener.waitForFlush(flushId, timeout). }
false;public;1;3;;public void clearAwaitingFlush(String flushId) {     flushListener.clear(flushId). }
false;public;0;3;;public void waitUntilRenormalizerIsIdle() {     renormalizer.waitUntilIdle(). }
true;public;0;3;/**  * If failed then there was an error parsing the results that cannot be recovered from  * @return true if failed  */ ;/**  * If failed then there was an error parsing the results that cannot be recovered from  * @return true if failed  */ public boolean isFailed() {     return failed. }
false;public;0;3;;public ModelSizeStats modelSizeStats() {     return latestModelSizeStats. }
