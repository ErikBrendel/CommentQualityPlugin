commented;modifiers;parameterAmount;loc;comment;code
false;public;1;4;;@Override public void onResponse(PersistentTask<?> persistentTask) {     taskRunner.runWhenJobIsOpened(task). }
false;public;1;4;;@Override public void onFailure(Exception e) {     finishHandler.accept(e). }
false;public;2;24;;public void run(TransportStartDatafeedAction.DatafeedTask task, Consumer<Exception> finishHandler) {     String datafeedId = task.getDatafeedId().     ActionListener<DatafeedJob> datafeedJobHandler = ActionListener.wrap(datafeedJob -> {         Holder holder = new Holder(task, datafeedId, datafeedJob, new ProblemTracker(auditor, datafeedJob.getJobId()), finishHandler).         runningDatafeedsOnThisNode.put(task.getAllocationId(), holder).         task.updatePersistentTaskState(DatafeedState.STARTED, new ActionListener<PersistentTask<?>>() {              @Override             public void onResponse(PersistentTask<?> persistentTask) {                 taskRunner.runWhenJobIsOpened(task).             }              @Override             public void onFailure(Exception e) {                 finishHandler.accept(e).             }         }).     }, finishHandler::accept).     datafeedJobBuilder.build(datafeedId, datafeedJobHandler). }
false;public;3;7;;public void stopDatafeed(TransportStartDatafeedAction.DatafeedTask task, String reason, TimeValue timeout) {     logger.info("[{}] attempt to stop datafeed [{}] [{}]", reason, task.getDatafeedId(), task.getAllocationId()).     Holder holder = runningDatafeedsOnThisNode.remove(task.getAllocationId()).     if (holder != null) {         holder.stop(reason, timeout, null).     } }
true;public;1;10;/**  * This is used when the license expires.  */ ;/**  * This is used when the license expires.  */ public void stopAllDatafeedsOnThisNode(String reason) {     int numDatafeeds = runningDatafeedsOnThisNode.size().     if (numDatafeeds != 0) {         logger.info("Closing [{}] datafeeds, because [{}]", numDatafeeds, reason).         for (Holder holder : runningDatafeedsOnThisNode.values()) {             holder.stop(reason, TimeValue.timeValueSeconds(20), null).         }     } }
true;public;0;10;/**  * This is used before the JVM is killed.  It differs from stopAllDatafeedsOnThisNode in that it leaves  * the datafeed tasks in the "started" state, so that they get restarted on a different node.  */ ;/**  * This is used before the JVM is killed.  It differs from stopAllDatafeedsOnThisNode in that it leaves  * the datafeed tasks in the "started" state, so that they get restarted on a different node.  */ public void isolateAllDatafeedsOnThisNodeBeforeShutdown() {     Iterator<Holder> iter = runningDatafeedsOnThisNode.values().iterator().     while (iter.hasNext()) {         Holder next = iter.next().         next.isolateDatafeed().         // TODO: it's not ideal that this "isolate" method does something a bit different to the one below         next.setNodeIsShuttingDown().         iter.remove().     } }
false;public;1;8;;public void isolateDatafeed(long allocationId) {     // This calls get() rather than remove() because we expect that the persistent task will     // be removed shortly afterwards and that operation needs to be able to find the holder     Holder holder = runningDatafeedsOnThisNode.get(allocationId).     if (holder != null) {         holder.isolateDatafeed().     } }
false;public;1;5;;@Override public void onFailure(Exception e) {     logger.error("Failed lookback import for job [" + holder.datafeedJob.getJobId() + "]", e).     holder.stop("general_lookback_failure", TimeValue.timeValueSeconds(20), e). }
false;protected;0;43;;@Override protected void doRun() {     Long next = null.     try {         next = holder.executeLookBack(startTime, endTime).     } catch (DatafeedJob.ExtractionProblemException e) {         if (endTime == null) {             next = e.nextDelayInMsSinceEpoch.         }         holder.problemTracker.reportExtractionProblem(e.getCause().getMessage()).     } catch (DatafeedJob.AnalysisProblemException e) {         if (endTime == null) {             next = e.nextDelayInMsSinceEpoch.         }         holder.problemTracker.reportAnalysisProblem(e.getCause().getMessage()).         if (e.shouldStop) {             holder.stop("lookback_analysis_error", TimeValue.timeValueSeconds(20), e).             return.         }     } catch (DatafeedJob.EmptyDataCountException e) {         if (endTime == null) {             holder.problemTracker.reportEmptyDataCount().             next = e.nextDelayInMsSinceEpoch.         } else {             // Notify that a lookback-only run found no data             String lookbackNoDataMsg = Messages.getMessage(Messages.JOB_AUDIT_DATAFEED_LOOKBACK_NO_DATA).             logger.warn("[{}] {}", holder.datafeedJob.getJobId(), lookbackNoDataMsg).             auditor.warning(holder.datafeedJob.getJobId(), lookbackNoDataMsg).         }     } catch (Exception e) {         logger.error("Failed lookback import for job [" + holder.datafeedJob.getJobId() + "]", e).         holder.stop("general_lookback_failure", TimeValue.timeValueSeconds(20), e).         return.     }     if (holder.isIsolated() == false) {         if (next != null) {             doDatafeedRealtime(next, holder.datafeedJob.getJobId(), holder).         } else {             holder.stop("no_realtime", TimeValue.timeValueSeconds(20), null).             holder.problemTracker.finishReport().         }     } }
true;private;3;55;// the DatafeedTask without stopping datafeed, which causes the datafeed to keep on running. ;// Important: Holder must be created and assigned to DatafeedTask before setting state to started, // otherwise if a stop datafeed call is made immediately after the start datafeed call we could cancel // the DatafeedTask without stopping datafeed, which causes the datafeed to keep on running. private void innerRun(Holder holder, long startTime, Long endTime) {     holder.cancellable = Scheduler.wrapAsCancellable(threadPool.executor(MachineLearning.DATAFEED_THREAD_POOL_NAME).submit(new AbstractRunnable() {          @Override         public void onFailure(Exception e) {             logger.error("Failed lookback import for job [" + holder.datafeedJob.getJobId() + "]", e).             holder.stop("general_lookback_failure", TimeValue.timeValueSeconds(20), e).         }          @Override         protected void doRun() {             Long next = null.             try {                 next = holder.executeLookBack(startTime, endTime).             } catch (DatafeedJob.ExtractionProblemException e) {                 if (endTime == null) {                     next = e.nextDelayInMsSinceEpoch.                 }                 holder.problemTracker.reportExtractionProblem(e.getCause().getMessage()).             } catch (DatafeedJob.AnalysisProblemException e) {                 if (endTime == null) {                     next = e.nextDelayInMsSinceEpoch.                 }                 holder.problemTracker.reportAnalysisProblem(e.getCause().getMessage()).                 if (e.shouldStop) {                     holder.stop("lookback_analysis_error", TimeValue.timeValueSeconds(20), e).                     return.                 }             } catch (DatafeedJob.EmptyDataCountException e) {                 if (endTime == null) {                     holder.problemTracker.reportEmptyDataCount().                     next = e.nextDelayInMsSinceEpoch.                 } else {                     // Notify that a lookback-only run found no data                     String lookbackNoDataMsg = Messages.getMessage(Messages.JOB_AUDIT_DATAFEED_LOOKBACK_NO_DATA).                     logger.warn("[{}] {}", holder.datafeedJob.getJobId(), lookbackNoDataMsg).                     auditor.warning(holder.datafeedJob.getJobId(), lookbackNoDataMsg).                 }             } catch (Exception e) {                 logger.error("Failed lookback import for job [" + holder.datafeedJob.getJobId() + "]", e).                 holder.stop("general_lookback_failure", TimeValue.timeValueSeconds(20), e).                 return.             }             if (holder.isIsolated() == false) {                 if (next != null) {                     doDatafeedRealtime(next, holder.datafeedJob.getJobId(), holder).                 } else {                     holder.stop("no_realtime", TimeValue.timeValueSeconds(20), null).                     holder.problemTracker.finishReport().                 }             }         }     })). }
false;public;1;5;;@Override public void onFailure(Exception e) {     logger.error("Unexpected datafeed failure for job [" + jobId + "] stopping...", e).     holder.stop("general_realtime_error", TimeValue.timeValueSeconds(20), e). }
false;protected;0;29;;@Override protected void doRun() {     long nextDelayInMsSinceEpoch.     try {         nextDelayInMsSinceEpoch = holder.executeRealTime().         holder.problemTracker.reportNoneEmptyCount().     } catch (DatafeedJob.ExtractionProblemException e) {         nextDelayInMsSinceEpoch = e.nextDelayInMsSinceEpoch.         holder.problemTracker.reportExtractionProblem(e.getCause().getMessage()).     } catch (DatafeedJob.AnalysisProblemException e) {         nextDelayInMsSinceEpoch = e.nextDelayInMsSinceEpoch.         holder.problemTracker.reportAnalysisProblem(e.getCause().getMessage()).         if (e.shouldStop) {             holder.stop("realtime_analysis_error", TimeValue.timeValueSeconds(20), e).             return.         }     } catch (DatafeedJob.EmptyDataCountException e) {         nextDelayInMsSinceEpoch = e.nextDelayInMsSinceEpoch.         holder.problemTracker.reportEmptyDataCount().     } catch (Exception e) {         logger.error("Unexpected datafeed failure for job [" + jobId + "] stopping...", e).         holder.stop("general_realtime_error", TimeValue.timeValueSeconds(20), e).         return.     }     holder.problemTracker.finishReport().     if (nextDelayInMsSinceEpoch >= 0) {         doDatafeedRealtime(nextDelayInMsSinceEpoch, jobId, holder).     } }
false;;3;44;;void doDatafeedRealtime(long delayInMsSinceEpoch, String jobId, Holder holder) {     if (holder.isRunning() && !holder.isIsolated()) {         TimeValue delay = computeNextDelay(delayInMsSinceEpoch).         logger.debug("Waiting [{}] before executing next realtime import for job [{}]", delay, jobId).         holder.cancellable = threadPool.schedule(new AbstractRunnable() {              @Override             public void onFailure(Exception e) {                 logger.error("Unexpected datafeed failure for job [" + jobId + "] stopping...", e).                 holder.stop("general_realtime_error", TimeValue.timeValueSeconds(20), e).             }              @Override             protected void doRun() {                 long nextDelayInMsSinceEpoch.                 try {                     nextDelayInMsSinceEpoch = holder.executeRealTime().                     holder.problemTracker.reportNoneEmptyCount().                 } catch (DatafeedJob.ExtractionProblemException e) {                     nextDelayInMsSinceEpoch = e.nextDelayInMsSinceEpoch.                     holder.problemTracker.reportExtractionProblem(e.getCause().getMessage()).                 } catch (DatafeedJob.AnalysisProblemException e) {                     nextDelayInMsSinceEpoch = e.nextDelayInMsSinceEpoch.                     holder.problemTracker.reportAnalysisProblem(e.getCause().getMessage()).                     if (e.shouldStop) {                         holder.stop("realtime_analysis_error", TimeValue.timeValueSeconds(20), e).                         return.                     }                 } catch (DatafeedJob.EmptyDataCountException e) {                     nextDelayInMsSinceEpoch = e.nextDelayInMsSinceEpoch.                     holder.problemTracker.reportEmptyDataCount().                 } catch (Exception e) {                     logger.error("Unexpected datafeed failure for job [" + jobId + "] stopping...", e).                     holder.stop("general_realtime_error", TimeValue.timeValueSeconds(20), e).                     return.                 }                 holder.problemTracker.finishReport().                 if (nextDelayInMsSinceEpoch >= 0) {                     doDatafeedRealtime(nextDelayInMsSinceEpoch, jobId, holder).                 }             }         }, delay, MachineLearning.DATAFEED_THREAD_POOL_NAME).     } }
false;private;1;3;;private String getJobId(TransportStartDatafeedAction.DatafeedTask task) {     return runningDatafeedsOnThisNode.get(task.getAllocationId()).getJobId(). }
false;private;2;3;;private JobState getJobState(PersistentTasksCustomMetaData tasks, TransportStartDatafeedAction.DatafeedTask datafeedTask) {     return MlTasks.getJobStateModifiedForReassignments(getJobId(datafeedTask), tasks). }
false;private;2;14;;private boolean jobHasOpenAutodetectCommunicator(PersistentTasksCustomMetaData tasks, TransportStartDatafeedAction.DatafeedTask datafeedTask) {     PersistentTasksCustomMetaData.PersistentTask<?> jobTask = MlTasks.getJobTask(getJobId(datafeedTask), tasks).     if (jobTask == null) {         return false.     }     JobTaskState state = (JobTaskState) jobTask.getState().     if (state == null || state.isStatusStale(jobTask)) {         return false.     }     return autodetectProcessManager.hasOpenAutodetectCommunicator(jobTask.getAllocationId()). }
false;private;1;3;;private TimeValue computeNextDelay(long next) {     return new TimeValue(Math.max(1, next - currentTimeSupplier.get())). }
true;;1;3;/**  * Visible for testing  */ ;/**  * Visible for testing  */ boolean isRunning(long allocationId) {     return runningDatafeedsOnThisNode.containsKey(allocationId). }
false;;0;3;;String getJobId() {     return datafeedJob.getJobId(). }
false;;0;3;;boolean isRunning() {     return datafeedJob.isRunning(). }
false;;0;3;;boolean isIsolated() {     return datafeedJob.isIsolated(). }
false;public;3;37;;public void stop(String source, TimeValue timeout, Exception e) {     if (isNodeShuttingDown) {         return.     }     logger.info("[{}] attempt to stop datafeed [{}] for job [{}]", source, datafeedId, datafeedJob.getJobId()).     if (datafeedJob.stop()) {         boolean acquired = false.         try {             logger.info("[{}] try lock [{}] to stop datafeed [{}] for job [{}]...", source, timeout, datafeedId, datafeedJob.getJobId()).             acquired = datafeedJobLock.tryLock(timeout.millis(), TimeUnit.MILLISECONDS).         } catch (InterruptedException e1) {             Thread.currentThread().interrupt().         } finally {             logger.info("[{}] stopping datafeed [{}] for job [{}], acquired [{}]...", source, datafeedId, datafeedJob.getJobId(), acquired).             runningDatafeedsOnThisNode.remove(allocationId).             if (cancellable != null) {                 cancellable.cancel().             }             auditor.info(datafeedJob.getJobId(), Messages.getMessage(isIsolated() ? Messages.JOB_AUDIT_DATAFEED_ISOLATED : Messages.JOB_AUDIT_DATAFEED_STOPPED)).             finishHandler.accept(e).             logger.info("[{}] datafeed [{}] for job [{}] has been stopped{}", source, datafeedId, datafeedJob.getJobId(), acquired ? "" : ", but there may be pending tasks as the timeout [" + timeout.getStringRep() + "] expired").             if (autoCloseJob && isIsolated() == false) {                 closeJob().             }             if (acquired) {                 datafeedJobLock.unlock().             }         }     } else {         logger.info("[{}] datafeed [{}] for job [{}] was already stopped", source, datafeedId, datafeedJob.getJobId()).     } }
true;public;0;3;/**  * This stops a datafeed WITHOUT updating the corresponding persistent task.  When called it  * will stop the datafeed from sending data to its job as quickly as possible.  The caller  * must do something sensible with the corresponding persistent task.  If the node is shutting  * down the task will automatically get reassigned.  Otherwise the caller must take action to  * remove or reassign the persistent task, or the datafeed will be left in limbo.  */ ;/**  * This stops a datafeed WITHOUT updating the corresponding persistent task.  When called it  * will stop the datafeed from sending data to its job as quickly as possible.  The caller  * must do something sensible with the corresponding persistent task.  If the node is shutting  * down the task will automatically get reassigned.  Otherwise the caller must take action to  * remove or reassign the persistent task, or the datafeed will be left in limbo.  */ public void isolateDatafeed() {     datafeedJob.isolate(). }
false;public;0;3;;public void setNodeIsShuttingDown() {     isNodeShuttingDown = true. }
false;private;2;12;;private Long executeLookBack(long startTime, Long endTime) throws Exception {     datafeedJobLock.lock().     try {         if (isRunning() && !isIsolated()) {             return datafeedJob.runLookBack(startTime, endTime).         } else {             return null.         }     } finally {         datafeedJobLock.unlock().     } }
false;private;0;12;;private long executeRealTime() throws Exception {     datafeedJobLock.lock().     try {         if (isRunning() && !isIsolated()) {             return datafeedJob.runRealtime().         } else {             return -1L.         }     } finally {         datafeedJobLock.unlock().     } }
false;public;1;6;;@Override public void onResponse(CloseJobAction.Response response) {     if (!response.isClosed()) {         logger.error("[{}] job close action was not acknowledged", getJobId()).     } }
false;public;1;12;;@Override public void onFailure(Exception e) {     // and it's misleading/worrying to log an error in this case.     if (e instanceof ElasticsearchStatusException && ((ElasticsearchStatusException) e).status() == RestStatus.CONFLICT) {         logger.debug("[{}] {}", getJobId(), e.getMessage()).     } else {         logger.error("[" + getJobId() + "] failed to auto-close job", e).     } }
false;public;1;41;;@Override public void onResponse(PersistentTask<StartDatafeedAction.DatafeedParams> persistentTask) {     CloseJobAction.Request closeJobRequest = new CloseJobAction.Request(getJobId()).     /*                         Enforces that for the close job api call the current node is the coordinating node.                         If we are in this callback then the local node's cluster state doesn't contain a persistent task                         for the datafeed and therefor the datafeed is stopped, so there is no need for the master node to                         be to coordinating node.                          Normally close job and stop datafeed are both executed via master node and both apis use master                         node's local cluster state for validation purposes. In case of auto close this isn't the case and                         if the job runs on a regular node then it may see the update before the close job api does in                         the master node's local cluster state. This can cause the close job api the fail with a validation                         error that the datafeed isn't stopped. To avoid this we use the current node as coordinating node                         for the close job api call.                     */     closeJobRequest.setLocal(true).     executeAsyncWithOrigin(client, ML_ORIGIN, CloseJobAction.INSTANCE, closeJobRequest, new ActionListener<CloseJobAction.Response>() {          @Override         public void onResponse(CloseJobAction.Response response) {             if (!response.isClosed()) {                 logger.error("[{}] job close action was not acknowledged", getJobId()).             }         }          @Override         public void onFailure(Exception e) {             // and it's misleading/worrying to log an error in this case.             if (e instanceof ElasticsearchStatusException && ((ElasticsearchStatusException) e).status() == RestStatus.CONFLICT) {                 logger.debug("[{}] {}", getJobId(), e.getMessage()).             } else {                 logger.error("[" + getJobId() + "] failed to auto-close job", e).             }         }     }). }
false;public;1;4;;@Override public void onFailure(Exception e) {     logger.error("Failed to remove datafeed persistent task - will not auto close job [" + getJobId() + "]", e). }
false;private;0;59;;private void closeJob() {     ClusterState clusterState = clusterService.state().     PersistentTasksCustomMetaData tasks = clusterState.getMetaData().custom(PersistentTasksCustomMetaData.TYPE).     JobState jobState = MlTasks.getJobState(getJobId(), tasks).     if (jobState != JobState.OPENED) {         logger.debug("[{}] No need to auto-close job as job state is [{}]", getJobId(), jobState).         return.     }     task.waitForPersistentTask(Objects::isNull, TimeValue.timeValueSeconds(20), new WaitForPersistentTaskListener<StartDatafeedAction.DatafeedParams>() {          @Override         public void onResponse(PersistentTask<StartDatafeedAction.DatafeedParams> persistentTask) {             CloseJobAction.Request closeJobRequest = new CloseJobAction.Request(getJobId()).             /*                         Enforces that for the close job api call the current node is the coordinating node.                         If we are in this callback then the local node's cluster state doesn't contain a persistent task                         for the datafeed and therefor the datafeed is stopped, so there is no need for the master node to                         be to coordinating node.                          Normally close job and stop datafeed are both executed via master node and both apis use master                         node's local cluster state for validation purposes. In case of auto close this isn't the case and                         if the job runs on a regular node then it may see the update before the close job api does in                         the master node's local cluster state. This can cause the close job api the fail with a validation                         error that the datafeed isn't stopped. To avoid this we use the current node as coordinating node                         for the close job api call.                     */             closeJobRequest.setLocal(true).             executeAsyncWithOrigin(client, ML_ORIGIN, CloseJobAction.INSTANCE, closeJobRequest, new ActionListener<CloseJobAction.Response>() {                  @Override                 public void onResponse(CloseJobAction.Response response) {                     if (!response.isClosed()) {                         logger.error("[{}] job close action was not acknowledged", getJobId()).                     }                 }                  @Override                 public void onFailure(Exception e) {                     // and it's misleading/worrying to log an error in this case.                     if (e instanceof ElasticsearchStatusException && ((ElasticsearchStatusException) e).status() == RestStatus.CONFLICT) {                         logger.debug("[{}] {}", getJobId(), e.getMessage()).                     } else {                         logger.error("[" + getJobId() + "] failed to auto-close job", e).                     }                 }             }).         }          @Override         public void onFailure(Exception e) {             logger.error("Failed to remove datafeed persistent task - will not auto close job [" + getJobId() + "]", e).         }     }). }
false;private;1;11;;private void runWhenJobIsOpened(TransportStartDatafeedAction.DatafeedTask datafeedTask) {     ClusterState clusterState = clusterService.state().     PersistentTasksCustomMetaData tasks = clusterState.getMetaData().custom(PersistentTasksCustomMetaData.TYPE).     if (getJobState(tasks, datafeedTask) == JobState.OPENED && jobHasOpenAutodetectCommunicator(tasks, datafeedTask)) {         runTask(datafeedTask).     } else {         logger.info("Datafeed [{}] is waiting for job [{}] to be opened", datafeedTask.getDatafeedId(), getJobId(datafeedTask)).         tasksToRun.add(datafeedTask).     } }
false;private;1;12;;private void runTask(TransportStartDatafeedAction.DatafeedTask task) {     // tests, leading to bugs not caught in CI due to many tests running in single node test clusters.     try (ThreadContext.StoredContext ignore = threadPool.getThreadContext().stashContext()) {         innerRun(runningDatafeedsOnThisNode.get(task.getAllocationId()), task.getDatafeedStartTime(), task.getEndTime()).     } }
false;public;1;29;;@Override public void clusterChanged(ClusterChangedEvent event) {     if (tasksToRun.isEmpty() || event.metaDataChanged() == false) {         return.     }     PersistentTasksCustomMetaData previousTasks = event.previousState().getMetaData().custom(PersistentTasksCustomMetaData.TYPE).     PersistentTasksCustomMetaData currentTasks = event.state().getMetaData().custom(PersistentTasksCustomMetaData.TYPE).     if (Objects.equals(previousTasks, currentTasks)) {         return.     }     List<TransportStartDatafeedAction.DatafeedTask> remainingTasks = new ArrayList<>().     for (TransportStartDatafeedAction.DatafeedTask datafeedTask : tasksToRun) {         if (runningDatafeedsOnThisNode.containsKey(datafeedTask.getAllocationId()) == false) {             continue.         }         JobState jobState = getJobState(currentTasks, datafeedTask).         if (jobState == JobState.OPENING || jobHasOpenAutodetectCommunicator(currentTasks, datafeedTask) == false) {             remainingTasks.add(datafeedTask).         } else if (jobState == JobState.OPENED) {             runTask(datafeedTask).         } else {             logger.warn("Datafeed [{}] is stopping because job [{}] state is [{}]", datafeedTask.getDatafeedId(), getJobId(datafeedTask), jobState).             datafeedTask.stop("job_never_opened", TimeValue.timeValueSeconds(20)).         }     }     tasksToRun.retainAll(remainingTasks). }
