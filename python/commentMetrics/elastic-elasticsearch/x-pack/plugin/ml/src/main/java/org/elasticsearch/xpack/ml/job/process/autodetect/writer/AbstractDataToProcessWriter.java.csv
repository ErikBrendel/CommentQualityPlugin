commented;modifiers;parameterAmount;loc;comment;code
true;;1;9;/**  * Set up the field index mappings. This must be called before  * {@linkplain DataToProcessWriter#write(InputStream, CategorizationAnalyzer, XContentType, BiConsumer)}  * <p>  * Finds the required input indexes in the <code>header</code> and sets the  * mappings to the corresponding output indexes.  */ ;/**  * Set up the field index mappings. This must be called before  * {@linkplain DataToProcessWriter#write(InputStream, CategorizationAnalyzer, XContentType, BiConsumer)}  * <p>  * Finds the required input indexes in the <code>header</code> and sets the  * mappings to the corresponding output indexes.  */ void buildFieldIndexMapping(String[] header) {     Collection<String> inputFields = inputFields().     inFieldIndexes = inputFieldIndexes(header, inputFields).     checkForMissingFields(inputFields, inFieldIndexes, header).     inputOutputMap = createInputOutputMap(inFieldIndexes).     // The time field doesn't count     dataCountsReporter.setAnalysedFieldsPerRecord(inputFields().size() - 1). }
true;public;0;15;/**  * Write the header.  * The header is created from the list of analysis input fields, the time field and the control field.  */ ;/**  * Write the header.  * The header is created from the list of analysis input fields, the time field and the control field.  */ @Override public void writeHeader() throws IOException {     Map<String, Integer> outFieldIndexes = outputFieldIndexes().     // header is all the analysis input fields + the time field + control field     int numFields = outFieldIndexes.size().     String[] record = new String[numFields].     for (Map.Entry<String, Integer> entry : outFieldIndexes.entrySet()) {         record[entry.getValue()] = entry.getKey().     }     // Write the header     autodetectProcess.writeRecord(record). }
true;protected;3;7;/**  * Tokenize the field that has been configured for categorization, and store the resulting list of tokens in CSV  * format in the appropriate field of the record to be sent to the analytics.  * @param categorizationAnalyzer   The analyzer to use to convert the categorization field to a list of tokens  * @param categorizationFieldValue The value of the categorization field to be tokenized  * @param record                   The record to be sent to the analytics  */ ;/**  * Tokenize the field that has been configured for categorization, and store the resulting list of tokens in CSV  * format in the appropriate field of the record to be sent to the analytics.  * @param categorizationAnalyzer   The analyzer to use to convert the categorization field to a list of tokens  * @param categorizationFieldValue The value of the categorization field to be tokenized  * @param record                   The record to be sent to the analytics  */ protected void tokenizeForCategorization(CategorizationAnalyzer categorizationAnalyzer, String categorizationFieldValue, String[] record) {     assert includeTokensField.     // -2 because last field is the control field, and last but one is the pre-tokenized tokens field     record[record.length - 2] = tokenizeForCategorization(categorizationAnalyzer, analysisConfig.getCategorizationFieldName(), categorizationFieldValue). }
true;static;3;17;/**  * Accessible for testing only.  */ ;/**  * Accessible for testing only.  */ static String tokenizeForCategorization(CategorizationAnalyzer categorizationAnalyzer, String categorizationFieldName, String categorizationFieldValue) {     StringBuilder builder = new StringBuilder().     CsvContext context = new CsvContext(0, 0, 0).     // Using the CsvEncoder directly is faster than using a CsvLineWriter with end-of-line set to the empty string     CsvEncoder encoder = new DefaultCsvEncoder().     boolean first = true.     for (String token : categorizationAnalyzer.tokenizeField(categorizationFieldName, categorizationFieldValue)) {         if (first) {             first = false.         } else {             builder.appendCodePoint(CsvPreference.STANDARD_PREFERENCE.getDelimiterChar()).         }         builder.append(encoder.encode(token, context, CsvPreference.STANDARD_PREFERENCE)).     }     return builder.toString(). }
true;protected;2;33;/**  * Transform the date in the input data and write all fields to the length encoded writer.  * <p>  * Fields  must be copied from input to output before this function is called.  *  * @param record             The record that will be written to the length encoded writer after the time has been transformed.  *                           This should be the same size as the number of output (analysis fields) i.e.  *                           the size of the map returned by {@linkplain #outputFieldIndexes()}  * @param numberOfFieldsRead The total number read not just those included in the analysis  */ ;/**  * Transform the date in the input data and write all fields to the length encoded writer.  * <p>  * Fields  must be copied from input to output before this function is called.  *  * @param record             The record that will be written to the length encoded writer after the time has been transformed.  *                           This should be the same size as the number of output (analysis fields) i.e.  *                           the size of the map returned by {@linkplain #outputFieldIndexes()}  * @param numberOfFieldsRead The total number read not just those included in the analysis  */ protected boolean transformTimeAndWrite(String[] record, long numberOfFieldsRead) throws IOException {     long epochMs.     try {         epochMs = dateTransformer.transform(record[TIME_FIELD_OUT_INDEX]).     } catch (CannotParseTimestampException e) {         dataCountsReporter.reportDateParseError(numberOfFieldsRead).         logger.error(e.getMessage()).         return false.     }     record[TIME_FIELD_OUT_INDEX] = Long.toString(epochMs / MS_IN_SECOND).     // Records have epoch seconds timestamp so compare for out of order in seconds     if (epochMs / MS_IN_SECOND < latestEpochMs / MS_IN_SECOND - latencySeconds) {         // out of order         dataCountsReporter.reportOutOfOrderRecord(numberOfFieldsRead).         if (epochMs > latestEpochMsThisUpload) {             // record this timestamp even if the record won't be processed             latestEpochMsThisUpload = epochMs.             dataCountsReporter.reportLatestTimeIncrementalStats(latestEpochMsThisUpload).         }         return false.     }     latestEpochMs = Math.max(latestEpochMs, epochMs).     latestEpochMsThisUpload = latestEpochMs.     autodetectProcess.writeRecord(record).     dataCountsReporter.reportRecordWritten(numberOfFieldsRead, epochMs).     return true. }
false;public;0;4;;@Override public void flushStream() throws IOException {     autodetectProcess.flushStream(). }
true;final;0;6;/**  * Get all the expected input fields i.e. all the fields we  * must see in the csv header  */ ;/**  * Get all the expected input fields i.e. all the fields we  * must see in the csv header  */ final Collection<String> inputFields() {     Set<String> requiredFields = analysisConfig.analysisFields().     requiredFields.add(dataDescription.getTimeField()).     requiredFields.remove(AnalysisConfig.ML_CATEGORY_FIELD).     return requiredFields. }
true;protected,final;2;14;/**  * Find the indexes of the input fields from the header  */ ;/**  * Find the indexes of the input fields from the header  */ protected final Map<String, Integer> inputFieldIndexes(String[] header, Collection<String> inputFields) {     // TODO header could be empty     List<String> headerList = Arrays.asList(header).     Map<String, Integer> fieldIndexes = new HashMap<>().     for (String field : inputFields) {         int index = headerList.indexOf(field).         if (index >= 0) {             fieldIndexes.put(field, index).         }     }     return fieldIndexes. }
false;;0;3;;Map<String, Integer> getInputFieldIndexes() {     return inFieldIndexes. }
true;protected,final;0;25;/**  * Create indexes of the output fields.  * This is the time field and all the fields configured for analysis  * and the control field.  * Time is the first field and the last is the control field  */ ;/**  * Create indexes of the output fields.  * This is the time field and all the fields configured for analysis  * and the control field.  * Time is the first field and the last is the control field  */ protected final Map<String, Integer> outputFieldIndexes() {     Map<String, Integer> fieldIndexes = new HashMap<>().     // time field     fieldIndexes.put(dataDescription.getTimeField(), TIME_FIELD_OUT_INDEX).     int index = TIME_FIELD_OUT_INDEX + 1.     for (String field : analysisConfig.analysisFields()) {         if (AnalysisConfig.ML_CATEGORY_FIELD.equals(field) == false) {             fieldIndexes.put(field, index++).         }     }     // field for categorization tokens     if (includeTokensField) {         fieldIndexes.put(LengthEncodedWriter.PRETOKENISED_TOKEN_FIELD, index++).     }     // control field     if (includeControlField) {         fieldIndexes.put(LengthEncodedWriter.CONTROL_FIELD_NAME, index++).     }     return fieldIndexes. }
true;protected;0;3;/**  * The number of fields used in the analysis field,  * the time field and (sometimes) the control field  */ ;/**  * The number of fields used in the analysis field,  * the time field and (sometimes) the control field  */ protected int outputFieldCount() {     return inputFields().size() + (includeControlField ? 1 : 0) + (includeTokensField ? 1 : 0). }
true;private;1;23;/**  * Create a map of input index to output index. This does not include the time or control fields.  *  * @param inFieldIndexes Map of field name to index in the input array  */ ;/**  * Create a map of input index to output index. This does not include the time or control fields.  *  * @param inFieldIndexes Map of field name to index in the input array  */ private List<InputOutputMap> createInputOutputMap(Map<String, Integer> inFieldIndexes) {     List<InputOutputMap> inputOutputMap = new ArrayList<>().     int outIndex = TIME_FIELD_OUT_INDEX.     Integer inIndex = inFieldIndexes.get(dataDescription.getTimeField()).     if (inIndex == null) {         throw new IllegalStateException(String.format(Locale.ROOT, "Input time field '%s' not found", dataDescription.getTimeField())).     }     inputOutputMap.add(new InputOutputMap(inIndex, outIndex)).     for (String field : analysisConfig.analysisFields()) {         if (AnalysisConfig.ML_CATEGORY_FIELD.equals(field) == false) {             ++outIndex.             inIndex = inFieldIndexes.get(field).             if (inIndex != null) {                 inputOutputMap.add(new InputOutputMap(inIndex, outIndex)).             }         }     }     return inputOutputMap. }
false;protected;0;3;;protected List<InputOutputMap> getInputOutputMap() {     return inputOutputMap. }
true;protected,abstract;3;2;/**  * Check that all the fields are present in the header.  * Either return true or throw a MissingFieldException  * <p>  * Every input field should have an entry in <code>inputFieldIndexes</code>  * otherwise the field cannot be found.  */ ;/**  * Check that all the fields are present in the header.  * Either return true or throw a MissingFieldException  * <p>  * Every input field should have an entry in <code>inputFieldIndexes</code>  * otherwise the field cannot be found.  */ protected abstract boolean checkForMissingFields(Collection<String> inputFields, Map<String, Integer> inputFieldIndexes, String[] header).
