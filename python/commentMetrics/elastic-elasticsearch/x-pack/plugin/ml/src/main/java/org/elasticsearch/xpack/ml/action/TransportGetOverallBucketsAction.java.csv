commented;modifiers;parameterAmount;loc;comment;code
false;protected;3;23;;@Override protected void doExecute(Task task, GetOverallBucketsAction.Request request, ActionListener<GetOverallBucketsAction.Response> listener) {     jobManager.expandJobs(request.getJobId(), request.allowNoJobs(), ActionListener.wrap(jobPage -> {         if (jobPage.count() == 0) {             listener.onResponse(new GetOverallBucketsAction.Response()).             return.         }         // As computing and potentially aggregating overall buckets might take a while,         // we run in a different thread to avoid blocking the network thread.         threadPool.executor(MachineLearning.UTILITY_THREAD_POOL_NAME).execute(() -> {             try {                 getOverallBuckets(request, jobPage.results(), listener).             } catch (Exception e) {                 listener.onFailure(e).             }         }).     }, listener::onFailure)). }
false;private;3;23;;private void getOverallBuckets(GetOverallBucketsAction.Request request, List<Job> jobs, ActionListener<GetOverallBucketsAction.Response> listener) {     JobsContext jobsContext = JobsContext.build(jobs, request).     ActionListener<List<OverallBucket>> overallBucketsListener = ActionListener.wrap(overallBuckets -> {         listener.onResponse(new GetOverallBucketsAction.Response(new QueryPage<>(overallBuckets, overallBuckets.size(), OverallBucket.RESULTS_FIELD))).     }, listener::onFailure).     ActionListener<ChunkedBucketSearcher> chunkedBucketSearcherListener = ActionListener.wrap(searcher -> {         if (searcher == null) {             listener.onResponse(new GetOverallBucketsAction.Response()).             return.         }         searcher.searchAndComputeOverallBuckets(overallBucketsListener).     }, listener::onFailure).     OverallBucketsProvider overallBucketsProvider = new OverallBucketsProvider(jobsContext.maxBucketSpan, request.getTopN(), request.getOverallScore()).     OverallBucketsProcessor overallBucketsProcessor = requiresAggregation(request, jobsContext.maxBucketSpan) ? new OverallBucketsAggregator(request.getBucketSpan()) : new OverallBucketsCollector().     initChunkedBucketSearcher(request, jobsContext, overallBucketsProvider, overallBucketsProcessor, chunkedBucketSearcherListener). }
false;private,static;2;3;;private static boolean requiresAggregation(GetOverallBucketsAction.Request request, TimeValue maxBucketSpan) {     return request.getBucketSpan() != null && !request.getBucketSpan().equals(maxBucketSpan). }
false;private,static;2;6;;private static void checkValidBucketSpan(TimeValue bucketSpan, TimeValue maxBucketSpan) {     if (bucketSpan != null && bucketSpan.compareTo(maxBucketSpan) < 0) {         throw ExceptionsHelper.badRequestException("Param [{}] must be greater or equal to the max bucket_span [{}]", GetOverallBucketsAction.Request.BUCKET_SPAN, maxBucketSpan.getStringRep()).     } }
false;private;5;26;;private void initChunkedBucketSearcher(GetOverallBucketsAction.Request request, JobsContext jobsContext, OverallBucketsProvider overallBucketsProvider, OverallBucketsProcessor overallBucketsProcessor, ActionListener<ChunkedBucketSearcher> listener) {     long maxBucketSpanMillis = jobsContext.maxBucketSpan.millis().     SearchRequest searchRequest = buildSearchRequest(request.getStart(), request.getEnd(), request.isExcludeInterim(), maxBucketSpanMillis, jobsContext.indices).     searchRequest.source().aggregation(AggregationBuilders.min(EARLIEST_TIME).field(Result.TIMESTAMP.getPreferredName())).     searchRequest.source().aggregation(AggregationBuilders.max(LATEST_TIME).field(Result.TIMESTAMP.getPreferredName())).     executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest, ActionListener.<SearchResponse>wrap(searchResponse -> {         long totalHits = searchResponse.getHits().getTotalHits().value.         if (totalHits > 0) {             Aggregations aggregations = searchResponse.getAggregations().             Min min = aggregations.get(EARLIEST_TIME).             long earliestTime = Intervals.alignToFloor((long) min.getValue(), maxBucketSpanMillis).             Max max = aggregations.get(LATEST_TIME).             long latestTime = Intervals.alignToCeil((long) max.getValue() + 1, maxBucketSpanMillis).             listener.onResponse(new ChunkedBucketSearcher(jobsContext, earliestTime, latestTime, request.isExcludeInterim(), overallBucketsProvider, overallBucketsProcessor)).         } else {             listener.onResponse(null).         }     }, listener::onFailure), client::search). }
false;private,static;2;19;;private static JobsContext build(List<Job> jobs, GetOverallBucketsAction.Request request) {     Set<String> indices = new HashSet<>().     TimeValue maxBucketSpan = TimeValue.ZERO.     for (Job job : jobs) {         indices.add(AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())).         TimeValue bucketSpan = job.getAnalysisConfig().getBucketSpan().         if (maxBucketSpan.compareTo(bucketSpan) < 0) {             maxBucketSpan = bucketSpan.         }     }     checkValidBucketSpan(request.getBucketSpan(), maxBucketSpan).     // If top_n is 1, we can use the request bucket_span in order to optimize the aggregations     if (request.getBucketSpan() != null && (request.getTopN() == 1 || jobs.size() <= 1)) {         maxBucketSpan = request.getBucketSpan().     }     return new JobsContext(jobs.size(), indices.toArray(new String[indices.size()]), maxBucketSpan). }
false;;1;20;;void searchAndComputeOverallBuckets(ActionListener<List<OverallBucket>> listener) {     if (curTime >= endTime) {         listener.onResponse(overallBucketsProcessor.finish()).         return.     }     executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, nextSearch(), ActionListener.<SearchResponse>wrap(searchResponse -> {         Histogram histogram = searchResponse.getAggregations().get(Result.TIMESTAMP.getPreferredName()).         overallBucketsProcessor.process(overallBucketsProvider.computeOverallBuckets(histogram)).         if (overallBucketsProcessor.size() > MAX_RESULT_COUNT) {             listener.onFailure(ExceptionsHelper.badRequestException("Unable to return more than [{}] results. please use " + "parameters [{}] and [{}] to limit the time range", MAX_RESULT_COUNT, GetOverallBucketsAction.Request.START, GetOverallBucketsAction.Request.END)).             return.         }         searchAndComputeOverallBuckets(listener).     }, listener::onFailure), client::search). }
false;;0;8;;SearchRequest nextSearch() {     long curEnd = Math.min(curTime + chunkMillis, endTime).     logger.debug("Search for buckets in: [{}, {})", curTime, curEnd).     SearchRequest searchRequest = buildSearchRequest(curTime, curEnd, excludeInterim, maxBucketSpanMillis, indices).     searchRequest.source().aggregation(aggs).     curTime += chunkMillis.     return searchRequest. }
false;private,static;5;18;;private static SearchRequest buildSearchRequest(Long start, Long end, boolean excludeInterim, long bucketSpanMillis, String[] indices) {     String startTime = start == null ? null : String.valueOf(Intervals.alignToCeil(start, bucketSpanMillis)).     String endTime = end == null ? null : String.valueOf(Intervals.alignToFloor(end, bucketSpanMillis)).     SearchSourceBuilder searchSourceBuilder = new BucketsQueryBuilder().size(0).includeInterim(excludeInterim == false).start(startTime).end(endTime).build().     searchSourceBuilder.trackTotalHits(true).     SearchRequest searchRequest = new SearchRequest(indices).     searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS)).     searchRequest.source(searchSourceBuilder).     return searchRequest. }
false;private,static;2;13;;private static AggregationBuilder buildAggregations(long maxBucketSpanMillis, int jobCount) {     AggregationBuilder overallScoreAgg = AggregationBuilders.max(OverallBucket.OVERALL_SCORE.getPreferredName()).field(Bucket.ANOMALY_SCORE.getPreferredName()).     AggregationBuilder jobsAgg = AggregationBuilders.terms(Job.ID.getPreferredName()).field(Job.ID.getPreferredName()).size(jobCount).subAggregation(overallScoreAgg).     AggregationBuilder interimAgg = AggregationBuilders.max(Result.IS_INTERIM.getPreferredName()).field(Result.IS_INTERIM.getPreferredName()).     return AggregationBuilders.dateHistogram(Result.TIMESTAMP.getPreferredName()).field(Result.TIMESTAMP.getPreferredName()).interval(maxBucketSpanMillis).subAggregation(jobsAgg).subAggregation(interimAgg). }
