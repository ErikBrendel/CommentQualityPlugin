commented;modifiers;parameterAmount;loc;comment;code
false;public;1;4;;public void init(ModelSnapshot modelSnapshot) throws IOException {     autodetectProcess.restoreState(stateStreamer, modelSnapshot).     createProcessWriter(Optional.empty()).writeHeader(). }
false;private;1;5;;private DataToProcessWriter createProcessWriter(Optional<DataDescription> dataDescription) {     return DataToProcessWriterFactory.create(true, includeTokensField, autodetectProcess, dataDescription.orElse(job.getDataDescription()), job.getAnalysisConfig(), dataCountsReporter, xContentRegistry). }
false;public;5;34;;public void writeToJob(InputStream inputStream, AnalysisRegistry analysisRegistry, XContentType xContentType, DataLoadParams params, BiConsumer<DataCounts, Exception> handler) {     submitOperation(() -> {         if (params.isResettingBuckets()) {             autodetectProcess.writeResetBucketsControlMessage(params).         }         CountingInputStream countingStream = new CountingInputStream(inputStream, dataCountsReporter).         DataToProcessWriter autoDetectWriter = createProcessWriter(params.getDataDescription()).         if (includeTokensField && categorizationAnalyzer == null) {             createCategorizationAnalyzer(analysisRegistry).         }         CountDownLatch latch = new CountDownLatch(1).         AtomicReference<DataCounts> dataCountsAtomicReference = new AtomicReference<>().         AtomicReference<Exception> exceptionAtomicReference = new AtomicReference<>().         autoDetectWriter.write(countingStream, categorizationAnalyzer, xContentType, (dataCounts, e) -> {             dataCountsAtomicReference.set(dataCounts).             exceptionAtomicReference.set(e).             latch.countDown().         }).         latch.await().         autoDetectWriter.flushStream().         if (exceptionAtomicReference.get() != null) {             throw exceptionAtomicReference.get().         } else {             return dataCountsAtomicReference.get().         }     }, handler). }
false;public;0;4;;@Override public void close() throws IOException {     close(false, null). }
true;public;2;33;/**  * Closes job this communicator is encapsulating.  *  * @param restart   Whether the job should be restarted by persistent tasks  * @param reason    The reason for closing the job  */ ;/**  * Closes job this communicator is encapsulating.  *  * @param restart   Whether the job should be restarted by persistent tasks  * @param reason    The reason for closing the job  */ public void close(boolean restart, String reason) {     Future<?> future = autodetectWorkerExecutor.submit(() -> {         checkProcessIsAlive().         try {             if (autodetectProcess.isReady()) {                 autodetectProcess.close().             } else {                 killProcess(false, false).                 stateStreamer.cancel().             }             autoDetectResultProcessor.awaitCompletion().         } finally {             onFinishHandler.accept(restart ? new ElasticsearchException(reason) : null, true).         }         LOGGER.info("[{}] job closed", job.getId()).         return null.     }).     try {         future.get().         autodetectWorkerExecutor.shutdown().     } catch (InterruptedException e) {         Thread.currentThread().interrupt().     } catch (ExecutionException e) {         if (processKilled) {             // In this case the original exception is spurious and highly misleading             throw ExceptionsHelper.conflictStatusException("Close job interrupted by kill request").         } else {             throw FutureUtils.rethrowExecutionException(e).         }     } finally {         destroyCategorizationAnalyzer().     } }
false;public;2;3;;public void killProcess(boolean awaitCompletion, boolean finish) throws IOException {     killProcess(awaitCompletion, finish, true). }
false;public;3;21;;public void killProcess(boolean awaitCompletion, boolean finish, boolean finalizeJob) throws IOException {     try {         processKilled = true.         autoDetectResultProcessor.setProcessKilled().         autodetectWorkerExecutor.shutdown().         autodetectProcess.kill().         if (awaitCompletion) {             try {                 autoDetectResultProcessor.awaitCompletion().             } catch (TimeoutException e) {                 LOGGER.warn(new ParameterizedMessage("[{}] Timed out waiting for killed job", job.getId()), e).             }         }     } finally {         if (finish) {             onFinishHandler.accept(null, finalizeJob).         }         destroyCategorizationAnalyzer().     } }
false;public;2;28;;public void writeUpdateProcessMessage(UpdateProcessMessage update, BiConsumer<Void, Exception> handler) {     submitOperation(() -> {         if (update.getModelPlotConfig() != null) {             autodetectProcess.writeUpdateModelPlotMessage(update.getModelPlotConfig()).         }         // Filters have to be written before detectors         if (update.getFilter() != null) {             autodetectProcess.writeUpdateFiltersMessage(Collections.singletonList(update.getFilter())).         }         // Add detector rules         if (update.getDetectorUpdates() != null) {             for (JobUpdate.DetectorUpdate detectorUpdate : update.getDetectorUpdates()) {                 if (detectorUpdate.getRules() != null) {                     autodetectProcess.writeUpdateDetectorRulesMessage(detectorUpdate.getDetectorIndex(), detectorUpdate.getRules()).                 }             }         }         // Add scheduled events. null means there's no update but an empty list means we should clear any events in the process         if (update.getScheduledEvents() != null) {             autodetectProcess.writeUpdateScheduledEventsMessage(update.getScheduledEvents(), job.getAnalysisConfig().getBucketSpan()).         }         return null.     }, handler). }
false;public;2;6;;public void flushJob(FlushJobParams params, BiConsumer<FlushAcknowledgement, Exception> handler) {     submitOperation(() -> {         String flushId = autodetectProcess.flushJob(params).         return waitFlushToCompletion(flushId).     }, handler). }
false;public;2;21;;public void forecastJob(ForecastParams params, BiConsumer<Void, Exception> handler) {     BiConsumer<Void, Exception> forecastConsumer = (aVoid, e) -> {         if (e == null) {             FlushJobParams flushParams = FlushJobParams.builder().build().             flushJob(flushParams, (flushAcknowledgement, flushException) -> {                 if (flushException != null) {                     String msg = String.format(Locale.ROOT, "[%s] exception while flushing job", job.getId()).                     handler.accept(null, ExceptionsHelper.serverError(msg, e)).                 } else {                     handler.accept(null, null).                 }             }).         } else {             handler.accept(null, e).         }     }.     submitOperation(() -> {         autodetectProcess.forecastJob(params).         return null.     }, forecastConsumer). }
false;public;1;6;;public void persistJob(BiConsumer<Void, Exception> handler) {     submitOperation(() -> {         autodetectProcess.persistState().         return null.     }, handler). }
false;;1;26;;@Nullable FlushAcknowledgement waitFlushToCompletion(String flushId) throws InterruptedException {     LOGGER.debug("[{}] waiting for flush", job.getId()).     FlushAcknowledgement flushAcknowledgement.     try {         flushAcknowledgement = autoDetectResultProcessor.waitForFlushAcknowledgement(flushId, FLUSH_PROCESS_CHECK_FREQUENCY).         while (flushAcknowledgement == null) {             checkProcessIsAlive().             checkResultsProcessorIsAlive().             flushAcknowledgement = autoDetectResultProcessor.waitForFlushAcknowledgement(flushId, FLUSH_PROCESS_CHECK_FREQUENCY).         }     } finally {         autoDetectResultProcessor.clearAwaitingFlush(flushId).     }     if (processKilled == false) {         // We also have to wait for the normalizer to become idle so that we block         // clients from querying results in the middle of normalization.         autoDetectResultProcessor.waitUntilRenormalizerIsIdle().         LOGGER.debug("[{}] Flush completed", job.getId()).     }     return flushAcknowledgement. }
true;private;0;6;/**  * Throws an exception if the process has exited  */ ;/**  * Throws an exception if the process has exited  */ private void checkProcessIsAlive() {     if (!autodetectProcess.isProcessAlive()) {         // Don't log here - it just causes double logging when the exception gets logged         throw new ElasticsearchException("[{}] Unexpected death of autodetect: {}", job.getId(), autodetectProcess.readError()).     } }
false;private;0;6;;private void checkResultsProcessorIsAlive() {     if (autoDetectResultProcessor.isFailed()) {         // Don't log here - it just causes double logging when the exception gets logged         throw new ElasticsearchException("[{}] Unexpected death of the result processor", job.getId()).     } }
false;public;0;3;;public ZonedDateTime getProcessStartTime() {     return autodetectProcess.getProcessStartTime(). }
false;public;0;3;;public ModelSizeStats getModelSizeStats() {     return autoDetectResultProcessor.modelSizeStats(). }
false;public;0;3;;public DataCounts getDataCounts() {     return dataCountsReporter.runningTotalStats(). }
true;;0;6;/**  * Care must be taken to ensure this method is not called while data is being posted.  * The methods in this class that call it wait for all processing to complete first.  * The expectation is that external calls are only made when cleaning up after a fatal  * error.  */ ;/**  * Care must be taken to ensure this method is not called while data is being posted.  * The methods in this class that call it wait for all processing to complete first.  * The expectation is that external calls are only made when cleaning up after a fatal  * error.  */ void destroyCategorizationAnalyzer() {     if (categorizationAnalyzer != null) {         categorizationAnalyzer.close().         categorizationAnalyzer = null.     } }
false;public;1;10;;@Override public void onFailure(Exception e) {     if (processKilled) {         handler.accept(null, ExceptionsHelper.conflictStatusException("[{}] Could not submit operation to process as it has been killed", job.getId())).     } else {         LOGGER.error(new ParameterizedMessage("[{}] Unexpected exception writing to process", job.getId()), e).         handler.accept(null, e).     } }
false;protected;0;10;;@Override protected void doRun() throws Exception {     if (processKilled) {         handler.accept(null, ExceptionsHelper.conflictStatusException("[{}] Could not submit operation to process as it has been killed", job.getId())).     } else {         checkProcessIsAlive().         handler.accept(operation.get(), null).     } }
false;private;2;25;;private <T> void submitOperation(CheckedSupplier<T, Exception> operation, BiConsumer<T, Exception> handler) {     autodetectWorkerExecutor.execute(new AbstractRunnable() {          @Override         public void onFailure(Exception e) {             if (processKilled) {                 handler.accept(null, ExceptionsHelper.conflictStatusException("[{}] Could not submit operation to process as it has been killed", job.getId())).             } else {                 LOGGER.error(new ParameterizedMessage("[{}] Unexpected exception writing to process", job.getId()), e).                 handler.accept(null, e).             }         }          @Override         protected void doRun() throws Exception {             if (processKilled) {                 handler.accept(null, ExceptionsHelper.conflictStatusException("[{}] Could not submit operation to process as it has been killed", job.getId())).             } else {                 checkProcessIsAlive().                 handler.accept(operation.get(), null).             }         }     }). }
false;private;1;9;;private void createCategorizationAnalyzer(AnalysisRegistry analysisRegistry) throws IOException {     AnalysisConfig analysisConfig = job.getAnalysisConfig().     CategorizationAnalyzerConfig categorizationAnalyzerConfig = analysisConfig.getCategorizationAnalyzerConfig().     if (categorizationAnalyzerConfig == null) {         categorizationAnalyzerConfig = CategorizationAnalyzerConfig.buildDefaultCategorizationAnalyzer(analysisConfig.getCategorizationFilters()).     }     categorizationAnalyzer = new CategorizationAnalyzer(analysisRegistry, environment, categorizationAnalyzerConfig). }
