commented;modifiers;parameterAmount;loc;comment;code
true;public;0;3;/**  * Tell the scores updater to shut down ASAP.  */ ;/**  * Tell the scores updater to shut down ASAP.  */ public void shutdown() {     shutdown = true. }
false;private;1;7;;private long getNormalizationWindowOrDefault(Job job) {     if (job.getRenormalizationWindowDays() != null) {         return job.getRenormalizationWindowDays() * SECONDS_IN_DAY * MILLISECONDS_IN_SECOND.     }     return Math.max(DEFAULT_RENORMALIZATION_WINDOW_MS, DEFAULT_BUCKETS_IN_RENORMALIZATION_WINDOW * bucketSpan * MILLISECONDS_IN_SECOND). }
true;public;3;13;/**  * Update the anomaly score field on all previously persisted buckets  * and all contained records  */ ;/**  * Update the anomaly score field on all previously persisted buckets  * and all contained records  */ public void update(String quantilesState, long endBucketEpochMs, long windowExtensionMs) {     Normalizer normalizer = normalizerFactory.create(jobId).     int[] counts = { 0, 0 }.     updateBuckets(normalizer, quantilesState, endBucketEpochMs, windowExtensionMs, counts).     updateRecords(normalizer, quantilesState, endBucketEpochMs, windowExtensionMs, counts).     updateInfluencers(normalizer, quantilesState, endBucketEpochMs, windowExtensionMs, counts).     // The updates will have been persisted in batches throughout the renormalization     // process - this call just catches any leftovers     updatesPersister.executeRequest().     LOGGER.debug("[{}] Normalization resulted in: {} updates, {} no-ops", jobId, counts[0], counts[1]). }
false;private;5;31;;private void updateBuckets(Normalizer normalizer, String quantilesState, long endBucketEpochMs, long windowExtensionMs, int[] counts) {     BatchedDocumentsIterator<Result<Bucket>> bucketsIterator = jobResultsProvider.newBatchedBucketsIterator(jobId).timeRange(calcNormalizationWindowStart(endBucketEpochMs, windowExtensionMs), endBucketEpochMs).includeInterim(false).     List<BucketNormalizable> bucketsToRenormalize = new ArrayList<>().     while (bucketsIterator.hasNext() && shutdown == false) {         Deque<Result<Bucket>> buckets = bucketsIterator.next().         if (buckets.isEmpty()) {             LOGGER.debug("[{}] No buckets to renormalize for job", jobId).             break.         }         while (!buckets.isEmpty() && shutdown == false) {             Result<Bucket> current = buckets.removeFirst().             if (current.result.isNormalizable()) {                 bucketsToRenormalize.add(new BucketNormalizable(current.result, current.index)).                 if (bucketsToRenormalize.size() >= TARGET_BUCKETS_TO_RENORMALIZE) {                     normalizeBuckets(normalizer, bucketsToRenormalize, quantilesState, counts).                     bucketsToRenormalize.clear().                 }             }         }     }     if (!bucketsToRenormalize.isEmpty()) {         normalizeBuckets(normalizer, bucketsToRenormalize, quantilesState, counts).     } }
false;private;2;3;;private long calcNormalizationWindowStart(long endEpochMs, long windowExtensionMs) {     return Math.max(0, endEpochMs - normalizationWindow - windowExtensionMs). }
false;private;4;13;;private void normalizeBuckets(Normalizer normalizer, List<BucketNormalizable> normalizableBuckets, String quantilesState, int[] counts) {     normalizer.normalize(bucketSpan, normalizableBuckets, quantilesState).     for (BucketNormalizable bucketNormalizable : normalizableBuckets) {         if (bucketNormalizable.hadBigNormalizedUpdate()) {             updatesPersister.updateBucket(bucketNormalizable).             ++counts[0].         } else {             ++counts[1].         }     } }
false;private;5;22;;private void updateRecords(Normalizer normalizer, String quantilesState, long endBucketEpochMs, long windowExtensionMs, int[] counts) {     BatchedDocumentsIterator<Result<AnomalyRecord>> recordsIterator = jobResultsProvider.newBatchedRecordsIterator(jobId).timeRange(calcNormalizationWindowStart(endBucketEpochMs, windowExtensionMs), endBucketEpochMs).includeInterim(false).     while (recordsIterator.hasNext() && shutdown == false) {         Deque<Result<AnomalyRecord>> records = recordsIterator.next().         if (records.isEmpty()) {             LOGGER.debug("[{}] No records to renormalize for job", jobId).             break.         }         LOGGER.debug("[{}] Will renormalize a batch of {} records", jobId, records.size()).         List<Normalizable> asNormalizables = records.stream().map(recordResultIndex -> new RecordNormalizable(recordResultIndex.result, recordResultIndex.index)).collect(Collectors.toList()).         normalizer.normalize(bucketSpan, asNormalizables, quantilesState).         persistChanged(counts, asNormalizables).     } }
false;private;5;22;;private void updateInfluencers(Normalizer normalizer, String quantilesState, long endBucketEpochMs, long windowExtensionMs, int[] counts) {     BatchedDocumentsIterator<Result<Influencer>> influencersIterator = jobResultsProvider.newBatchedInfluencersIterator(jobId).timeRange(calcNormalizationWindowStart(endBucketEpochMs, windowExtensionMs), endBucketEpochMs).includeInterim(false).     while (influencersIterator.hasNext() && shutdown == false) {         Deque<Result<Influencer>> influencers = influencersIterator.next().         if (influencers.isEmpty()) {             LOGGER.debug("[{}] No influencers to renormalize for job", jobId).             break.         }         LOGGER.debug("[{}] Will renormalize a batch of {} influencers", jobId, influencers.size()).         List<Normalizable> asNormalizables = influencers.stream().map(influencerResultIndex -> new InfluencerNormalizable(influencerResultIndex.result, influencerResultIndex.index)).collect(Collectors.toList()).         normalizer.normalize(bucketSpan, asNormalizables, quantilesState).         persistChanged(counts, asNormalizables).     } }
false;private;2;13;;private void persistChanged(int[] counts, List<? extends Normalizable> asNormalizables) {     if (shutdown) {         return.     }     List<Normalizable> toUpdate = asNormalizables.stream().filter(Normalizable::hadBigNormalizedUpdate).collect(Collectors.toList()).     counts[0] += toUpdate.size().     counts[1] += asNormalizables.size() - toUpdate.size().     if (!toUpdate.isEmpty()) {         updatesPersister.updateResults(toUpdate).     } }
false;;0;3;;long getNormalizationWindow() {     return normalizationWindow. }
