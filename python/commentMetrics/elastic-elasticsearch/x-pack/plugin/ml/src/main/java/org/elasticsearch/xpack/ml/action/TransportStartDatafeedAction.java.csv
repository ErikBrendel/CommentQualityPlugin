commented;modifiers;parameterAmount;loc;comment;code
false;static;3;9;;static void validate(Job job, DatafeedConfig datafeedConfig, PersistentTasksCustomMetaData tasks) {     DatafeedJobValidator.validate(datafeedConfig, job).     DatafeedConfig.validateAggregations(datafeedConfig.getParsedAggregations()).     JobState jobState = MlTasks.getJobState(datafeedConfig.getJobId(), tasks).     if (jobState.isAnyOf(JobState.OPENING, JobState.OPENED) == false) {         throw ExceptionsHelper.conflictStatusException("cannot start datafeed [" + datafeedConfig.getId() + "] because job [" + job.getId() + "] is " + jobState).     } }
true;static;3;11;// Get the deprecation warnings from the parsed query and aggs to audit ;// Get the deprecation warnings from the parsed query and aggs to audit static void auditDeprecations(DatafeedConfig datafeed, Job job, Auditor auditor) {     List<String> deprecationWarnings = new ArrayList<>().     deprecationWarnings.addAll(datafeed.getAggDeprecations()).     deprecationWarnings.addAll(datafeed.getQueryDeprecations()).     if (deprecationWarnings.isEmpty() == false) {         String msg = "datafeed [" + datafeed.getId() + "] configuration has deprecations. [" + Strings.collectionToDelimitedString(deprecationWarnings, ", ") + "]".         auditor.warning(job.getId(), msg).     } }
false;protected;0;6;;@Override protected String executor() {     // so we can do this on the network thread     return ThreadPool.Names.SAME. }
false;protected;0;4;;@Override protected AcknowledgedResponse newResponse() {     return new AcknowledgedResponse(). }
false;public;1;5;;@Override public void onResponse(PersistentTasksCustomMetaData.PersistentTask<StartDatafeedAction.DatafeedParams> persistentTask) {     waitForDatafeedStarted(persistentTask.getId(), params, listener). }
false;public;1;9;;@Override public void onFailure(Exception e) {     if (e instanceof ResourceAlreadyExistsException) {         logger.debug("datafeed already started", e).         e = new ElasticsearchStatusException("cannot start datafeed [" + params.getDatafeedId() + "] because it has already been started", RestStatus.CONFLICT).     }     listener.onFailure(e). }
false;protected;3;95;;@Override protected void masterOperation(StartDatafeedAction.Request request, ClusterState state, ActionListener<AcknowledgedResponse> listener) {     StartDatafeedAction.DatafeedParams params = request.getParams().     if (licenseState.isMachineLearningAllowed() == false) {         listener.onFailure(LicenseUtils.newComplianceException(XPackField.MACHINE_LEARNING)).         return.     }     if (migrationEligibilityCheck.datafeedIsEligibleForMigration(request.getParams().getDatafeedId(), state)) {         listener.onFailure(ExceptionsHelper.configHasNotBeenMigrated("start datafeed", request.getParams().getDatafeedId())).         return.     }     AtomicReference<DatafeedConfig> datafeedConfigHolder = new AtomicReference<>().     PersistentTasksCustomMetaData tasks = state.getMetaData().custom(PersistentTasksCustomMetaData.TYPE).     ActionListener<PersistentTasksCustomMetaData.PersistentTask<StartDatafeedAction.DatafeedParams>> waitForTaskListener = new ActionListener<PersistentTasksCustomMetaData.PersistentTask<StartDatafeedAction.DatafeedParams>>() {          @Override         public void onResponse(PersistentTasksCustomMetaData.PersistentTask<StartDatafeedAction.DatafeedParams> persistentTask) {             waitForDatafeedStarted(persistentTask.getId(), params, listener).         }          @Override         public void onFailure(Exception e) {             if (e instanceof ResourceAlreadyExistsException) {                 logger.debug("datafeed already started", e).                 e = new ElasticsearchStatusException("cannot start datafeed [" + params.getDatafeedId() + "] because it has already been started", RestStatus.CONFLICT).             }             listener.onFailure(e).         }     }.     // Verify data extractor factory can be created, then start persistent task     Consumer<Job> createDataExtrator = job -> {         if (RemoteClusterLicenseChecker.containsRemoteIndex(params.getDatafeedIndices())) {             final RemoteClusterLicenseChecker remoteClusterLicenseChecker = new RemoteClusterLicenseChecker(client, XPackLicenseState::isMachineLearningAllowedForOperationMode).             remoteClusterLicenseChecker.checkRemoteClusterLicenses(RemoteClusterLicenseChecker.remoteClusterAliases(transportService.getRemoteClusterService().getRegisteredRemoteClusterNames(), params.getDatafeedIndices()), ActionListener.wrap(response -> {                 if (response.isSuccess() == false) {                     listener.onFailure(createUnlicensedError(params.getDatafeedId(), response)).                 } else {                     createDataExtractor(job, datafeedConfigHolder.get(), params, waitForTaskListener).                 }             }, e -> listener.onFailure(createUnknownLicenseError(params.getDatafeedId(), RemoteClusterLicenseChecker.remoteIndices(params.getDatafeedIndices()), e)))).         } else {             createDataExtractor(job, datafeedConfigHolder.get(), params, waitForTaskListener).         }     }.     ActionListener<Job.Builder> jobListener = ActionListener.wrap(jobBuilder -> {         try {             Job job = jobBuilder.build().             validate(job, datafeedConfigHolder.get(), tasks).             auditDeprecations(datafeedConfigHolder.get(), job, auditor).             createDataExtrator.accept(job).         } catch (Exception e) {             listener.onFailure(e).         }     }, listener::onFailure).     ActionListener<DatafeedConfig.Builder> datafeedListener = ActionListener.wrap(datafeedBuilder -> {         try {             DatafeedConfig datafeedConfig = datafeedBuilder.build().             params.setDatafeedIndices(datafeedConfig.getIndices()).             params.setJobId(datafeedConfig.getJobId()).             datafeedConfigHolder.set(datafeedConfig).             jobConfigProvider.getJob(datafeedConfig.getJobId(), jobListener).         } catch (Exception e) {             listener.onFailure(e).         }     }, listener::onFailure).     datafeedConfigProvider.getDatafeedConfig(params.getDatafeedId(), datafeedListener). }
false;private;4;9;;private void createDataExtractor(Job job, DatafeedConfig datafeed, StartDatafeedAction.DatafeedParams params, ActionListener<PersistentTasksCustomMetaData.PersistentTask<StartDatafeedAction.DatafeedParams>> listener) {     DataExtractorFactory.create(client, datafeed, job, ActionListener.wrap(dataExtractorFactory -> persistentTasksService.sendStartRequest(MlTasks.datafeedTaskId(params.getDatafeedId()), MlTasks.DATAFEED_TASK_NAME, params, listener), listener::onFailure)). }
false;protected;2;7;;@Override protected ClusterBlockException checkBlock(StartDatafeedAction.Request request, ClusterState state) {     // because PersistentTasksService will then fail.     return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE). }
false;public;1;11;;@Override public void onResponse(PersistentTasksCustomMetaData.PersistentTask<StartDatafeedAction.DatafeedParams> persistentTask) {     if (predicate.exception != null) {         // We want to return to the caller without leaving an unassigned persistent task, to match         // what would have happened if the error had been detected in the "fast fail" validation         cancelDatafeedStart(persistentTask, predicate.exception, listener).     } else {         listener.onResponse(new AcknowledgedResponse(true)).     } }
false;public;1;4;;@Override public void onFailure(Exception e) {     listener.onFailure(e). }
false;public;1;5;;@Override public void onTimeout(TimeValue timeout) {     listener.onFailure(new ElasticsearchException("Starting datafeed [" + params.getDatafeedId() + "] timed out after [" + timeout + "]")). }
false;private;3;29;;private void waitForDatafeedStarted(String taskId, StartDatafeedAction.DatafeedParams params, ActionListener<AcknowledgedResponse> listener) {     DatafeedPredicate predicate = new DatafeedPredicate().     persistentTasksService.waitForPersistentTaskCondition(taskId, predicate, params.getTimeout(), new PersistentTasksService.WaitForPersistentTaskListener<StartDatafeedAction.DatafeedParams>() {          @Override         public void onResponse(PersistentTasksCustomMetaData.PersistentTask<StartDatafeedAction.DatafeedParams> persistentTask) {             if (predicate.exception != null) {                 // We want to return to the caller without leaving an unassigned persistent task, to match                 // what would have happened if the error had been detected in the "fast fail" validation                 cancelDatafeedStart(persistentTask, predicate.exception, listener).             } else {                 listener.onResponse(new AcknowledgedResponse(true)).             }         }          @Override         public void onFailure(Exception e) {             listener.onFailure(e).         }          @Override         public void onTimeout(TimeValue timeout) {             listener.onFailure(new ElasticsearchException("Starting datafeed [" + params.getDatafeedId() + "] timed out after [" + timeout + "]")).         }     }). }
false;public;1;6;;@Override public void onResponse(PersistentTasksCustomMetaData.PersistentTask<?> task) {     // We succeeded in cancelling the persistent task, but the     // problem that caused us to cancel it is the overall result     listener.onFailure(exception). }
false;public;1;6;;@Override public void onFailure(Exception e) {     logger.error("[" + persistentTask.getParams().getDatafeedId() + "] Failed to cancel persistent task that could " + "not be assigned due to [" + exception.getMessage() + "]", e).     listener.onFailure(exception). }
false;private;3;20;;private void cancelDatafeedStart(PersistentTasksCustomMetaData.PersistentTask<StartDatafeedAction.DatafeedParams> persistentTask, Exception exception, ActionListener<AcknowledgedResponse> listener) {     persistentTasksService.sendRemoveRequest(persistentTask.getId(), new ActionListener<PersistentTasksCustomMetaData.PersistentTask<?>>() {          @Override         public void onResponse(PersistentTasksCustomMetaData.PersistentTask<?> task) {             // We succeeded in cancelling the persistent task, but the             // problem that caused us to cancel it is the overall result             listener.onFailure(exception).         }          @Override         public void onFailure(Exception e) {             logger.error("[" + persistentTask.getParams().getDatafeedId() + "] Failed to cancel persistent task that could " + "not be assigned due to [" + exception.getMessage() + "]", e).             listener.onFailure(exception).         }     }). }
false;private;2;13;;private ElasticsearchStatusException createUnlicensedError(final String datafeedId, final RemoteClusterLicenseChecker.LicenseCheck licenseCheck) {     final String message = String.format(Locale.ROOT, "cannot start datafeed [%s] as it is configured to use indices on remote cluster [%s] that is not licensed for ml. %s", datafeedId, licenseCheck.remoteClusterLicenseInfo().clusterAlias(), RemoteClusterLicenseChecker.buildErrorMessage("ml", licenseCheck.remoteClusterLicenseInfo(), RemoteClusterLicenseChecker::isLicensePlatinumOrTrial)).     return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST). }
false;private;3;17;;private ElasticsearchStatusException createUnknownLicenseError(final String datafeedId, final List<String> remoteIndices, final Exception cause) {     final int numberOfRemoteClusters = RemoteClusterLicenseChecker.remoteClusterAliases(transportService.getRemoteClusterService().getRegisteredRemoteClusterNames(), remoteIndices).size().     assert numberOfRemoteClusters > 0.     final String remoteClusterQualifier = numberOfRemoteClusters == 1 ? "a remote cluster" : "remote clusters".     final String licenseTypeQualifier = numberOfRemoteClusters == 1 ? "" : "s".     final String message = String.format(Locale.ROOT, "cannot start datafeed [%s] as it uses indices on %s %s but the license type%s could not be verified", datafeedId, remoteClusterQualifier, remoteIndices, licenseTypeQualifier).     return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST, cause). }
false;public;2;6;;@Override public PersistentTasksCustomMetaData.Assignment getAssignment(StartDatafeedAction.DatafeedParams params, ClusterState clusterState) {     return new DatafeedNodeSelector(clusterState, resolver, params.getDatafeedId(), params.getJobId(), params.getDatafeedIndices()).selectNode(). }
false;public;2;5;;@Override public void validate(StartDatafeedAction.DatafeedParams params, ClusterState clusterState) {     new DatafeedNodeSelector(clusterState, resolver, params.getDatafeedId(), params.getJobId(), params.getDatafeedIndices()).checkDatafeedTaskCanBeCreated(). }
false;protected;3;15;;@Override protected void nodeOperation(final AllocatedPersistentTask allocatedPersistentTask, final StartDatafeedAction.DatafeedParams params, final PersistentTaskState state) {     DatafeedTask datafeedTask = (DatafeedTask) allocatedPersistentTask.     datafeedTask.datafeedManager = datafeedManager.     datafeedManager.run(datafeedTask, (error) -> {         if (error != null) {             datafeedTask.markAsFailed(error).         } else {             datafeedTask.markAsCompleted().         }     }). }
false;protected;6;7;;@Override protected AllocatedPersistentTask createTask(long id, String type, String action, TaskId parentTaskId, PersistentTasksCustomMetaData.PersistentTask<StartDatafeedAction.DatafeedParams> persistentTask, Map<String, String> headers) {     return new DatafeedTask(id, type, action, parentTaskId, persistentTask.getParams(), headers). }
false;public;0;3;;public String getDatafeedId() {     return datafeedId. }
false;public;0;3;;public long getDatafeedStartTime() {     return startTime. }
false;public;0;4;;@Nullable public Long getEndTime() {     return endTime. }
false;public;0;3;;public boolean isLookbackOnly() {     return endTime != null. }
false;protected;0;8;;@Override protected void onCancelled() {     // If the persistent task framework wants us to stop then we should do so immediately and     // we should wait for an existing datafeed import to realize we want it to stop.     // Note that this only applied when task cancel is invoked and stop datafeed api doesn't use this.     // Also stop datafeed api will obey the timeout.     stop(getReasonCancelled(), TimeValue.ZERO). }
false;public;2;5;;public void stop(String reason, TimeValue timeout) {     if (datafeedManager != null) {         datafeedManager.stopDatafeed(this, reason, timeout).     } }
false;public;0;5;;public void isolate() {     if (datafeedManager != null) {         datafeedManager.isolateDatafeed(getAllocationId()).     } }
false;public;1;16;;@Override public boolean test(PersistentTasksCustomMetaData.PersistentTask<?> persistentTask) {     if (persistentTask == null) {         return false.     }     PersistentTasksCustomMetaData.Assignment assignment = persistentTask.getAssignment().     if (assignment != null && assignment.equals(PersistentTasksCustomMetaData.INITIAL_ASSIGNMENT) == false && assignment.isAssigned() == false) {         // Assignment has failed despite passing our "fast fail" validation         exception = new ElasticsearchStatusException("Could not start datafeed, allocation explanation [" + assignment.getExplanation() + "]", RestStatus.TOO_MANY_REQUESTS).         return true.     }     DatafeedState datafeedState = (DatafeedState) persistentTask.getState().     return datafeedState == DatafeedState.STARTED. }
