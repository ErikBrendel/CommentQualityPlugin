# id;timestamp;commentText;codeText;commentWords;codeWords
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1533230566;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1534362961;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1536314350;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1537806831;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1540847035;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1544035746;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1544205697;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1545155131;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1546880335;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1547215421;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1548668326;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,                                          Consumer<Exception> errorHandler);1550064927;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses the internal client, so runs as the _xpack user;public void bucketsViaInternalClient(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler,_                                         Consumer<Exception> errorHandler) {_        buckets(jobId, query, handler, errorHandler, client)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,the,internal,client,so,runs,as,the,user;public,void,buckets,via,internal,client,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,buckets,job,id,query,handler,error,handler,client
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1533230566;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1534362961;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1536314350;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1537806831;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1540847035;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1544035746;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1544205697;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1545155131;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1546880335;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1547215421;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1548668326;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler);1550064927;Get the job's model size stats.;public void modelSizeStats(String jobId, Consumer<ModelSizeStats> handler, Consumer<Exception> errorHandler) {_        LOGGER.trace("ES API CALL: search latest {} for job {}", ModelSizeStats.RESULT_TYPE_VALUE, jobId)___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, ModelSizeStats.RESULT_TYPE_VALUE, createLatestModelSizeStatsSearch(indexName),_                ModelSizeStats.LENIENT_PARSER,_                result -> handler.accept(result.result.build()), errorHandler,_                () -> new ModelSizeStats.Builder(jobId))__    };get,the,job,s,model,size,stats;public,void,model,size,stats,string,job,id,consumer,model,size,stats,handler,consumer,exception,error,handler,logger,trace,es,api,call,search,latest,for,job,model,size,stats,job,id,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,model,size,stats,create,latest,model,size,stats,search,index,name,model,size,stats,result,handler,accept,result,result,build,error,handler,new,model,size,stats,builder,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1533230566;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1534362961;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1536314350;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1537806831;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1540847035;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1544035746;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1544205697;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1545155131;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1546880335;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1547215421;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1548668326;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId);1550064927;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of buckets of the given job._The bucket and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a bucket {@link BatchedResultsIterator};public BatchedResultsIterator<Bucket> newBatchedBucketsIterator(String jobId) {_        return new BatchedBucketsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,buckets,of,the,given,job,the,bucket,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,bucket,link,batched,results,iterator;public,batched,results,iterator,bucket,new,batched,buckets,iterator,string,job,id,return,new,batched,buckets,iterator,client,with,origin,client,job,id
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1533230566;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1534362961;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1536314350;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1537806831;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1540847035;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1544035746;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1544205697;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1545155131;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1546880335;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1547215421;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1548668326;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,                                  Consumer<Exception> errorHandler);1550064927;Get a job's model snapshot by its id;public void getModelSnapshot(String jobId, @Nullable String modelSnapshotId, Consumer<Result<ModelSnapshot>> handler,_                                 Consumer<Exception> errorHandler) {_        if (modelSnapshotId == null) {_            handler.accept(null)__            return__        }_        String resultsIndex = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequestBuilder search = createDocIdSearch(resultsIndex, ModelSnapshot.documentId(jobId, modelSnapshotId))__        searchSingleResult(jobId, ModelSnapshot.TYPE.getPreferredName(), search, ModelSnapshot.LENIENT_PARSER,_                result -> handler.accept(result.result == null ? null : new Result<ModelSnapshot>(result.index, result.result.build())),_                errorHandler, () -> null)__    };get,a,job,s,model,snapshot,by,its,id;public,void,get,model,snapshot,string,job,id,nullable,string,model,snapshot,id,consumer,result,model,snapshot,handler,consumer,exception,error,handler,if,model,snapshot,id,null,handler,accept,null,return,string,results,index,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,builder,search,create,doc,id,search,results,index,model,snapshot,document,id,job,id,model,snapshot,id,search,single,result,job,id,model,snapshot,type,get,preferred,name,search,model,snapshot,result,handler,accept,result,result,null,null,new,result,model,snapshot,result,index,result,result,build,error,handler,null
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1533230566;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1534362961;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1536314350;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1537806831;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1540847035;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1544035746;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1544205697;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1545155131;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1546880335;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1547215421;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1548668326;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1550064927;Get model snapshots for the job ordered by descending timestamp (newest first).__@param jobId the job id_@param from  number of snapshots to from_@param size  number of snapshots to retrieve;public void modelSnapshots(String jobId, int from, int size, Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        modelSnapshots(jobId, from, size, null, true, QueryBuilders.matchAllQuery(), handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,timestamp,newest,first,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve;public,void,model,snapshots,string,job,id,int,from,int,size,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,model,snapshots,job,id,from,size,null,true,query,builders,match,all,query,handler,error,handler
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1533230566;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1534362961;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1536314350;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1537806831;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1540847035;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1544035746;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1544205697;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1545155131;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1546880335;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1547215421;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1548668326;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler);1550064927;Get the job's data counts__@param jobId The job id;public void dataCounts(String jobId, Consumer<DataCounts> handler, Consumer<Exception> errorHandler) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        searchSingleResult(jobId, DataCounts.TYPE.getPreferredName(), createLatestDataCountsSearch(indexName, jobId),_                DataCounts.PARSER, result -> handler.accept(result.result), errorHandler, () -> new DataCounts(jobId))__    };get,the,job,s,data,counts,param,job,id,the,job,id;public,void,data,counts,string,job,id,consumer,data,counts,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,single,result,job,id,data,counts,type,get,preferred,name,create,latest,data,counts,search,index,name,job,id,data,counts,parser,result,handler,accept,result,result,error,handler,new,data,counts,job,id
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1533230566;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,lenient,expand,open,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,lenient,expand,open,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1534362961;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,lenient,expand,open,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,lenient,expand,open,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1536314350;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,lenient,expand,open,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,lenient,expand,open,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1537806831;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,lenient,expand,open,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,lenient,expand,open,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1540847035;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,lenient,expand,open,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,lenient,expand,open,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1544035746;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,lenient,expand,open,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,lenient,expand,open,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1544205697;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,lenient,expand,open,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,lenient,expand,open,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1545155131;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,lenient,expand,open,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,lenient,expand,open,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1546880335;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexName())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,lenient,expand,open,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,name,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,lenient,expand,open,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1547215421;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexPattern())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.strictExpand())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexPattern())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.strictExpand())___        String resultsIndexName = job.getResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,pattern,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,strict,expand,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,pattern,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,strict,expand,string,results,index,name,job,get,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1548668326;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexPattern())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.strictExpand())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexPattern())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.strictExpand())___        String resultsIndexName = job.getInitialResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,pattern,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,strict,expand,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,pattern,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,strict,expand,string,results,index,name,job,get,initial,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener);1550064927;Check that a previously deleted job with the same Id has not left any result_or categorizer state documents due to a failed delete. Any left over results would_appear to be part of the new job.__We can't check for model state as the Id is based on the snapshot Id which is_a timestamp and so unpredictable however, it is unlikely a new job would have_the same snapshot Id as an old one.__@param job  Job configuration_@param listener The ActionListener;public void checkForLeftOverDocuments(Job job, ActionListener<Boolean> listener) {__        SearchRequestBuilder stateDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexPattern())_                .setQuery(QueryBuilders.idsQuery().addIds(CategorizerState.documentId(job.getId(), 1),_                        CategorizerState.v54DocumentId(job.getId(), 1)))_                .setIndicesOptions(IndicesOptions.strictExpand())___        SearchRequestBuilder quantilesDocSearch = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexPattern())_                .setQuery(QueryBuilders.idsQuery().addIds(Quantiles.documentId(job.getId()), Quantiles.v54DocumentId(job.getId())))_                .setIndicesOptions(IndicesOptions.strictExpand())___        String resultsIndexName = job.getInitialResultsIndexName()__        SearchRequestBuilder resultDocSearch = client.prepareSearch(resultsIndexName)_                .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                .setQuery(QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                .setSize(1)___        MultiSearchRequestBuilder msearch = client.prepareMultiSearch()_                .add(stateDocSearch)_                .add(resultDocSearch)_                .add(quantilesDocSearch)___        ActionListener<MultiSearchResponse> searchResponseActionListener = new ActionListener<MultiSearchResponse>() {_            @Override_            public void onResponse(MultiSearchResponse response) {_                List<SearchHit> searchHits = new ArrayList<>()__                _                for (int i = 0_ i < response.getResponses().length_ i++) {_                    MultiSearchResponse.Item itemResponse = response.getResponses()[i]__                    if (itemResponse.isFailure()) {_                        Exception e = itemResponse.getFailure()__                        _                        _                        if (e instanceof ClusterBlockException) {_                            for (ClusterBlock block : ((ClusterBlockException) e).blocks()) {_                                if ("index closed".equals(block.description())) {_                                    SearchRequest searchRequest = msearch.request().requests().get(i)__                                    _                                    _                                    e = ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}",_                                            job.getId(), searchRequest.indices())__                                }_                            }_                        }_                        listener.onFailure(e)__                        return__                    }_                    searchHits.addAll(Arrays.asList(itemResponse.getResponse().getHits().getHits()))__                }__                if (searchHits.isEmpty()) {_                    listener.onResponse(true)__                } else {_                    int quantileDocCount = 0__                    int categorizerStateDocCount = 0__                    int resultDocCount = 0__                    for (SearchHit hit : searchHits) {_                        if (hit.getId().equals(Quantiles.documentId(job.getId())) ||_                                hit.getId().equals(Quantiles.v54DocumentId(job.getId()))) {_                            quantileDocCount++__                        } else if (hit.getId().startsWith(CategorizerState.documentPrefix(job.getId())) ||_                                hit.getId().startsWith(CategorizerState.v54DocumentPrefix(job.getId()))) {_                            categorizerStateDocCount++__                        } else {_                            resultDocCount++__                        }_                    }__                    LOGGER.warn("{} result, {} quantile state and {} categorizer state documents exist for a prior job with Id [{}]",_                            resultDocCount, quantileDocCount, categorizerStateDocCount, job.getId())___                    listener.onFailure(ExceptionsHelper.conflictStatusException(_                                    "[" + resultDocCount + "] result and [" + (quantileDocCount + categorizerStateDocCount) +_                            "] state documents exist for a prior job with Id [" + job.getId() + "]. " +_                                            "Please create the job with a different Id"))__                }_            }__            @Override_            public void onFailure(Exception e) {_                listener.onFailure(e)__            }_        }___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, msearch.request(), searchResponseActionListener,_                client::multiSearch)__    };check,that,a,previously,deleted,job,with,the,same,id,has,not,left,any,result,or,categorizer,state,documents,due,to,a,failed,delete,any,left,over,results,would,appear,to,be,part,of,the,new,job,we,can,t,check,for,model,state,as,the,id,is,based,on,the,snapshot,id,which,is,a,timestamp,and,so,unpredictable,however,it,is,unlikely,a,new,job,would,have,the,same,snapshot,id,as,an,old,one,param,job,job,configuration,param,listener,the,action,listener;public,void,check,for,left,over,documents,job,job,action,listener,boolean,listener,search,request,builder,state,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,pattern,set,query,query,builders,ids,query,add,ids,categorizer,state,document,id,job,get,id,1,categorizer,state,v54document,id,job,get,id,1,set,indices,options,indices,options,strict,expand,search,request,builder,quantiles,doc,search,client,prepare,search,anomaly,detectors,index,job,state,index,pattern,set,query,query,builders,ids,query,add,ids,quantiles,document,id,job,get,id,quantiles,v54document,id,job,get,id,set,indices,options,indices,options,strict,expand,string,results,index,name,job,get,initial,results,index,name,search,request,builder,result,doc,search,client,prepare,search,results,index,name,set,indices,options,indices,options,lenient,expand,open,set,query,query,builders,term,query,job,id,get,preferred,name,job,get,id,set,size,1,multi,search,request,builder,msearch,client,prepare,multi,search,add,state,doc,search,add,result,doc,search,add,quantiles,doc,search,action,listener,multi,search,response,search,response,action,listener,new,action,listener,multi,search,response,override,public,void,on,response,multi,search,response,response,list,search,hit,search,hits,new,array,list,for,int,i,0,i,response,get,responses,length,i,multi,search,response,item,item,response,response,get,responses,i,if,item,response,is,failure,exception,e,item,response,get,failure,if,e,instanceof,cluster,block,exception,for,cluster,block,block,cluster,block,exception,e,blocks,if,index,closed,equals,block,description,search,request,search,request,msearch,request,requests,get,i,e,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,search,request,indices,listener,on,failure,e,return,search,hits,add,all,arrays,as,list,item,response,get,response,get,hits,get,hits,if,search,hits,is,empty,listener,on,response,true,else,int,quantile,doc,count,0,int,categorizer,state,doc,count,0,int,result,doc,count,0,for,search,hit,hit,search,hits,if,hit,get,id,equals,quantiles,document,id,job,get,id,hit,get,id,equals,quantiles,v54document,id,job,get,id,quantile,doc,count,else,if,hit,get,id,starts,with,categorizer,state,document,prefix,job,get,id,hit,get,id,starts,with,categorizer,state,v54document,prefix,job,get,id,categorizer,state,doc,count,else,result,doc,count,logger,warn,result,quantile,state,and,categorizer,state,documents,exist,for,a,prior,job,with,id,result,doc,count,quantile,doc,count,categorizer,state,doc,count,job,get,id,listener,on,failure,exceptions,helper,conflict,status,exception,result,doc,count,result,and,quantile,doc,count,categorizer,state,doc,count,state,documents,exist,for,a,prior,job,with,id,job,get,id,please,create,the,job,with,a,different,id,override,public,void,on,failure,exception,e,listener,on,failure,e,execute,async,with,origin,client,thread,pool,get,thread,context,msearch,request,search,response,action,listener,client,multi,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1533230566;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits(), Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1534362961;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits(), Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1536314350;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits(), Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1537806831;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits(), Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1540847035;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits(), Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1544035746;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits().value, Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,value,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1544205697;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb).trackTotalHits(true))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits().value, Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,track,total,hits,true,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,value,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1545155131;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb).trackTotalHits(true))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits().value, Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,track,total,hits,true,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,value,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1546880335;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb).trackTotalHits(true))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits().value, Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,track,total,hits,true,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,value,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1547215421;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb).trackTotalHits(true))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits().value, Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,track,total,hits,true,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,value,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1548668326;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb).trackTotalHits(true))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits().value, Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,track,total,hits,true,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,value,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,                             Consumer<Exception> errorHandler, Client client);1550064927;Return a page of influencers for the given job and within the given date range_Uses a supplied client, so may run as the currently authenticated user_@param jobId The job ID for which influencers are requested_@param query the query;public void influencers(String jobId, InfluencersQuery query, Consumer<QueryPage<Influencer>> handler,_                            Consumer<Exception> errorHandler, Client client) {_        QueryBuilder fb = new ResultsFilterBuilder()_                .timeRange(Result.TIMESTAMP.getPreferredName(), query.getStart(), query.getEnd())_                .score(Influencer.INFLUENCER_SCORE.getPreferredName(), query.getInfluencerScoreFilter())_                .interim(query.isIncludeInterim())_                .build()___        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of influencers from index {}{}  with filter from {} size {}", () -> indexName,_                () -> (query.getSortField() != null) ?_                        " with sort " + (query.isSortDescending() ? "descending" : "ascending") + " on field " + query.getSortField() : "",_                query::getFrom, query::getSize)___        QueryBuilder qb = new BoolQueryBuilder()_                .filter(fb)_                .filter(new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), Influencer.RESULT_TYPE_VALUE))___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        FieldSortBuilder sb = query.getSortField() == null ? SortBuilders.fieldSort(ElasticsearchMappings.ES_DOC)_                : new FieldSortBuilder(query.getSortField()).order(query.isSortDescending() ? SortOrder.DESC : SortOrder.ASC)__        searchRequest.source(new SearchSourceBuilder().query(qb).from(query.getFrom()).size(query.getSize()).sort(sb).trackTotalHits(true))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(response -> {_                    List<Influencer> influencers = new ArrayList<>()__                    for (SearchHit hit : response.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            influencers.add(Influencer.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse influencer", e)__                        }_                    }_                    QueryPage<Influencer> result =_                            new QueryPage<>(influencers, response.getHits().getTotalHits().value, Influencer.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetInfluencersAction.NAME))), client::search)__    };return,a,page,of,influencers,for,the,given,job,and,within,the,given,date,range,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,for,which,influencers,are,requested,param,query,the,query;public,void,influencers,string,job,id,influencers,query,query,consumer,query,page,influencer,handler,consumer,exception,error,handler,client,client,query,builder,fb,new,results,filter,builder,time,range,result,timestamp,get,preferred,name,query,get,start,query,get,end,score,influencer,get,preferred,name,query,get,influencer,score,filter,interim,query,is,include,interim,build,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,influencers,from,index,with,filter,from,size,index,name,query,get,sort,field,null,with,sort,query,is,sort,descending,descending,ascending,on,field,query,get,sort,field,query,get,from,query,get,size,query,builder,qb,new,bool,query,builder,filter,fb,filter,new,terms,query,builder,result,get,preferred,name,influencer,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,field,sort,builder,sb,query,get,sort,field,null,sort,builders,field,sort,elasticsearch,mappings,new,field,sort,builder,query,get,sort,field,order,query,is,sort,descending,sort,order,desc,sort,order,asc,search,request,source,new,search,source,builder,query,qb,from,query,get,from,size,query,get,size,sort,sb,track,total,hits,true,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,influencer,influencers,new,array,list,for,search,hit,hit,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,influencers,add,influencer,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,influencer,e,query,page,influencer,result,new,query,page,influencers,response,get,hits,get,total,hits,value,influencer,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,influencers,action,name,client,search
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1533230566;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1534362961;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1536314350;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1537806831;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1540847035;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1544035746;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1544205697;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1545155131;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1546880335;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1547215421;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1548668326;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId);1550064927;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of influencers of the given job__@param jobId the id of the job for which influencers are requested_@return an influencer {@link BatchedResultsIterator};public BatchedResultsIterator<Influencer> newBatchedInfluencersIterator(String jobId) {_        return new BatchedInfluencersIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,influencers,of,the,given,job,param,job,id,the,id,of,the,job,for,which,influencers,are,requested,return,an,influencer,link,batched,results,iterator;public,batched,results,iterator,influencer,new,batched,influencers,iterator,string,job,id,return,new,batched,influencers,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1533230566;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1534362961;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1536314350;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1537806831;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1540847035;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1544035746;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1544205697;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1545155131;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1546880335;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1547215421;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1548668326;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId);1550064927;Returns a {@link BatchedResultsIterator} that allows querying_and iterating over a large number of records in the given job_The records and source indexes are returned by the iterator.__@param jobId the id of the job for which buckets are requested_@return a record {@link BatchedResultsIterator};public BatchedResultsIterator<AnomalyRecord> newBatchedRecordsIterator(String jobId) {_        return new BatchedRecordsIterator(clientWithOrigin(client, ML_ORIGIN), jobId)__    };returns,a,link,batched,results,iterator,that,allows,querying,and,iterating,over,a,large,number,of,records,in,the,given,job,the,records,and,source,indexes,are,returned,by,the,iterator,param,job,id,the,id,of,the,job,for,which,buckets,are,requested,return,a,record,link,batched,results,iterator;public,batched,results,iterator,anomaly,record,new,batched,records,iterator,string,job,id,return,new,batched,records,iterator,client,with,origin,client,job,id
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1533230566;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1534362961;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1536314350;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1537806831;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1540847035;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1544035746;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1544205697;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1545155131;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1546880335;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1547215421;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1548668326;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,                              Consumer<Exception> errorHandler, Client client);1550064927;Expand a bucket with its records;public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, Consumer<Integer> consumer,_                             Consumer<Exception> errorHandler, Client client) {_        Consumer<QueryPage<AnomalyRecord>> h = page -> {_            bucket.getRecords().addAll(page.results())__            consumer.accept(bucket.getRecords().size())__        }__        bucketRecords(jobId, bucket, 0, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),_                false, h, errorHandler, client)__    };expand,a,bucket,with,its,records;public,void,expand,bucket,string,job,id,boolean,include,interim,bucket,bucket,consumer,integer,consumer,consumer,exception,error,handler,client,client,consumer,query,page,anomaly,record,h,page,bucket,get,records,add,all,page,results,consumer,accept,bucket,get,records,size,bucket,records,job,id,bucket,0,include,interim,anomaly,record,probability,get,preferred,name,false,h,error,handler,client
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1533230566;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1534362961;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1536314350;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1537806831;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1540847035;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1544035746;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1544205697;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1545155131;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1546880335;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1547215421;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1548668326;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName);1550064927;Maps authorization failures when querying ML indexes to job-specific authorization failures attributed to the ML actions._Works by replacing the action name with another provided by the caller, and appending the job ID._This is designed to improve understandability when an admin has applied index or document level security to the .ml-anomalies_indexes to allow some users to have access to certain job results but not others._For example, if user ml_test is allowed to see some results, but not the ones for job "farequote" then:__action [indices:data/read/search] is unauthorized for user [ml_test]__gets mapped to:__action [cluster:monitor/xpack/ml/anomaly_detectors/results/buckets/get] is unauthorized for user [ml_test] for job [farequote]__Exceptions that are not related to authorization are returned unaltered._@param e An exception that occurred while getting ML data_@param jobId The job ID_@param mappedActionName The outermost action name, that will make sense to the user who made the request;static Exception mapAuthFailure(Exception e, String jobId, String mappedActionName) {_        if (e instanceof ElasticsearchStatusException) {_            if (((ElasticsearchStatusException)e).status() == RestStatus.FORBIDDEN) {_                e = Exceptions.authorizationError(_                        e.getMessage().replaceFirst("action \\[.*?\\]", "action [" + mappedActionName + "]") + " for job [{}]", jobId)__            }_        }_        return e__    };maps,authorization,failures,when,querying,ml,indexes,to,job,specific,authorization,failures,attributed,to,the,ml,actions,works,by,replacing,the,action,name,with,another,provided,by,the,caller,and,appending,the,job,id,this,is,designed,to,improve,understandability,when,an,admin,has,applied,index,or,document,level,security,to,the,ml,anomalies,indexes,to,allow,some,users,to,have,access,to,certain,job,results,but,not,others,for,example,if,user,is,allowed,to,see,some,results,but,not,the,ones,for,job,farequote,then,action,indices,data,read,search,is,unauthorized,for,user,gets,mapped,to,action,cluster,monitor,xpack,ml,results,buckets,get,is,unauthorized,for,user,for,job,farequote,exceptions,that,are,not,related,to,authorization,are,returned,unaltered,param,e,an,exception,that,occurred,while,getting,ml,data,param,job,id,the,job,id,param,mapped,action,name,the,outermost,action,name,that,will,make,sense,to,the,user,who,made,the,request;static,exception,map,auth,failure,exception,e,string,job,id,string,mapped,action,name,if,e,instanceof,elasticsearch,status,exception,if,elasticsearch,status,exception,e,status,rest,status,forbidden,e,exceptions,authorization,error,e,get,message,replace,first,action,action,mapped,action,name,for,job,job,id,return,e
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1533230566;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<IndicesAliasesResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,indices,aliases,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1534362961;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1536314350;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1537806831;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1540847035;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1544035746;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1544205697;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1545155131;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1546880335;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1547215421;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1548668326;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String indexName = job.getInitialResultsIndexName()___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,index,name,job,get,initial,results,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener);1550064927;Create the Elasticsearch index and the mappings;public void createJobResultIndex(Job job, ClusterState state, final ActionListener<Boolean> finalListener) {_        Collection<String> termFields = (job.getAnalysisConfig() != null) ? job.getAnalysisConfig().termFields() : Collections.emptyList()___        String readAliasName = AnomalyDetectorsIndex.jobResultsAliasedName(job.getId())__        String writeAliasName = AnomalyDetectorsIndex.resultsWriteAlias(job.getId())__        String tempIndexName = job.getInitialResultsIndexName()___        _        _        if (state.getMetaData().hasAlias(tempIndexName)) {_            IndexNameExpressionResolver resolver = new IndexNameExpressionResolver()__            String[] concreteIndices = resolver.concreteIndexNames(state, IndicesOptions.lenientExpandOpen(), tempIndexName)___            _            _            if (concreteIndices.length == 0) {_                finalListener.onFailure(_                    ExceptionsHelper.badRequestException("Cannot create job [{}] as it requires closed index {}", job.getId(),_                        tempIndexName))__                return__            }_            tempIndexName = concreteIndices[0]__        }_        final String indexName = tempIndexName___        final ActionListener<Boolean> createAliasListener = ActionListener.wrap(success -> {_            final IndicesAliasesRequest request = client.admin().indices().prepareAliases()_                    .addAlias(indexName, readAliasName, QueryBuilders.termQuery(Job.ID.getPreferredName(), job.getId()))_                    .addAlias(indexName, writeAliasName).request()__            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request,_                    ActionListener.<AcknowledgedResponse>wrap(r -> finalListener.onResponse(true), finalListener::onFailure),_                    client.admin().indices()::aliases)__            }, finalListener::onFailure)___        _        _        if (!state.getMetaData().hasIndex(indexName)) {_            LOGGER.trace("ES API CALL: create index {}", indexName)__            CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName)__            _            _            try (XContentBuilder termFieldsMapping = ElasticsearchMappings.termFieldsMapping(ElasticsearchMappings.DOC_TYPE, termFields)) {_                createIndexRequest.mapping(ElasticsearchMappings.DOC_TYPE, termFieldsMapping)__            }_            executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest,_                    ActionListener.<CreateIndexResponse>wrap(_                            r -> createAliasListener.onResponse(r.isAcknowledged()),_                            e -> {_                                _                                _                                if (e instanceof ResourceAlreadyExistsException) {_                                    LOGGER.info("Index already exists")__                                    _                                    createAliasListener.onResponse(true)__                                } else {_                                    finalListener.onFailure(e)__                                }_                            }_                    ), client.admin().indices()::create)__        } else {_            long fieldCountLimit = MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.get(settings)__            if (violatedFieldCountLimit(indexName, termFields.size(), fieldCountLimit, state)) {_                String message = "Cannot create job in index '" + indexName + "' as the " +_                        MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey() + " setting will be violated"__                finalListener.onFailure(new IllegalArgumentException(message))__            } else {_                updateIndexMappingWithTermFields(indexName, termFields,_                        ActionListener.wrap(createAliasListener::onResponse, finalListener::onFailure))__            }_        }_    };create,the,elasticsearch,index,and,the,mappings;public,void,create,job,result,index,job,job,cluster,state,state,final,action,listener,boolean,final,listener,collection,string,term,fields,job,get,analysis,config,null,job,get,analysis,config,term,fields,collections,empty,list,string,read,alias,name,anomaly,detectors,index,job,results,aliased,name,job,get,id,string,write,alias,name,anomaly,detectors,index,results,write,alias,job,get,id,string,temp,index,name,job,get,initial,results,index,name,if,state,get,meta,data,has,alias,temp,index,name,index,name,expression,resolver,resolver,new,index,name,expression,resolver,string,concrete,indices,resolver,concrete,index,names,state,indices,options,lenient,expand,open,temp,index,name,if,concrete,indices,length,0,final,listener,on,failure,exceptions,helper,bad,request,exception,cannot,create,job,as,it,requires,closed,index,job,get,id,temp,index,name,return,temp,index,name,concrete,indices,0,final,string,index,name,temp,index,name,final,action,listener,boolean,create,alias,listener,action,listener,wrap,success,final,indices,aliases,request,request,client,admin,indices,prepare,aliases,add,alias,index,name,read,alias,name,query,builders,term,query,job,id,get,preferred,name,job,get,id,add,alias,index,name,write,alias,name,request,execute,async,with,origin,client,thread,pool,get,thread,context,request,action,listener,acknowledged,response,wrap,r,final,listener,on,response,true,final,listener,on,failure,client,admin,indices,aliases,final,listener,on,failure,if,state,get,meta,data,has,index,index,name,logger,trace,es,api,call,create,index,index,name,create,index,request,create,index,request,new,create,index,request,index,name,try,xcontent,builder,term,fields,mapping,elasticsearch,mappings,term,fields,mapping,elasticsearch,mappings,term,fields,create,index,request,mapping,elasticsearch,mappings,term,fields,mapping,execute,async,with,origin,client,thread,pool,get,thread,context,create,index,request,action,listener,create,index,response,wrap,r,create,alias,listener,on,response,r,is,acknowledged,e,if,e,instanceof,resource,already,exists,exception,logger,info,index,already,exists,create,alias,listener,on,response,true,else,final,listener,on,failure,e,client,admin,indices,create,else,long,field,count,limit,mapper,service,get,settings,if,violated,field,count,limit,index,name,term,fields,size,field,count,limit,state,string,message,cannot,create,job,in,index,index,name,as,the,mapper,service,get,key,setting,will,be,violated,final,listener,on,failure,new,illegal,argument,exception,message,else,update,index,mapping,with,term,fields,index,name,term,fields,action,listener,wrap,create,alias,listener,on,response,final,listener,on,failure
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1533230566;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build())__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results, searchResponse.getHits().getTotalHits(), Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1534362961;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build())__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results, searchResponse.getHits().getTotalHits(), Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1536314350;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build())__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results, searchResponse.getHits().getTotalHits(), Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1537806831;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build())__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results, searchResponse.getHits().getTotalHits(), Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1540847035;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build())__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results, searchResponse.getHits().getTotalHits(), Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1544035746;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build())__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results,_                        searchResponse.getHits().getTotalHits().value, Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,value,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1544205697;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build().trackTotalHits(true))__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results,_                        searchResponse.getHits().getTotalHits().value, Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,track,total,hits,true,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,value,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1545155131;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build().trackTotalHits(true))__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results,_                        searchResponse.getHits().getTotalHits().value, Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,track,total,hits,true,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,value,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1546880335;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build().trackTotalHits(true))__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results,_                        searchResponse.getHits().getTotalHits().value, Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,track,total,hits,true,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,value,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1547215421;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build().trackTotalHits(true))__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results,_                        searchResponse.getHits().getTotalHits().value, Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,track,total,hits,true,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,value,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1548668326;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build().trackTotalHits(true))__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results,_                        searchResponse.getHits().getTotalHits().value, Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,track,total,hits,true,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,value,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,                         Client client) throws ResourceNotFoundException;1550064927;Search for buckets with the parameters in the {@link BucketsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void buckets(String jobId, BucketsQueryBuilder query, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler,_                        Client client) throws ResourceNotFoundException {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.source(query.build().trackTotalHits(true))__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(SearchRequest.DEFAULT_INDICES_OPTIONS))___        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHits hits = searchResponse.getHits()__                    List<Bucket> results = new ArrayList<>()__                    for (SearchHit hit : hits.getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            Bucket bucket = Bucket.LENIENT_PARSER.apply(parser, null)__                            results.add(bucket)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse bucket", e)__                        }_                    }__                    if (query.hasTimestamp() && results.isEmpty()) {_                        throw QueryPage.emptyQueryPage(Bucket.RESULTS_FIELD)__                    }__                    QueryPage<Bucket> buckets = new QueryPage<>(results,_                        searchResponse.getHits().getTotalHits().value, Bucket.RESULTS_FIELD)___                    if (query.isExpand()) {_                        Iterator<Bucket> bucketsToExpand = buckets.results().stream()_                                .filter(bucket -> bucket.getBucketInfluencers().size() > 0).iterator()__                        expandBuckets(jobId, query, buckets, bucketsToExpand, handler, errorHandler, client)__                    } else {_                        handler.accept(buckets)__                    }_                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetBucketsAction.NAME))), client::search)__    };search,for,buckets,with,the,parameters,in,the,link,buckets,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,buckets,string,job,id,buckets,query,builder,query,consumer,query,page,bucket,handler,consumer,exception,error,handler,client,client,throws,resource,not,found,exception,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,request,search,request,new,search,request,index,name,search,request,source,query,build,track,total,hits,true,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hits,hits,search,response,get,hits,list,bucket,results,new,array,list,for,search,hit,hit,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,bucket,bucket,bucket,apply,parser,null,results,add,bucket,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,bucket,e,if,query,has,timestamp,results,is,empty,throw,query,page,empty,query,page,bucket,query,page,bucket,buckets,new,query,page,results,search,response,get,hits,get,total,hits,value,bucket,if,query,is,expand,iterator,bucket,buckets,to,expand,buckets,results,stream,filter,bucket,bucket,get,bucket,influencers,size,0,iterator,expand,buckets,job,id,query,buckets,buckets,to,expand,handler,error,handler,client,else,handler,accept,buckets,e,error,handler,accept,map,auth,failure,e,job,id,get,buckets,action,name,client,search
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1533230566;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1534362961;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1536314350;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1537806831;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1540847035;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1544035746;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1544205697;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1545155131;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1546880335;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1547215421;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1548668326;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void modelSnapshots(String jobId,                                int from,                                int size,                                String startEpochMs,                                String endEpochMs,                                String sortField,                                boolean sortDescending,                                String snapshotId,                                Consumer<QueryPage<ModelSnapshot>> handler,                                Consumer<Exception> errorHandler);1550064927;Get model snapshots for the job ordered by descending restore priority.__@param jobId          the job id_@param from           number of snapshots to from_@param size           number of snapshots to retrieve_@param startEpochMs   earliest time to include (inclusive)_@param endEpochMs     latest time to include (exclusive)_@param sortField      optional sort field name (may be null)_@param sortDescending Sort in descending order_@param snapshotId     optional snapshot ID to match (null for all);public void modelSnapshots(String jobId,_                               int from,_                               int size,_                               String startEpochMs,_                               String endEpochMs,_                               String sortField,_                               boolean sortDescending,_                               String snapshotId,_                               Consumer<QueryPage<ModelSnapshot>> handler,_                               Consumer<Exception> errorHandler) {_        ResultsFilterBuilder fb = new ResultsFilterBuilder()__        if (snapshotId != null && !snapshotId.isEmpty()) {_            fb.term(ModelSnapshotField.SNAPSHOT_ID.getPreferredName(), snapshotId)__        }__        QueryBuilder qb = fb.timeRange(Result.TIMESTAMP.getPreferredName(), startEpochMs, endEpochMs).build()__        modelSnapshots(jobId, from, size, sortField, sortDescending, qb, handler, errorHandler)__    };get,model,snapshots,for,the,job,ordered,by,descending,restore,priority,param,job,id,the,job,id,param,from,number,of,snapshots,to,from,param,size,number,of,snapshots,to,retrieve,param,start,epoch,ms,earliest,time,to,include,inclusive,param,end,epoch,ms,latest,time,to,include,exclusive,param,sort,field,optional,sort,field,name,may,be,null,param,sort,descending,sort,in,descending,order,param,snapshot,id,optional,snapshot,id,to,match,null,for,all;public,void,model,snapshots,string,job,id,int,from,int,size,string,start,epoch,ms,string,end,epoch,ms,string,sort,field,boolean,sort,descending,string,snapshot,id,consumer,query,page,model,snapshot,handler,consumer,exception,error,handler,results,filter,builder,fb,new,results,filter,builder,if,snapshot,id,null,snapshot,id,is,empty,fb,term,model,snapshot,field,get,preferred,name,snapshot,id,query,builder,qb,fb,time,range,result,timestamp,get,preferred,name,start,epoch,ms,end,epoch,ms,build,model,snapshots,job,id,from,size,sort,field,sort,descending,qb,handler,error,handler
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1533230566;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build())___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1534362961;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build())___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1536314350;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build())___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1537806831;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build())___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1540847035;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build())___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1544035746;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build())___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,value,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1544205697;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build().trackTotalHits(true))___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,track,total,hits,true,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,value,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1545155131;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build().trackTotalHits(true))___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,track,total,hits,true,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,value,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1546880335;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build().trackTotalHits(true))___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,track,total,hits,true,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,value,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1547215421;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build().trackTotalHits(true))___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,track,total,hits,true,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,value,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1548668326;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build().trackTotalHits(true))___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,track,total,hits,true,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,value,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,                          Consumer<Exception> errorHandler, Client client);1550064927;Search for anomaly records with the parameters in the_{@link RecordsQueryBuilder}_Uses a supplied client, so may run as the currently authenticated user;public void records(String jobId, RecordsQueryBuilder recordsQueryBuilder, Consumer<QueryPage<AnomalyRecord>> handler,_                         Consumer<Exception> errorHandler, Client client) {_        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        SearchSourceBuilder searchSourceBuilder = recordsQueryBuilder.build()__        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        searchRequest.source(recordsQueryBuilder.build().trackTotalHits(true))___        LOGGER.trace("ES API CALL: search all of records from index {} with query {}", indexName, searchSourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    List<AnomalyRecord> results = new ArrayList<>()__                    for (SearchHit hit : searchResponse.getHits().getHits()) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            results.add(AnomalyRecord.LENIENT_PARSER.apply(parser, null))__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse records", e)__                        }_                    }_                    QueryPage<AnomalyRecord> queryPage =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, AnomalyRecord.RESULTS_FIELD)__                    handler.accept(queryPage)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetRecordsAction.NAME))), client::search)__    };search,for,anomaly,records,with,the,parameters,in,the,link,records,query,builder,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user;public,void,records,string,job,id,records,query,builder,records,query,builder,consumer,query,page,anomaly,record,handler,consumer,exception,error,handler,client,client,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,search,source,builder,search,source,builder,records,query,builder,build,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,request,source,records,query,builder,build,track,total,hits,true,logger,trace,es,api,call,search,all,of,records,from,index,with,query,index,name,search,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,list,anomaly,record,results,new,array,list,for,search,hit,hit,search,response,get,hits,get,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,results,add,anomaly,record,apply,parser,null,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,records,e,query,page,anomaly,record,query,page,new,query,page,results,search,response,get,hits,get,total,hits,value,anomaly,record,handler,accept,query,page,e,error,handler,accept,map,auth,failure,e,job,id,get,records,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1533230566;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1534362961;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1536314350;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1537806831;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1540847035;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits(), CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1544035746;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,value,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1544205697;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        sourceBuilder.trackTotalHits(true)__        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,source,builder,track,total,hits,true,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,value,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1545155131;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        sourceBuilder.trackTotalHits(true)__        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,source,builder,track,total,hits,true,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,value,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1546880335;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        sourceBuilder.trackTotalHits(true)__        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,source,builder,track,total,hits,true,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,value,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1547215421;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        sourceBuilder.trackTotalHits(true)__        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,source,builder,track,total,hits,true,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,value,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1548668326;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        sourceBuilder.trackTotalHits(true)__        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,source,builder,track,total,hits,true,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,value,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,                                     Consumer<QueryPage<CategoryDefinition>> handler,                                     Consumer<Exception> errorHandler, Client client);1550064927;Get a page of {@linkplain CategoryDefinition}s for the given <code>jobId</code>._Uses a supplied client, so may run as the currently authenticated user_@param jobId the job id_@param augment Should the category definition be augmented with a Grok pattern?_@param from  Skip the first N categories. This parameter is for paging_@param size  Take only this number of categories;public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size,_                                    Consumer<QueryPage<CategoryDefinition>> handler,_                                    Consumer<Exception> errorHandler, Client client) {_        if (categoryId != null && (from != null || size != null)) {_            throw new IllegalStateException("Both categoryId and pageParams are specified")__        }__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)__        LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}",_                indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size)___        SearchRequest searchRequest = new SearchRequest(indexName)__        searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions()))__        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()__        if (categoryId != null) {_            sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId))__        } else if (from != null && size != null) {_            sourceBuilder.from(from).size(size)_                    .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName()))_                    .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC))__        } else {_            throw new IllegalStateException("Both categoryId and pageParams are not specified")__        }_        sourceBuilder.trackTotalHits(true)__        searchRequest.source(sourceBuilder)__        executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest,_                ActionListener.<SearchResponse>wrap(searchResponse -> {_                    SearchHit[] hits = searchResponse.getHits().getHits()__                    List<CategoryDefinition> results = new ArrayList<>(hits.length)__                    for (SearchHit hit : hits) {_                        BytesReference source = hit.getSourceRef()__                        try (InputStream stream = source.streamInput()__                             XContentParser parser = XContentFactory.xContent(XContentType.JSON)_                                     .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {_                            CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null)__                            if (augment) {_                                augmentWithGrokPattern(categoryDefinition)__                            }_                            results.add(categoryDefinition)__                        } catch (IOException e) {_                            throw new ElasticsearchParseException("failed to parse category definition", e)__                        }_                    }_                    QueryPage<CategoryDefinition> result =_                            new QueryPage<>(results, searchResponse.getHits().getTotalHits().value, CategoryDefinition.RESULTS_FIELD)__                    handler.accept(result)__                }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search)__    };get,a,page,of,linkplain,category,definition,s,for,the,given,code,job,id,code,uses,a,supplied,client,so,may,run,as,the,currently,authenticated,user,param,job,id,the,job,id,param,augment,should,the,category,definition,be,augmented,with,a,grok,pattern,param,from,skip,the,first,n,categories,this,parameter,is,for,paging,param,size,take,only,this,number,of,categories;public,void,category,definitions,string,job,id,long,category,id,boolean,augment,integer,from,integer,size,consumer,query,page,category,definition,handler,consumer,exception,error,handler,client,client,if,category,id,null,from,null,size,null,throw,new,illegal,state,exception,both,category,id,and,page,params,are,specified,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,logger,trace,es,api,call,search,all,of,category,definitions,from,index,sort,ascending,from,size,index,name,category,definition,get,preferred,name,from,size,search,request,search,request,new,search,request,index,name,search,request,indices,options,ml,indices,utils,add,ignore,unavailable,search,request,indices,options,search,source,builder,source,builder,new,search,source,builder,if,category,id,null,source,builder,query,query,builders,term,query,category,definition,get,preferred,name,category,id,else,if,from,null,size,null,source,builder,from,from,size,size,query,query,builders,exists,query,category,definition,get,preferred,name,sort,new,field,sort,builder,category,definition,get,preferred,name,order,sort,order,asc,else,throw,new,illegal,state,exception,both,category,id,and,page,params,are,not,specified,source,builder,track,total,hits,true,search,request,source,source,builder,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,search,response,search,hit,hits,search,response,get,hits,get,hits,list,category,definition,results,new,array,list,hits,length,for,search,hit,hit,hits,bytes,reference,source,hit,get,source,ref,try,input,stream,stream,source,stream,input,xcontent,parser,parser,xcontent,factory,x,content,xcontent,type,json,create,parser,named,xcontent,registry,empty,logging,deprecation,handler,instance,stream,category,definition,category,definition,category,definition,apply,parser,null,if,augment,augment,with,grok,pattern,category,definition,results,add,category,definition,catch,ioexception,e,throw,new,elasticsearch,parse,exception,failed,to,parse,category,definition,e,query,page,category,definition,result,new,query,page,results,search,response,get,hits,get,total,hits,value,category,definition,handler,accept,result,e,error,handler,accept,map,auth,failure,e,job,id,get,categories,action,name,client,search
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1533230566;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1534362961;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1536314350;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1537806831;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1540847035;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1544035746;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1544205697;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1545155131;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1546880335;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1547215421;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1548668326;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
JobResultsProvider -> public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,                                           Consumer<Long> handler, Consumer<Exception> errorHandler);1550064927;Get the "established" memory usage of a job, if it has one._In order for a job to be considered to have established memory usage it must:_- Have generated at least <code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets of results_- Have generated at least one model size stats document_- Have low variability of model bytes in model size stats documents in the time period covered by the last_<code>BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE</code> buckets, which is defined as having a coefficient of variation_of no more than <code>ESTABLISHED_MEMORY_CV_THRESHOLD</code>_@param jobId the id of the job for which established memory usage is required_@param latestBucketTimestamp the latest bucket timestamp to be used for the calculation, if known, otherwise_<code>null</code>, implying the latest bucket that exists in the results index_@param latestModelSizeStats the latest model size stats for the job, if known, otherwise <code>null</code> - supplying_these when available avoids one search_@param handler if the method succeeds, this will be passed the established memory usage (in bytes) of the_specified job, or 0 if memory usage is not yet established_@param errorHandler if a problem occurs, the exception will be passed to this handler;public void getEstablishedMemoryUsage(String jobId, Date latestBucketTimestamp, ModelSizeStats latestModelSizeStats,_                                          Consumer<Long> handler, Consumer<Exception> errorHandler) {__        String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId)___        _        _        Consumer<QueryPage<Bucket>> bucketHandler = buckets -> {_            if (buckets.results().size() == 1) {_                String searchFromTimeMs = Long.toString(buckets.results().get(0).getTimestamp().getTime())__                SearchRequestBuilder search = client.prepareSearch(indexName)_                        .setSize(0)_                        .setIndicesOptions(IndicesOptions.lenientExpandOpen())_                        .setQuery(new BoolQueryBuilder()_                                .filter(QueryBuilders.rangeQuery(Result.TIMESTAMP.getPreferredName()).gte(searchFromTimeMs))_                                .filter(QueryBuilders.termQuery(Result.RESULT_TYPE.getPreferredName(), ModelSizeStats.RESULT_TYPE_VALUE)))_                        .addAggregation(AggregationBuilders.extendedStats("es").field(ModelSizeStats.MODEL_BYTES_FIELD.getPreferredName()))___                executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, search.request(),_                        ActionListener.<SearchResponse>wrap(_                                response -> {_                                    List<Aggregation> aggregations = response.getAggregations().asList()__                                    if (aggregations.size() == 1) {_                                        ExtendedStats extendedStats = (ExtendedStats) aggregations.get(0)__                                        long count = extendedStats.getCount()__                                        if (count <= 0) {_                                            _                                            _                                            handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                        } else if (count == 1) {_                                            _                                            handler.accept((long) extendedStats.getAvg())__                                        } else {_                                            double coefficientOfVaration = extendedStats.getStdDeviation() / extendedStats.getAvg()__                                            LOGGER.trace("[{}] Coefficient of variation [{}] when calculating established memory use",_                                                    jobId, coefficientOfVaration)__                                            _                                            if (coefficientOfVaration <= ESTABLISHED_MEMORY_CV_THRESHOLD) {_                                                _                                                handleLatestModelSizeStats(jobId, latestModelSizeStats, handler, errorHandler)__                                            } else {_                                                _                                                handler.accept(0L)__                                            }_                                        }_                                    } else {_                                        handler.accept(0L)__                                    }_                                }, errorHandler_                        ), client::search)__            } else {_                LOGGER.trace("[{}] Insufficient history to calculate established memory use", jobId)__                handler.accept(0L)__            }_        }___        _        _        BucketsQueryBuilder bucketQuery = new BucketsQueryBuilder()_                .end(latestBucketTimestamp != null ? Long.toString(latestBucketTimestamp.getTime() + 1) : null)_                .sortField(Result.TIMESTAMP.getPreferredName())_                .sortDescending(true).from(BUCKETS_FOR_ESTABLISHED_MEMORY_SIZE - 1).size(1)_                .includeInterim(false)__        bucketsViaInternalClient(jobId, bucketQuery, bucketHandler, e -> {_            if (e instanceof ResourceNotFoundException) {_                handler.accept(0L)__            } else {_                errorHandler.accept(e)__            }_        })__    };get,the,established,memory,usage,of,a,job,if,it,has,one,in,order,for,a,job,to,be,considered,to,have,established,memory,usage,it,must,have,generated,at,least,code,code,buckets,of,results,have,generated,at,least,one,model,size,stats,document,have,low,variability,of,model,bytes,in,model,size,stats,documents,in,the,time,period,covered,by,the,last,code,code,buckets,which,is,defined,as,having,a,coefficient,of,variation,of,no,more,than,code,code,param,job,id,the,id,of,the,job,for,which,established,memory,usage,is,required,param,latest,bucket,timestamp,the,latest,bucket,timestamp,to,be,used,for,the,calculation,if,known,otherwise,code,null,code,implying,the,latest,bucket,that,exists,in,the,results,index,param,latest,model,size,stats,the,latest,model,size,stats,for,the,job,if,known,otherwise,code,null,code,supplying,these,when,available,avoids,one,search,param,handler,if,the,method,succeeds,this,will,be,passed,the,established,memory,usage,in,bytes,of,the,specified,job,or,0,if,memory,usage,is,not,yet,established,param,error,handler,if,a,problem,occurs,the,exception,will,be,passed,to,this,handler;public,void,get,established,memory,usage,string,job,id,date,latest,bucket,timestamp,model,size,stats,latest,model,size,stats,consumer,long,handler,consumer,exception,error,handler,string,index,name,anomaly,detectors,index,job,results,aliased,name,job,id,consumer,query,page,bucket,bucket,handler,buckets,if,buckets,results,size,1,string,search,from,time,ms,long,to,string,buckets,results,get,0,get,timestamp,get,time,search,request,builder,search,client,prepare,search,index,name,set,size,0,set,indices,options,indices,options,lenient,expand,open,set,query,new,bool,query,builder,filter,query,builders,range,query,result,timestamp,get,preferred,name,gte,search,from,time,ms,filter,query,builders,term,query,result,get,preferred,name,model,size,stats,add,aggregation,aggregation,builders,extended,stats,es,field,model,size,stats,get,preferred,name,execute,async,with,origin,client,thread,pool,get,thread,context,search,request,action,listener,search,response,wrap,response,list,aggregation,aggregations,response,get,aggregations,as,list,if,aggregations,size,1,extended,stats,extended,stats,extended,stats,aggregations,get,0,long,count,extended,stats,get,count,if,count,0,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,if,count,1,handler,accept,long,extended,stats,get,avg,else,double,coefficient,of,varation,extended,stats,get,std,deviation,extended,stats,get,avg,logger,trace,coefficient,of,variation,when,calculating,established,memory,use,job,id,coefficient,of,varation,if,coefficient,of,varation,handle,latest,model,size,stats,job,id,latest,model,size,stats,handler,error,handler,else,handler,accept,0l,else,handler,accept,0l,error,handler,client,search,else,logger,trace,insufficient,history,to,calculate,established,memory,use,job,id,handler,accept,0l,buckets,query,builder,bucket,query,new,buckets,query,builder,end,latest,bucket,timestamp,null,long,to,string,latest,bucket,timestamp,get,time,1,null,sort,field,result,timestamp,get,preferred,name,sort,descending,true,from,1,size,1,include,interim,false,buckets,via,internal,client,job,id,bucket,query,bucket,handler,e,if,e,instanceof,resource,not,found,exception,handler,accept,0l,else,error,handler,accept,e
