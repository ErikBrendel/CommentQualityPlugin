# id;timestamp;commentText;codeText;commentWords;codeWords
ChunkedDataExtractor -> DataSummary -> private long estimateChunk();1524684173;The heuristic here is that we want a time interval where we expect roughly scrollSize documents_(assuming data are uniformly spread over time)._We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk._Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits._However, assuming this as the chunk span may often lead to half-filled pages or empty searches._It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.;private long estimateChunk() {_            long dataTimeSpread = getDataTimeSpread()__            if (totalHits <= 0 || dataTimeSpread <= 0) {_                return context.end - currentEnd__            }_            long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits__            return Math.max(estimatedChunk, MIN_CHUNK_SPAN)__        };the,heuristic,here,is,that,we,want,a,time,interval,where,we,expect,roughly,scroll,size,documents,assuming,data,are,uniformly,spread,over,time,we,have,total,hits,documents,over,data,time,spread,latest,time,earliest,time,we,want,scroll,size,documents,over,chunk,thus,the,interval,would,be,scroll,size,data,time,spread,total,hits,however,assuming,this,as,the,chunk,span,may,often,lead,to,half,filled,pages,or,empty,searches,it,is,beneficial,to,take,a,multiple,of,that,based,on,benchmarking,we,set,this,to,10x;private,long,estimate,chunk,long,data,time,spread,get,data,time,spread,if,total,hits,0,data,time,spread,0,return,context,end,current,end,long,estimated,chunk,10,context,scroll,size,get,data,time,spread,total,hits,return,math,max,estimated,chunk
ChunkedDataExtractor -> DataSummary -> private long estimateChunk();1526467406;The heuristic here is that we want a time interval where we expect roughly scrollSize documents_(assuming data are uniformly spread over time)._We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk._Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits._However, assuming this as the chunk span may often lead to half-filled pages or empty searches._It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.;private long estimateChunk() {_            long dataTimeSpread = getDataTimeSpread()__            if (totalHits <= 0 || dataTimeSpread <= 0) {_                return context.end - currentEnd__            }_            long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits__            return Math.max(estimatedChunk, MIN_CHUNK_SPAN)__        };the,heuristic,here,is,that,we,want,a,time,interval,where,we,expect,roughly,scroll,size,documents,assuming,data,are,uniformly,spread,over,time,we,have,total,hits,documents,over,data,time,spread,latest,time,earliest,time,we,want,scroll,size,documents,over,chunk,thus,the,interval,would,be,scroll,size,data,time,spread,total,hits,however,assuming,this,as,the,chunk,span,may,often,lead,to,half,filled,pages,or,empty,searches,it,is,beneficial,to,take,a,multiple,of,that,based,on,benchmarking,we,set,this,to,10x;private,long,estimate,chunk,long,data,time,spread,get,data,time,spread,if,total,hits,0,data,time,spread,0,return,context,end,current,end,long,estimated,chunk,10,context,scroll,size,get,data,time,spread,total,hits,return,math,max,estimated,chunk
ChunkedDataExtractor -> DataSummary -> private long estimateChunk();1527840262;The heuristic here is that we want a time interval where we expect roughly scrollSize documents_(assuming data are uniformly spread over time)._We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk._Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits._However, assuming this as the chunk span may often lead to half-filled pages or empty searches._It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.;private long estimateChunk() {_            long dataTimeSpread = getDataTimeSpread()__            if (totalHits <= 0 || dataTimeSpread <= 0) {_                return context.end - currentEnd__            }_            long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits__            return Math.max(estimatedChunk, MIN_CHUNK_SPAN)__        };the,heuristic,here,is,that,we,want,a,time,interval,where,we,expect,roughly,scroll,size,documents,assuming,data,are,uniformly,spread,over,time,we,have,total,hits,documents,over,data,time,spread,latest,time,earliest,time,we,want,scroll,size,documents,over,chunk,thus,the,interval,would,be,scroll,size,data,time,spread,total,hits,however,assuming,this,as,the,chunk,span,may,often,lead,to,half,filled,pages,or,empty,searches,it,is,beneficial,to,take,a,multiple,of,that,based,on,benchmarking,we,set,this,to,10x;private,long,estimate,chunk,long,data,time,spread,get,data,time,spread,if,total,hits,0,data,time,spread,0,return,context,end,current,end,long,estimated,chunk,10,context,scroll,size,get,data,time,spread,total,hits,return,math,max,estimated,chunk
ChunkedDataExtractor -> DataSummary -> private long estimateChunk();1536314350;The heuristic here is that we want a time interval where we expect roughly scrollSize documents_(assuming data are uniformly spread over time)._We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk._Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits._However, assuming this as the chunk span may often lead to half-filled pages or empty searches._It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.;private long estimateChunk() {_            long dataTimeSpread = getDataTimeSpread()__            if (totalHits <= 0 || dataTimeSpread <= 0) {_                return context.end - currentEnd__            }_            long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits__            return Math.max(estimatedChunk, MIN_CHUNK_SPAN)__        };the,heuristic,here,is,that,we,want,a,time,interval,where,we,expect,roughly,scroll,size,documents,assuming,data,are,uniformly,spread,over,time,we,have,total,hits,documents,over,data,time,spread,latest,time,earliest,time,we,want,scroll,size,documents,over,chunk,thus,the,interval,would,be,scroll,size,data,time,spread,total,hits,however,assuming,this,as,the,chunk,span,may,often,lead,to,half,filled,pages,or,empty,searches,it,is,beneficial,to,take,a,multiple,of,that,based,on,benchmarking,we,set,this,to,10x;private,long,estimate,chunk,long,data,time,spread,get,data,time,spread,if,total,hits,0,data,time,spread,0,return,context,end,current,end,long,estimated,chunk,10,context,scroll,size,get,data,time,spread,total,hits,return,math,max,estimated,chunk
ChunkedDataExtractor -> DataSummary -> private long estimateChunk();1540847035;The heuristic here is that we want a time interval where we expect roughly scrollSize documents_(assuming data are uniformly spread over time)._We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk._Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits._However, assuming this as the chunk span may often lead to half-filled pages or empty searches._It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.;private long estimateChunk() {_            long dataTimeSpread = getDataTimeSpread()__            if (totalHits <= 0 || dataTimeSpread <= 0) {_                return context.end - currentEnd__            }_            long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits__            return Math.max(estimatedChunk, MIN_CHUNK_SPAN)__        };the,heuristic,here,is,that,we,want,a,time,interval,where,we,expect,roughly,scroll,size,documents,assuming,data,are,uniformly,spread,over,time,we,have,total,hits,documents,over,data,time,spread,latest,time,earliest,time,we,want,scroll,size,documents,over,chunk,thus,the,interval,would,be,scroll,size,data,time,spread,total,hits,however,assuming,this,as,the,chunk,span,may,often,lead,to,half,filled,pages,or,empty,searches,it,is,beneficial,to,take,a,multiple,of,that,based,on,benchmarking,we,set,this,to,10x;private,long,estimate,chunk,long,data,time,spread,get,data,time,spread,if,total,hits,0,data,time,spread,0,return,context,end,current,end,long,estimated,chunk,10,context,scroll,size,get,data,time,spread,total,hits,return,math,max,estimated,chunk
ChunkedDataExtractor -> ScrolledDataSummary -> @Override         public long estimateChunk();1541092382;The heuristic here is that we want a time interval where we expect roughly scrollSize documents_(assuming data are uniformly spread over time)._We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk._Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits._However, assuming this as the chunk span may often lead to half-filled pages or empty searches._It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.;@Override_        public long estimateChunk() {_            long dataTimeSpread = getDataTimeSpread()__            if (totalHits <= 0 || dataTimeSpread <= 0) {_                return context.end - currentEnd__            }_            long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits__            return Math.max(estimatedChunk, MIN_CHUNK_SPAN)__        };the,heuristic,here,is,that,we,want,a,time,interval,where,we,expect,roughly,scroll,size,documents,assuming,data,are,uniformly,spread,over,time,we,have,total,hits,documents,over,data,time,spread,latest,time,earliest,time,we,want,scroll,size,documents,over,chunk,thus,the,interval,would,be,scroll,size,data,time,spread,total,hits,however,assuming,this,as,the,chunk,span,may,often,lead,to,half,filled,pages,or,empty,searches,it,is,beneficial,to,take,a,multiple,of,that,based,on,benchmarking,we,set,this,to,10x;override,public,long,estimate,chunk,long,data,time,spread,get,data,time,spread,if,total,hits,0,data,time,spread,0,return,context,end,current,end,long,estimated,chunk,10,context,scroll,size,get,data,time,spread,total,hits,return,math,max,estimated,chunk
ChunkedDataExtractor -> ScrolledDataSummary -> @Override         public long estimateChunk();1544035746;The heuristic here is that we want a time interval where we expect roughly scrollSize documents_(assuming data are uniformly spread over time)._We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk._Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits._However, assuming this as the chunk span may often lead to half-filled pages or empty searches._It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.;@Override_        public long estimateChunk() {_            long dataTimeSpread = getDataTimeSpread()__            if (totalHits <= 0 || dataTimeSpread <= 0) {_                return context.end - currentEnd__            }_            long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits__            return Math.max(estimatedChunk, MIN_CHUNK_SPAN)__        };the,heuristic,here,is,that,we,want,a,time,interval,where,we,expect,roughly,scroll,size,documents,assuming,data,are,uniformly,spread,over,time,we,have,total,hits,documents,over,data,time,spread,latest,time,earliest,time,we,want,scroll,size,documents,over,chunk,thus,the,interval,would,be,scroll,size,data,time,spread,total,hits,however,assuming,this,as,the,chunk,span,may,often,lead,to,half,filled,pages,or,empty,searches,it,is,beneficial,to,take,a,multiple,of,that,based,on,benchmarking,we,set,this,to,10x;override,public,long,estimate,chunk,long,data,time,spread,get,data,time,spread,if,total,hits,0,data,time,spread,0,return,context,end,current,end,long,estimated,chunk,10,context,scroll,size,get,data,time,spread,total,hits,return,math,max,estimated,chunk
ChunkedDataExtractor -> ScrolledDataSummary -> @Override         public long estimateChunk();1544205697;The heuristic here is that we want a time interval where we expect roughly scrollSize documents_(assuming data are uniformly spread over time)._We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk._Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits._However, assuming this as the chunk span may often lead to half-filled pages or empty searches._It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.;@Override_        public long estimateChunk() {_            long dataTimeSpread = getDataTimeSpread()__            if (totalHits <= 0 || dataTimeSpread <= 0) {_                return context.end - currentEnd__            }_            long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits__            return Math.max(estimatedChunk, MIN_CHUNK_SPAN)__        };the,heuristic,here,is,that,we,want,a,time,interval,where,we,expect,roughly,scroll,size,documents,assuming,data,are,uniformly,spread,over,time,we,have,total,hits,documents,over,data,time,spread,latest,time,earliest,time,we,want,scroll,size,documents,over,chunk,thus,the,interval,would,be,scroll,size,data,time,spread,total,hits,however,assuming,this,as,the,chunk,span,may,often,lead,to,half,filled,pages,or,empty,searches,it,is,beneficial,to,take,a,multiple,of,that,based,on,benchmarking,we,set,this,to,10x;override,public,long,estimate,chunk,long,data,time,spread,get,data,time,spread,if,total,hits,0,data,time,spread,0,return,context,end,current,end,long,estimated,chunk,10,context,scroll,size,get,data,time,spread,total,hits,return,math,max,estimated,chunk
ChunkedDataExtractor -> ScrolledDataSummary -> @Override         public long estimateChunk();1546587824;The heuristic here is that we want a time interval where we expect roughly scrollSize documents_(assuming data are uniformly spread over time)._We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk._Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits._However, assuming this as the chunk span may often lead to half-filled pages or empty searches._It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.;@Override_        public long estimateChunk() {_            long dataTimeSpread = getDataTimeSpread()__            if (totalHits <= 0 || dataTimeSpread <= 0) {_                return context.end - currentEnd__            }_            long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits__            return Math.max(estimatedChunk, MIN_CHUNK_SPAN)__        };the,heuristic,here,is,that,we,want,a,time,interval,where,we,expect,roughly,scroll,size,documents,assuming,data,are,uniformly,spread,over,time,we,have,total,hits,documents,over,data,time,spread,latest,time,earliest,time,we,want,scroll,size,documents,over,chunk,thus,the,interval,would,be,scroll,size,data,time,spread,total,hits,however,assuming,this,as,the,chunk,span,may,often,lead,to,half,filled,pages,or,empty,searches,it,is,beneficial,to,take,a,multiple,of,that,based,on,benchmarking,we,set,this,to,10x;override,public,long,estimate,chunk,long,data,time,spread,get,data,time,spread,if,total,hits,0,data,time,spread,0,return,context,end,current,end,long,estimated,chunk,10,context,scroll,size,get,data,time,spread,total,hits,return,math,max,estimated,chunk
ChunkedDataExtractor -> DataSummaryFactory -> private DataSummary buildDataSummary() throws IOException;1541092382;If there are aggregations, an AggregatedDataSummary object is created. It returns a ScrollingDataSummary otherwise.__By default a DatafeedConfig with aggregations, should already have a manual ChunkingConfig created._However, the end user could have specifically set the ChunkingConfig to AUTO, which would not really work for aggregations._So, if we need to gather an appropriate chunked time for aggregations, we can utilize the AggregatedDataSummary__@return DataSummary object_@throws IOException when timefield range search fails;private DataSummary buildDataSummary() throws IOException {_            return context.hasAggregations ? newAggregatedDataSummary() : newScrolledDataSummary()__        };if,there,are,aggregations,an,aggregated,data,summary,object,is,created,it,returns,a,scrolling,data,summary,otherwise,by,default,a,datafeed,config,with,aggregations,should,already,have,a,manual,chunking,config,created,however,the,end,user,could,have,specifically,set,the,chunking,config,to,auto,which,would,not,really,work,for,aggregations,so,if,we,need,to,gather,an,appropriate,chunked,time,for,aggregations,we,can,utilize,the,aggregated,data,summary,return,data,summary,object,throws,ioexception,when,timefield,range,search,fails;private,data,summary,build,data,summary,throws,ioexception,return,context,has,aggregations,new,aggregated,data,summary,new,scrolled,data,summary
ChunkedDataExtractor -> DataSummaryFactory -> private DataSummary buildDataSummary() throws IOException;1544035746;If there are aggregations, an AggregatedDataSummary object is created. It returns a ScrollingDataSummary otherwise.__By default a DatafeedConfig with aggregations, should already have a manual ChunkingConfig created._However, the end user could have specifically set the ChunkingConfig to AUTO, which would not really work for aggregations._So, if we need to gather an appropriate chunked time for aggregations, we can utilize the AggregatedDataSummary__@return DataSummary object_@throws IOException when timefield range search fails;private DataSummary buildDataSummary() throws IOException {_            return context.hasAggregations ? newAggregatedDataSummary() : newScrolledDataSummary()__        };if,there,are,aggregations,an,aggregated,data,summary,object,is,created,it,returns,a,scrolling,data,summary,otherwise,by,default,a,datafeed,config,with,aggregations,should,already,have,a,manual,chunking,config,created,however,the,end,user,could,have,specifically,set,the,chunking,config,to,auto,which,would,not,really,work,for,aggregations,so,if,we,need,to,gather,an,appropriate,chunked,time,for,aggregations,we,can,utilize,the,aggregated,data,summary,return,data,summary,object,throws,ioexception,when,timefield,range,search,fails;private,data,summary,build,data,summary,throws,ioexception,return,context,has,aggregations,new,aggregated,data,summary,new,scrolled,data,summary
ChunkedDataExtractor -> DataSummaryFactory -> private DataSummary buildDataSummary() throws IOException;1544205697;If there are aggregations, an AggregatedDataSummary object is created. It returns a ScrollingDataSummary otherwise.__By default a DatafeedConfig with aggregations, should already have a manual ChunkingConfig created._However, the end user could have specifically set the ChunkingConfig to AUTO, which would not really work for aggregations._So, if we need to gather an appropriate chunked time for aggregations, we can utilize the AggregatedDataSummary__@return DataSummary object_@throws IOException when timefield range search fails;private DataSummary buildDataSummary() throws IOException {_            return context.hasAggregations ? newAggregatedDataSummary() : newScrolledDataSummary()__        };if,there,are,aggregations,an,aggregated,data,summary,object,is,created,it,returns,a,scrolling,data,summary,otherwise,by,default,a,datafeed,config,with,aggregations,should,already,have,a,manual,chunking,config,created,however,the,end,user,could,have,specifically,set,the,chunking,config,to,auto,which,would,not,really,work,for,aggregations,so,if,we,need,to,gather,an,appropriate,chunked,time,for,aggregations,we,can,utilize,the,aggregated,data,summary,return,data,summary,object,throws,ioexception,when,timefield,range,search,fails;private,data,summary,build,data,summary,throws,ioexception,return,context,has,aggregations,new,aggregated,data,summary,new,scrolled,data,summary
ChunkedDataExtractor -> DataSummaryFactory -> private DataSummary buildDataSummary() throws IOException;1546587824;If there are aggregations, an AggregatedDataSummary object is created. It returns a ScrollingDataSummary otherwise.__By default a DatafeedConfig with aggregations, should already have a manual ChunkingConfig created._However, the end user could have specifically set the ChunkingConfig to AUTO, which would not really work for aggregations._So, if we need to gather an appropriate chunked time for aggregations, we can utilize the AggregatedDataSummary__@return DataSummary object_@throws IOException when timefield range search fails;private DataSummary buildDataSummary() throws IOException {_            return context.hasAggregations ? newAggregatedDataSummary() : newScrolledDataSummary()__        };if,there,are,aggregations,an,aggregated,data,summary,object,is,created,it,returns,a,scrolling,data,summary,otherwise,by,default,a,datafeed,config,with,aggregations,should,already,have,a,manual,chunking,config,created,however,the,end,user,could,have,specifically,set,the,chunking,config,to,auto,which,would,not,really,work,for,aggregations,so,if,we,need,to,gather,an,appropriate,chunked,time,for,aggregations,we,can,utilize,the,aggregated,data,summary,return,data,summary,object,throws,ioexception,when,timefield,range,search,fails;private,data,summary,build,data,summary,throws,ioexception,return,context,has,aggregations,new,aggregated,data,summary,new,scrolled,data,summary
ChunkedDataExtractor -> AggregatedDataSummary -> @Override         public long estimateChunk();1541092382;This heuristic is a direct copy of the manual chunking config auto-creation done in {@link DatafeedConfig.Builder};@Override_        public long estimateChunk() {_            return DatafeedConfig.Builder.DEFAULT_AGGREGATION_CHUNKING_BUCKETS * histogramIntervalMillis__        };this,heuristic,is,a,direct,copy,of,the,manual,chunking,config,auto,creation,done,in,link,datafeed,config,builder;override,public,long,estimate,chunk,return,datafeed,config,builder,histogram,interval,millis
ChunkedDataExtractor -> AggregatedDataSummary -> @Override         public long estimateChunk();1544035746;This heuristic is a direct copy of the manual chunking config auto-creation done in {@link DatafeedConfig.Builder};@Override_        public long estimateChunk() {_            return DatafeedConfig.Builder.DEFAULT_AGGREGATION_CHUNKING_BUCKETS * histogramIntervalMillis__        };this,heuristic,is,a,direct,copy,of,the,manual,chunking,config,auto,creation,done,in,link,datafeed,config,builder;override,public,long,estimate,chunk,return,datafeed,config,builder,histogram,interval,millis
ChunkedDataExtractor -> AggregatedDataSummary -> @Override         public long estimateChunk();1544205697;This heuristic is a direct copy of the manual chunking config auto-creation done in {@link DatafeedConfig.Builder};@Override_        public long estimateChunk() {_            return DatafeedConfig.Builder.DEFAULT_AGGREGATION_CHUNKING_BUCKETS * histogramIntervalMillis__        };this,heuristic,is,a,direct,copy,of,the,manual,chunking,config,auto,creation,done,in,link,datafeed,config,builder;override,public,long,estimate,chunk,return,datafeed,config,builder,histogram,interval,millis
ChunkedDataExtractor -> AggregatedDataSummary -> @Override         public long estimateChunk();1546587824;This heuristic is a direct copy of the manual chunking config auto-creation done in {@link DatafeedConfig.Builder};@Override_        public long estimateChunk() {_            return DatafeedConfig.Builder.DEFAULT_AGGREGATION_CHUNKING_BUCKETS * histogramIntervalMillis__        };this,heuristic,is,a,direct,copy,of,the,manual,chunking,config,auto,creation,done,in,link,datafeed,config,builder;override,public,long,estimate,chunk,return,datafeed,config,builder,histogram,interval,millis
