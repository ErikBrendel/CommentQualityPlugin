commented;modifiers;parameterAmount;loc;comment;code
false;;0;1;;long estimateChunk().
false;;0;1;;boolean hasData().
false;;0;1;;long earliestTime().
false;;0;1;;long getDataTimeSpread().
false;public;0;8;;@Override public boolean hasNext() {     boolean currentHasNext = currentExtractor != null && currentExtractor.hasNext().     if (isCancelled()) {         return currentHasNext.     }     return currentHasNext || currentEnd < context.end. }
false;public;0;13;;@Override public Optional<InputStream> next() throws IOException {     if (!hasNext()) {         throw new NoSuchElementException().     }     if (currentExtractor == null) {         // This is the first time next is called         setUpChunkedSearch().     }     return getNextStream(). }
false;private;0;14;;private void setUpChunkedSearch() throws IOException {     DataSummary dataSummary = dataSummaryFactory.buildDataSummary().     if (dataSummary.hasData()) {         currentStart = context.timeAligner.alignToFloor(dataSummary.earliestTime()).         currentEnd = currentStart.         chunkSpan = context.chunkSpan == null ? dataSummary.estimateChunk() : context.chunkSpan.getMillis().         chunkSpan = context.timeAligner.alignToCeil(chunkSpan).         LOGGER.debug("[{}]Chunked search configured: kind = {}, dataTimeSpread = {} ms, chunk span = {} ms", context.jobId, dataSummary.getClass().getSimpleName(), dataSummary.getDataTimeSpread(), chunkSpan).     } else {         // search is over         currentEnd = context.end.     } }
false;protected;1;3;;protected SearchResponse executeSearchRequest(ActionRequestBuilder<SearchRequest, SearchResponse> searchRequestBuilder) {     return ClientHelper.executeWithHeaders(context.headers, ClientHelper.ML_ORIGIN, client, searchRequestBuilder::get). }
false;private;0;23;;private Optional<InputStream> getNextStream() throws IOException {     while (hasNext()) {         boolean isNewSearch = false.         if (currentExtractor == null || currentExtractor.hasNext() == false) {             // First search or the current search finished. we can advance to the next search             advanceTime().             isNewSearch = true.         }         Optional<InputStream> nextStream = currentExtractor.next().         if (nextStream.isPresent()) {             return nextStream.         }         if (isNewSearch && hasNext()) {             // If it was a new search it means it returned 0 results. Thus,             // we reconfigure and jump to the next time interval where there are data.             setUpChunkedSearch().         }     }     return Optional.empty(). }
false;private;0;6;;private void advanceTime() {     currentStart = currentEnd.     currentEnd = Math.min(currentStart + chunkSpan, context.end).     currentExtractor = dataExtractorFactory.newExtractor(currentStart, currentEnd).     LOGGER.trace("[{}] advances time to [{}, {})", context.jobId, currentStart, currentEnd). }
false;public;0;4;;@Override public boolean isCancelled() {     return isCancelled. }
false;public;0;7;;@Override public void cancel() {     if (currentExtractor != null) {         currentExtractor.cancel().     }     isCancelled = true. }
false;;0;3;;ChunkedDataExtractorContext getContext() {     return context. }
true;private;0;3;/**  * If there are aggregations, an AggregatedDataSummary object is created. It returns a ScrollingDataSummary otherwise.  *  * By default a DatafeedConfig with aggregations, should already have a manual ChunkingConfig created.  * However, the end user could have specifically set the ChunkingConfig to AUTO, which would not really work for aggregations.  * So, if we need to gather an appropriate chunked time for aggregations, we can utilize the AggregatedDataSummary  *  * @return DataSummary object  * @throws IOException when timefield range search fails  */ ;/**  * If there are aggregations, an AggregatedDataSummary object is created. It returns a ScrollingDataSummary otherwise.  *  * By default a DatafeedConfig with aggregations, should already have a manual ChunkingConfig created.  * However, the end user could have specifically set the ChunkingConfig to AUTO, which would not really work for aggregations.  * So, if we need to gather an appropriate chunked time for aggregations, we can utilize the AggregatedDataSummary  *  * @return DataSummary object  * @throws IOException when timefield range search fails  */ private DataSummary buildDataSummary() throws IOException {     return context.hasAggregations ? newAggregatedDataSummary() : newScrolledDataSummary(). }
false;private;0;20;;private DataSummary newScrolledDataSummary() throws IOException {     SearchRequestBuilder searchRequestBuilder = rangeSearchRequest().     SearchResponse response = executeSearchRequest(searchRequestBuilder).     LOGGER.debug("[{}] Scrolling Data summary response was obtained", context.jobId).     ExtractorUtils.checkSearchWasSuccessful(context.jobId, response).     Aggregations aggregations = response.getAggregations().     long earliestTime = 0.     long latestTime = 0.     long totalHits = response.getHits().getTotalHits().value.     if (totalHits > 0) {         Min min = aggregations.get(EARLIEST_TIME).         earliestTime = (long) min.getValue().         Max max = aggregations.get(LATEST_TIME).         latestTime = (long) max.getValue().     }     return new ScrolledDataSummary(earliestTime, latestTime, totalHits). }
false;private;0;14;;private DataSummary newAggregatedDataSummary() throws IOException {     // TODO: once RollupSearchAction is changed from indices:admin* to indices:data/read/* this branch is not needed     ActionRequestBuilder<SearchRequest, SearchResponse> searchRequestBuilder = dataExtractorFactory instanceof RollupDataExtractorFactory ? rollupRangeSearchRequest() : rangeSearchRequest().     SearchResponse response = executeSearchRequest(searchRequestBuilder).     LOGGER.debug("[{}] Aggregating Data summary response was obtained", context.jobId).     ExtractorUtils.checkSearchWasSuccessful(context.jobId, response).     Aggregations aggregations = response.getAggregations().     Min min = aggregations.get(EARLIEST_TIME).     Max max = aggregations.get(LATEST_TIME).     return new AggregatedDataSummary(min.getValue(), max.getValue(), context.histogramInterval). }
false;private;0;7;;private SearchSourceBuilder rangeSearchBuilder() {     return new SearchSourceBuilder().size(0).query(ExtractorUtils.wrapInTimeRangeQuery(context.query, context.timeField, currentStart, context.end)).aggregation(AggregationBuilders.min(EARLIEST_TIME).field(context.timeField)).aggregation(AggregationBuilders.max(LATEST_TIME).field(context.timeField)). }
false;private;0;6;;private SearchRequestBuilder rangeSearchRequest() {     return new SearchRequestBuilder(client, SearchAction.INSTANCE).setIndices(context.indices).setSource(rangeSearchBuilder()).setTrackTotalHits(true). }
false;private;0;4;;private RollupSearchAction.RequestBuilder rollupRangeSearchRequest() {     SearchRequest searchRequest = new SearchRequest().indices(context.indices).source(rangeSearchBuilder()).     return new RollupSearchAction.RequestBuilder(client, searchRequest). }
false;public;0;4;;@Override public long earliestTime() {     return earliestTime. }
false;public;0;4;;@Override public long getDataTimeSpread() {     return latestTime - earliestTime. }
true;public;0;9;/**  *  The heuristic here is that we want a time interval where we expect roughly scrollSize documents  * (assuming data are uniformly spread over time).  * We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk.  * Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits.  * However, assuming this as the chunk span may often lead to half-filled pages or empty searches.  * It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.  */ ;/**  *  The heuristic here is that we want a time interval where we expect roughly scrollSize documents  * (assuming data are uniformly spread over time).  * We have totalHits documents over dataTimeSpread (latestTime - earliestTime), we want scrollSize documents over chunk.  * Thus, the interval would be (scrollSize * dataTimeSpread) / totalHits.  * However, assuming this as the chunk span may often lead to half-filled pages or empty searches.  * It is beneficial to take a multiple of that. Based on benchmarking, we set this to 10x.  */ @Override public long estimateChunk() {     long dataTimeSpread = getDataTimeSpread().     if (totalHits <= 0 || dataTimeSpread <= 0) {         return context.end - currentEnd.     }     long estimatedChunk = 10 * (context.scrollSize * getDataTimeSpread()) / totalHits.     return Math.max(estimatedChunk, MIN_CHUNK_SPAN). }
false;public;0;4;;@Override public boolean hasData() {     return totalHits > 0. }
true;public;0;4;/**  * This heuristic is a direct copy of the manual chunking config auto-creation done in {@link DatafeedConfig.Builder}  */ ;/**  * This heuristic is a direct copy of the manual chunking config auto-creation done in {@link DatafeedConfig.Builder}  */ @Override public long estimateChunk() {     return DatafeedConfig.Builder.DEFAULT_AGGREGATION_CHUNKING_BUCKETS * histogramIntervalMillis. }
false;public;0;4;;@Override public boolean hasData() {     return (Double.isInfinite(earliestTime) || Double.isInfinite(latestTime)) == false. }
false;public;0;4;;@Override public long earliestTime() {     return (long) earliestTime. }
false;public;0;4;;@Override public long getDataTimeSpread() {     return (long) latestTime - (long) earliestTime. }
