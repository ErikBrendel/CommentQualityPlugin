commented;modifiers;parameterAmount;loc;comment;code
false;public;0;4;;@Override public boolean hasNext() {     return hasNext. }
false;public;0;4;;@Override public boolean isCancelled() {     return isCancelled. }
false;public;0;6;;@Override public void cancel() {     LOGGER.debug("[{}] Data extractor received cancel request", context.jobId).     isCancelled = true.     hasNext = false. }
false;public;0;17;;@Override public Optional<InputStream> next() throws IOException {     if (!hasNext()) {         throw new NoSuchElementException().     }     if (aggregationToJsonProcessor == null) {         Aggregations aggs = search().         if (aggs == null) {             hasNext = false.             return Optional.empty().         }         initAggregationProcessor(aggs).     }     return Optional.ofNullable(processNextBatch()). }
false;private;0;7;;private Aggregations search() throws IOException {     LOGGER.debug("[{}] Executing aggregated search", context.jobId).     SearchResponse searchResponse = executeSearchRequest(buildSearchRequest(buildBaseSearchSource())).     LOGGER.debug("[{}] Search response was obtained", context.jobId).     ExtractorUtils.checkSearchWasSuccessful(context.jobId, searchResponse).     return validateAggs(searchResponse.getAggregations()). }
false;private;1;5;;private void initAggregationProcessor(Aggregations aggs) throws IOException {     aggregationToJsonProcessor = new AggregationToJsonProcessor(context.timeField, context.fields, context.includeDocCount, context.start).     aggregationToJsonProcessor.process(aggs). }
false;protected;1;3;;protected SearchResponse executeSearchRequest(T searchRequestBuilder) {     return ClientHelper.executeWithHeaders(context.headers, ClientHelper.ML_ORIGIN, client, searchRequestBuilder::get). }
false;private;0;14;;private SearchSourceBuilder buildBaseSearchSource() {     // For derivative aggregations the first bucket will always be null     // so query one extra histogram bucket back and hope there is data     // in that bucket     long histogramSearchStartTime = Math.max(0, context.start - ExtractorUtils.getHistogramIntervalMillis(context.aggs)).     SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().size(0).query(ExtractorUtils.wrapInTimeRangeQuery(context.query, context.timeField, histogramSearchStartTime, context.end)).     context.aggs.getAggregatorFactories().forEach(searchSourceBuilder::aggregation).     context.aggs.getPipelineAggregatorFactories().forEach(searchSourceBuilder::aggregation).     return searchSourceBuilder. }
false;protected,abstract;1;1;;protected abstract T buildSearchRequest(SearchSourceBuilder searchRequestBuilder).
false;private;1;15;;private Aggregations validateAggs(@Nullable Aggregations aggs) {     if (aggs == null) {         return null.     }     List<Aggregation> aggsAsList = aggs.asList().     if (aggsAsList.isEmpty()) {         return null.     }     if (aggsAsList.size() > 1) {         throw new IllegalArgumentException("Multiple top level aggregations not supported. found: " + aggsAsList.stream().map(Aggregation::getName).collect(Collectors.toList())).     }     return aggs. }
false;private;0;6;;private InputStream processNextBatch() throws IOException {     outputStream.reset().     hasNext = aggregationToJsonProcessor.writeDocs(BATCH_KEY_VALUE_PAIRS, outputStream).     return new ByteArrayInputStream(outputStream.toByteArray()). }
false;protected;0;3;;protected long getHistogramInterval() {     return ExtractorUtils.getHistogramIntervalMillis(context.aggs). }
false;public;0;3;;public AggregationDataExtractorContext getContext() {     return context. }
