commented;modifiers;parameterAmount;loc;comment;code
false;;0;3;;void isolate() {     isIsolated = true. }
false;;0;3;;boolean isIsolated() {     return isIsolated. }
false;public;0;3;;public String getJobId() {     return jobId. }
false;;2;43;;Long runLookBack(long startTime, Long endTime) throws Exception {     lookbackStartTimeMs = skipToStartTime(startTime).     Optional<Long> endMs = Optional.ofNullable(endTime).     long lookbackEnd = endMs.orElse(currentTimeSupplier.get() - queryDelayMs).     boolean isLookbackOnly = endMs.isPresent().     if (lookbackEnd <= lookbackStartTimeMs) {         if (isLookbackOnly) {             return null.         } else {             auditor.info(jobId, Messages.getMessage(Messages.JOB_AUDIT_DATAFEED_STARTED_REALTIME)).             return nextRealtimeTimestamp().         }     }     String msg = Messages.getMessage(Messages.JOB_AUDIT_DATAFEED_STARTED_FROM_TO, DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.formatMillis(lookbackStartTimeMs), endTime == null ? "real-time" : DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.formatMillis(lookbackEnd), TimeValue.timeValueMillis(frequencyMs).getStringRep()).     auditor.info(jobId, msg).     LOGGER.info("[{}] {}", jobId, msg).     FlushJobAction.Request request = new FlushJobAction.Request(jobId).     request.setCalcInterim(true).     run(lookbackStartTimeMs, lookbackEnd, request).     if (shouldPersistAfterLookback(isLookbackOnly)) {         sendPersistRequest().     }     if (isRunning() && !isIsolated) {         auditor.info(jobId, Messages.getMessage(Messages.JOB_AUDIT_DATAFEED_LOOKBACK_COMPLETED)).         LOGGER.info("[{}] Lookback has finished", jobId).         if (isLookbackOnly) {             return null.         } else {             auditor.info(jobId, Messages.getMessage(Messages.JOB_AUDIT_DATAFEED_CONTINUED_REALTIME)).             return nextRealtimeTimestamp().         }     }     if (!isIsolated) {         LOGGER.debug("[{}] Lookback finished after being stopped", jobId).     }     return null. }
false;private;1;15;;private long skipToStartTime(long startTime) {     if (lastEndTimeMs != null) {         if (lastEndTimeMs + 1 > startTime) {             // start time is before last checkpoint, thus continue from checkpoint             return lastEndTimeMs + 1.         }         // start time is after last checkpoint, thus we need to skip time         FlushJobAction.Request request = new FlushJobAction.Request(jobId).         request.setSkipTime(String.valueOf(startTime)).         FlushJobAction.Response flushResponse = flushJob(request).         LOGGER.info("[{}] Skipped to time [{}]", jobId, flushResponse.getLastFinalizedBucketEnd().getTime()).         return flushResponse.getLastFinalizedBucketEnd().getTime().     }     return startTime. }
false;;0;11;;long runRealtime() throws Exception {     long start = lastEndTimeMs == null ? lookbackStartTimeMs : Math.max(lookbackStartTimeMs, lastEndTimeMs + 1).     long nowMinusQueryDelay = currentTimeSupplier.get() - queryDelayMs.     long end = toIntervalStartEpochMs(nowMinusQueryDelay).     FlushJobAction.Request request = new FlushJobAction.Request(jobId).     request.setCalcInterim(true).     request.setAdvanceTime(String.valueOf(end)).     run(start, end, request).     checkForMissingDataIfNecessary().     return nextRealtimeTimestamp(). }
false;private;0;40;;private void checkForMissingDataIfNecessary() {     if (isRunning() && !isIsolated && checkForMissingDataTriggered()) {         // Keep track of the last bucket time for which we did a missing data check         this.lastDataCheckTimeMs = this.currentTimeSupplier.get().         List<BucketWithMissingData> missingDataBuckets = delayedDataDetector.detectMissingData(latestFinalBucketEndTimeMs).         if (missingDataBuckets.isEmpty() == false) {             long totalRecordsMissing = missingDataBuckets.stream().mapToLong(BucketWithMissingData::getMissingDocumentCount).sum().             Bucket lastBucket = missingDataBuckets.get(missingDataBuckets.size() - 1).getBucket().             // Get the end of the last bucket and make it milliseconds             Date endTime = new Date((lastBucket.getEpoch() + lastBucket.getBucketSpan()) * 1000).             String msg = Messages.getMessage(Messages.JOB_AUDIT_DATAFEED_MISSING_DATA, totalRecordsMissing, XContentElasticsearchExtension.DEFAULT_DATE_PRINTER.print(lastBucket.getTimestamp().getTime())).             Annotation annotation = createAnnotation(missingDataBuckets.get(0).getBucket().getTimestamp(), endTime, msg).             // Cannot use annotation.equals(other) as that checks createTime             if (lastDataCheckAnnotation != null && annotation.getAnnotation().equals(lastDataCheckAnnotation.getAnnotation()) && annotation.getTimestamp().equals(lastDataCheckAnnotation.getTimestamp()) && annotation.getEndTimestamp().equals(lastDataCheckAnnotation.getEndTimestamp())) {                 return.             }             // Creating a warning in addition to updating/creating our annotation. This allows the issue to be plainly visible             // in the job list page.             auditor.warning(jobId, msg).             if (lastDataCheckAnnotationId != null) {                 updateAnnotation(annotation).             } else {                 lastDataCheckAnnotationId = addAndSetDelayedDataAnnotation(annotation).             }         }     } }
false;private;3;12;;private Annotation createAnnotation(Date startTime, Date endTime, String msg) {     Date currentTime = new Date(currentTimeSupplier.get()).     return new Annotation(msg, currentTime, XPackUser.NAME, startTime, endTime, jobId, currentTime, XPackUser.NAME, "annotation"). }
false;private;1;16;;private String addAndSetDelayedDataAnnotation(Annotation annotation) {     try (XContentBuilder xContentBuilder = annotation.toXContent(XContentFactory.jsonBuilder(), ToXContent.EMPTY_PARAMS)) {         IndexRequest request = new IndexRequest(AnnotationIndex.WRITE_ALIAS_NAME).         request.source(xContentBuilder).         try (ThreadContext.StoredContext ignore = stashWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN)) {             IndexResponse response = client.index(request).actionGet().             lastDataCheckAnnotation = annotation.             return response.getId().         }     } catch (IOException ex) {         String errorMessage = "[" + jobId + "] failed to create annotation for delayed data checker.".         LOGGER.error(errorMessage, ex).         auditor.error(jobId, errorMessage).         return null.     } }
false;private;1;21;;private void updateAnnotation(Annotation annotation) {     Annotation updatedAnnotation = new Annotation(lastDataCheckAnnotation).     updatedAnnotation.setModifiedUsername(XPackUser.NAME).     updatedAnnotation.setModifiedTime(new Date(currentTimeSupplier.get())).     updatedAnnotation.setAnnotation(annotation.getAnnotation()).     updatedAnnotation.setTimestamp(annotation.getTimestamp()).     updatedAnnotation.setEndTimestamp(annotation.getEndTimestamp()).     try (XContentBuilder xContentBuilder = updatedAnnotation.toXContent(XContentFactory.jsonBuilder(), ToXContent.EMPTY_PARAMS)) {         IndexRequest indexRequest = new IndexRequest(AnnotationIndex.WRITE_ALIAS_NAME).         indexRequest.id(lastDataCheckAnnotationId).         indexRequest.source(xContentBuilder).         try (ThreadContext.StoredContext ignore = stashWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN)) {             client.index(indexRequest).actionGet().             lastDataCheckAnnotation = updatedAnnotation.         }     } catch (IOException ex) {         String errorMessage = "[" + jobId + "] failed to update annotation for delayed data checker.".         LOGGER.error(errorMessage, ex).         auditor.error(jobId, errorMessage).     } }
true;private;0;4;/**  * We wait a static interval of 15 minutes till the next missing data check.  *  * However, if our delayed data window is smaller than that, we will probably want to check at every available window (if freq. allows).  * This is to help to miss as few buckets in the delayed data check as possible.  *  * If our frequency/query delay are longer then our default interval or window size, we will end up looking for missing data on  * every real-time trigger. This should be OK as the we are pulling from the Index as such a slow pace, another query will  * probably not even be noticeable at such a large timescale.  */ ;/**  * We wait a static interval of 15 minutes till the next missing data check.  *  * However, if our delayed data window is smaller than that, we will probably want to check at every available window (if freq. allows).  * This is to help to miss as few buckets in the delayed data check as possible.  *  * If our frequency/query delay are longer then our default interval or window size, we will end up looking for missing data on  * every real-time trigger. This should be OK as the we are pulling from the Index as such a slow pace, another query will  * probably not even be noticeable at such a large timescale.  */ private boolean checkForMissingDataTriggered() {     return this.currentTimeSupplier.get() > this.lastDataCheckTimeMs + Math.min(MISSING_DATA_CHECK_INTERVAL_MS, delayedDataDetector.getWindow()). }
true;public;0;7;/**  * Stops the datafeed job  *  * @return <code>true</code> when the datafeed was running and this method invocation stopped it,  *         otherwise <code>false</code> is returned  */ ;/**  * Stops the datafeed job  *  * @return <code>true</code> when the datafeed was running and this method invocation stopped it,  *         otherwise <code>false</code> is returned  */ public boolean stop() {     if (running.compareAndSet(true, false)) {         return true.     } else {         return false.     } }
false;public;0;3;;public boolean isRunning() {     return running.get(). }
false;private;3;98;;private void run(long start, long end, FlushJobAction.Request flushRequest) throws IOException {     if (end <= start) {         return.     }     LOGGER.trace("[{}] Searching data in: [{}, {})", jobId, start, end).     // A storage for errors that should only be thrown after advancing time     RuntimeException error = null.     long recordCount = 0.     DataExtractor dataExtractor = dataExtractorFactory.newExtractor(start, end).     while (dataExtractor.hasNext()) {         if ((isIsolated || !isRunning()) && !dataExtractor.isCancelled()) {             dataExtractor.cancel().         }         if (isIsolated) {             return.         }         Optional<InputStream> extractedData.         try {             extractedData = dataExtractor.next().         } catch (Exception e) {             LOGGER.debug("[" + jobId + "] error while extracting data", e).             // deep in the exception.             if (e.toString().contains("doc values")) {                 throw new ExtractionProblemException(nextRealtimeTimestamp(), new IllegalArgumentException("One or more fields do not have doc values. please enable doc values for all analysis fields for datafeeds" + " using aggregations")).             }             throw new ExtractionProblemException(nextRealtimeTimestamp(), e).         }         if (isIsolated) {             return.         }         if (extractedData.isPresent()) {             DataCounts counts.             try (InputStream in = extractedData.get()) {                 counts = postData(in, XContentType.JSON).                 LOGGER.trace("[{}] Processed another {} records", jobId, counts.getProcessedRecordCount()).             } catch (Exception e) {                 if (e instanceof InterruptedException) {                     Thread.currentThread().interrupt().                 }                 if (isIsolated) {                     return.                 }                 LOGGER.debug("[" + jobId + "] error while posting data", e).                 // a conflict exception means the job state is not open any more.                 // we should therefore stop the datafeed.                 boolean shouldStop = isConflictException(e).                 // When an analysis problem occurs, it means something catastrophic has                 // happened to the c++ process. We sent a batch of data to the c++ process                 // yet we do not know how many of those were processed. It is better to                 // advance time in order to avoid importing duplicate data.                 error = new AnalysisProblemException(nextRealtimeTimestamp(), shouldStop, e).                 break.             }             recordCount += counts.getProcessedRecordCount().             if (counts.getLatestRecordTimeStamp() != null) {                 lastEndTimeMs = counts.getLatestRecordTimeStamp().getTime().             }         }     }     lastEndTimeMs = Math.max(lastEndTimeMs == null ? 0 : lastEndTimeMs, end - 1).     LOGGER.debug("[{}] Complete iterating data extractor [{}], [{}], [{}], [{}], [{}]", jobId, error, recordCount, lastEndTimeMs, isRunning(), dataExtractor.isCancelled()).     // We can now throw any stored error as we have updated time.     if (error != null) {         throw error.     }     // datafeed is still running.     if (isRunning() && !isIsolated) {         Date lastFinalizedBucketEnd = flushJob(flushRequest).getLastFinalizedBucketEnd().         if (lastFinalizedBucketEnd != null) {             this.latestFinalBucketEndTimeMs = lastFinalizedBucketEnd.getTime().         }     }     if (recordCount == 0) {         throw new EmptyDataCountException(nextRealtimeTimestamp()).     } }
false;private;2;12;;private DataCounts postData(InputStream inputStream, XContentType xContentType) throws IOException {     PostDataAction.Request request = new PostDataAction.Request(jobId).     request.setDataDescription(dataDescription).     ByteArrayOutputStream outputStream = new ByteArrayOutputStream().     Streams.copy(inputStream, outputStream).     request.setContent(new BytesArray(outputStream.toByteArray()), xContentType).     try (ThreadContext.StoredContext ignore = stashWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN)) {         PostDataAction.Response response = client.execute(PostDataAction.INSTANCE, request).actionGet().         return response.getDataCounts().     } }
false;private;1;4;;private boolean isConflictException(Exception e) {     return e instanceof ElasticsearchStatusException && ((ElasticsearchStatusException) e).status() == RestStatus.CONFLICT. }
false;private;0;10;;private long nextRealtimeTimestamp() {     // We find the timestamp of the start of the next frequency interval.     // The goal is to minimize any lag. To do so,     // we offset the time by the query delay modulo frequency.     // For example, if frequency is 60s and query delay 90s,     // we run 30s past the minute. If frequency is 1s and query delay 10s,     // we don't add anything and we'll run every second.     long next = currentTimeSupplier.get() + frequencyMs.     return toIntervalStartEpochMs(next) + queryDelayMs % frequencyMs + NEXT_TASK_DELAY_MS. }
false;private;1;3;;private long toIntervalStartEpochMs(long epochMs) {     return (epochMs / frequencyMs) * frequencyMs. }
false;private;1;20;;private FlushJobAction.Response flushJob(FlushJobAction.Request flushRequest) {     try {         LOGGER.trace("[" + jobId + "] Sending flush request").         try (ThreadContext.StoredContext ignore = stashWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN)) {             return client.execute(FlushJobAction.INSTANCE, flushRequest).actionGet().         }     } catch (Exception e) {         LOGGER.debug("[" + jobId + "] error while flushing job", e).         // a conflict exception means the job state is not open any more.         // we should therefore stop the datafeed.         boolean shouldStop = isConflictException(e).         // advance time in order to avoid importing duplicate data.         throw new AnalysisProblemException(nextRealtimeTimestamp(), shouldStop, e).     } }
false;private;1;3;;private boolean shouldPersistAfterLookback(boolean isLookbackOnly) {     return isLookbackOnly == false && isIsolated == false && isRunning(). }
false;private;0;10;;private void sendPersistRequest() {     try {         LOGGER.trace("[" + jobId + "] Sending persist request").         try (ThreadContext.StoredContext ignore = stashWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN)) {             client.execute(PersistJobAction.INSTANCE, new PersistJobAction.Request(jobId)).         }     } catch (Exception e) {         LOGGER.debug("[" + jobId + "] error while persisting job", e).     } }
true;;0;3;/**  * Visible for testing  */ ;/**  * Visible for testing  */ Long lastEndTimeMs() {     return lastEndTimeMs. }
