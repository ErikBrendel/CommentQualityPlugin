commented;modifiers;parameterAmount;loc;comment;code
false;protected;0;7;;@Override protected Settings restClientSettings() {     String token = "Basic " + Base64.getEncoder().encodeToString("test_user:x-pack-test-password".getBytes(StandardCharsets.UTF_8)).     return Settings.builder().put(ThreadContext.PREFIX + ".Authorization", token).build(). }
false;public;0;5;;@Before public void waitForMlTemplates() throws Exception {     List<String> templatesToWaitFor = XPackRestTestHelper.ML_POST_V660_TEMPLATES.     XPackRestTestHelper.waitForTemplates(client(), templatesToWaitFor). }
false;private;0;10;;private void createTestIndex() throws IOException {     Request createTestIndex = new Request("PUT", "/airline-data").     createTestIndex.setJsonEntity("{\"mappings\": { \"doc\": {\"properties\": {" + "\"time\": {\"type\": \"date\"}," + "\"airline\": {\"type\": \"keyword\"}," + "\"responsetime\": {\"type\": \"float\"}" + "}}}}").     createTestIndex.setOptions(allowTypesRemovalWarnings()).     client().performRequest(createTestIndex). }
false;public;0;9;;@AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/36816") public void testMigration() throws Exception {     if (isRunningAgainstOldCluster()) {         createTestIndex().         oldClusterTests().     } else {         upgradedClusterTests().     } }
false;private;0;43;;private void oldClusterTests() throws IOException {     // create jobs and datafeeds     Detector.Builder d = new Detector.Builder("metric", "responsetime").     d.setByFieldName("airline").     AnalysisConfig.Builder analysisConfig = new AnalysisConfig.Builder(Collections.singletonList(d.build())).     analysisConfig.setBucketSpan(TimeValue.timeValueMinutes(10)).     Job.Builder closedJob = new Job.Builder(OLD_CLUSTER_CLOSED_JOB_ID).     closedJob.setAnalysisConfig(analysisConfig).     closedJob.setDataDescription(new DataDescription.Builder()).     Request putClosedJob = new Request("PUT", "/_xpack/ml/anomaly_detectors/" + OLD_CLUSTER_CLOSED_JOB_ID).     putClosedJob.setJsonEntity(Strings.toString(closedJob)).     client().performRequest(putClosedJob).     DatafeedConfig.Builder stoppedDfBuilder = new DatafeedConfig.Builder(OLD_CLUSTER_STOPPED_DATAFEED_ID, OLD_CLUSTER_CLOSED_JOB_ID).     stoppedDfBuilder.setIndices(Collections.singletonList("airline-data")).     Request putStoppedDatafeed = new Request("PUT", "/_xpack/ml/datafeeds/" + OLD_CLUSTER_STOPPED_DATAFEED_ID).     putStoppedDatafeed.setJsonEntity(Strings.toString(stoppedDfBuilder.build())).     client().performRequest(putStoppedDatafeed).     // open job and started datafeed     Job.Builder openJob = new Job.Builder(OLD_CLUSTER_OPEN_JOB_ID).     openJob.setAnalysisConfig(analysisConfig).     openJob.setDataDescription(new DataDescription.Builder()).     Request putOpenJob = new Request("PUT", "_xpack/ml/anomaly_detectors/" + OLD_CLUSTER_OPEN_JOB_ID).     putOpenJob.setJsonEntity(Strings.toString(openJob)).     client().performRequest(putOpenJob).     Request openOpenJob = new Request("POST", "_xpack/ml/anomaly_detectors/" + OLD_CLUSTER_OPEN_JOB_ID + "/_open").     client().performRequest(openOpenJob).     DatafeedConfig.Builder dfBuilder = new DatafeedConfig.Builder(OLD_CLUSTER_STARTED_DATAFEED_ID, OLD_CLUSTER_OPEN_JOB_ID).     dfBuilder.setIndices(Collections.singletonList("airline-data")).     Request putDatafeed = new Request("PUT", "_xpack/ml/datafeeds/" + OLD_CLUSTER_STARTED_DATAFEED_ID).     putDatafeed.setJsonEntity(Strings.toString(dfBuilder.build())).     client().performRequest(putDatafeed).     Request startDatafeed = new Request("POST", "_xpack/ml/datafeeds/" + OLD_CLUSTER_STARTED_DATAFEED_ID + "/_start").     client().performRequest(startDatafeed). }
false;private;0;20;;private void upgradedClusterTests() throws Exception {     // wait for the closed and open jobs and datafeed to be migrated     waitForMigration(Arrays.asList(OLD_CLUSTER_CLOSED_JOB_ID, OLD_CLUSTER_OPEN_JOB_ID), Arrays.asList(OLD_CLUSTER_STOPPED_DATAFEED_ID, OLD_CLUSTER_STARTED_DATAFEED_ID)).     waitForJobToBeAssigned(OLD_CLUSTER_OPEN_JOB_ID).     waitForDatafeedToBeAssigned(OLD_CLUSTER_STARTED_DATAFEED_ID).     // The persistent task params for the job & datafeed left open     // during upgrade should be updated with new fields     checkTaskParamsAreUpdated(OLD_CLUSTER_OPEN_JOB_ID, OLD_CLUSTER_STARTED_DATAFEED_ID).     // open the migrated job and datafeed     Request openJob = new Request("POST", "_ml/anomaly_detectors/" + OLD_CLUSTER_CLOSED_JOB_ID + "/_open").     client().performRequest(openJob).     Request startDatafeed = new Request("POST", "_ml/datafeeds/" + OLD_CLUSTER_STOPPED_DATAFEED_ID + "/_start").     client().performRequest(startDatafeed).     waitForJobToBeAssigned(OLD_CLUSTER_CLOSED_JOB_ID).     waitForDatafeedToBeAssigned(OLD_CLUSTER_STOPPED_DATAFEED_ID). }
false;private;1;16;;@SuppressWarnings("unchecked") private void waitForJobToBeAssigned(String jobId) throws Exception {     assertBusy(() -> {         Request getJobStats = new Request("GET", "_ml/anomaly_detectors/" + jobId + "/_stats").         Response response = client().performRequest(getJobStats).         Map<String, Object> stats = entityAsMap(response).         List<Map<String, Object>> jobStats = (List<Map<String, Object>>) XContentMapValues.extractValue("jobs", stats).         assertEquals(jobId, XContentMapValues.extractValue("job_id", jobStats.get(0))).         assertEquals("opened", XContentMapValues.extractValue("state", jobStats.get(0))).         assertThat((String) XContentMapValues.extractValue("assignment_explanation", jobStats.get(0)), isEmptyOrNullString()).         assertNotNull(XContentMapValues.extractValue("node", jobStats.get(0))).     }, 30, TimeUnit.SECONDS). }
false;private;1;15;;@SuppressWarnings("unchecked") private void waitForDatafeedToBeAssigned(String datafeedId) throws Exception {     assertBusy(() -> {         Request getDatafeedStats = new Request("GET", "_ml/datafeeds/" + datafeedId + "/_stats").         Response response = client().performRequest(getDatafeedStats).         Map<String, Object> stats = entityAsMap(response).         List<Map<String, Object>> datafeedStats = (List<Map<String, Object>>) XContentMapValues.extractValue("datafeeds", stats).         assertEquals(datafeedId, XContentMapValues.extractValue("datafeed_id", datafeedStats.get(0))).         assertEquals("started", XContentMapValues.extractValue("state", datafeedStats.get(0))).         assertThat((String) XContentMapValues.extractValue("assignment_explanation", datafeedStats.get(0)), isEmptyOrNullString()).         assertNotNull(XContentMapValues.extractValue("node", datafeedStats.get(0))).     }, 30, TimeUnit.SECONDS). }
false;private;2;33;;@SuppressWarnings("unchecked") private void waitForMigration(List<String> expectedMigratedJobs, List<String> expectedMigratedDatafeeds) throws Exception {     // After v6.6.0 jobs are created in the index so no migration will take place     if (getOldClusterVersion().onOrAfter(Version.V_6_6_0)) {         return.     }     assertBusy(() -> {         // wait for the eligible configs to be moved from the clusterstate         Request getClusterState = new Request("GET", "/_cluster/state/metadata").         Response response = client().performRequest(getClusterState).         Map<String, Object> responseMap = entityAsMap(response).         List<Map<String, Object>> jobs = (List<Map<String, Object>>) XContentMapValues.extractValue("metadata.ml.jobs", responseMap).         if (jobs != null) {             for (String jobId : expectedMigratedJobs) {                 assertJobNotPresent(jobId, jobs).             }         }         List<Map<String, Object>> datafeeds = (List<Map<String, Object>>) XContentMapValues.extractValue("metadata.ml.datafeeds", responseMap).         if (datafeeds != null) {             for (String datafeedId : expectedMigratedDatafeeds) {                 assertDatafeedNotPresent(datafeedId, datafeeds).             }         }     }, 30, TimeUnit.SECONDS). }
false;private;2;24;;@SuppressWarnings("unchecked") private void checkTaskParamsAreUpdated(String jobId, String datafeedId) throws Exception {     Request getClusterState = new Request("GET", "/_cluster/state/metadata").     Response response = client().performRequest(getClusterState).     Map<String, Object> responseMap = entityAsMap(response).     List<Map<String, Object>> tasks = (List<Map<String, Object>>) XContentMapValues.extractValue("metadata.persistent_tasks.tasks", responseMap).     assertNotNull(tasks).     for (Map<String, Object> task : tasks) {         String id = (String) task.get("id").         assertNotNull(id).         if (id.equals(MlTasks.jobTaskId(jobId))) {             Object jobParam = XContentMapValues.extractValue("task.xpack/ml/job.params.job", task).             assertNotNull(jobParam).         } else if (id.equals(MlTasks.datafeedTaskId(datafeedId))) {             Object jobIdParam = XContentMapValues.extractValue("task.xpack/ml/datafeed.params.job_id", task).             assertNotNull(jobIdParam).             Object indices = XContentMapValues.extractValue("task.xpack/ml/datafeed.params.indices", task).             assertNotNull(indices).         }     } }
false;private;2;5;;private void assertDatafeedNotPresent(String datafeedId, List<Map<String, Object>> datafeeds) {     Optional<Object> config = datafeeds.stream().map(map -> map.get("datafeed_id")).filter(id -> id.equals(datafeedId)).findFirst().     assertFalse(config.isPresent()). }
false;private;2;5;;private void assertJobNotPresent(String jobId, List<Map<String, Object>> jobs) {     Optional<Object> config = jobs.stream().map(map -> map.get("job_id")).filter(id -> id.equals(jobId)).findFirst().     assertFalse(config.isPresent()). }
