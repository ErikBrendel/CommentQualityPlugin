commented;modifiers;parameterAmount;loc;comment;code
false;public;0;85;;public void testHAFailoverWithRepository() throws Exception {     RestClient client = client().     String esKerberosPrincipal = System.getProperty("test.krb5.principal.es").     String hdfsKerberosPrincipal = System.getProperty("test.krb5.principal.hdfs").     String kerberosKeytabLocation = System.getProperty("test.krb5.keytab.hdfs").     boolean securityEnabled = hdfsKerberosPrincipal != null.     Configuration hdfsConfiguration = new Configuration().     hdfsConfiguration.set("dfs.nameservices", "ha-hdfs").     hdfsConfiguration.set("dfs.ha.namenodes.ha-hdfs", "nn1,nn2").     hdfsConfiguration.set("dfs.namenode.rpc-address.ha-hdfs.nn1", "localhost:10001").     hdfsConfiguration.set("dfs.namenode.rpc-address.ha-hdfs.nn2", "localhost:10002").     hdfsConfiguration.set("dfs.client.failover.proxy.provider.ha-hdfs", "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider").     AccessController.doPrivileged((PrivilegedExceptionAction<Void>) () -> {         if (securityEnabled) {             // ensure that keytab exists             Path kt = PathUtils.get(kerberosKeytabLocation).             if (Files.exists(kt) == false) {                 throw new IllegalStateException("Could not locate keytab at " + kerberosKeytabLocation).             }             if (Files.isReadable(kt) != true) {                 throw new IllegalStateException("Could not read keytab at " + kerberosKeytabLocation).             }             logger.info("Keytab Length: " + Files.readAllBytes(kt).length).             // set principal names             hdfsConfiguration.set("dfs.namenode.kerberos.principal", hdfsKerberosPrincipal).             hdfsConfiguration.set("dfs.datanode.kerberos.principal", hdfsKerberosPrincipal).             hdfsConfiguration.set("dfs.data.transfer.protection", "authentication").             SecurityUtil.setAuthenticationMethod(UserGroupInformation.AuthenticationMethod.KERBEROS, hdfsConfiguration).             UserGroupInformation.setConfiguration(hdfsConfiguration).             UserGroupInformation.loginUserFromKeytab(hdfsKerberosPrincipal, kerberosKeytabLocation).         } else {             SecurityUtil.setAuthenticationMethod(UserGroupInformation.AuthenticationMethod.SIMPLE, hdfsConfiguration).             UserGroupInformation.setConfiguration(hdfsConfiguration).             UserGroupInformation.getCurrentUser().         }         return null.     }).     // Create repository     {         Request request = new Request("PUT", "/_snapshot/hdfs_ha_repo_read").         request.setJsonEntity("{" + "\"type\":\"hdfs\"," + "\"settings\":{" + "\"uri\": \"hdfs://ha-hdfs/\",\n" + "\"path\": \"/user/elasticsearch/existing/readonly-repository\"," + "\"readonly\": \"true\"," + securityCredentials(securityEnabled, esKerberosPrincipal) + "\"conf.dfs.nameservices\": \"ha-hdfs\"," + "\"conf.dfs.ha.namenodes.ha-hdfs\": \"nn1,nn2\"," + "\"conf.dfs.namenode.rpc-address.ha-hdfs.nn1\": \"localhost:10001\"," + "\"conf.dfs.namenode.rpc-address.ha-hdfs.nn2\": \"localhost:10002\"," + "\"conf.dfs.client.failover.proxy.provider.ha-hdfs\": " + "\"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"" + "}" + "}").         Response response = client.performRequest(request).         Assert.assertEquals(200, response.getStatusLine().getStatusCode()).     }     // Get repository     {         Response response = client.performRequest(new Request("GET", "/_snapshot/hdfs_ha_repo_read/_all")).         Assert.assertEquals(200, response.getStatusLine().getStatusCode()).     }     // Failover the namenode to the second.     failoverHDFS("nn1", "nn2", hdfsConfiguration).     // Get repository again     {         Response response = client.performRequest(new Request("GET", "/_snapshot/hdfs_ha_repo_read/_all")).         Assert.assertEquals(200, response.getStatusLine().getStatusCode()).     } }
false;private;2;8;;private String securityCredentials(boolean securityEnabled, String kerberosPrincipal) {     if (securityEnabled) {         return "\"security.principal\": \"" + kerberosPrincipal + "\"," + "\"conf.dfs.data.transfer.protection\": \"authentication\",".     } else {         return "".     } }
false;public;0;4;;@Override public InetSocketAddress getAddress() {     return delegate.getAddress(). }
false;public;0;4;;@Override public InetSocketAddress getHealthMonitorAddress() {     return delegate.getHealthMonitorAddress(). }
false;public;0;4;;@Override public InetSocketAddress getZKFCAddress() {     return delegate.getZKFCAddress(). }
false;public;0;4;;@Override public NodeFencer getFencer() {     return delegate.getFencer(). }
false;public;0;4;;@Override public void checkFencingConfigured() throws BadFencingConfigurationException {     delegate.checkFencingConfigured(). }
false;public;2;6;;@Override public HAServiceProtocol getProxy(Configuration conf, int timeoutMs) throws IOException {     HAServiceProtocol proxy = delegate.getProxy(conf, timeoutMs).     protocolsToClose.add(proxy).     return proxy. }
false;public;2;4;;@Override public HAServiceProtocol getHealthMonitorProxy(Configuration conf, int timeoutMs) throws IOException {     return delegate.getHealthMonitorProxy(conf, timeoutMs). }
false;public;2;4;;@Override public ZKFCProtocol getZKFCProxy(Configuration conf, int timeoutMs) throws IOException {     return delegate.getZKFCProxy(conf, timeoutMs). }
false;public;0;4;;@Override public boolean isAutoFailoverEnabled() {     return delegate.isAutoFailoverEnabled(). }
false;private;0;7;;private void close() {     for (HAServiceProtocol protocol : protocolsToClose) {         if (protocol instanceof HAServiceProtocolClientSideTranslatorPB) {             ((HAServiceProtocolClientSideTranslatorPB) protocol).close().         }     } }
false;protected;1;6;;@Override protected HAServiceTarget resolveTarget(String nnId) {     CloseableHAServiceTarget target = new CloseableHAServiceTarget(super.resolveTarget(nnId)).     serviceTargets.add(target).     return target. }
false;public;1;4;;@Override public int run(String[] argv) throws Exception {     return runCmd(argv). }
false;public;1;3;;public int transitionToStandby(String namenodeID) throws Exception {     return run(new String[] { "-transitionToStandby", namenodeID }). }
false;public;1;3;;public int transitionToActive(String namenodeID) throws Exception {     return run(new String[] { "-transitionToActive", namenodeID }). }
false;public;0;5;;public void close() {     for (CloseableHAServiceTarget serviceTarget : serviceTargets) {         serviceTarget.close().     } }
true;private;3;18;/**  * Performs a two-phase leading namenode transition.  * @param from Namenode ID to transition to standby  * @param to Namenode ID to transition to active  * @param configuration Client configuration for HAAdmin tool  * @throws IOException In the event of a raised exception during namenode failover.  */ ;/**  * Performs a two-phase leading namenode transition.  * @param from Namenode ID to transition to standby  * @param to Namenode ID to transition to active  * @param configuration Client configuration for HAAdmin tool  * @throws IOException In the event of a raised exception during namenode failover.  */ private void failoverHDFS(String from, String to, Configuration configuration) throws IOException {     logger.info("Swapping active namenodes: [{}] to standby and [{}] to active", from, to).     try {         AccessController.doPrivileged((PrivilegedExceptionAction<Void>) () -> {             CloseableHAAdmin haAdmin = new CloseableHAAdmin().             haAdmin.setConf(configuration).             try {                 haAdmin.transitionToStandby(from).                 haAdmin.transitionToActive(to).             } finally {                 haAdmin.close().             }             return null.         }).     } catch (PrivilegedActionException pae) {         throw new IOException("Unable to perform namenode failover", pae).     } }
