commented;modifiers;parameterAmount;loc;comment;code
false;private;3;49;;private HdfsBlobStore createBlobstore(URI uri, String path, Settings repositorySettings) {     Configuration hadoopConfiguration = new Configuration(repositorySettings.getAsBoolean("load_defaults", true)).     hadoopConfiguration.setClassLoader(HdfsRepository.class.getClassLoader()).     hadoopConfiguration.reloadConfiguration().     final Settings confSettings = repositorySettings.getByPrefix("conf.").     for (String key : confSettings.keySet()) {         logger.debug("Adding configuration to HDFS Client Configuration : {} = {}", key, confSettings.get(key)).         hadoopConfiguration.set(key, confSettings.get(key)).     }     // Disable FS cache     hadoopConfiguration.setBoolean("fs.hdfs.impl.disable.cache", true).     // Create a hadoop user     UserGroupInformation ugi = login(hadoopConfiguration, repositorySettings).     // Sense if HA is enabled     // HA requires elevated permissions during regular usage in the event that a failover operation     // occurs and a new connection is required.     String host = uri.getHost().     String configKey = HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + "." + host.     Class<?> ret = hadoopConfiguration.getClass(configKey, null, FailoverProxyProvider.class).     boolean haEnabled = ret != null.     int bufferSize = repositorySettings.getAsBytesSize("buffer_size", DEFAULT_BUFFER_SIZE).bytesAsInt().     // Create the filecontext with our user information     // This will correctly configure the filecontext to have our UGI as its internal user.     FileContext fileContext = ugi.doAs((PrivilegedAction<FileContext>) () -> {         try {             AbstractFileSystem fs = AbstractFileSystem.get(uri, hadoopConfiguration).             return FileContext.getFileContext(fs, hadoopConfiguration).         } catch (UnsupportedFileSystemException e) {             throw new UncheckedIOException(e).         }     }).     logger.debug("Using file-system [{}] for URI [{}], path [{}]", fileContext.getDefaultFileSystem(), fileContext.getDefaultFileSystem().getUri(), path).     try {         return new HdfsBlobStore(fileContext, path, bufferSize, isReadOnly(), haEnabled).     } catch (IOException e) {         throw new UncheckedIOException(String.format(Locale.ROOT, "Cannot create HDFS repository for uri [%s]", uri), e).     } }
false;private;2;42;;private UserGroupInformation login(Configuration hadoopConfiguration, Settings repositorySettings) {     // Validate the authentication method:     AuthenticationMethod authMethod = SecurityUtil.getAuthenticationMethod(hadoopConfiguration).     if (authMethod.equals(AuthenticationMethod.SIMPLE) == false && authMethod.equals(AuthenticationMethod.KERBEROS) == false) {         throw new RuntimeException("Unsupported authorization mode [" + authMethod + "]").     }     // Check if the user added a principal to use, and that there is a keytab file provided     String kerberosPrincipal = repositorySettings.get(CONF_SECURITY_PRINCIPAL).     // Check to see if the authentication method is compatible     if (kerberosPrincipal != null && authMethod.equals(AuthenticationMethod.SIMPLE)) {         logger.warn("Hadoop authentication method is set to [SIMPLE], but a Kerberos principal is " + "specified. Continuing with [KERBEROS] authentication.").         SecurityUtil.setAuthenticationMethod(AuthenticationMethod.KERBEROS, hadoopConfiguration).     } else if (kerberosPrincipal == null && authMethod.equals(AuthenticationMethod.KERBEROS)) {         throw new RuntimeException("HDFS Repository does not support [KERBEROS] authentication without " + "a valid Kerberos principal and keytab. Please specify a principal in the repository settings with [" + CONF_SECURITY_PRINCIPAL + "].").     }     // Now we can initialize the UGI with the configuration.     UserGroupInformation.setConfiguration(hadoopConfiguration).     // Debugging     logger.debug("Hadoop security enabled: [{}]", UserGroupInformation.isSecurityEnabled()).     logger.debug("Using Hadoop authentication method: [{}]", SecurityUtil.getAuthenticationMethod(hadoopConfiguration)).     // UserGroupInformation (UGI) instance is just a Hadoop specific wrapper around a Java Subject     try {         if (UserGroupInformation.isSecurityEnabled()) {             String principal = preparePrincipal(kerberosPrincipal).             String keytab = HdfsSecurityContext.locateKeytabFile(environment).toString().             logger.debug("Using kerberos principal [{}] and keytab located at [{}]", principal, keytab).             return UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytab).         }         return UserGroupInformation.getCurrentUser().     } catch (IOException e) {         throw new UncheckedIOException("Could not retrieve the current user information", e).     } }
true;private,static;1;17;// Convert principals of the format 'service/_HOST@REALM' by subbing in the local address for '_HOST'. ;// Convert principals of the format 'service/_HOST@REALM' by subbing in the local address for '_HOST'. private static String preparePrincipal(String originalPrincipal) {     String finalPrincipal = originalPrincipal.     // Don't worry about host name resolution if they don't have the _HOST pattern in the name.     if (originalPrincipal.contains("_HOST")) {         try {             finalPrincipal = SecurityUtil.getServerPrincipal(originalPrincipal, getHostName()).         } catch (IOException e) {             throw new UncheckedIOException(e).         }         if (originalPrincipal.equals(finalPrincipal) == false) {             logger.debug("Found service principal. Converted original principal name [{}] to server principal [{}]", originalPrincipal, finalPrincipal).         }     }     return finalPrincipal. }
false;private,static;0;13;;@SuppressForbidden(reason = "InetAddress.getLocalHost(). Needed for filling in hostname for a kerberos principal name pattern.") private static String getHostName() {     try {         /*              * This should not block since it should already be resolved via Log4J and Netty. The              * host information is cached by the JVM and the TTL for the cache entry is infinite              * when the SecurityManager is activated.              */         return InetAddress.getLocalHost().getCanonicalHostName().     } catch (UnknownHostException e) {         throw new RuntimeException("Could not locate host information", e).     } }
false;protected;0;9;;@Override protected HdfsBlobStore createBlobStore() {     // initialize our blobstore using elevated privileges.     SpecialPermission.check().     final HdfsBlobStore blobStore = AccessController.doPrivileged((PrivilegedAction<HdfsBlobStore>) () -> createBlobstore(uri, pathSetting, getMetadata().settings())).     return blobStore. }
false;protected;0;4;;@Override protected BlobPath basePath() {     return basePath. }
false;protected;0;4;;@Override protected ByteSizeValue chunkSize() {     return chunkSize. }
