# id;timestamp;commentText;codeText;commentWords;codeWords
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1480685315;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			this.wrappedProducerBase.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,wrapped,producer,base,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1495175928;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			this.wrappedProducerBase.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,wrapped,producer,base,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1495175928;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			this.wrappedProducerBase.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,wrapped,producer,base,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1495923077;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			this.wrappedProducerBase.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,wrapped,producer,base,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1499314317;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			this.wrappedProducerBase.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,wrapped,producer,base,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1505994399;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			producer.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1509636341;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			producer.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1509723634;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			producer.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1515212142;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			producer.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1515212342;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			producer.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1515212364;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			producer.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1515212365;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			producer.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setFlushOnCheckpoint(boolean flush);1515757409;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_			producer.setFlushOnCheckpoint(flush)__		};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1495175928;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)_@deprecated Use {@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1495175928;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1495923077;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1499314317;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1505994399;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1509636341;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1509723634;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515212142;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515212342;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515212364;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515212365;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515757409;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1480685315;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			this.wrappedProducerBase.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,wrapped,producer,base,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1495175928;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			this.wrappedProducerBase.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,wrapped,producer,base,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1495175928;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			this.wrappedProducerBase.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,wrapped,producer,base,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1495923077;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			this.wrappedProducerBase.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,wrapped,producer,base,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1499314317;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			this.wrappedProducerBase.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,wrapped,producer,base,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1505994399;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			producer.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1509636341;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			producer.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1509723634;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			producer.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1515212142;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			producer.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1515212342;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			producer.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1515212364;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			producer.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1515212365;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			producer.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setLogFailuresOnly(boolean logFailuresOnly);1515757409;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_			producer.setLogFailuresOnly(logFailuresOnly)__		};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._@deprecated Use {@link FlinkKafkaProducer010Configuration#writeToKafkaWithTimestamps(DataStream, String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,use,link,flink,kafka,producer010configuration,write,to,kafka,with,timestamps,data,stream,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,generic,type,info,object,object,type,info,new,generic,type,info,object,class,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,single,output,stream,operator,object,transformation,in,stream,transform,flin,kafka,producer,0,10,x,object,type,info,kafka,producer,return,new,flink,kafka,producer010configuration,transformation,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,generic,type,info,object,object,type,info,new,generic,type,info,object,class,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,single,output,stream,operator,object,transformation,in,stream,transform,flin,kafka,producer,0,10,x,object,type,info,kafka,producer,return,new,flink,kafka,producer010configuration,transformation,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1495923077;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,generic,type,info,object,object,type,info,new,generic,type,info,object,class,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,single,output,stream,operator,object,transformation,in,stream,transform,flin,kafka,producer,0,10,x,object,type,info,kafka,producer,return,new,flink,kafka,producer010configuration,transformation,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1499314317;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,generic,type,info,object,object,type,info,new,generic,type,info,object,class,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,single,output,stream,operator,object,transformation,in,stream,transform,flin,kafka,producer,0,10,x,object,type,info,kafka,producer,return,new,flink,kafka,producer010configuration,transformation,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1505994399;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<T>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,t,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1509636341;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<T>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,t,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1509723634;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<T>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,t,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1515212142;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<T>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,t,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1515212342;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<T>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,t,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1515212364;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<T>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,t,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1515212365;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<T>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,t,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					KafkaPartitioner<T> customPartitioner);1515757409;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated This is a deprecated since it does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					KafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer =_				new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner))__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<T>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,this,is,a,deprecated,since,it,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,t,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1480685315;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1495175928;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1495175928;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1495923077;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1499314317;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1505994399;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1509636341;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1509723634;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1515212142;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1515212342;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1515212364;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1515212365;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig);1515757409;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema._@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1480685315;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1495923077;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1499314317;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1505994399;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1509636341;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1509723634;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1515212142;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1515212342;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1515212364;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1515212365;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig);1515757409;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages_@param producerConfig_Properties with the producer configuration.;public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig) {_		this(topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,this,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1480685315;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1495923077;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1499314317;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1480685315;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1495175928;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1495175928;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1495923077;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1499314317;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,generic,type,info,object,object,type,info,new,generic,type,info,object,class,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,single,output,stream,operator,object,transformation,in,stream,transform,flin,kafka,producer,0,10,x,object,type,info,kafka,producer,return,new,flink,kafka,producer010configuration,transformation,kafka,producer
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,generic,type,info,object,object,type,info,new,generic,type,info,object,class,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,single,output,stream,operator,object,transformation,in,stream,transform,flin,kafka,producer,0,10,x,object,type,info,kafka,producer,return,new,flink,kafka,producer010configuration,transformation,kafka,producer
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1495923077;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,generic,type,info,object,object,type,info,new,generic,type,info,object,class,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,single,output,stream,operator,object,transformation,in,stream,transform,flin,kafka,producer,0,10,x,object,type,info,kafka,producer,return,new,flink,kafka,producer010configuration,transformation,kafka,producer
FlinkKafkaProducer010 -> public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1499314317;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.;public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {__		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class)__		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions;public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,generic,type,info,object,object,type,info,new,generic,type,info,object,class,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,single,output,stream,operator,object,transformation,in,stream,transform,flin,kafka,producer,0,10,x,object,type,info,kafka,producer,return,new,flink,kafka,producer010configuration,transformation,kafka,producer
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			SerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515212142;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			SerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			SerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515212342;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			SerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			SerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515212364;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			SerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			SerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515212365;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			SerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			SerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515757409;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a key-less {@link SerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>Since a key-less {@link SerializationSchema} is used, all records sent to Kafka will not have an_attached key. Therefore, if a partitioner is also not provided, records will be distributed to Kafka_partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A key-less serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be distributed to Kafka partitions_in a round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			SerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,key,less,link,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,since,a,key,less,link,serialization,schema,is,used,all,records,sent,to,kafka,will,not,have,an,attached,key,therefore,if,a,partitioner,is,also,not,provided,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,key,less,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			KeyedSerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515212142;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			KeyedSerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			KeyedSerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515212342;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			KeyedSerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			KeyedSerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515212364;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			KeyedSerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			KeyedSerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515212365;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			KeyedSerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010( 			String topicId, 			KeyedSerializationSchema<T> serializationSchema, 			Properties producerConfig, 			@Nullable FlinkKafkaPartitioner<T> customPartitioner);1515757409;Creates a FlinkKafkaProducer for a given topic. The sink produces its input to_the topic. It accepts a keyed {@link KeyedSerializationSchema} and possibly a custom {@link FlinkKafkaPartitioner}.__<p>If a partitioner is not provided, written records will be partitioned by the attached key of each_record (as determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If written records do not_have a key (i.e., {@link KeyedSerializationSchema#serializeKey(Object)} returns {@code null}), they_will be distributed to Kafka partitions in a round-robin fashion.__@param topicId The topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions._If set to {@code null}, records will be partitioned by the key of each record_(determined by {@link KeyedSerializationSchema#serializeKey(Object)}). If the keys_are {@code null}, then records will be distributed to Kafka partitions in a_round-robin fashion.;public FlinkKafkaProducer010(_			String topicId,_			KeyedSerializationSchema<T> serializationSchema,_			Properties producerConfig,_			@Nullable FlinkKafkaPartitioner<T> customPartitioner) {__		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,its,input,to,the,topic,it,accepts,a,keyed,link,keyed,serialization,schema,and,possibly,a,custom,link,flink,kafka,partitioner,p,if,a,partitioner,is,not,provided,written,records,will,be,partitioned,by,the,attached,key,of,each,record,as,determined,by,link,keyed,serialization,schema,serialize,key,object,if,written,records,do,not,have,a,key,i,e,link,keyed,serialization,schema,serialize,key,object,returns,code,null,they,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,if,set,to,code,null,records,will,be,partitioned,by,the,key,of,each,record,determined,by,link,keyed,serialization,schema,serialize,key,object,if,the,keys,are,code,null,then,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,nullable,flink,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Override 	public void processElement(StreamRecord<T> element) throws Exception;1480685315;Process method for using the sink with timestamp support.__This method is used for approach (b) (see above);@Override_	public void processElement(StreamRecord<T> element) throws Exception {_		invokeInternal(element.getValue(), element.getTimestamp())__	};process,method,for,using,the,sink,with,timestamp,support,this,method,is,used,for,approach,b,see,above;override,public,void,process,element,stream,record,t,element,throws,exception,invoke,internal,element,get,value,element,get,timestamp
FlinkKafkaProducer010 -> @Override 	public void processElement(StreamRecord<T> element) throws Exception;1495175928;Process method for using the sink with timestamp support.__This method is used for approach (b) (see above);@Override_	public void processElement(StreamRecord<T> element) throws Exception {_		invokeInternal(element.getValue(), element.getTimestamp())__	};process,method,for,using,the,sink,with,timestamp,support,this,method,is,used,for,approach,b,see,above;override,public,void,process,element,stream,record,t,element,throws,exception,invoke,internal,element,get,value,element,get,timestamp
FlinkKafkaProducer010 -> @Override 	public void processElement(StreamRecord<T> element) throws Exception;1495175928;Process method for using the sink with timestamp support.__This method is used for approach (b) (see above);@Override_	public void processElement(StreamRecord<T> element) throws Exception {_		invokeInternal(element.getValue(), element.getTimestamp())__	};process,method,for,using,the,sink,with,timestamp,support,this,method,is,used,for,approach,b,see,above;override,public,void,process,element,stream,record,t,element,throws,exception,invoke,internal,element,get,value,element,get,timestamp
FlinkKafkaProducer010 -> @Override 	public void processElement(StreamRecord<T> element) throws Exception;1495923077;Process method for using the sink with timestamp support.__<p>This method is used for approach (b) (see above);@Override_	public void processElement(StreamRecord<T> element) throws Exception {_		invokeInternal(element.getValue(), element.getTimestamp())__	};process,method,for,using,the,sink,with,timestamp,support,p,this,method,is,used,for,approach,b,see,above;override,public,void,process,element,stream,record,t,element,throws,exception,invoke,internal,element,get,value,element,get,timestamp
FlinkKafkaProducer010 -> @Override 	public void processElement(StreamRecord<T> element) throws Exception;1499314317;Process method for using the sink with timestamp support.__<p>This method is used for approach (b) (see above);@Override_	public void processElement(StreamRecord<T> element) throws Exception {_		invokeInternal(element.getValue(), element.getTimestamp())__	};process,method,for,using,the,sink,with,timestamp,support,p,this,method,is,used,for,approach,b,see,above;override,public,void,process,element,stream,record,t,element,throws,exception,invoke,internal,element,get,value,element,get,timestamp
FlinkKafkaProducer010 -> @Override 	public IterationRuntimeContext getIterationRuntimeContext();1480685315;This method is used for approach (a) (see above);@Override_	public IterationRuntimeContext getIterationRuntimeContext() {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		return internalProducer.getIterationRuntimeContext()__	};this,method,is,used,for,approach,a,see,above;override,public,iteration,runtime,context,get,iteration,runtime,context,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,return,internal,producer,get,iteration,runtime,context
FlinkKafkaProducer010 -> @Override 	public IterationRuntimeContext getIterationRuntimeContext();1495175928;This method is used for approach (a) (see above);@Override_	public IterationRuntimeContext getIterationRuntimeContext() {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		return internalProducer.getIterationRuntimeContext()__	};this,method,is,used,for,approach,a,see,above;override,public,iteration,runtime,context,get,iteration,runtime,context,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,return,internal,producer,get,iteration,runtime,context
FlinkKafkaProducer010 -> @Override 	public IterationRuntimeContext getIterationRuntimeContext();1495175928;This method is used for approach (a) (see above);@Override_	public IterationRuntimeContext getIterationRuntimeContext() {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		return internalProducer.getIterationRuntimeContext()__	};this,method,is,used,for,approach,a,see,above;override,public,iteration,runtime,context,get,iteration,runtime,context,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,return,internal,producer,get,iteration,runtime,context
FlinkKafkaProducer010 -> @Override 	public IterationRuntimeContext getIterationRuntimeContext();1495923077;This method is used for approach (a) (see above).;@Override_	public IterationRuntimeContext getIterationRuntimeContext() {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		return internalProducer.getIterationRuntimeContext()__	};this,method,is,used,for,approach,a,see,above;override,public,iteration,runtime,context,get,iteration,runtime,context,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,return,internal,producer,get,iteration,runtime,context
FlinkKafkaProducer010 -> @Override 	public IterationRuntimeContext getIterationRuntimeContext();1499314317;This method is used for approach (a) (see above).;@Override_	public IterationRuntimeContext getIterationRuntimeContext() {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		return internalProducer.getIterationRuntimeContext()__	};this,method,is,used,for,approach,a,see,above;override,public,iteration,runtime,context,get,iteration,runtime,context,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,return,internal,producer,get,iteration,runtime,context
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1505994399;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1509636341;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1509723634;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515212142;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515212342;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515212364;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515212365;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515757409;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined serialization schema supporting key/value messages_@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Override 	public void invoke(T value) throws Exception;1480685315;Invoke method for using the Sink as DataStream.addSink() sink.__This method is used for approach (a) (see above)__@param value The input record.;@Override_	public void invoke(T value) throws Exception {_		invokeInternal(value, Long.MAX_VALUE)__	};invoke,method,for,using,the,sink,as,data,stream,add,sink,sink,this,method,is,used,for,approach,a,see,above,param,value,the,input,record;override,public,void,invoke,t,value,throws,exception,invoke,internal,value,long
FlinkKafkaProducer010 -> @Override 	public void invoke(T value) throws Exception;1495175928;Invoke method for using the Sink as DataStream.addSink() sink.__This method is used for approach (a) (see above)__@param value The input record.;@Override_	public void invoke(T value) throws Exception {_		invokeInternal(value, Long.MAX_VALUE)__	};invoke,method,for,using,the,sink,as,data,stream,add,sink,sink,this,method,is,used,for,approach,a,see,above,param,value,the,input,record;override,public,void,invoke,t,value,throws,exception,invoke,internal,value,long
FlinkKafkaProducer010 -> @Override 	public void invoke(T value) throws Exception;1495175928;Invoke method for using the Sink as DataStream.addSink() sink.__This method is used for approach (a) (see above)__@param value The input record.;@Override_	public void invoke(T value) throws Exception {_		invokeInternal(value, Long.MAX_VALUE)__	};invoke,method,for,using,the,sink,as,data,stream,add,sink,sink,this,method,is,used,for,approach,a,see,above,param,value,the,input,record;override,public,void,invoke,t,value,throws,exception,invoke,internal,value,long
FlinkKafkaProducer010 -> @Override 	public void invoke(T value) throws Exception;1495923077;Invoke method for using the Sink as DataStream.addSink() sink.__<p>This method is used for approach (a) (see above)__@param value The input record.;@Override_	public void invoke(T value) throws Exception {_		invokeInternal(value, Long.MAX_VALUE)__	};invoke,method,for,using,the,sink,as,data,stream,add,sink,sink,p,this,method,is,used,for,approach,a,see,above,param,value,the,input,record;override,public,void,invoke,t,value,throws,exception,invoke,internal,value,long
FlinkKafkaProducer010 -> @Override 	public void invoke(T value) throws Exception;1499314317;Invoke method for using the Sink as DataStream.addSink() sink.__<p>This method is used for approach (a) (see above)__@param value The input record.;@Override_	public void invoke(T value) throws Exception {_		invokeInternal(value, Long.MAX_VALUE)__	};invoke,method,for,using,the,sink,as,data,stream,add,sink,sink,p,this,method,is,used,for,approach,a,see,above,param,value,the,input,record;override,public,void,invoke,t,value,throws,exception,invoke,internal,value,long
FlinkKafkaProducer010 -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1505994399;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1509636341;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1509723634;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515212142;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515212342;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515212364;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515212365;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515757409;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_		this.writeTimestampToKafka = writeTimestampToKafka__	};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> @Override 	public void setRuntimeContext(RuntimeContext t);1480685315;This method is used for approach (a) (see above);@Override_	public void setRuntimeContext(RuntimeContext t) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setRuntimeContext(t)__	};this,method,is,used,for,approach,a,see,above;override,public,void,set,runtime,context,runtime,context,t,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,runtime,context,t
FlinkKafkaProducer010 -> @Override 	public void setRuntimeContext(RuntimeContext t);1495175928;This method is used for approach (a) (see above);@Override_	public void setRuntimeContext(RuntimeContext t) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setRuntimeContext(t)__	};this,method,is,used,for,approach,a,see,above;override,public,void,set,runtime,context,runtime,context,t,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,runtime,context,t
FlinkKafkaProducer010 -> @Override 	public void setRuntimeContext(RuntimeContext t);1495175928;This method is used for approach (a) (see above);@Override_	public void setRuntimeContext(RuntimeContext t) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setRuntimeContext(t)__	};this,method,is,used,for,approach,a,see,above;override,public,void,set,runtime,context,runtime,context,t,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,runtime,context,t
FlinkKafkaProducer010 -> @Override 	public void setRuntimeContext(RuntimeContext t);1495923077;This method is used for approach (a) (see above).;@Override_	public void setRuntimeContext(RuntimeContext t) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setRuntimeContext(t)__	};this,method,is,used,for,approach,a,see,above;override,public,void,set,runtime,context,runtime,context,t,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,runtime,context,t
FlinkKafkaProducer010 -> @Override 	public void setRuntimeContext(RuntimeContext t);1499314317;This method is used for approach (a) (see above).;@Override_	public void setRuntimeContext(RuntimeContext t) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setRuntimeContext(t)__	};this,method,is,used,for,approach,a,see,above;override,public,void,set,runtime,context,runtime,context,t,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,runtime,context,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1480685315;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1495923077;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1499314317;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1505994399;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1509636341;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1509723634;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1515212142;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1515212342;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1515212364;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1515212365;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema);1515757409;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined serialization schema supporting key/value messages;public FlinkKafkaProducer010(String brokerList, String topicId, KeyedSerializationSchema<T> serializationSchema) {_		this(topicId, serializationSchema, getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,serialization,schema,supporting,key,value,messages;public,flink,kafka,producer010,string,broker,list,string,topic,id,keyed,serialization,schema,t,serialization,schema,this,topic,id,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public void setFlushOnCheckpoint(boolean flush);1480685315;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__Method is only accessible for approach (a) (see above)__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setFlushOnCheckpoint(flush)__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,method,is,only,accessible,for,approach,a,see,above,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> public void setFlushOnCheckpoint(boolean flush);1495175928;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__Method is only accessible for approach (a) (see above)__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setFlushOnCheckpoint(flush)__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,method,is,only,accessible,for,approach,a,see,above,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> public void setFlushOnCheckpoint(boolean flush);1495175928;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__Method is only accessible for approach (a) (see above)__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setFlushOnCheckpoint(flush)__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,method,is,only,accessible,for,approach,a,see,above,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> public void setFlushOnCheckpoint(boolean flush);1495923077;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__<p>Method is only accessible for approach (a) (see above)__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setFlushOnCheckpoint(flush)__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,p,method,is,only,accessible,for,approach,a,see,above,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> public void setFlushOnCheckpoint(boolean flush);1499314317;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__<p>Method is only accessible for approach (a) (see above)__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setFlushOnCheckpoint(flush)__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,p,method,is,only,accessible,for,approach,a,see,above,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,flush,on,checkpoint,flush
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1495175928;Create Kafka producer__This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above);public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		_		_		super(new FlinkKafkaProducer09<>(topicId, serializationSchema, producerConfig, customPartitioner))__	};create,kafka,producer,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,super,new,flink,kafka,producer09,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1495175928;Create Kafka producer__This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above);public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		_		_		super(new FlinkKafkaProducer09<>(topicId, serializationSchema, producerConfig, customPartitioner))__	};create,kafka,producer,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,super,new,flink,kafka,producer09,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1495923077;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above);public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		_		_		super(new FlinkKafkaProducer09<>(topicId, serializationSchema, producerConfig, customPartitioner))__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,super,new,flink,kafka,producer09,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1499314317;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above);public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		_		_		super(new FlinkKafkaProducer09<>(topicId, serializationSchema, producerConfig, customPartitioner))__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,super,new,flink,kafka,producer09,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1505994399;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above);public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1509636341;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above);public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1509723634;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above);public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above;public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Override 	public void open(Configuration parameters) throws Exception;1480685315;This method is used for approach (a) (see above);@Override_	public void open(Configuration parameters) throws Exception {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.open(parameters)__	};this,method,is,used,for,approach,a,see,above;override,public,void,open,configuration,parameters,throws,exception,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,open,parameters
FlinkKafkaProducer010 -> @Override 	public void open(Configuration parameters) throws Exception;1495175928;This method is used for approach (a) (see above);@Override_	public void open(Configuration parameters) throws Exception {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.open(parameters)__	};this,method,is,used,for,approach,a,see,above;override,public,void,open,configuration,parameters,throws,exception,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,open,parameters
FlinkKafkaProducer010 -> @Override 	public void open(Configuration parameters) throws Exception;1495175928;This method is used for approach (a) (see above);@Override_	public void open(Configuration parameters) throws Exception {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.open(parameters)__	};this,method,is,used,for,approach,a,see,above;override,public,void,open,configuration,parameters,throws,exception,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,open,parameters
FlinkKafkaProducer010 -> @Override 	public void open(Configuration parameters) throws Exception;1495923077;This method is used for approach (a) (see above).;@Override_	public void open(Configuration parameters) throws Exception {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.open(parameters)__	};this,method,is,used,for,approach,a,see,above;override,public,void,open,configuration,parameters,throws,exception,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,open,parameters
FlinkKafkaProducer010 -> @Override 	public void open(Configuration parameters) throws Exception;1499314317;This method is used for approach (a) (see above).;@Override_	public void open(Configuration parameters) throws Exception {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.open(parameters)__	};this,method,is,used,for,approach,a,see,above;override,public,void,open,configuration,parameters,throws,exception,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,open,parameters
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1495175928;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner);public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1495175928;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner);public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1495923077;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner);public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1499314317;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner);public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1505994399;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner);public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1509636341;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner);public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner);1509723634;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__@param topicId The topic to write data to_@param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner);public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<T> customPartitioner) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,topic,id,the,topic,to,write,data,to,param,serialization,schema,a,keyless,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,when,passing,null,we,ll,use,kafka,s,partitioner;public,flink,kafka,producer010,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1480685315;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			this.producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1495175928;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			this.producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1495175928;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			this.producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1495923077;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			this.producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1499314317;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			this.producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,this,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1505994399;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1509636341;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1509723634;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515212142;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515212342;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515212364;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515212365;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> FlinkKafkaProducer010Configuration -> public void setWriteTimestampToKafka(boolean writeTimestampToKafka);1515757409;If set to true, Flink will write the (event time) timestamp attached to each record into Kafka._Timestamps must be positive for Kafka to accept them.__@param writeTimestampToKafka Flag indicating if Flink's internal timestamps are written to Kafka.;public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {_			producer.writeTimestampToKafka = writeTimestampToKafka__		};if,set,to,true,flink,will,write,the,event,time,timestamp,attached,to,each,record,into,kafka,timestamps,must,be,positive,for,kafka,to,accept,them,param,write,timestamp,to,kafka,flag,indicating,if,flink,s,internal,timestamps,are,written,to,kafka;public,void,set,write,timestamp,to,kafka,boolean,write,timestamp,to,kafka,producer,write,timestamp,to,kafka,write,timestamp,to,kafka
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1495175928;Create Kafka producer__This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)_@deprecated Use {@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)} instead;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(new FlinkKafkaProducer09<>(topicId, serializationSchema, producerConfig, customPartitioner))__	};create,kafka,producer,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,use,link,flink,kafka,producer010,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,new,flink,kafka,producer09,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1495175928;Create Kafka producer__This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(new FlinkKafkaProducer09<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner)))__	};create,kafka,producer,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,new,flink,kafka,producer09,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1495923077;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(new FlinkKafkaProducer09<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner)))__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,new,flink,kafka,producer09,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1499314317;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(new FlinkKafkaProducer09<>(topicId, serializationSchema, producerConfig, new FlinkKafkaDelegatePartitioner<>(customPartitioner)))__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,new,flink,kafka,producer09,topic,id,serialization,schema,producer,config,new,flink,kafka,delegate,partitioner,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1505994399;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1509636341;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1509723634;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515212142;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515212342;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515212364;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515212365;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner);1515757409;Create Kafka producer.__<p>This constructor does not allow writing timestamps to Kafka, it follow approach (a) (see above)__@deprecated This is a deprecated constructor that does not correctly handle partitioning when_producing to multiple topics. Use_{@link FlinkKafkaProducer010#FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.;@Deprecated_	public FlinkKafkaProducer010(String topicId, KeyedSerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {_		_		_		super(topicId, serializationSchema, producerConfig, customPartitioner)__	};create,kafka,producer,p,this,constructor,does,not,allow,writing,timestamps,to,kafka,it,follow,approach,a,see,above,deprecated,this,is,a,deprecated,constructor,that,does,not,correctly,handle,partitioning,when,producing,to,multiple,topics,use,link,flink,kafka,producer010,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead;deprecated,public,flink,kafka,producer010,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,kafka,partitioner,t,custom,partitioner,super,topic,id,serialization,schema,producer,config,custom,partitioner
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1505994399;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(streamSink, inStream, kafkaProducer)___	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1509636341;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(streamSink, inStream, kafkaProducer)___	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1509723634;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(streamSink, inStream, kafkaProducer)___	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1515212142;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {__		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(streamSink, inStream, kafkaProducer)___	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1515212342;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {_		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1515212364;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {_		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1515212365;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {_		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					KeyedSerializationSchema<T> serializationSchema, 																					Properties producerConfig, 																					FlinkKafkaPartitioner<T> customPartitioner);1515757409;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId The name of the target topic_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties, FlinkKafkaPartitioner)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					KeyedSerializationSchema<T> serializationSchema,_																					Properties producerConfig,_																					FlinkKafkaPartitioner<T> customPartitioner) {_		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner)__		DataStreamSink<T> streamSink = inStream.addSink(kafkaProducer)__		return new FlinkKafkaProducer010Configuration<>(streamSink, inStream, kafkaProducer)__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,the,name,of,the,target,topic,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,flink,kafka,partitioner,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,keyed,serialization,schema,t,serialization,schema,properties,producer,config,flink,kafka,partitioner,t,custom,partitioner,flink,kafka,producer010,t,kafka,producer,new,flink,kafka,producer010,topic,id,serialization,schema,producer,config,custom,partitioner,data,stream,sink,t,stream,sink,in,stream,add,sink,kafka,producer,return,new,flink,kafka,producer010configuration,stream,sink,in,stream,kafka,producer
FlinkKafkaProducer010 -> public void setLogFailuresOnly(boolean logFailuresOnly);1480685315;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__Method is only accessible for approach (a) (see above)__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setLogFailuresOnly(logFailuresOnly)__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,method,is,only,accessible,for,approach,a,see,above,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> public void setLogFailuresOnly(boolean logFailuresOnly);1495175928;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__Method is only accessible for approach (a) (see above)__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setLogFailuresOnly(logFailuresOnly)__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,method,is,only,accessible,for,approach,a,see,above,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> public void setLogFailuresOnly(boolean logFailuresOnly);1495175928;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__Method is only accessible for approach (a) (see above)__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setLogFailuresOnly(logFailuresOnly)__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,method,is,only,accessible,for,approach,a,see,above,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> public void setLogFailuresOnly(boolean logFailuresOnly);1495923077;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__<p>Method is only accessible for approach (a) (see above)__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setLogFailuresOnly(logFailuresOnly)__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,p,method,is,only,accessible,for,approach,a,see,above,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> public void setLogFailuresOnly(boolean logFailuresOnly);1499314317;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__<p>Method is only accessible for approach (a) (see above)__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		final FlinkKafkaProducerBase<T> internalProducer = (FlinkKafkaProducerBase<T>) userFunction__		internalProducer.setLogFailuresOnly(logFailuresOnly)__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,p,method,is,only,accessible,for,approach,a,see,above,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,final,flink,kafka,producer,base,t,internal,producer,flink,kafka,producer,base,t,user,function,internal,producer,set,log,failures,only,log,failures,only
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1480685315;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1495175928;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1495923077;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1499314317;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1505994399;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1509636341;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1509723634;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined (keyless) serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1515212142;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1515212342;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1515212364;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1515212365;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema);1515757409;Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to_the topic.__<p>Using this constructor, the default {@link FlinkFixedPartitioner} will be used as_the partitioner. This default partitioner maps each sink subtask to a single Kafka_partition (i.e. all records received by a sink subtask will end up in the same_Kafka partition).__<p>To use a custom partitioner, please use_{@link #FlinkKafkaProducer010(String, SerializationSchema, Properties, FlinkKafkaPartitioner)} instead.__@param brokerList_Comma separated addresses of the brokers_@param topicId_ID of the Kafka topic._@param serializationSchema_User defined key-less serialization schema.;public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {_		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,using,this,constructor,the,default,link,flink,fixed,partitioner,will,be,used,as,the,partitioner,this,default,partitioner,maps,each,sink,subtask,to,a,single,kafka,partition,i,e,all,records,received,by,a,sink,subtask,will,end,up,in,the,same,kafka,partition,p,to,use,a,custom,partitioner,please,use,link,flink,kafka,producer010,string,serialization,schema,properties,flink,kafka,partitioner,instead,param,broker,list,comma,separated,addresses,of,the,brokers,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,key,less,serialization,schema;public,flink,kafka,producer010,string,broker,list,string,topic,id,serialization,schema,t,serialization,schema,this,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,get,properties,from,broker,list,broker,list,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1505994399;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1509636341;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1509723634;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515212142;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515212342;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515212364;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515212365;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
FlinkKafkaProducer010 -> @Deprecated 	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream, 																					String topicId, 																					SerializationSchema<T> serializationSchema, 																					Properties producerConfig);1515757409;Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to_the topic.__<p>This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)__@param inStream The stream to write to Kafka_@param topicId ID of the Kafka topic._@param serializationSchema User defined (keyless) serialization schema._@param producerConfig Properties with the producer configuration.__@deprecated Use {@link #FlinkKafkaProducer010(String, KeyedSerializationSchema, Properties)}_and call {@link #setWriteTimestampToKafka(boolean)}.;@Deprecated_	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,_																					String topicId,_																					SerializationSchema<T> serializationSchema,_																					Properties producerConfig) {_		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FlinkFixedPartitioner<T>())__	};creates,a,flink,kafka,producer,for,a,given,topic,the,sink,produces,a,data,stream,to,the,topic,p,this,constructor,allows,writing,timestamps,to,kafka,it,follow,approach,b,see,above,param,in,stream,the,stream,to,write,to,kafka,param,topic,id,id,of,the,kafka,topic,param,serialization,schema,user,defined,keyless,serialization,schema,param,producer,config,properties,with,the,producer,configuration,deprecated,use,link,flink,kafka,producer010,string,keyed,serialization,schema,properties,and,call,link,set,write,timestamp,to,kafka,boolean;deprecated,public,static,t,flink,kafka,producer010configuration,t,write,to,kafka,with,timestamps,data,stream,t,in,stream,string,topic,id,serialization,schema,t,serialization,schema,properties,producer,config,return,write,to,kafka,with,timestamps,in,stream,topic,id,new,keyed,serialization,schema,wrapper,serialization,schema,producer,config,new,flink,fixed,partitioner,t
