# id;timestamp;commentText;codeText;commentWords;codeWords
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1489510697;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1489764760;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1491500150;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1493821466;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1494830990;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1495923077;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1498894422;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1501249949;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1501249949;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1501249950;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1503598628;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1509013576;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1512405587;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1515757408;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1515757408;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1515757408;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1515757409;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1517943538;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1519658056;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1519973085;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1519973085;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1520598638;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1525116534;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1542613709;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1548931626;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1550834388;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1550834396;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints);1550834396;Specifies whether or not the consumer should commit offsets back to Kafka on checkpoints.__<p>This setting will only have effect if checkpointing is enabled for the job._If checkpointing isn't enabled, only the "auto.commit.enable" (for 0.8) / "enable.auto.commit" (for 0.9+)_property settings will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setCommitOffsetsOnCheckpoints(boolean commitOnCheckpoints) {_		this.enableCommitOnCheckpoints = commitOnCheckpoints__		return this__	};specifies,whether,or,not,the,consumer,should,commit,offsets,back,to,kafka,on,checkpoints,p,this,setting,will,only,have,effect,if,checkpointing,is,enabled,for,the,job,if,checkpointing,isn,t,enabled,only,the,auto,commit,enable,for,0,8,enable,auto,commit,for,0,9,property,settings,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,commit,offsets,on,checkpoints,boolean,commit,on,checkpoints,this,enable,commit,on,checkpoints,commit,on,checkpoints,return,this
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			List<KafkaTopicPartition> thisSubtaskPartitions, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext) throws Exception_;1480685315;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param thisSubtaskPartitions The set of partitions that this subtask should handle._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			List<KafkaTopicPartition> thisSubtaskPartitions,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,this,subtask,partitions,the,set,of,partitions,that,this,subtask,should,handle,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,list,kafka,topic,partition,this,subtask,partitions,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			List<KafkaTopicPartition> thisSubtaskPartitions, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext) throws Exception_;1482244974;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param thisSubtaskPartitions The set of partitions that this subtask should handle._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			List<KafkaTopicPartition> thisSubtaskPartitions,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,this,subtask,partitions,the,set,of,partitions,that,this,subtask,should,handle,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,list,kafka,topic,partition,this,subtask,partitions,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			List<KafkaTopicPartition> thisSubtaskPartitions, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext) throws Exception_;1482244974;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param thisSubtaskPartitions The set of partitions that this subtask should handle._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			List<KafkaTopicPartition> thisSubtaskPartitions,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,this,subtask,partitions,the,set,of,partitions,that,this,subtask,should,handle,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,list,kafka,topic,partition,this,subtask,partitions,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1489510697;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1489764760;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1491500150;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1493821466;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1494830990;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1495923077;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1498894422;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1501249949;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1501249949;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1501249950;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1503598628;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1509013576;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1512405587;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1515757408;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1515757408;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1515757408;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode) throws Exception_;1515757409;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,throws,exception
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1498894422;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1501249949;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1501249949;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1501249950;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1503598628;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1509013576;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1512405587;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1515757408;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1515757408;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1515757408;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis);1515757409;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext) throws Exception_;1488214488;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext) throws Exception_;1489419493;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1517943538;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1519658056;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1519973085;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1519973085;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1520598638;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1525116534;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1542613709;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1548931626;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1550834388;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1550834396;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> protected abstract AbstractFetcher<T, ?> createFetcher( 			SourceContext<T> sourceContext, 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic, 			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated, 			StreamingRuntimeContext runtimeContext, 			OffsetCommitMode offsetCommitMode, 			MetricGroup kafkaMetricGroup, 			boolean useMetrics) throws Exception_;1550834396;Creates the fetcher that connect to the Kafka brokers, pulls data, deserialized the_data, and emits it into the data streams.__@param sourceContext The source context to emit data to._@param subscribedPartitionsToStartOffsets The set of partitions that this subtask should handle, with their start offsets._@param watermarksPeriodic Optional, a serialized timestamp extractor / periodic watermark generator._@param watermarksPunctuated Optional, a serialized timestamp extractor / punctuated watermark generator._@param runtimeContext The task's runtime context.__@return The instantiated fetcher__@throws Exception The method should forward exceptions;protected abstract AbstractFetcher<T, ?> createFetcher(_			SourceContext<T> sourceContext,_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,_			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,_			StreamingRuntimeContext runtimeContext,_			OffsetCommitMode offsetCommitMode,_			MetricGroup kafkaMetricGroup,_			boolean useMetrics) throws Exception_;creates,the,fetcher,that,connect,to,the,kafka,brokers,pulls,data,deserialized,the,data,and,emits,it,into,the,data,streams,param,source,context,the,source,context,to,emit,data,to,param,subscribed,partitions,to,start,offsets,the,set,of,partitions,that,this,subtask,should,handle,with,their,start,offsets,param,watermarks,periodic,optional,a,serialized,timestamp,extractor,periodic,watermark,generator,param,watermarks,punctuated,optional,a,serialized,timestamp,extractor,punctuated,watermark,generator,param,runtime,context,the,task,s,runtime,context,return,the,instantiated,fetcher,throws,exception,the,method,should,forward,exceptions;protected,abstract,abstract,fetcher,t,create,fetcher,source,context,t,source,context,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,serialized,value,assigner,with,periodic,watermarks,t,watermarks,periodic,serialized,value,assigner,with,punctuated,watermarks,t,watermarks,punctuated,streaming,runtime,context,runtime,context,offset,commit,mode,offset,commit,mode,metric,group,kafka,metric,group,boolean,use,metrics,throws,exception
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1487173364;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1488214488;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1489419493;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1489510697;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1489764760;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1491500150;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1493821466;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1494830990;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1495923077;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1498894422;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1501249949;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1501249949;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1501249950;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1503598628;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1509013576;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1512405587;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1515757408;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1515757408;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1515757408;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1515757409;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1517943538;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1519658056;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1519973085;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1519973085;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1520598638;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1525116534;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1542613709;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1548931626;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1550834388;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1550834396;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets();1550834396;Specifies the consumer to start reading from any committed group offsets found_in Zookeeper / Kafka brokers. The "group.id" property must be set in the configuration_properties. If no offset can be found for a partition, the behaviour in "auto.offset.reset"_set in the configuration properties will be used for the partition.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,the,group,id,property,must,be,set,in,the,configuration,properties,if,no,offset,can,be,found,for,a,partition,the,behaviour,in,auto,offset,reset,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,group,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected static void initializeSubscribedPartitionsToStartOffsets( 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			List<KafkaTopicPartition> kafkaTopicPartitions, 			int indexOfThisSubtask, 			int numParallelSubtasks, 			StartupMode startupMode, 			Map<KafkaTopicPartition, Long> specificStartupOffsets);1489419493;Initializes {@link FlinkKafkaConsumerBase#subscribedPartitionsToStartOffsets} with appropriate_values. The method decides which partitions this consumer instance should subscribe to, and also_sets the initial offset each subscribed partition should be started from based on the configured startup mode.__@param subscribedPartitionsToStartOffsets to subscribedPartitionsToStartOffsets to initialize_@param kafkaTopicPartitions the complete list of all Kafka partitions_@param indexOfThisSubtask the index of this consumer instance_@param numParallelSubtasks total number of parallel consumer instances_@param startupMode the configured startup mode for the consumer_@param specificStartupOffsets specific partition offsets to start from_(only relevant if startupMode is {@link StartupMode#SPECIFIC_OFFSETS})__Note: This method is also exposed for testing.;protected static void initializeSubscribedPartitionsToStartOffsets(_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			List<KafkaTopicPartition> kafkaTopicPartitions,_			int indexOfThisSubtask,_			int numParallelSubtasks,_			StartupMode startupMode,_			Map<KafkaTopicPartition, Long> specificStartupOffsets) {__		for (int i = 0_ i < kafkaTopicPartitions.size()_ i++) {_			if (i % numParallelSubtasks == indexOfThisSubtask) {_				if (startupMode != StartupMode.SPECIFIC_OFFSETS) {_					subscribedPartitionsToStartOffsets.put(kafkaTopicPartitions.get(i), startupMode.getStateSentinel())__				} else {_					if (specificStartupOffsets == null) {_						throw new IllegalArgumentException(_							"Startup mode for the consumer set to " + StartupMode.SPECIFIC_OFFSETS +_								", but no specific offsets were specified")__					}__					KafkaTopicPartition partition = kafkaTopicPartitions.get(i)___					Long specificOffset = specificStartupOffsets.get(partition)__					if (specificOffset != null) {_						_						_						subscribedPartitionsToStartOffsets.put(partition, specificOffset - 1)__					} else {_						subscribedPartitionsToStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__					}_				}_			}_		}_	};initializes,link,flink,kafka,consumer,base,subscribed,partitions,to,start,offsets,with,appropriate,values,the,method,decides,which,partitions,this,consumer,instance,should,subscribe,to,and,also,sets,the,initial,offset,each,subscribed,partition,should,be,started,from,based,on,the,configured,startup,mode,param,subscribed,partitions,to,start,offsets,to,subscribed,partitions,to,start,offsets,to,initialize,param,kafka,topic,partitions,the,complete,list,of,all,kafka,partitions,param,index,of,this,subtask,the,index,of,this,consumer,instance,param,num,parallel,subtasks,total,number,of,parallel,consumer,instances,param,startup,mode,the,configured,startup,mode,for,the,consumer,param,specific,startup,offsets,specific,partition,offsets,to,start,from,only,relevant,if,startup,mode,is,link,startup,mode,note,this,method,is,also,exposed,for,testing;protected,static,void,initialize,subscribed,partitions,to,start,offsets,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,list,kafka,topic,partition,kafka,topic,partitions,int,index,of,this,subtask,int,num,parallel,subtasks,startup,mode,startup,mode,map,kafka,topic,partition,long,specific,startup,offsets,for,int,i,0,i,kafka,topic,partitions,size,i,if,i,num,parallel,subtasks,index,of,this,subtask,if,startup,mode,startup,mode,subscribed,partitions,to,start,offsets,put,kafka,topic,partitions,get,i,startup,mode,get,state,sentinel,else,if,specific,startup,offsets,null,throw,new,illegal,argument,exception,startup,mode,for,the,consumer,set,to,startup,mode,but,no,specific,offsets,were,specified,kafka,topic,partition,partition,kafka,topic,partitions,get,i,long,specific,offset,specific,startup,offsets,get,partition,if,specific,offset,null,subscribed,partitions,to,start,offsets,put,partition,specific,offset,1,else,subscribed,partitions,to,start,offsets,put,partition,kafka,topic,partition,state,sentinel
FlinkKafkaConsumerBase -> protected static void initializeSubscribedPartitionsToStartOffsets( 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			List<KafkaTopicPartition> kafkaTopicPartitions, 			int indexOfThisSubtask, 			int numParallelSubtasks, 			StartupMode startupMode, 			Map<KafkaTopicPartition, Long> specificStartupOffsets);1489510697;Initializes {@link FlinkKafkaConsumerBase#subscribedPartitionsToStartOffsets} with appropriate_values. The method decides which partitions this consumer instance should subscribe to, and also_sets the initial offset each subscribed partition should be started from based on the configured startup mode.__@param subscribedPartitionsToStartOffsets to subscribedPartitionsToStartOffsets to initialize_@param kafkaTopicPartitions the complete list of all Kafka partitions_@param indexOfThisSubtask the index of this consumer instance_@param numParallelSubtasks total number of parallel consumer instances_@param startupMode the configured startup mode for the consumer_@param specificStartupOffsets specific partition offsets to start from_(only relevant if startupMode is {@link StartupMode#SPECIFIC_OFFSETS})__Note: This method is also exposed for testing.;protected static void initializeSubscribedPartitionsToStartOffsets(_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			List<KafkaTopicPartition> kafkaTopicPartitions,_			int indexOfThisSubtask,_			int numParallelSubtasks,_			StartupMode startupMode,_			Map<KafkaTopicPartition, Long> specificStartupOffsets) {__		for (int i = 0_ i < kafkaTopicPartitions.size()_ i++) {_			if (i % numParallelSubtasks == indexOfThisSubtask) {_				if (startupMode != StartupMode.SPECIFIC_OFFSETS) {_					subscribedPartitionsToStartOffsets.put(kafkaTopicPartitions.get(i), startupMode.getStateSentinel())__				} else {_					if (specificStartupOffsets == null) {_						throw new IllegalArgumentException(_							"Startup mode for the consumer set to " + StartupMode.SPECIFIC_OFFSETS +_								", but no specific offsets were specified")__					}__					KafkaTopicPartition partition = kafkaTopicPartitions.get(i)___					Long specificOffset = specificStartupOffsets.get(partition)__					if (specificOffset != null) {_						_						_						subscribedPartitionsToStartOffsets.put(partition, specificOffset - 1)__					} else {_						subscribedPartitionsToStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__					}_				}_			}_		}_	};initializes,link,flink,kafka,consumer,base,subscribed,partitions,to,start,offsets,with,appropriate,values,the,method,decides,which,partitions,this,consumer,instance,should,subscribe,to,and,also,sets,the,initial,offset,each,subscribed,partition,should,be,started,from,based,on,the,configured,startup,mode,param,subscribed,partitions,to,start,offsets,to,subscribed,partitions,to,start,offsets,to,initialize,param,kafka,topic,partitions,the,complete,list,of,all,kafka,partitions,param,index,of,this,subtask,the,index,of,this,consumer,instance,param,num,parallel,subtasks,total,number,of,parallel,consumer,instances,param,startup,mode,the,configured,startup,mode,for,the,consumer,param,specific,startup,offsets,specific,partition,offsets,to,start,from,only,relevant,if,startup,mode,is,link,startup,mode,note,this,method,is,also,exposed,for,testing;protected,static,void,initialize,subscribed,partitions,to,start,offsets,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,list,kafka,topic,partition,kafka,topic,partitions,int,index,of,this,subtask,int,num,parallel,subtasks,startup,mode,startup,mode,map,kafka,topic,partition,long,specific,startup,offsets,for,int,i,0,i,kafka,topic,partitions,size,i,if,i,num,parallel,subtasks,index,of,this,subtask,if,startup,mode,startup,mode,subscribed,partitions,to,start,offsets,put,kafka,topic,partitions,get,i,startup,mode,get,state,sentinel,else,if,specific,startup,offsets,null,throw,new,illegal,argument,exception,startup,mode,for,the,consumer,set,to,startup,mode,but,no,specific,offsets,were,specified,kafka,topic,partition,partition,kafka,topic,partitions,get,i,long,specific,offset,specific,startup,offsets,get,partition,if,specific,offset,null,subscribed,partitions,to,start,offsets,put,partition,specific,offset,1,else,subscribed,partitions,to,start,offsets,put,partition,kafka,topic,partition,state,sentinel
FlinkKafkaConsumerBase -> protected static void initializeSubscribedPartitionsToStartOffsets( 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			List<KafkaTopicPartition> kafkaTopicPartitions, 			int indexOfThisSubtask, 			int numParallelSubtasks, 			StartupMode startupMode, 			Map<KafkaTopicPartition, Long> specificStartupOffsets);1489764760;Initializes {@link FlinkKafkaConsumerBase#subscribedPartitionsToStartOffsets} with appropriate_values. The method decides which partitions this consumer instance should subscribe to, and also_sets the initial offset each subscribed partition should be started from based on the configured startup mode.__@param subscribedPartitionsToStartOffsets to subscribedPartitionsToStartOffsets to initialize_@param kafkaTopicPartitions the complete list of all Kafka partitions_@param indexOfThisSubtask the index of this consumer instance_@param numParallelSubtasks total number of parallel consumer instances_@param startupMode the configured startup mode for the consumer_@param specificStartupOffsets specific partition offsets to start from_(only relevant if startupMode is {@link StartupMode#SPECIFIC_OFFSETS})__Note: This method is also exposed for testing.;protected static void initializeSubscribedPartitionsToStartOffsets(_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			List<KafkaTopicPartition> kafkaTopicPartitions,_			int indexOfThisSubtask,_			int numParallelSubtasks,_			StartupMode startupMode,_			Map<KafkaTopicPartition, Long> specificStartupOffsets) {__		for (int i = 0_ i < kafkaTopicPartitions.size()_ i++) {_			if (i % numParallelSubtasks == indexOfThisSubtask) {_				if (startupMode != StartupMode.SPECIFIC_OFFSETS) {_					subscribedPartitionsToStartOffsets.put(kafkaTopicPartitions.get(i), startupMode.getStateSentinel())__				} else {_					if (specificStartupOffsets == null) {_						throw new IllegalArgumentException(_							"Startup mode for the consumer set to " + StartupMode.SPECIFIC_OFFSETS +_								", but no specific offsets were specified")__					}__					KafkaTopicPartition partition = kafkaTopicPartitions.get(i)___					Long specificOffset = specificStartupOffsets.get(partition)__					if (specificOffset != null) {_						_						_						subscribedPartitionsToStartOffsets.put(partition, specificOffset - 1)__					} else {_						subscribedPartitionsToStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__					}_				}_			}_		}_	};initializes,link,flink,kafka,consumer,base,subscribed,partitions,to,start,offsets,with,appropriate,values,the,method,decides,which,partitions,this,consumer,instance,should,subscribe,to,and,also,sets,the,initial,offset,each,subscribed,partition,should,be,started,from,based,on,the,configured,startup,mode,param,subscribed,partitions,to,start,offsets,to,subscribed,partitions,to,start,offsets,to,initialize,param,kafka,topic,partitions,the,complete,list,of,all,kafka,partitions,param,index,of,this,subtask,the,index,of,this,consumer,instance,param,num,parallel,subtasks,total,number,of,parallel,consumer,instances,param,startup,mode,the,configured,startup,mode,for,the,consumer,param,specific,startup,offsets,specific,partition,offsets,to,start,from,only,relevant,if,startup,mode,is,link,startup,mode,note,this,method,is,also,exposed,for,testing;protected,static,void,initialize,subscribed,partitions,to,start,offsets,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,list,kafka,topic,partition,kafka,topic,partitions,int,index,of,this,subtask,int,num,parallel,subtasks,startup,mode,startup,mode,map,kafka,topic,partition,long,specific,startup,offsets,for,int,i,0,i,kafka,topic,partitions,size,i,if,i,num,parallel,subtasks,index,of,this,subtask,if,startup,mode,startup,mode,subscribed,partitions,to,start,offsets,put,kafka,topic,partitions,get,i,startup,mode,get,state,sentinel,else,if,specific,startup,offsets,null,throw,new,illegal,argument,exception,startup,mode,for,the,consumer,set,to,startup,mode,but,no,specific,offsets,were,specified,kafka,topic,partition,partition,kafka,topic,partitions,get,i,long,specific,offset,specific,startup,offsets,get,partition,if,specific,offset,null,subscribed,partitions,to,start,offsets,put,partition,specific,offset,1,else,subscribed,partitions,to,start,offsets,put,partition,kafka,topic,partition,state,sentinel
FlinkKafkaConsumerBase -> protected static void initializeSubscribedPartitionsToStartOffsets( 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			List<KafkaTopicPartition> kafkaTopicPartitions, 			int indexOfThisSubtask, 			int numParallelSubtasks, 			StartupMode startupMode, 			Map<KafkaTopicPartition, Long> specificStartupOffsets);1491500150;Initializes {@link FlinkKafkaConsumerBase#subscribedPartitionsToStartOffsets} with appropriate_values. The method decides which partitions this consumer instance should subscribe to, and also_sets the initial offset each subscribed partition should be started from based on the configured startup mode.__@param subscribedPartitionsToStartOffsets to subscribedPartitionsToStartOffsets to initialize_@param kafkaTopicPartitions the complete list of all Kafka partitions_@param indexOfThisSubtask the index of this consumer instance_@param numParallelSubtasks total number of parallel consumer instances_@param startupMode the configured startup mode for the consumer_@param specificStartupOffsets specific partition offsets to start from_(only relevant if startupMode is {@link StartupMode#SPECIFIC_OFFSETS})__Note: This method is also exposed for testing.;protected static void initializeSubscribedPartitionsToStartOffsets(_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			List<KafkaTopicPartition> kafkaTopicPartitions,_			int indexOfThisSubtask,_			int numParallelSubtasks,_			StartupMode startupMode,_			Map<KafkaTopicPartition, Long> specificStartupOffsets) {__		for (int i = 0_ i < kafkaTopicPartitions.size()_ i++) {_			if (i % numParallelSubtasks == indexOfThisSubtask) {_				if (startupMode != StartupMode.SPECIFIC_OFFSETS) {_					subscribedPartitionsToStartOffsets.put(kafkaTopicPartitions.get(i), startupMode.getStateSentinel())__				} else {_					if (specificStartupOffsets == null) {_						throw new IllegalArgumentException(_							"Startup mode for the consumer set to " + StartupMode.SPECIFIC_OFFSETS +_								", but no specific offsets were specified")__					}__					KafkaTopicPartition partition = kafkaTopicPartitions.get(i)___					Long specificOffset = specificStartupOffsets.get(partition)__					if (specificOffset != null) {_						_						_						subscribedPartitionsToStartOffsets.put(partition, specificOffset - 1)__					} else {_						subscribedPartitionsToStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__					}_				}_			}_		}_	};initializes,link,flink,kafka,consumer,base,subscribed,partitions,to,start,offsets,with,appropriate,values,the,method,decides,which,partitions,this,consumer,instance,should,subscribe,to,and,also,sets,the,initial,offset,each,subscribed,partition,should,be,started,from,based,on,the,configured,startup,mode,param,subscribed,partitions,to,start,offsets,to,subscribed,partitions,to,start,offsets,to,initialize,param,kafka,topic,partitions,the,complete,list,of,all,kafka,partitions,param,index,of,this,subtask,the,index,of,this,consumer,instance,param,num,parallel,subtasks,total,number,of,parallel,consumer,instances,param,startup,mode,the,configured,startup,mode,for,the,consumer,param,specific,startup,offsets,specific,partition,offsets,to,start,from,only,relevant,if,startup,mode,is,link,startup,mode,note,this,method,is,also,exposed,for,testing;protected,static,void,initialize,subscribed,partitions,to,start,offsets,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,list,kafka,topic,partition,kafka,topic,partitions,int,index,of,this,subtask,int,num,parallel,subtasks,startup,mode,startup,mode,map,kafka,topic,partition,long,specific,startup,offsets,for,int,i,0,i,kafka,topic,partitions,size,i,if,i,num,parallel,subtasks,index,of,this,subtask,if,startup,mode,startup,mode,subscribed,partitions,to,start,offsets,put,kafka,topic,partitions,get,i,startup,mode,get,state,sentinel,else,if,specific,startup,offsets,null,throw,new,illegal,argument,exception,startup,mode,for,the,consumer,set,to,startup,mode,but,no,specific,offsets,were,specified,kafka,topic,partition,partition,kafka,topic,partitions,get,i,long,specific,offset,specific,startup,offsets,get,partition,if,specific,offset,null,subscribed,partitions,to,start,offsets,put,partition,specific,offset,1,else,subscribed,partitions,to,start,offsets,put,partition,kafka,topic,partition,state,sentinel
FlinkKafkaConsumerBase -> protected static void initializeSubscribedPartitionsToStartOffsets( 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			List<KafkaTopicPartition> kafkaTopicPartitions, 			int indexOfThisSubtask, 			int numParallelSubtasks, 			StartupMode startupMode, 			Map<KafkaTopicPartition, Long> specificStartupOffsets);1493821466;Initializes {@link FlinkKafkaConsumerBase#subscribedPartitionsToStartOffsets} with appropriate_values. The method decides which partitions this consumer instance should subscribe to, and also_sets the initial offset each subscribed partition should be started from based on the configured startup mode.__@param subscribedPartitionsToStartOffsets to subscribedPartitionsToStartOffsets to initialize_@param kafkaTopicPartitions the complete list of all Kafka partitions_@param indexOfThisSubtask the index of this consumer instance_@param numParallelSubtasks total number of parallel consumer instances_@param startupMode the configured startup mode for the consumer_@param specificStartupOffsets specific partition offsets to start from_(only relevant if startupMode is {@link StartupMode#SPECIFIC_OFFSETS})__Note: This method is also exposed for testing.;protected static void initializeSubscribedPartitionsToStartOffsets(_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			List<KafkaTopicPartition> kafkaTopicPartitions,_			int indexOfThisSubtask,_			int numParallelSubtasks,_			StartupMode startupMode,_			Map<KafkaTopicPartition, Long> specificStartupOffsets) {__		for (int i = 0_ i < kafkaTopicPartitions.size()_ i++) {_			if (i % numParallelSubtasks == indexOfThisSubtask) {_				if (startupMode != StartupMode.SPECIFIC_OFFSETS) {_					subscribedPartitionsToStartOffsets.put(kafkaTopicPartitions.get(i), startupMode.getStateSentinel())__				} else {_					if (specificStartupOffsets == null) {_						throw new IllegalArgumentException(_							"Startup mode for the consumer set to " + StartupMode.SPECIFIC_OFFSETS +_								", but no specific offsets were specified")__					}__					KafkaTopicPartition partition = kafkaTopicPartitions.get(i)___					Long specificOffset = specificStartupOffsets.get(partition)__					if (specificOffset != null) {_						_						_						subscribedPartitionsToStartOffsets.put(partition, specificOffset - 1)__					} else {_						subscribedPartitionsToStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__					}_				}_			}_		}_	};initializes,link,flink,kafka,consumer,base,subscribed,partitions,to,start,offsets,with,appropriate,values,the,method,decides,which,partitions,this,consumer,instance,should,subscribe,to,and,also,sets,the,initial,offset,each,subscribed,partition,should,be,started,from,based,on,the,configured,startup,mode,param,subscribed,partitions,to,start,offsets,to,subscribed,partitions,to,start,offsets,to,initialize,param,kafka,topic,partitions,the,complete,list,of,all,kafka,partitions,param,index,of,this,subtask,the,index,of,this,consumer,instance,param,num,parallel,subtasks,total,number,of,parallel,consumer,instances,param,startup,mode,the,configured,startup,mode,for,the,consumer,param,specific,startup,offsets,specific,partition,offsets,to,start,from,only,relevant,if,startup,mode,is,link,startup,mode,note,this,method,is,also,exposed,for,testing;protected,static,void,initialize,subscribed,partitions,to,start,offsets,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,list,kafka,topic,partition,kafka,topic,partitions,int,index,of,this,subtask,int,num,parallel,subtasks,startup,mode,startup,mode,map,kafka,topic,partition,long,specific,startup,offsets,for,int,i,0,i,kafka,topic,partitions,size,i,if,i,num,parallel,subtasks,index,of,this,subtask,if,startup,mode,startup,mode,subscribed,partitions,to,start,offsets,put,kafka,topic,partitions,get,i,startup,mode,get,state,sentinel,else,if,specific,startup,offsets,null,throw,new,illegal,argument,exception,startup,mode,for,the,consumer,set,to,startup,mode,but,no,specific,offsets,were,specified,kafka,topic,partition,partition,kafka,topic,partitions,get,i,long,specific,offset,specific,startup,offsets,get,partition,if,specific,offset,null,subscribed,partitions,to,start,offsets,put,partition,specific,offset,1,else,subscribed,partitions,to,start,offsets,put,partition,kafka,topic,partition,state,sentinel
FlinkKafkaConsumerBase -> protected static void initializeSubscribedPartitionsToStartOffsets( 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			List<KafkaTopicPartition> kafkaTopicPartitions, 			int indexOfThisSubtask, 			int numParallelSubtasks, 			StartupMode startupMode, 			Map<KafkaTopicPartition, Long> specificStartupOffsets);1494830990;Initializes {@link FlinkKafkaConsumerBase#subscribedPartitionsToStartOffsets} with appropriate_values. The method decides which partitions this consumer instance should subscribe to, and also_sets the initial offset each subscribed partition should be started from based on the configured startup mode.__@param subscribedPartitionsToStartOffsets to subscribedPartitionsToStartOffsets to initialize_@param kafkaTopicPartitions the complete list of all Kafka partitions_@param indexOfThisSubtask the index of this consumer instance_@param numParallelSubtasks total number of parallel consumer instances_@param startupMode the configured startup mode for the consumer_@param specificStartupOffsets specific partition offsets to start from_(only relevant if startupMode is {@link StartupMode#SPECIFIC_OFFSETS})__Note: This method is also exposed for testing.;protected static void initializeSubscribedPartitionsToStartOffsets(_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			List<KafkaTopicPartition> kafkaTopicPartitions,_			int indexOfThisSubtask,_			int numParallelSubtasks,_			StartupMode startupMode,_			Map<KafkaTopicPartition, Long> specificStartupOffsets) {__		for (int i = 0_ i < kafkaTopicPartitions.size()_ i++) {_			if (i % numParallelSubtasks == indexOfThisSubtask) {_				if (startupMode != StartupMode.SPECIFIC_OFFSETS) {_					subscribedPartitionsToStartOffsets.put(kafkaTopicPartitions.get(i), startupMode.getStateSentinel())__				} else {_					if (specificStartupOffsets == null) {_						throw new IllegalArgumentException(_							"Startup mode for the consumer set to " + StartupMode.SPECIFIC_OFFSETS +_								", but no specific offsets were specified")__					}__					KafkaTopicPartition partition = kafkaTopicPartitions.get(i)___					Long specificOffset = specificStartupOffsets.get(partition)__					if (specificOffset != null) {_						_						_						subscribedPartitionsToStartOffsets.put(partition, specificOffset - 1)__					} else {_						subscribedPartitionsToStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__					}_				}_			}_		}_	};initializes,link,flink,kafka,consumer,base,subscribed,partitions,to,start,offsets,with,appropriate,values,the,method,decides,which,partitions,this,consumer,instance,should,subscribe,to,and,also,sets,the,initial,offset,each,subscribed,partition,should,be,started,from,based,on,the,configured,startup,mode,param,subscribed,partitions,to,start,offsets,to,subscribed,partitions,to,start,offsets,to,initialize,param,kafka,topic,partitions,the,complete,list,of,all,kafka,partitions,param,index,of,this,subtask,the,index,of,this,consumer,instance,param,num,parallel,subtasks,total,number,of,parallel,consumer,instances,param,startup,mode,the,configured,startup,mode,for,the,consumer,param,specific,startup,offsets,specific,partition,offsets,to,start,from,only,relevant,if,startup,mode,is,link,startup,mode,note,this,method,is,also,exposed,for,testing;protected,static,void,initialize,subscribed,partitions,to,start,offsets,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,list,kafka,topic,partition,kafka,topic,partitions,int,index,of,this,subtask,int,num,parallel,subtasks,startup,mode,startup,mode,map,kafka,topic,partition,long,specific,startup,offsets,for,int,i,0,i,kafka,topic,partitions,size,i,if,i,num,parallel,subtasks,index,of,this,subtask,if,startup,mode,startup,mode,subscribed,partitions,to,start,offsets,put,kafka,topic,partitions,get,i,startup,mode,get,state,sentinel,else,if,specific,startup,offsets,null,throw,new,illegal,argument,exception,startup,mode,for,the,consumer,set,to,startup,mode,but,no,specific,offsets,were,specified,kafka,topic,partition,partition,kafka,topic,partitions,get,i,long,specific,offset,specific,startup,offsets,get,partition,if,specific,offset,null,subscribed,partitions,to,start,offsets,put,partition,specific,offset,1,else,subscribed,partitions,to,start,offsets,put,partition,kafka,topic,partition,state,sentinel
FlinkKafkaConsumerBase -> protected static void initializeSubscribedPartitionsToStartOffsets( 			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets, 			List<KafkaTopicPartition> kafkaTopicPartitions, 			int indexOfThisSubtask, 			int numParallelSubtasks, 			StartupMode startupMode, 			Map<KafkaTopicPartition, Long> specificStartupOffsets);1495923077;Initializes {@link FlinkKafkaConsumerBase#subscribedPartitionsToStartOffsets} with appropriate_values. The method decides which partitions this consumer instance should subscribe to, and also_sets the initial offset each subscribed partition should be started from based on the configured startup mode.__@param subscribedPartitionsToStartOffsets to subscribedPartitionsToStartOffsets to initialize_@param kafkaTopicPartitions the complete list of all Kafka partitions_@param indexOfThisSubtask the index of this consumer instance_@param numParallelSubtasks total number of parallel consumer instances_@param startupMode the configured startup mode for the consumer_@param specificStartupOffsets specific partition offsets to start from_(only relevant if startupMode is {@link StartupMode#SPECIFIC_OFFSETS})__Note: This method is also exposed for testing.;protected static void initializeSubscribedPartitionsToStartOffsets(_			Map<KafkaTopicPartition, Long> subscribedPartitionsToStartOffsets,_			List<KafkaTopicPartition> kafkaTopicPartitions,_			int indexOfThisSubtask,_			int numParallelSubtasks,_			StartupMode startupMode,_			Map<KafkaTopicPartition, Long> specificStartupOffsets) {__		for (int i = 0_ i < kafkaTopicPartitions.size()_ i++) {_			if (i % numParallelSubtasks == indexOfThisSubtask) {_				if (startupMode != StartupMode.SPECIFIC_OFFSETS) {_					subscribedPartitionsToStartOffsets.put(kafkaTopicPartitions.get(i), startupMode.getStateSentinel())__				} else {_					if (specificStartupOffsets == null) {_						throw new IllegalArgumentException(_							"Startup mode for the consumer set to " + StartupMode.SPECIFIC_OFFSETS +_								", but no specific offsets were specified")__					}__					KafkaTopicPartition partition = kafkaTopicPartitions.get(i)___					Long specificOffset = specificStartupOffsets.get(partition)__					if (specificOffset != null) {_						_						_						subscribedPartitionsToStartOffsets.put(partition, specificOffset - 1)__					} else {_						subscribedPartitionsToStartOffsets.put(partition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET)__					}_				}_			}_		}_	};initializes,link,flink,kafka,consumer,base,subscribed,partitions,to,start,offsets,with,appropriate,values,the,method,decides,which,partitions,this,consumer,instance,should,subscribe,to,and,also,sets,the,initial,offset,each,subscribed,partition,should,be,started,from,based,on,the,configured,startup,mode,param,subscribed,partitions,to,start,offsets,to,subscribed,partitions,to,start,offsets,to,initialize,param,kafka,topic,partitions,the,complete,list,of,all,kafka,partitions,param,index,of,this,subtask,the,index,of,this,consumer,instance,param,num,parallel,subtasks,total,number,of,parallel,consumer,instances,param,startup,mode,the,configured,startup,mode,for,the,consumer,param,specific,startup,offsets,specific,partition,offsets,to,start,from,only,relevant,if,startup,mode,is,link,startup,mode,note,this,method,is,also,exposed,for,testing;protected,static,void,initialize,subscribed,partitions,to,start,offsets,map,kafka,topic,partition,long,subscribed,partitions,to,start,offsets,list,kafka,topic,partition,kafka,topic,partitions,int,index,of,this,subtask,int,num,parallel,subtasks,startup,mode,startup,mode,map,kafka,topic,partition,long,specific,startup,offsets,for,int,i,0,i,kafka,topic,partitions,size,i,if,i,num,parallel,subtasks,index,of,this,subtask,if,startup,mode,startup,mode,subscribed,partitions,to,start,offsets,put,kafka,topic,partitions,get,i,startup,mode,get,state,sentinel,else,if,specific,startup,offsets,null,throw,new,illegal,argument,exception,startup,mode,for,the,consumer,set,to,startup,mode,but,no,specific,offsets,were,specified,kafka,topic,partition,partition,kafka,topic,partitions,get,i,long,specific,offset,specific,startup,offsets,get,partition,if,specific,offset,null,subscribed,partitions,to,start,offsets,put,partition,specific,offset,1,else,subscribed,partitions,to,start,offsets,put,partition,kafka,topic,partition,state,sentinel
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1487173364;Specifies the consumer to start reading from the earliest offset for all partitions._This ignores any committed group offsets in Zookeeper / Kafka brokers.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,ignores,any,committed,group,offsets,in,zookeeper,kafka,brokers,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1488214488;Specifies the consumer to start reading from the earliest offset for all partitions._This ignores any committed group offsets in Zookeeper / Kafka brokers.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,ignores,any,committed,group,offsets,in,zookeeper,kafka,brokers,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1489419493;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1489510697;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1489764760;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1491500150;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1493821466;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1494830990;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1495923077;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1498894422;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1501249949;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1501249949;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1501249950;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1503598628;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1509013576;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1512405587;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1515757408;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1515757408;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1515757408;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1515757409;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1517943538;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1519658056;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1519973085;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1519973085;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1520598638;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1525116534;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1542613709;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1548931626;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1550834388;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1550834396;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromEarliest();1550834396;Specifies the consumer to start reading from the earliest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,earliest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,earliest,this,startup,mode,startup,mode,earliest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1498894422;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1501249949;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1501249949;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1501249950;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1503598628;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1509013576;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1512405587;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1515757408;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1515757408;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1515757408;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1515757409;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1517943538;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1519658056;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1519973085;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1519973085;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1520598638;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1525116534;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1542613709;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1548931626;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1550834388;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1550834396;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer( 			KafkaTopicsDescriptor topicsDescriptor, 			int indexOfThisSubtask, 			int numParallelSubtasks)_;1550834396;Creates the partition discoverer that is used to find new partitions for this subtask.__@param topicsDescriptor Descriptor that describes whether we are discovering partitions for fixed topics or a topic pattern._@param indexOfThisSubtask The index of this consumer subtask._@param numParallelSubtasks The total number of parallel consumer subtasks.__@return The instantiated partition discoverer;protected abstract AbstractPartitionDiscoverer createPartitionDiscoverer(_			KafkaTopicsDescriptor topicsDescriptor,_			int indexOfThisSubtask,_			int numParallelSubtasks)_;creates,the,partition,discoverer,that,is,used,to,find,new,partitions,for,this,subtask,param,topics,descriptor,descriptor,that,describes,whether,we,are,discovering,partitions,for,fixed,topics,or,a,topic,pattern,param,index,of,this,subtask,the,index,of,this,consumer,subtask,param,num,parallel,subtasks,the,total,number,of,parallel,consumer,subtasks,return,the,instantiated,partition,discoverer;protected,abstract,abstract,partition,discoverer,create,partition,discoverer,kafka,topics,descriptor,topics,descriptor,int,index,of,this,subtask,int,num,parallel,subtasks
FlinkKafkaConsumerBase -> protected void setSubscribedPartitions(List<KafkaTopicPartition> allSubscribedPartitions);1480685315;This method must be called from the subclasses, to set the list of all subscribed partitions_that this consumer will fetch from (across all subtasks).__@param allSubscribedPartitions The list of all partitions that all subtasks together should fetch from.;protected void setSubscribedPartitions(List<KafkaTopicPartition> allSubscribedPartitions) {_		checkNotNull(allSubscribedPartitions)__		this.subscribedPartitions = Collections.unmodifiableList(allSubscribedPartitions)__	};this,method,must,be,called,from,the,subclasses,to,set,the,list,of,all,subscribed,partitions,that,this,consumer,will,fetch,from,across,all,subtasks,param,all,subscribed,partitions,the,list,of,all,partitions,that,all,subtasks,together,should,fetch,from;protected,void,set,subscribed,partitions,list,kafka,topic,partition,all,subscribed,partitions,check,not,null,all,subscribed,partitions,this,subscribed,partitions,collections,unmodifiable,list,all,subscribed,partitions
FlinkKafkaConsumerBase -> protected void setSubscribedPartitions(List<KafkaTopicPartition> allSubscribedPartitions);1482244974;This method must be called from the subclasses, to set the list of all subscribed partitions_that this consumer will fetch from (across all subtasks).__@param allSubscribedPartitions The list of all partitions that all subtasks together should fetch from.;protected void setSubscribedPartitions(List<KafkaTopicPartition> allSubscribedPartitions) {_		checkNotNull(allSubscribedPartitions)__		this.subscribedPartitions = Collections.unmodifiableList(allSubscribedPartitions)__	};this,method,must,be,called,from,the,subclasses,to,set,the,list,of,all,subscribed,partitions,that,this,consumer,will,fetch,from,across,all,subtasks,param,all,subscribed,partitions,the,list,of,all,partitions,that,all,subtasks,together,should,fetch,from;protected,void,set,subscribed,partitions,list,kafka,topic,partition,all,subscribed,partitions,check,not,null,all,subscribed,partitions,this,subscribed,partitions,collections,unmodifiable,list,all,subscribed,partitions
FlinkKafkaConsumerBase -> protected void setSubscribedPartitions(List<KafkaTopicPartition> allSubscribedPartitions);1482244974;This method must be called from the subclasses, to set the list of all subscribed partitions_that this consumer will fetch from (across all subtasks).__@param allSubscribedPartitions The list of all partitions that all subtasks together should fetch from.;protected void setSubscribedPartitions(List<KafkaTopicPartition> allSubscribedPartitions) {_		checkNotNull(allSubscribedPartitions)__		this.subscribedPartitions = Collections.unmodifiableList(allSubscribedPartitions)__	};this,method,must,be,called,from,the,subclasses,to,set,the,list,of,all,subscribed,partitions,that,this,consumer,will,fetch,from,across,all,subtasks,param,all,subscribed,partitions,the,list,of,all,partitions,that,all,subtasks,together,should,fetch,from;protected,void,set,subscribed,partitions,list,kafka,topic,partition,all,subscribed,partitions,check,not,null,all,subscribed,partitions,this,subscribed,partitions,collections,unmodifiable,list,all,subscribed,partitions
FlinkKafkaConsumerBase -> protected void setSubscribedPartitions(List<KafkaTopicPartition> allSubscribedPartitions);1487173364;This method must be called from the subclasses, to set the list of all subscribed partitions_that this consumer will fetch from (across all subtasks).__@param allSubscribedPartitions The list of all partitions that all subtasks together should fetch from.;protected void setSubscribedPartitions(List<KafkaTopicPartition> allSubscribedPartitions) {_		checkNotNull(allSubscribedPartitions)__		this.subscribedPartitions = Collections.unmodifiableList(allSubscribedPartitions)__	};this,method,must,be,called,from,the,subclasses,to,set,the,list,of,all,subscribed,partitions,that,this,consumer,will,fetch,from,across,all,subtasks,param,all,subscribed,partitions,the,list,of,all,partitions,that,all,subtasks,together,should,fetch,from;protected,void,set,subscribed,partitions,list,kafka,topic,partition,all,subscribed,partitions,check,not,null,all,subscribed,partitions,this,subscribed,partitions,collections,unmodifiable,list,all,subscribed,partitions
FlinkKafkaConsumerBase -> protected static List<KafkaTopicPartition> assignPartitions( 			List<KafkaTopicPartition> allPartitions, 			int numConsumers, int consumerIndex);1480685315;Selects which of the given partitions should be handled by a specific consumer,_given a certain number of consumers.__@param allPartitions The partitions to select from_@param numConsumers The number of consumers_@param consumerIndex The index of the specific consumer__@return The sublist of partitions to be handled by that consumer.;protected static List<KafkaTopicPartition> assignPartitions(_			List<KafkaTopicPartition> allPartitions,_			int numConsumers, int consumerIndex) {_		final List<KafkaTopicPartition> thisSubtaskPartitions = new ArrayList<>(_				allPartitions.size() / numConsumers + 1)___		for (int i = 0_ i < allPartitions.size()_ i++) {_			if (i % numConsumers == consumerIndex) {_				thisSubtaskPartitions.add(allPartitions.get(i))__			}_		}_		_		return thisSubtaskPartitions__	};selects,which,of,the,given,partitions,should,be,handled,by,a,specific,consumer,given,a,certain,number,of,consumers,param,all,partitions,the,partitions,to,select,from,param,num,consumers,the,number,of,consumers,param,consumer,index,the,index,of,the,specific,consumer,return,the,sublist,of,partitions,to,be,handled,by,that,consumer;protected,static,list,kafka,topic,partition,assign,partitions,list,kafka,topic,partition,all,partitions,int,num,consumers,int,consumer,index,final,list,kafka,topic,partition,this,subtask,partitions,new,array,list,all,partitions,size,num,consumers,1,for,int,i,0,i,all,partitions,size,i,if,i,num,consumers,consumer,index,this,subtask,partitions,add,all,partitions,get,i,return,this,subtask,partitions
FlinkKafkaConsumerBase -> protected static List<KafkaTopicPartition> assignPartitions( 			List<KafkaTopicPartition> allPartitions, 			int numConsumers, int consumerIndex);1482244974;Selects which of the given partitions should be handled by a specific consumer,_given a certain number of consumers.__@param allPartitions The partitions to select from_@param numConsumers The number of consumers_@param consumerIndex The index of the specific consumer__@return The sublist of partitions to be handled by that consumer.;protected static List<KafkaTopicPartition> assignPartitions(_			List<KafkaTopicPartition> allPartitions,_			int numConsumers, int consumerIndex) {_		final List<KafkaTopicPartition> thisSubtaskPartitions = new ArrayList<>(_				allPartitions.size() / numConsumers + 1)___		for (int i = 0_ i < allPartitions.size()_ i++) {_			if (i % numConsumers == consumerIndex) {_				thisSubtaskPartitions.add(allPartitions.get(i))__			}_		}_		_		return thisSubtaskPartitions__	};selects,which,of,the,given,partitions,should,be,handled,by,a,specific,consumer,given,a,certain,number,of,consumers,param,all,partitions,the,partitions,to,select,from,param,num,consumers,the,number,of,consumers,param,consumer,index,the,index,of,the,specific,consumer,return,the,sublist,of,partitions,to,be,handled,by,that,consumer;protected,static,list,kafka,topic,partition,assign,partitions,list,kafka,topic,partition,all,partitions,int,num,consumers,int,consumer,index,final,list,kafka,topic,partition,this,subtask,partitions,new,array,list,all,partitions,size,num,consumers,1,for,int,i,0,i,all,partitions,size,i,if,i,num,consumers,consumer,index,this,subtask,partitions,add,all,partitions,get,i,return,this,subtask,partitions
FlinkKafkaConsumerBase -> protected static List<KafkaTopicPartition> assignPartitions( 			List<KafkaTopicPartition> allPartitions, 			int numConsumers, int consumerIndex);1482244974;Selects which of the given partitions should be handled by a specific consumer,_given a certain number of consumers.__@param allPartitions The partitions to select from_@param numConsumers The number of consumers_@param consumerIndex The index of the specific consumer__@return The sublist of partitions to be handled by that consumer.;protected static List<KafkaTopicPartition> assignPartitions(_			List<KafkaTopicPartition> allPartitions,_			int numConsumers, int consumerIndex) {_		final List<KafkaTopicPartition> thisSubtaskPartitions = new ArrayList<>(_				allPartitions.size() / numConsumers + 1)___		for (int i = 0_ i < allPartitions.size()_ i++) {_			if (i % numConsumers == consumerIndex) {_				thisSubtaskPartitions.add(allPartitions.get(i))__			}_		}_		_		return thisSubtaskPartitions__	};selects,which,of,the,given,partitions,should,be,handled,by,a,specific,consumer,given,a,certain,number,of,consumers,param,all,partitions,the,partitions,to,select,from,param,num,consumers,the,number,of,consumers,param,consumer,index,the,index,of,the,specific,consumer,return,the,sublist,of,partitions,to,be,handled,by,that,consumer;protected,static,list,kafka,topic,partition,assign,partitions,list,kafka,topic,partition,all,partitions,int,num,consumers,int,consumer,index,final,list,kafka,topic,partition,this,subtask,partitions,new,array,list,all,partitions,size,num,consumers,1,for,int,i,0,i,all,partitions,size,i,if,i,num,consumers,consumer,index,this,subtask,partitions,add,all,partitions,get,i,return,this,subtask,partitions
FlinkKafkaConsumerBase -> protected static List<KafkaTopicPartition> assignPartitions( 			List<KafkaTopicPartition> allPartitions, 			int numConsumers, int consumerIndex);1487173364;Selects which of the given partitions should be handled by a specific consumer,_given a certain number of consumers.__@param allPartitions The partitions to select from_@param numConsumers The number of consumers_@param consumerIndex The index of the specific consumer__@return The sublist of partitions to be handled by that consumer.;protected static List<KafkaTopicPartition> assignPartitions(_			List<KafkaTopicPartition> allPartitions,_			int numConsumers, int consumerIndex) {_		final List<KafkaTopicPartition> thisSubtaskPartitions = new ArrayList<>(_				allPartitions.size() / numConsumers + 1)___		for (int i = 0_ i < allPartitions.size()_ i++) {_			if (i % numConsumers == consumerIndex) {_				thisSubtaskPartitions.add(allPartitions.get(i))__			}_		}_		_		return thisSubtaskPartitions__	};selects,which,of,the,given,partitions,should,be,handled,by,a,specific,consumer,given,a,certain,number,of,consumers,param,all,partitions,the,partitions,to,select,from,param,num,consumers,the,number,of,consumers,param,consumer,index,the,index,of,the,specific,consumer,return,the,sublist,of,partitions,to,be,handled,by,that,consumer;protected,static,list,kafka,topic,partition,assign,partitions,list,kafka,topic,partition,all,partitions,int,num,consumers,int,consumer,index,final,list,kafka,topic,partition,this,subtask,partitions,new,array,list,all,partitions,size,num,consumers,1,for,int,i,0,i,all,partitions,size,i,if,i,num,consumers,consumer,index,this,subtask,partitions,add,all,partitions,get,i,return,this,subtask,partitions
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1489419493;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1489510697;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1489764760;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1491500150;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1493821466;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1494830990;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1495923077;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1498894422;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1501249949;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1501249949;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1501249950;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1503598628;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1509013576;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1512405587;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1515757408;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1515757408;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1515757408;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1515757409;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1517943538;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1519658056;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1519973085;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1519973085;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1520598638;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1525116534;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1542613709;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1548931626;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1550834388;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1550834396;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets);1550834396;Specifies the consumer to start reading partitions from specific offsets, set independently for each partition._The specified offset should be the offset of the next record that will be read from partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>If the provided map of offsets contains entries whose {@link KafkaTopicPartition} is not subscribed by the_consumer, the entry will be ignored. If the consumer subscribes to a partition that does not exist in the provided_map of offsets, the consumer will fallback to the default group offset behaviour (see_{@link FlinkKafkaConsumerBase#setStartFromGroupOffsets()}) for that particular partition.__<p>If the specified offset for a partition is invalid, or the behaviour for that partition is defaulted to group_offsets but still no group offset could be found for it, then the "auto.offset.reset" behaviour set in the_configuration properties will be used for the partition__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = checkNotNull(specificStartupOffsets)__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,the,specified,offset,should,be,the,offset,of,the,next,record,that,will,be,read,from,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,if,the,provided,map,of,offsets,contains,entries,whose,link,kafka,topic,partition,is,not,subscribed,by,the,consumer,the,entry,will,be,ignored,if,the,consumer,subscribes,to,a,partition,that,does,not,exist,in,the,provided,map,of,offsets,the,consumer,will,fallback,to,the,default,group,offset,behaviour,see,link,flink,kafka,consumer,base,set,start,from,group,offsets,for,that,particular,partition,p,if,the,specified,offset,for,a,partition,is,invalid,or,the,behaviour,for,that,partition,is,defaulted,to,group,offsets,but,still,no,group,offset,could,be,found,for,it,then,the,auto,offset,reset,behaviour,set,in,the,configuration,properties,will,be,used,for,the,partition,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,specific,offsets,map,kafka,topic,partition,long,specific,startup,offsets,this,startup,mode,startup,mode,this,startup,offsets,timestamp,null,this,specific,startup,offsets,check,not,null,specific,startup,offsets,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1517943538;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1519658056;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1519973085;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1519973085;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1520598638;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1525116534;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1542613709;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1548931626;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1550834388;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase( 			List<String> topics, 			Pattern topicPattern, 			KeyedDeserializationSchema<T> deserializer, 			long discoveryIntervalMillis, 			boolean useMetrics);1550834396;Base constructor.__@param topics fixed list of topics to subscribe to (null, if using topic pattern)_@param topicPattern the topic pattern to subscribe to (null, if using fixed topics)_@param deserializer The deserializer to turn raw byte messages into Java/Scala objects._@param discoveryIntervalMillis the topic / partition discovery interval, in_milliseconds (0 if discovery is disabled).;public FlinkKafkaConsumerBase(_			List<String> topics,_			Pattern topicPattern,_			KeyedDeserializationSchema<T> deserializer,_			long discoveryIntervalMillis,_			boolean useMetrics) {_		this.topicsDescriptor = new KafkaTopicsDescriptor(topics, topicPattern)__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")___		checkArgument(_			discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED || discoveryIntervalMillis >= 0,_			"Cannot define a negative value for the topic / partition discovery interval.")__		this.discoveryIntervalMillis = discoveryIntervalMillis___		this.useMetrics = useMetrics__	};base,constructor,param,topics,fixed,list,of,topics,to,subscribe,to,null,if,using,topic,pattern,param,topic,pattern,the,topic,pattern,to,subscribe,to,null,if,using,fixed,topics,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects,param,discovery,interval,millis,the,topic,partition,discovery,interval,in,milliseconds,0,if,discovery,is,disabled;public,flink,kafka,consumer,base,list,string,topics,pattern,topic,pattern,keyed,deserialization,schema,t,deserializer,long,discovery,interval,millis,boolean,use,metrics,this,topics,descriptor,new,kafka,topics,descriptor,topics,topic,pattern,this,deserializer,check,not,null,deserializer,value,deserializer,check,argument,discovery,interval,millis,discovery,interval,millis,0,cannot,define,a,negative,value,for,the,topic,partition,discovery,interval,this,discovery,interval,millis,discovery,interval,millis,this,use,metrics,use,metrics
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1480685315;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1482244974;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1482244974;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1487173364;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1488214488;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1489419493;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1489510697;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1489764760;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1491500150;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1493821466;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1494830990;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1495923077;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1498894422;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1501249949;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1501249949;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1501249950;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1503598628;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1509013576;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1512405587;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1515757408;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1515757408;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1515757408;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1515757409;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1517943538;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1519658056;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1519973085;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1519973085;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1520598638;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1525116534;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1542613709;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1548931626;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1550834388;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1550834396;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner);1550834396;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.periodicWatermarkAssigner != null) {_			throw new IllegalStateException("A periodic watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.punctuatedWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,punctuated,watermarks,t,assigner,check,not,null,assigner,if,this,periodic,watermark,assigner,null,throw,new,illegal,state,exception,a,periodic,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,punctuated,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> static void adjustAutoCommitConfig(Properties properties, OffsetCommitMode offsetCommitMode);1550834388;Make sure that auto commit is disabled when our offset commit mode is ON_CHECKPOINTS._This overwrites whatever setting the user configured in the properties._@param properties - Kafka configuration properties to be adjusted_@param offsetCommitMode offset commit mode;static void adjustAutoCommitConfig(Properties properties, OffsetCommitMode offsetCommitMode) {_		if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS || offsetCommitMode == OffsetCommitMode.DISABLED) {_			properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false")__		}_	};make,sure,that,auto,commit,is,disabled,when,our,offset,commit,mode,is,this,overwrites,whatever,setting,the,user,configured,in,the,properties,param,properties,kafka,configuration,properties,to,be,adjusted,param,offset,commit,mode,offset,commit,mode;static,void,adjust,auto,commit,config,properties,properties,offset,commit,mode,offset,commit,mode,if,offset,commit,mode,offset,commit,mode,offset,commit,mode,offset,commit,mode,disabled,properties,set,property,consumer,config,false
FlinkKafkaConsumerBase -> static void adjustAutoCommitConfig(Properties properties, OffsetCommitMode offsetCommitMode);1550834396;Make sure that auto commit is disabled when our offset commit mode is ON_CHECKPOINTS._This overwrites whatever setting the user configured in the properties._@param properties - Kafka configuration properties to be adjusted_@param offsetCommitMode offset commit mode;static void adjustAutoCommitConfig(Properties properties, OffsetCommitMode offsetCommitMode) {_		if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS || offsetCommitMode == OffsetCommitMode.DISABLED) {_			properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false")__		}_	};make,sure,that,auto,commit,is,disabled,when,our,offset,commit,mode,is,this,overwrites,whatever,setting,the,user,configured,in,the,properties,param,properties,kafka,configuration,properties,to,be,adjusted,param,offset,commit,mode,offset,commit,mode;static,void,adjust,auto,commit,config,properties,properties,offset,commit,mode,offset,commit,mode,if,offset,commit,mode,offset,commit,mode,offset,commit,mode,offset,commit,mode,disabled,properties,set,property,consumer,config,false
FlinkKafkaConsumerBase -> static void adjustAutoCommitConfig(Properties properties, OffsetCommitMode offsetCommitMode);1550834396;Make sure that auto commit is disabled when our offset commit mode is ON_CHECKPOINTS._This overwrites whatever setting the user configured in the properties._@param properties - Kafka configuration properties to be adjusted_@param offsetCommitMode offset commit mode;static void adjustAutoCommitConfig(Properties properties, OffsetCommitMode offsetCommitMode) {_		if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS || offsetCommitMode == OffsetCommitMode.DISABLED) {_			properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false")__		}_	};make,sure,that,auto,commit,is,disabled,when,our,offset,commit,mode,is,this,overwrites,whatever,setting,the,user,configured,in,the,properties,param,properties,kafka,configuration,properties,to,be,adjusted,param,offset,commit,mode,offset,commit,mode;static,void,adjust,auto,commit,config,properties,properties,offset,commit,mode,offset,commit,mode,if,offset,commit,mode,offset,commit,mode,offset,commit,mode,offset,commit,mode,disabled,properties,set,property,consumer,config,false
FlinkKafkaConsumerBase -> protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp);1519973085;Specifies the consumer to start reading partitions from a specified timestamp._The specified timestamp must be before the current timestamp._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>The consumer will look up the earliest offset whose timestamp is greater than or equal_to the specific timestamp from Kafka. If there's no such offset, the consumer will use the_latest offset to read data from kafka.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@param startupOffsetsTimestamp timestamp for the startup offsets, as milliseconds from epoch.__@return The consumer object, to allow function chaining.;protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp) {_		checkArgument(startupOffsetsTimestamp >= 0, "The provided value for the startup offsets timestamp is invalid.")___		long currentTimestamp = System.currentTimeMillis()__		checkArgument(startupOffsetsTimestamp <= currentTimestamp,_			"Startup time[%s] must be before current time[%s].", startupOffsetsTimestamp, currentTimestamp)___		this.startupMode = StartupMode.TIMESTAMP__		this.startupOffsetsTimestamp = startupOffsetsTimestamp__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,a,specified,timestamp,the,specified,timestamp,must,be,before,the,current,timestamp,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,the,consumer,will,look,up,the,earliest,offset,whose,timestamp,is,greater,than,or,equal,to,the,specific,timestamp,from,kafka,if,there,s,no,such,offset,the,consumer,will,use,the,latest,offset,to,read,data,from,kafka,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,param,startup,offsets,timestamp,timestamp,for,the,startup,offsets,as,milliseconds,from,epoch,return,the,consumer,object,to,allow,function,chaining;protected,flink,kafka,consumer,base,t,set,start,from,timestamp,long,startup,offsets,timestamp,check,argument,startup,offsets,timestamp,0,the,provided,value,for,the,startup,offsets,timestamp,is,invalid,long,current,timestamp,system,current,time,millis,check,argument,startup,offsets,timestamp,current,timestamp,startup,time,s,must,be,before,current,time,s,startup,offsets,timestamp,current,timestamp,this,startup,mode,startup,mode,timestamp,this,startup,offsets,timestamp,startup,offsets,timestamp,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp);1520598638;Specifies the consumer to start reading partitions from a specified timestamp._The specified timestamp must be before the current timestamp._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>The consumer will look up the earliest offset whose timestamp is greater than or equal_to the specific timestamp from Kafka. If there's no such offset, the consumer will use the_latest offset to read data from kafka.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@param startupOffsetsTimestamp timestamp for the startup offsets, as milliseconds from epoch.__@return The consumer object, to allow function chaining.;protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp) {_		checkArgument(startupOffsetsTimestamp >= 0, "The provided value for the startup offsets timestamp is invalid.")___		long currentTimestamp = System.currentTimeMillis()__		checkArgument(startupOffsetsTimestamp <= currentTimestamp,_			"Startup time[%s] must be before current time[%s].", startupOffsetsTimestamp, currentTimestamp)___		this.startupMode = StartupMode.TIMESTAMP__		this.startupOffsetsTimestamp = startupOffsetsTimestamp__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,a,specified,timestamp,the,specified,timestamp,must,be,before,the,current,timestamp,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,the,consumer,will,look,up,the,earliest,offset,whose,timestamp,is,greater,than,or,equal,to,the,specific,timestamp,from,kafka,if,there,s,no,such,offset,the,consumer,will,use,the,latest,offset,to,read,data,from,kafka,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,param,startup,offsets,timestamp,timestamp,for,the,startup,offsets,as,milliseconds,from,epoch,return,the,consumer,object,to,allow,function,chaining;protected,flink,kafka,consumer,base,t,set,start,from,timestamp,long,startup,offsets,timestamp,check,argument,startup,offsets,timestamp,0,the,provided,value,for,the,startup,offsets,timestamp,is,invalid,long,current,timestamp,system,current,time,millis,check,argument,startup,offsets,timestamp,current,timestamp,startup,time,s,must,be,before,current,time,s,startup,offsets,timestamp,current,timestamp,this,startup,mode,startup,mode,timestamp,this,startup,offsets,timestamp,startup,offsets,timestamp,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp);1525116534;Specifies the consumer to start reading partitions from a specified timestamp._The specified timestamp must be before the current timestamp._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>The consumer will look up the earliest offset whose timestamp is greater than or equal_to the specific timestamp from Kafka. If there's no such offset, the consumer will use the_latest offset to read data from kafka.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@param startupOffsetsTimestamp timestamp for the startup offsets, as milliseconds from epoch.__@return The consumer object, to allow function chaining.;protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp) {_		checkArgument(startupOffsetsTimestamp >= 0, "The provided value for the startup offsets timestamp is invalid.")___		long currentTimestamp = System.currentTimeMillis()__		checkArgument(startupOffsetsTimestamp <= currentTimestamp,_			"Startup time[%s] must be before current time[%s].", startupOffsetsTimestamp, currentTimestamp)___		this.startupMode = StartupMode.TIMESTAMP__		this.startupOffsetsTimestamp = startupOffsetsTimestamp__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,a,specified,timestamp,the,specified,timestamp,must,be,before,the,current,timestamp,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,the,consumer,will,look,up,the,earliest,offset,whose,timestamp,is,greater,than,or,equal,to,the,specific,timestamp,from,kafka,if,there,s,no,such,offset,the,consumer,will,use,the,latest,offset,to,read,data,from,kafka,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,param,startup,offsets,timestamp,timestamp,for,the,startup,offsets,as,milliseconds,from,epoch,return,the,consumer,object,to,allow,function,chaining;protected,flink,kafka,consumer,base,t,set,start,from,timestamp,long,startup,offsets,timestamp,check,argument,startup,offsets,timestamp,0,the,provided,value,for,the,startup,offsets,timestamp,is,invalid,long,current,timestamp,system,current,time,millis,check,argument,startup,offsets,timestamp,current,timestamp,startup,time,s,must,be,before,current,time,s,startup,offsets,timestamp,current,timestamp,this,startup,mode,startup,mode,timestamp,this,startup,offsets,timestamp,startup,offsets,timestamp,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp);1542613709;Specifies the consumer to start reading partitions from a specified timestamp._The specified timestamp must be before the current timestamp._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>The consumer will look up the earliest offset whose timestamp is greater than or equal_to the specific timestamp from Kafka. If there's no such offset, the consumer will use the_latest offset to read data from kafka.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@param startupOffsetsTimestamp timestamp for the startup offsets, as milliseconds from epoch.__@return The consumer object, to allow function chaining.;protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp) {_		checkArgument(startupOffsetsTimestamp >= 0, "The provided value for the startup offsets timestamp is invalid.")___		long currentTimestamp = System.currentTimeMillis()__		checkArgument(startupOffsetsTimestamp <= currentTimestamp,_			"Startup time[%s] must be before current time[%s].", startupOffsetsTimestamp, currentTimestamp)___		this.startupMode = StartupMode.TIMESTAMP__		this.startupOffsetsTimestamp = startupOffsetsTimestamp__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,a,specified,timestamp,the,specified,timestamp,must,be,before,the,current,timestamp,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,the,consumer,will,look,up,the,earliest,offset,whose,timestamp,is,greater,than,or,equal,to,the,specific,timestamp,from,kafka,if,there,s,no,such,offset,the,consumer,will,use,the,latest,offset,to,read,data,from,kafka,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,param,startup,offsets,timestamp,timestamp,for,the,startup,offsets,as,milliseconds,from,epoch,return,the,consumer,object,to,allow,function,chaining;protected,flink,kafka,consumer,base,t,set,start,from,timestamp,long,startup,offsets,timestamp,check,argument,startup,offsets,timestamp,0,the,provided,value,for,the,startup,offsets,timestamp,is,invalid,long,current,timestamp,system,current,time,millis,check,argument,startup,offsets,timestamp,current,timestamp,startup,time,s,must,be,before,current,time,s,startup,offsets,timestamp,current,timestamp,this,startup,mode,startup,mode,timestamp,this,startup,offsets,timestamp,startup,offsets,timestamp,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp);1548931626;Specifies the consumer to start reading partitions from a specified timestamp._The specified timestamp must be before the current timestamp._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>The consumer will look up the earliest offset whose timestamp is greater than or equal_to the specific timestamp from Kafka. If there's no such offset, the consumer will use the_latest offset to read data from kafka.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@param startupOffsetsTimestamp timestamp for the startup offsets, as milliseconds from epoch.__@return The consumer object, to allow function chaining.;protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp) {_		checkArgument(startupOffsetsTimestamp >= 0, "The provided value for the startup offsets timestamp is invalid.")___		long currentTimestamp = System.currentTimeMillis()__		checkArgument(startupOffsetsTimestamp <= currentTimestamp,_			"Startup time[%s] must be before current time[%s].", startupOffsetsTimestamp, currentTimestamp)___		this.startupMode = StartupMode.TIMESTAMP__		this.startupOffsetsTimestamp = startupOffsetsTimestamp__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,a,specified,timestamp,the,specified,timestamp,must,be,before,the,current,timestamp,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,the,consumer,will,look,up,the,earliest,offset,whose,timestamp,is,greater,than,or,equal,to,the,specific,timestamp,from,kafka,if,there,s,no,such,offset,the,consumer,will,use,the,latest,offset,to,read,data,from,kafka,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,param,startup,offsets,timestamp,timestamp,for,the,startup,offsets,as,milliseconds,from,epoch,return,the,consumer,object,to,allow,function,chaining;protected,flink,kafka,consumer,base,t,set,start,from,timestamp,long,startup,offsets,timestamp,check,argument,startup,offsets,timestamp,0,the,provided,value,for,the,startup,offsets,timestamp,is,invalid,long,current,timestamp,system,current,time,millis,check,argument,startup,offsets,timestamp,current,timestamp,startup,time,s,must,be,before,current,time,s,startup,offsets,timestamp,current,timestamp,this,startup,mode,startup,mode,timestamp,this,startup,offsets,timestamp,startup,offsets,timestamp,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp);1550834388;Specifies the consumer to start reading partitions from a specified timestamp._The specified timestamp must be before the current timestamp._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>The consumer will look up the earliest offset whose timestamp is greater than or equal_to the specific timestamp from Kafka. If there's no such offset, the consumer will use the_latest offset to read data from kafka.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@param startupOffsetsTimestamp timestamp for the startup offsets, as milliseconds from epoch.__@return The consumer object, to allow function chaining.;protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp) {_		checkArgument(startupOffsetsTimestamp >= 0, "The provided value for the startup offsets timestamp is invalid.")___		long currentTimestamp = System.currentTimeMillis()__		checkArgument(startupOffsetsTimestamp <= currentTimestamp,_			"Startup time[%s] must be before current time[%s].", startupOffsetsTimestamp, currentTimestamp)___		this.startupMode = StartupMode.TIMESTAMP__		this.startupOffsetsTimestamp = startupOffsetsTimestamp__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,a,specified,timestamp,the,specified,timestamp,must,be,before,the,current,timestamp,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,the,consumer,will,look,up,the,earliest,offset,whose,timestamp,is,greater,than,or,equal,to,the,specific,timestamp,from,kafka,if,there,s,no,such,offset,the,consumer,will,use,the,latest,offset,to,read,data,from,kafka,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,param,startup,offsets,timestamp,timestamp,for,the,startup,offsets,as,milliseconds,from,epoch,return,the,consumer,object,to,allow,function,chaining;protected,flink,kafka,consumer,base,t,set,start,from,timestamp,long,startup,offsets,timestamp,check,argument,startup,offsets,timestamp,0,the,provided,value,for,the,startup,offsets,timestamp,is,invalid,long,current,timestamp,system,current,time,millis,check,argument,startup,offsets,timestamp,current,timestamp,startup,time,s,must,be,before,current,time,s,startup,offsets,timestamp,current,timestamp,this,startup,mode,startup,mode,timestamp,this,startup,offsets,timestamp,startup,offsets,timestamp,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp);1550834396;Specifies the consumer to start reading partitions from a specified timestamp._The specified timestamp must be before the current timestamp._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>The consumer will look up the earliest offset whose timestamp is greater than or equal_to the specific timestamp from Kafka. If there's no such offset, the consumer will use the_latest offset to read data from kafka.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@param startupOffsetsTimestamp timestamp for the startup offsets, as milliseconds from epoch.__@return The consumer object, to allow function chaining.;protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp) {_		checkArgument(startupOffsetsTimestamp >= 0, "The provided value for the startup offsets timestamp is invalid.")___		long currentTimestamp = System.currentTimeMillis()__		checkArgument(startupOffsetsTimestamp <= currentTimestamp,_			"Startup time[%s] must be before current time[%s].", startupOffsetsTimestamp, currentTimestamp)___		this.startupMode = StartupMode.TIMESTAMP__		this.startupOffsetsTimestamp = startupOffsetsTimestamp__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,a,specified,timestamp,the,specified,timestamp,must,be,before,the,current,timestamp,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,the,consumer,will,look,up,the,earliest,offset,whose,timestamp,is,greater,than,or,equal,to,the,specific,timestamp,from,kafka,if,there,s,no,such,offset,the,consumer,will,use,the,latest,offset,to,read,data,from,kafka,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,param,startup,offsets,timestamp,timestamp,for,the,startup,offsets,as,milliseconds,from,epoch,return,the,consumer,object,to,allow,function,chaining;protected,flink,kafka,consumer,base,t,set,start,from,timestamp,long,startup,offsets,timestamp,check,argument,startup,offsets,timestamp,0,the,provided,value,for,the,startup,offsets,timestamp,is,invalid,long,current,timestamp,system,current,time,millis,check,argument,startup,offsets,timestamp,current,timestamp,startup,time,s,must,be,before,current,time,s,startup,offsets,timestamp,current,timestamp,this,startup,mode,startup,mode,timestamp,this,startup,offsets,timestamp,startup,offsets,timestamp,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp);1550834396;Specifies the consumer to start reading partitions from a specified timestamp._The specified timestamp must be before the current timestamp._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>The consumer will look up the earliest offset whose timestamp is greater than or equal_to the specific timestamp from Kafka. If there's no such offset, the consumer will use the_latest offset to read data from kafka.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@param startupOffsetsTimestamp timestamp for the startup offsets, as milliseconds from epoch.__@return The consumer object, to allow function chaining.;protected FlinkKafkaConsumerBase<T> setStartFromTimestamp(long startupOffsetsTimestamp) {_		checkArgument(startupOffsetsTimestamp >= 0, "The provided value for the startup offsets timestamp is invalid.")___		long currentTimestamp = System.currentTimeMillis()__		checkArgument(startupOffsetsTimestamp <= currentTimestamp,_			"Startup time[%s] must be before current time[%s].", startupOffsetsTimestamp, currentTimestamp)___		this.startupMode = StartupMode.TIMESTAMP__		this.startupOffsetsTimestamp = startupOffsetsTimestamp__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,partitions,from,a,specified,timestamp,the,specified,timestamp,must,be,before,the,current,timestamp,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,the,consumer,will,look,up,the,earliest,offset,whose,timestamp,is,greater,than,or,equal,to,the,specific,timestamp,from,kafka,if,there,s,no,such,offset,the,consumer,will,use,the,latest,offset,to,read,data,from,kafka,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,param,startup,offsets,timestamp,timestamp,for,the,startup,offsets,as,milliseconds,from,epoch,return,the,consumer,object,to,allow,function,chaining;protected,flink,kafka,consumer,base,t,set,start,from,timestamp,long,startup,offsets,timestamp,check,argument,startup,offsets,timestamp,0,the,provided,value,for,the,startup,offsets,timestamp,is,invalid,long,current,timestamp,system,current,time,millis,check,argument,startup,offsets,timestamp,current,timestamp,startup,time,s,must,be,before,current,time,s,startup,offsets,timestamp,current,timestamp,this,startup,mode,startup,mode,timestamp,this,startup,offsets,timestamp,startup,offsets,timestamp,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1480685315;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1482244974;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1482244974;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1487173364;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1488214488;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1489419493;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1489510697;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1489764760;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1491500150;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1493821466;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1494830990;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")__		_		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}_		_		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos);1495923077;Logs the partition information in INFO level.__@param logger The logger to log to._@param partitionInfos List of subscribed partitions;protected static void logPartitionInfo(Logger logger, List<KafkaTopicPartition> partitionInfos) {_		Map<String, Integer> countPerTopic = new HashMap<>()__		for (KafkaTopicPartition partition : partitionInfos) {_			Integer count = countPerTopic.get(partition.getTopic())__			if (count == null) {_				count = 1__			} else {_				count++__			}_			countPerTopic.put(partition.getTopic(), count)__		}_		StringBuilder sb = new StringBuilder(_				"Consumer is going to read the following topics (with number of partitions): ")___		for (Map.Entry<String, Integer> e : countPerTopic.entrySet()) {_			sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ")__		}__		logger.info(sb.toString())__	};logs,the,partition,information,in,info,level,param,logger,the,logger,to,log,to,param,partition,infos,list,of,subscribed,partitions;protected,static,void,log,partition,info,logger,logger,list,kafka,topic,partition,partition,infos,map,string,integer,count,per,topic,new,hash,map,for,kafka,topic,partition,partition,partition,infos,integer,count,count,per,topic,get,partition,get,topic,if,count,null,count,1,else,count,count,per,topic,put,partition,get,topic,count,string,builder,sb,new,string,builder,consumer,is,going,to,read,the,following,topics,with,number,of,partitions,for,map,entry,string,integer,e,count,per,topic,entry,set,sb,append,e,get,key,append,append,e,get,value,append,logger,info,sb,to,string
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1487173364;Specifies the consumer to start reading from the latest offset for all partitions._This ignores any committed group offsets in Zookeeper / Kafka brokers.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,ignores,any,committed,group,offsets,in,zookeeper,kafka,brokers,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1488214488;Specifies the consumer to start reading from the latest offset for all partitions._This ignores any committed group offsets in Zookeeper / Kafka brokers.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,ignores,any,committed,group,offsets,in,zookeeper,kafka,brokers,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1489419493;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1489510697;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1489764760;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1491500150;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1493821466;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1494830990;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1495923077;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1498894422;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1501249949;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1501249949;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1501249950;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1503598628;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1509013576;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1512405587;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1515757408;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1515757408;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1515757408;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1515757409;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1517943538;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1519658056;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1519973085;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not effect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,effect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1519973085;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1520598638;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1525116534;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1542613709;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1548931626;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1550834388;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1550834396;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> setStartFromLatest();1550834396;Specifies the consumer to start reading from the latest offset for all partitions._This lets the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.__<p>This method does not affect where partitions are read from when the consumer is restored_from a checkpoint or savepoint. When the consumer is restored from a checkpoint or_savepoint, only the offsets in the restored state will be used.__@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> setStartFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.startupOffsetsTimestamp = null__		this.specificStartupOffsets = null__		return this__	};specifies,the,consumer,to,start,reading,from,the,latest,offset,for,all,partitions,this,lets,the,consumer,ignore,any,committed,group,offsets,in,zookeeper,kafka,brokers,p,this,method,does,not,affect,where,partitions,are,read,from,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,when,the,consumer,is,restored,from,a,checkpoint,or,savepoint,only,the,offsets,in,the,restored,state,will,be,used,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,set,start,from,latest,this,startup,mode,startup,mode,latest,this,startup,offsets,timestamp,null,this,specific,startup,offsets,null,return,this
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1480685315;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1482244974;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1482244974;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1487173364;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1488214488;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1489419493;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1489510697;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1489764760;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1491500150;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1493821466;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1494830990;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer);1495923077;Base constructor.__@param deserializer_The deserializer to turn raw byte messages into Java/Scala objects.;public FlinkKafkaConsumerBase(List<String> topics, KeyedDeserializationSchema<T> deserializer) {_		this.topics = checkNotNull(topics)__		checkArgument(topics.size() > 0, "You have to define at least one topic.")__		this.deserializer = checkNotNull(deserializer, "valueDeserializer")__	};base,constructor,param,deserializer,the,deserializer,to,turn,raw,byte,messages,into,java,scala,objects;public,flink,kafka,consumer,base,list,string,topics,keyed,deserialization,schema,t,deserializer,this,topics,check,not,null,topics,check,argument,topics,size,0,you,have,to,define,at,least,one,topic,this,deserializer,check,not,null,deserializer,value,deserializer
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1480685315;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1482244974;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1482244974;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1487173364;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1488214488;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1489419493;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1489510697;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1489764760;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1491500150;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1493821466;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1494830990;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)__		_		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1495923077;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1498894422;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1501249949;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1501249949;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1501249950;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1503598628;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1509013576;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1512405587;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1515757408;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1515757408;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1515757408;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1515757409;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1517943538;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1519658056;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1519973085;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1519973085;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1520598638;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1525116534;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1542613709;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1548931626;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1550834388;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1550834396;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
FlinkKafkaConsumerBase -> public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner);1550834396;Specifies an {@link AssignerWithPunctuatedWatermarks} to emit watermarks in a punctuated manner._The watermark extractor will run per Kafka partition, watermarks will be merged across partitions_in the same way as in the Flink runtime, when streams are merged.__<p>When a subtask of a FlinkKafkaConsumer source reads multiple Kafka partitions,_the streams from the partitions are unioned in a "first come first serve" fashion. Per-partition_characteristics are usually lost that way. For example, if the timestamps are strictly ascending_per Kafka partition, they will not be strictly ascending in the resulting Flink DataStream, if the_parallel source subtask reads more that one partition.__<p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka_partition, allows users to let them exploit the per-partition characteristics.__<p>Note: One can use either an {@link AssignerWithPunctuatedWatermarks} or an_{@link AssignerWithPeriodicWatermarks}, not both at the same time.__@param assigner The timestamp assigner / watermark generator to use._@return The consumer object, to allow function chaining.;public FlinkKafkaConsumerBase<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> assigner) {_		checkNotNull(assigner)___		if (this.punctuatedWatermarkAssigner != null) {_			throw new IllegalStateException("A punctuated watermark emitter has already been set.")__		}_		try {_			ClosureCleaner.clean(assigner, true)__			this.periodicWatermarkAssigner = new SerializedValue<>(assigner)__			return this__		} catch (Exception e) {_			throw new IllegalArgumentException("The given assigner is not serializable", e)__		}_	};specifies,an,link,assigner,with,punctuated,watermarks,to,emit,watermarks,in,a,punctuated,manner,the,watermark,extractor,will,run,per,kafka,partition,watermarks,will,be,merged,across,partitions,in,the,same,way,as,in,the,flink,runtime,when,streams,are,merged,p,when,a,subtask,of,a,flink,kafka,consumer,source,reads,multiple,kafka,partitions,the,streams,from,the,partitions,are,unioned,in,a,first,come,first,serve,fashion,per,partition,characteristics,are,usually,lost,that,way,for,example,if,the,timestamps,are,strictly,ascending,per,kafka,partition,they,will,not,be,strictly,ascending,in,the,resulting,flink,data,stream,if,the,parallel,source,subtask,reads,more,that,one,partition,p,running,timestamp,extractors,watermark,generators,directly,inside,the,kafka,source,per,kafka,partition,allows,users,to,let,them,exploit,the,per,partition,characteristics,p,note,one,can,use,either,an,link,assigner,with,punctuated,watermarks,or,an,link,assigner,with,periodic,watermarks,not,both,at,the,same,time,param,assigner,the,timestamp,assigner,watermark,generator,to,use,return,the,consumer,object,to,allow,function,chaining;public,flink,kafka,consumer,base,t,assign,timestamps,and,watermarks,assigner,with,periodic,watermarks,t,assigner,check,not,null,assigner,if,this,punctuated,watermark,assigner,null,throw,new,illegal,state,exception,a,punctuated,watermark,emitter,has,already,been,set,try,closure,cleaner,clean,assigner,true,this,periodic,watermark,assigner,new,serialized,value,assigner,return,this,catch,exception,e,throw,new,illegal,argument,exception,the,given,assigner,is,not,serializable,e
