# id;timestamp;commentText;codeText;commentWords;codeWords
FlinkKafkaProducerBase -> protected abstract void flush()_;1480685315;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1480691398;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1487783817;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1495175928;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1495175928;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1495175928;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1495923077;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1505994399;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1508317940;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1515757409;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1518770239;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> protected abstract void flush()_;1520440672;Flush pending records.;protected abstract void flush()_;flush,pending,records;protected,abstract,void,flush
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner);1495175928;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner.;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig__		this.flinkKafkaPartitioner = customPartitioner___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.topicPartitionsMap = new HashMap<>()__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,this,flink,kafka,partitioner,custom,partitioner,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,topic,partitions,map,new,hash,map
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner);1495175928;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner.;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig__		this.flinkKafkaPartitioner = customPartitioner___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.topicPartitionsMap = new HashMap<>()__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,this,flink,kafka,partitioner,custom,partitioner,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,topic,partitions,map,new,hash,map
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner);1495175928;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner.;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig__		this.flinkKafkaPartitioner = customPartitioner___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.topicPartitionsMap = new HashMap<>()__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,this,flink,kafka,partitioner,custom,partitioner,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,topic,partitions,map,new,hash,map
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner);1495923077;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner.;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig__		this.flinkKafkaPartitioner = customPartitioner___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.topicPartitionsMap = new HashMap<>()__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,this,flink,kafka,partitioner,custom,partitioner,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,topic,partitions,map,new,hash,map
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner);1505994399;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner.;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig__		this.flinkKafkaPartitioner = customPartitioner___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.topicPartitionsMap = new HashMap<>()__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,this,flink,kafka,partitioner,custom,partitioner,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,topic,partitions,map,new,hash,map
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner);1508317940;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner.;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig__		this.flinkKafkaPartitioner = customPartitioner___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.topicPartitionsMap = new HashMap<>()__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,this,flink,kafka,partitioner,custom,partitioner,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,topic,partitions,map,new,hash,map
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner);1515757409;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner.;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig__		this.flinkKafkaPartitioner = customPartitioner___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.topicPartitionsMap = new HashMap<>()__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,this,flink,kafka,partitioner,custom,partitioner,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,topic,partitions,map,new,hash,map
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner);1518770239;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner.;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig__		this.flinkKafkaPartitioner = customPartitioner___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.topicPartitionsMap = new HashMap<>()__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,this,flink,kafka,partitioner,custom,partitioner,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,topic,partitions,map,new,hash,map
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner);1520440672;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner.;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, FlinkKafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig__		this.flinkKafkaPartitioner = customPartitioner___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.topicPartitionsMap = new HashMap<>()__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,flink,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,this,flink,kafka,partitioner,custom,partitioner,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,topic,partitions,map,new,hash,map
FlinkKafkaProducerBase -> @VisibleForTesting 	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props);1495923077;Used for testing only.;@VisibleForTesting_	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;visible,for,testing,protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> @VisibleForTesting 	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props);1505994399;Used for testing only.;@VisibleForTesting_	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;visible,for,testing,protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> @VisibleForTesting 	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props);1508317940;Used for testing only.;@VisibleForTesting_	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;visible,for,testing,protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> @VisibleForTesting 	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props);1515757409;Used for testing only.;@VisibleForTesting_	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;visible,for,testing,protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> @VisibleForTesting 	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props);1518770239;Used for testing only.;@VisibleForTesting_	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;visible,for,testing,protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> @VisibleForTesting 	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props);1520440672;Used for testing only.;@VisibleForTesting_	protected <K, V> KafkaProducer<K, V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;visible,for,testing,protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1480685315;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		_		List<PartitionInfo> partitionsList = new ArrayList<>(producer.partitionsFor(defaultTopicId))___		_		Collections.sort(partitionsList, new Comparator<PartitionInfo>() {_			@Override_			public int compare(PartitionInfo o1, PartitionInfo o2) {_				return Integer.compare(o1.partition(), o2.partition())__			}_		})___		partitions = new int[partitionsList.size()]__		for (int i = 0_ i < partitions.length_ i++) {_			partitions[i] = partitionsList.get(i).partition()__		}__		RuntimeContext ctx = getRuntimeContext()__		if (partitioner != null) {_			partitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks(), partitions)__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into topic {}", _				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext)this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,list,partition,info,partitions,list,new,array,list,producer,partitions,for,default,topic,id,collections,sort,partitions,list,new,comparator,partition,info,override,public,int,compare,partition,info,o1,partition,info,o2,return,integer,compare,o1,partition,o2,partition,partitions,new,int,partitions,list,size,for,int,i,0,i,partitions,length,i,partitions,i,partitions,list,get,i,partition,runtime,context,ctx,get,runtime,context,if,partitioner,null,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,partitions,log,info,starting,flink,kafka,producer,to,produce,into,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1480691398;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()__		if (partitioner != null) {_			_			List<PartitionInfo> partitionsList = new ArrayList<>(producer.partitionsFor(defaultTopicId))___			_			Collections.sort(partitionsList, new Comparator<PartitionInfo>() {_				@Override_				public int compare(PartitionInfo o1, PartitionInfo o2) {_					return Integer.compare(o1.partition(), o2.partition())__				}_			})___			partitions = new int[partitionsList.size()]__			for (int i = 0_ i < partitions.length_ i++) {_				partitions[i] = partitionsList.get(i).partition()__			}__			partitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks(), partitions)__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into topic {}", _				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext)this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,partitioner,null,list,partition,info,partitions,list,new,array,list,producer,partitions,for,default,topic,id,collections,sort,partitions,list,new,comparator,partition,info,override,public,int,compare,partition,info,o1,partition,info,o2,return,integer,compare,o1,partition,o2,partition,partitions,new,int,partitions,list,size,for,int,i,0,i,partitions,length,i,partitions,i,partitions,list,get,i,partition,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,partitions,log,info,starting,flink,kafka,producer,to,produce,into,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1487783817;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()__		if (partitioner != null) {_			_			List<PartitionInfo> partitionsList = new ArrayList<>(producer.partitionsFor(defaultTopicId))___			_			Collections.sort(partitionsList, new Comparator<PartitionInfo>() {_				@Override_				public int compare(PartitionInfo o1, PartitionInfo o2) {_					return Integer.compare(o1.partition(), o2.partition())__				}_			})___			partitions = new int[partitionsList.size()]__			for (int i = 0_ i < partitions.length_ i++) {_				partitions[i] = partitionsList.get(i).partition()__			}__			partitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks(), partitions)__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into topic {}", _				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext)this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,partitioner,null,list,partition,info,partitions,list,new,array,list,producer,partitions,for,default,topic,id,collections,sort,partitions,list,new,comparator,partition,info,override,public,int,compare,partition,info,o1,partition,info,o2,return,integer,compare,o1,partition,o2,partition,partitions,new,int,partitions,list,size,for,int,i,0,i,partitions,length,i,partitions,i,partitions,list,get,i,partition,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,partitions,log,info,starting,flink,kafka,producer,to,produce,into,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1495175928;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()__		if(null != flinkKafkaPartitioner) {_			if(flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {_				((FlinkKafkaDelegatePartitioner)flinkKafkaPartitioner).setPartitions(getPartitionsByTopic(this.defaultTopicId, this.producer))__			}_			flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks())__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}",_				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext)this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,null,flink,kafka,partitioner,if,flink,kafka,partitioner,instanceof,flink,kafka,delegate,partitioner,flink,kafka,delegate,partitioner,flink,kafka,partitioner,set,partitions,get,partitions,by,topic,this,default,topic,id,this,producer,flink,kafka,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,log,info,starting,flink,kafka,producer,to,produce,into,default,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1495175928;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()___		if(null != flinkKafkaPartitioner) {_			if(flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {_				((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(_						getPartitionsByTopic(this.defaultTopicId, this.producer))__			}_			flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks())__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}",_				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext)this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,null,flink,kafka,partitioner,if,flink,kafka,partitioner,instanceof,flink,kafka,delegate,partitioner,flink,kafka,delegate,partitioner,flink,kafka,partitioner,set,partitions,get,partitions,by,topic,this,default,topic,id,this,producer,flink,kafka,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,log,info,starting,flink,kafka,producer,to,produce,into,default,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1495175928;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()___		if(null != flinkKafkaPartitioner) {_			if(flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {_				((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(_						getPartitionsByTopic(this.defaultTopicId, this.producer))__			}_			flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks())__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}",_				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext)this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,null,flink,kafka,partitioner,if,flink,kafka,partitioner,instanceof,flink,kafka,delegate,partitioner,flink,kafka,delegate,partitioner,flink,kafka,partitioner,set,partitions,get,partitions,by,topic,this,default,topic,id,this,producer,flink,kafka,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,log,info,starting,flink,kafka,producer,to,produce,into,default,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1495923077;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()___		if (null != flinkKafkaPartitioner) {_			if (flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {_				((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(_						getPartitionsByTopic(this.defaultTopicId, this.producer))__			}_			flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks())__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}",_				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext) this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,null,flink,kafka,partitioner,if,flink,kafka,partitioner,instanceof,flink,kafka,delegate,partitioner,flink,kafka,delegate,partitioner,flink,kafka,partitioner,set,partitions,get,partitions,by,topic,this,default,topic,id,this,producer,flink,kafka,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,log,info,starting,flink,kafka,producer,to,produce,into,default,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1505994399;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()___		if (null != flinkKafkaPartitioner) {_			if (flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {_				((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(_						getPartitionsByTopic(this.defaultTopicId, this.producer))__			}_			flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks())__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}",_				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext) this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,null,flink,kafka,partitioner,if,flink,kafka,partitioner,instanceof,flink,kafka,delegate,partitioner,flink,kafka,delegate,partitioner,flink,kafka,partitioner,set,partitions,get,partitions,by,topic,this,default,topic,id,this,producer,flink,kafka,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,log,info,starting,flink,kafka,producer,to,produce,into,default,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1508317940;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()___		if (null != flinkKafkaPartitioner) {_			if (flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {_				((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(_						getPartitionsByTopic(this.defaultTopicId, this.producer))__			}_			flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks())__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}",_				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext) this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,null,flink,kafka,partitioner,if,flink,kafka,partitioner,instanceof,flink,kafka,delegate,partitioner,flink,kafka,delegate,partitioner,flink,kafka,partitioner,set,partitions,get,partitions,by,topic,this,default,topic,id,this,producer,flink,kafka,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,log,info,starting,flink,kafka,producer,to,produce,into,default,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1515757409;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()___		if (null != flinkKafkaPartitioner) {_			if (flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {_				((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(_						getPartitionsByTopic(this.defaultTopicId, this.producer))__			}_			flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks())__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}",_				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext) this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,null,flink,kafka,partitioner,if,flink,kafka,partitioner,instanceof,flink,kafka,delegate,partitioner,flink,kafka,delegate,partitioner,flink,kafka,partitioner,set,partitions,get,partitions,by,topic,this,default,topic,id,this,producer,flink,kafka,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,log,info,starting,flink,kafka,producer,to,produce,into,default,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1518770239;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()___		if (null != flinkKafkaPartitioner) {_			if (flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {_				((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(_						getPartitionsByTopic(this.defaultTopicId, this.producer))__			}_			flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks())__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}",_				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext) this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,null,flink,kafka,partitioner,if,flink,kafka,partitioner,instanceof,flink,kafka,delegate,partitioner,flink,kafka,delegate,partitioner,flink,kafka,partitioner,set,partitions,get,partitions,by,topic,this,default,topic,id,this,producer,flink,kafka,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,log,info,starting,flink,kafka,producer,to,produce,into,default,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> @Override 	public void open(Configuration configuration);1520440672;Initializes the connection to Kafka.;@Override_	public void open(Configuration configuration) {_		producer = getKafkaProducer(this.producerConfig)___		RuntimeContext ctx = getRuntimeContext()___		if (null != flinkKafkaPartitioner) {_			if (flinkKafkaPartitioner instanceof FlinkKafkaDelegatePartitioner) {_				((FlinkKafkaDelegatePartitioner) flinkKafkaPartitioner).setPartitions(_						getPartitionsByTopic(this.defaultTopicId, this.producer))__			}_			flinkKafkaPartitioner.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks())__		}__		LOG.info("Starting FlinkKafkaProducer ({}/{}) to produce into default topic {}",_				ctx.getIndexOfThisSubtask() + 1, ctx.getNumberOfParallelSubtasks(), defaultTopicId)___		_		if (!Boolean.parseBoolean(producerConfig.getProperty(KEY_DISABLE_METRICS, "false"))) {_			Map<MetricName, ? extends Metric> metrics = this.producer.metrics()___			if (metrics == null) {_				_				LOG.info("Producer implementation does not support metrics")__			} else {_				final MetricGroup kafkaMetricGroup = getRuntimeContext().getMetricGroup().addGroup("KafkaProducer")__				for (Map.Entry<MetricName, ? extends Metric> metric: metrics.entrySet()) {_					kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()))__				}_			}_		}__		if (flushOnCheckpoint && !((StreamingRuntimeContext) this.getRuntimeContext()).isCheckpointingEnabled()) {_			LOG.warn("Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.")__			flushOnCheckpoint = false__		}__		if (logFailuresOnly) {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception e) {_					if (e != null) {_						LOG.error("Error while sending record to Kafka: " + e.getMessage(), e)__					}_					acknowledgeMessage()__				}_			}__		}_		else {_			callback = new Callback() {_				@Override_				public void onCompletion(RecordMetadata metadata, Exception exception) {_					if (exception != null && asyncException == null) {_						asyncException = exception__					}_					acknowledgeMessage()__				}_			}__		}_	};initializes,the,connection,to,kafka;override,public,void,open,configuration,configuration,producer,get,kafka,producer,this,producer,config,runtime,context,ctx,get,runtime,context,if,null,flink,kafka,partitioner,if,flink,kafka,partitioner,instanceof,flink,kafka,delegate,partitioner,flink,kafka,delegate,partitioner,flink,kafka,partitioner,set,partitions,get,partitions,by,topic,this,default,topic,id,this,producer,flink,kafka,partitioner,open,ctx,get,index,of,this,subtask,ctx,get,number,of,parallel,subtasks,log,info,starting,flink,kafka,producer,to,produce,into,default,topic,ctx,get,index,of,this,subtask,1,ctx,get,number,of,parallel,subtasks,default,topic,id,if,boolean,parse,boolean,producer,config,get,property,false,map,metric,name,extends,metric,metrics,this,producer,metrics,if,metrics,null,log,info,producer,implementation,does,not,support,metrics,else,final,metric,group,kafka,metric,group,get,runtime,context,get,metric,group,add,group,kafka,producer,for,map,entry,metric,name,extends,metric,metric,metrics,entry,set,kafka,metric,group,gauge,metric,get,key,name,new,kafka,metric,wrapper,metric,get,value,if,flush,on,checkpoint,streaming,runtime,context,this,get,runtime,context,is,checkpointing,enabled,log,warn,flushing,on,checkpoint,is,enabled,but,checkpointing,is,not,enabled,disabling,flushing,flush,on,checkpoint,false,if,log,failures,only,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,e,if,e,null,log,error,error,while,sending,record,to,kafka,e,get,message,e,acknowledge,message,else,callback,new,callback,override,public,void,on,completion,record,metadata,metadata,exception,exception,if,exception,null,async,exception,null,async,exception,exception,acknowledge,message
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, KafkaPartitioner<IN> customPartitioner);1480685315;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, KafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.partitioner = customPartitioner__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,partitioner,custom,partitioner
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, KafkaPartitioner<IN> customPartitioner);1480691398;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, KafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.partitioner = customPartitioner__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,partitioner,custom,partitioner
FlinkKafkaProducerBase -> public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, KafkaPartitioner<IN> customPartitioner);1487783817;The main constructor for creating a FlinkKafkaProducer.__@param defaultTopicId The default topic to write data to_@param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages_@param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument._@param customPartitioner A serializable partitioner for assigning messages to Kafka partitions. Passing null will use Kafka's partitioner;public FlinkKafkaProducerBase(String defaultTopicId, KeyedSerializationSchema<IN> serializationSchema, Properties producerConfig, KafkaPartitioner<IN> customPartitioner) {_		requireNonNull(defaultTopicId, "TopicID not set")__		requireNonNull(serializationSchema, "serializationSchema not set")__		requireNonNull(producerConfig, "producerConfig not set")__		ClosureCleaner.clean(customPartitioner, true)__		ClosureCleaner.ensureSerializable(serializationSchema)___		this.defaultTopicId = defaultTopicId__		this.schema = serializationSchema__		this.producerConfig = producerConfig___		_		if (!producerConfig.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)__		}__		if (!producerConfig.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)) {_			this.producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getCanonicalName())__		} else {_			LOG.warn("Overwriting the '{}' is not recommended", ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)__		}__		_		if (!this.producerConfig.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)) {_			throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must be supplied in the producer config properties.")__		}__		this.partitioner = customPartitioner__	};the,main,constructor,for,creating,a,flink,kafka,producer,param,default,topic,id,the,default,topic,to,write,data,to,param,serialization,schema,a,serializable,serialization,schema,for,turning,user,objects,into,a,kafka,consumable,byte,supporting,key,value,messages,param,producer,config,configuration,properties,for,the,kafka,producer,bootstrap,servers,is,the,only,required,argument,param,custom,partitioner,a,serializable,partitioner,for,assigning,messages,to,kafka,partitions,passing,null,will,use,kafka,s,partitioner;public,flink,kafka,producer,base,string,default,topic,id,keyed,serialization,schema,in,serialization,schema,properties,producer,config,kafka,partitioner,in,custom,partitioner,require,non,null,default,topic,id,topic,id,not,set,require,non,null,serialization,schema,serialization,schema,not,set,require,non,null,producer,config,producer,config,not,set,closure,cleaner,clean,custom,partitioner,true,closure,cleaner,ensure,serializable,serialization,schema,this,default,topic,id,default,topic,id,this,schema,serialization,schema,this,producer,config,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,producer,config,contains,key,producer,config,this,producer,config,put,producer,config,byte,array,serializer,class,get,canonical,name,else,log,warn,overwriting,the,is,not,recommended,producer,config,if,this,producer,config,contains,key,producer,config,throw,new,illegal,argument,exception,producer,config,must,be,supplied,in,the,producer,config,properties,this,partitioner,custom,partitioner
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1480685315;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1480691398;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1487783817;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1495175928;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1495175928;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1495175928;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1495923077;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1505994399;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1508317940;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1515757409;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1518770239;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> public void setFlushOnCheckpoint(boolean flush);1520440672;If set to true, the Flink producer will wait for all outstanding messages in the Kafka buffers_to be acknowledged by the Kafka producer on a checkpoint._This way, the producer can guarantee that messages in the Kafka buffers are part of the checkpoint.__@param flush Flag indicating the flushing mode (true = flush on checkpoint);public void setFlushOnCheckpoint(boolean flush) {_		this.flushOnCheckpoint = flush__	};if,set,to,true,the,flink,producer,will,wait,for,all,outstanding,messages,in,the,kafka,buffers,to,be,acknowledged,by,the,kafka,producer,on,a,checkpoint,this,way,the,producer,can,guarantee,that,messages,in,the,kafka,buffers,are,part,of,the,checkpoint,param,flush,flag,indicating,the,flushing,mode,true,flush,on,checkpoint;public,void,set,flush,on,checkpoint,boolean,flush,this,flush,on,checkpoint,flush
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next, Context context) throws Exception;1505994399;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next, Context context) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		int[] partitions = this.topicPartitionsMap.get(targetTopic)__		if (null == partitions) {_			partitions = getPartitionsByTopic(targetTopic, producer)__			this.topicPartitionsMap.put(targetTopic, partitions)__		}__		ProducerRecord<byte[], byte[]> record__		if (flinkKafkaPartitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(_					targetTopic,_					flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),_					serializedKey,_					serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,context,context,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,int,partitions,this,topic,partitions,map,get,target,topic,if,null,partitions,partitions,get,partitions,by,topic,target,topic,producer,this,topic,partitions,map,put,target,topic,partitions,producer,record,byte,byte,record,if,flink,kafka,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,flink,kafka,partitioner,partition,next,serialized,key,serialized,value,target,topic,partitions,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next, Context context) throws Exception;1508317940;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next, Context context) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		int[] partitions = this.topicPartitionsMap.get(targetTopic)__		if (null == partitions) {_			partitions = getPartitionsByTopic(targetTopic, producer)__			this.topicPartitionsMap.put(targetTopic, partitions)__		}__		ProducerRecord<byte[], byte[]> record__		if (flinkKafkaPartitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(_					targetTopic,_					flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),_					serializedKey,_					serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,context,context,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,int,partitions,this,topic,partitions,map,get,target,topic,if,null,partitions,partitions,get,partitions,by,topic,target,topic,producer,this,topic,partitions,map,put,target,topic,partitions,producer,record,byte,byte,record,if,flink,kafka,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,flink,kafka,partitioner,partition,next,serialized,key,serialized,value,target,topic,partitions,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next, Context context) throws Exception;1515757409;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next, Context context) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		int[] partitions = this.topicPartitionsMap.get(targetTopic)__		if (null == partitions) {_			partitions = getPartitionsByTopic(targetTopic, producer)__			this.topicPartitionsMap.put(targetTopic, partitions)__		}__		ProducerRecord<byte[], byte[]> record__		if (flinkKafkaPartitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(_					targetTopic,_					flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),_					serializedKey,_					serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,context,context,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,int,partitions,this,topic,partitions,map,get,target,topic,if,null,partitions,partitions,get,partitions,by,topic,target,topic,producer,this,topic,partitions,map,put,target,topic,partitions,producer,record,byte,byte,record,if,flink,kafka,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,flink,kafka,partitioner,partition,next,serialized,key,serialized,value,target,topic,partitions,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next, Context context) throws Exception;1518770239;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next, Context context) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		int[] partitions = this.topicPartitionsMap.get(targetTopic)__		if (null == partitions) {_			partitions = getPartitionsByTopic(targetTopic, producer)__			this.topicPartitionsMap.put(targetTopic, partitions)__		}__		ProducerRecord<byte[], byte[]> record__		if (flinkKafkaPartitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(_					targetTopic,_					flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),_					serializedKey,_					serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,context,context,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,int,partitions,this,topic,partitions,map,get,target,topic,if,null,partitions,partitions,get,partitions,by,topic,target,topic,producer,this,topic,partitions,map,put,target,topic,partitions,producer,record,byte,byte,record,if,flink,kafka,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,flink,kafka,partitioner,partition,next,serialized,key,serialized,value,target,topic,partitions,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next, Context context) throws Exception;1520440672;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next, Context context) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		int[] partitions = this.topicPartitionsMap.get(targetTopic)__		if (null == partitions) {_			partitions = getPartitionsByTopic(targetTopic, producer)__			this.topicPartitionsMap.put(targetTopic, partitions)__		}__		ProducerRecord<byte[], byte[]> record__		if (flinkKafkaPartitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(_					targetTopic,_					flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),_					serializedKey,_					serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,context,context,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,int,partitions,this,topic,partitions,map,get,target,topic,if,null,partitions,partitions,get,partitions,by,topic,target,topic,producer,this,topic,partitions,map,put,target,topic,partitions,producer,record,byte,byte,record,if,flink,kafka,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,flink,kafka,partitioner,partition,next,serialized,key,serialized,value,target,topic,partitions,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next) throws Exception;1480685315;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		ProducerRecord<byte[], byte[]> record__		if (partitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(targetTopic, partitioner.partition(next, serializedKey, serializedValue, partitions.length), serializedKey, serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,producer,record,byte,byte,record,if,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,partitioner,partition,next,serialized,key,serialized,value,partitions,length,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next) throws Exception;1480691398;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		ProducerRecord<byte[], byte[]> record__		if (partitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(targetTopic, partitioner.partition(next, serializedKey, serializedValue, partitions.length), serializedKey, serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,producer,record,byte,byte,record,if,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,partitioner,partition,next,serialized,key,serialized,value,partitions,length,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next) throws Exception;1487783817;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		ProducerRecord<byte[], byte[]> record__		if (partitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(targetTopic, partitioner.partition(next, serializedKey, serializedValue, partitions.length), serializedKey, serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,producer,record,byte,byte,record,if,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,partitioner,partition,next,serialized,key,serialized,value,partitions,length,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next) throws Exception;1495175928;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		int[] partitions = this.topicPartitionsMap.get(targetTopic)__		if(null == partitions) {_			partitions = this.getPartitionsByTopic(targetTopic, producer)__			this.topicPartitionsMap.put(targetTopic, partitions)__		}__		ProducerRecord<byte[], byte[]> record__		if (flinkKafkaPartitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(targetTopic, flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions), serializedKey, serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,int,partitions,this,topic,partitions,map,get,target,topic,if,null,partitions,partitions,this,get,partitions,by,topic,target,topic,producer,this,topic,partitions,map,put,target,topic,partitions,producer,record,byte,byte,record,if,flink,kafka,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,flink,kafka,partitioner,partition,next,serialized,key,serialized,value,target,topic,partitions,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next) throws Exception;1495175928;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		int[] partitions = this.topicPartitionsMap.get(targetTopic)__		if(null == partitions) {_			partitions = getPartitionsByTopic(targetTopic, producer)__			this.topicPartitionsMap.put(targetTopic, partitions)__		}__		ProducerRecord<byte[], byte[]> record__		if (flinkKafkaPartitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(_					targetTopic,_					flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),_					serializedKey,_					serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,int,partitions,this,topic,partitions,map,get,target,topic,if,null,partitions,partitions,get,partitions,by,topic,target,topic,producer,this,topic,partitions,map,put,target,topic,partitions,producer,record,byte,byte,record,if,flink,kafka,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,flink,kafka,partitioner,partition,next,serialized,key,serialized,value,target,topic,partitions,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next) throws Exception;1495175928;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		int[] partitions = this.topicPartitionsMap.get(targetTopic)__		if(null == partitions) {_			partitions = getPartitionsByTopic(targetTopic, producer)__			this.topicPartitionsMap.put(targetTopic, partitions)__		}__		ProducerRecord<byte[], byte[]> record__		if (flinkKafkaPartitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(_					targetTopic,_					flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),_					serializedKey,_					serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,int,partitions,this,topic,partitions,map,get,target,topic,if,null,partitions,partitions,get,partitions,by,topic,target,topic,producer,this,topic,partitions,map,put,target,topic,partitions,producer,record,byte,byte,record,if,flink,kafka,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,flink,kafka,partitioner,partition,next,serialized,key,serialized,value,target,topic,partitions,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> @Override 	public void invoke(IN next) throws Exception;1495923077;Called when new data arrives to the sink, and forwards it to Kafka.__@param next_The incoming data;@Override_	public void invoke(IN next) throws Exception {_		_		checkErroneous()___		byte[] serializedKey = schema.serializeKey(next)__		byte[] serializedValue = schema.serializeValue(next)__		String targetTopic = schema.getTargetTopic(next)__		if (targetTopic == null) {_			targetTopic = defaultTopicId__		}__		int[] partitions = this.topicPartitionsMap.get(targetTopic)__		if (null == partitions) {_			partitions = getPartitionsByTopic(targetTopic, producer)__			this.topicPartitionsMap.put(targetTopic, partitions)__		}__		ProducerRecord<byte[], byte[]> record__		if (flinkKafkaPartitioner == null) {_			record = new ProducerRecord<>(targetTopic, serializedKey, serializedValue)__		} else {_			record = new ProducerRecord<>(_					targetTopic,_					flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),_					serializedKey,_					serializedValue)__		}_		if (flushOnCheckpoint) {_			synchronized (pendingRecordsLock) {_				pendingRecords++__			}_		}_		producer.send(record, callback)__	};called,when,new,data,arrives,to,the,sink,and,forwards,it,to,kafka,param,next,the,incoming,data;override,public,void,invoke,in,next,throws,exception,check,erroneous,byte,serialized,key,schema,serialize,key,next,byte,serialized,value,schema,serialize,value,next,string,target,topic,schema,get,target,topic,next,if,target,topic,null,target,topic,default,topic,id,int,partitions,this,topic,partitions,map,get,target,topic,if,null,partitions,partitions,get,partitions,by,topic,target,topic,producer,this,topic,partitions,map,put,target,topic,partitions,producer,record,byte,byte,record,if,flink,kafka,partitioner,null,record,new,producer,record,target,topic,serialized,key,serialized,value,else,record,new,producer,record,target,topic,flink,kafka,partitioner,partition,next,serialized,key,serialized,value,target,topic,partitions,serialized,key,serialized,value,if,flush,on,checkpoint,synchronized,pending,records,lock,pending,records,producer,send,record,callback
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1480685315;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1480691398;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1487783817;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1495175928;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1495175928;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1495175928;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1495923077;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1505994399;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1508317940;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1515757409;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1518770239;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> public void setLogFailuresOnly(boolean logFailuresOnly);1520440672;Defines whether the producer should fail on errors, or only log them._If this is set to true, then exceptions will be only logged, if set to false,_exceptions will be eventually thrown and cause the streaming program to_fail (and enter recovery).__@param logFailuresOnly The flag to indicate logging-only on exceptions.;public void setLogFailuresOnly(boolean logFailuresOnly) {_		this.logFailuresOnly = logFailuresOnly__	};defines,whether,the,producer,should,fail,on,errors,or,only,log,them,if,this,is,set,to,true,then,exceptions,will,be,only,logged,if,set,to,false,exceptions,will,be,eventually,thrown,and,cause,the,streaming,program,to,fail,and,enter,recovery,param,log,failures,only,the,flag,to,indicate,logging,only,on,exceptions;public,void,set,log,failures,only,boolean,log,failures,only,this,log,failures,only,log,failures,only
FlinkKafkaProducerBase -> protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props);1480685315;Used for testing only;protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props);1480691398;Used for testing only;protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props);1487783817;Used for testing only;protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props);1495175928;Used for testing only;protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props);1495175928;Used for testing only;protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
FlinkKafkaProducerBase -> protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props);1495175928;Used for testing only;protected <K,V> KafkaProducer<K,V> getKafkaProducer(Properties props) {_		return new KafkaProducer<>(props)__	};used,for,testing,only;protected,k,v,kafka,producer,k,v,get,kafka,producer,properties,props,return,new,kafka,producer,props
