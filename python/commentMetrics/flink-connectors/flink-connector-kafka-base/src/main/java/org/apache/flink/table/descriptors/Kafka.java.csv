# id;timestamp;commentText;codeText;commentWords;codeWords
Kafka -> public Kafka startFromGroupOffsets();1519754754;Configures to start reading from any committed group offsets found in Zookeeper / Kafka brokers.__@see FlinkKafkaConsumerBase#setStartFromGroupOffsets();public Kafka startFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificOffsets = null__		return this__	};configures,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,see,flink,kafka,consumer,base,set,start,from,group,offsets;public,kafka,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,offsets,null,return,this
Kafka -> public Kafka startFromGroupOffsets();1533133404;Configures to start reading from any committed group offsets found in Zookeeper / Kafka brokers.__@see FlinkKafkaConsumerBase#setStartFromGroupOffsets();public Kafka startFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificOffsets = null__		return this__	};configures,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,see,flink,kafka,consumer,base,set,start,from,group,offsets;public,kafka,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,offsets,null,return,this
Kafka -> public Kafka startFromGroupOffsets();1540894120;Configures to start reading from any committed group offsets found in Zookeeper / Kafka brokers.__@see FlinkKafkaConsumerBase#setStartFromGroupOffsets();public Kafka startFromGroupOffsets() {_		this.startupMode = StartupMode.GROUP_OFFSETS__		this.specificOffsets = null__		return this__	};configures,to,start,reading,from,any,committed,group,offsets,found,in,zookeeper,kafka,brokers,see,flink,kafka,consumer,base,set,start,from,group,offsets;public,kafka,start,from,group,offsets,this,startup,mode,startup,mode,this,specific,offsets,null,return,this
Kafka -> public Kafka startFromSpecificOffset(int partition, long specificOffset);1519754754;Configures to start reading partitions from specific offsets and specifies the given offset for_the given partition.__@param partition partition index_@param specificOffset partition offset to start reading from_@see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map);public Kafka startFromSpecificOffset(int partition, long specificOffset) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		if (this.specificOffsets == null) {_			this.specificOffsets = new HashMap<>()__		}_		this.specificOffsets.put(partition, specificOffset)__		return this__	};configures,to,start,reading,partitions,from,specific,offsets,and,specifies,the,given,offset,for,the,given,partition,param,partition,partition,index,param,specific,offset,partition,offset,to,start,reading,from,see,flink,kafka,consumer,base,set,start,from,specific,offsets,map;public,kafka,start,from,specific,offset,int,partition,long,specific,offset,this,startup,mode,startup,mode,if,this,specific,offsets,null,this,specific,offsets,new,hash,map,this,specific,offsets,put,partition,specific,offset,return,this
Kafka -> public Kafka startFromSpecificOffset(int partition, long specificOffset);1533133404;Configures to start reading partitions from specific offsets and specifies the given offset for_the given partition.__@param partition partition index_@param specificOffset partition offset to start reading from_@see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map);public Kafka startFromSpecificOffset(int partition, long specificOffset) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		if (this.specificOffsets == null) {_			this.specificOffsets = new HashMap<>()__		}_		this.specificOffsets.put(partition, specificOffset)__		return this__	};configures,to,start,reading,partitions,from,specific,offsets,and,specifies,the,given,offset,for,the,given,partition,param,partition,partition,index,param,specific,offset,partition,offset,to,start,reading,from,see,flink,kafka,consumer,base,set,start,from,specific,offsets,map;public,kafka,start,from,specific,offset,int,partition,long,specific,offset,this,startup,mode,startup,mode,if,this,specific,offsets,null,this,specific,offsets,new,hash,map,this,specific,offsets,put,partition,specific,offset,return,this
Kafka -> public Kafka startFromSpecificOffset(int partition, long specificOffset);1540894120;Configures to start reading partitions from specific offsets and specifies the given offset for_the given partition.__@param partition partition index_@param specificOffset partition offset to start reading from_@see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map);public Kafka startFromSpecificOffset(int partition, long specificOffset) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		if (this.specificOffsets == null) {_			this.specificOffsets = new HashMap<>()__		}_		this.specificOffsets.put(partition, specificOffset)__		return this__	};configures,to,start,reading,partitions,from,specific,offsets,and,specifies,the,given,offset,for,the,given,partition,param,partition,partition,index,param,specific,offset,partition,offset,to,start,reading,from,see,flink,kafka,consumer,base,set,start,from,specific,offsets,map;public,kafka,start,from,specific,offset,int,partition,long,specific,offset,this,startup,mode,startup,mode,if,this,specific,offsets,null,this,specific,offsets,new,hash,map,this,specific,offsets,put,partition,specific,offset,return,this
Kafka -> public Kafka sinkPartitionerFixed();1533133404;Configures how to partition records from Flink's partitions into Kafka's partitions.__<p>This strategy ensures that each Flink partition ends up in one Kafka partition.__<p>Note: One Kafka partition can contain multiple Flink partitions. Examples:__<p>More Flink partitions than Kafka partitions. Some (or all) Kafka partitions contain_the output of more than one flink partition:_<pre>_Flink Sinks            Kafka Partitions_1    ----------------&gt_    1_2    --------------/_3    -------------/_4    ------------/_</pre>___<p>Fewer Flink partitions than Kafka partitions:_<pre>_Flink Sinks            Kafka Partitions_1    ----------------&gt_    1_2    ----------------&gt_    2____</pre>__@see org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner;public Kafka sinkPartitionerFixed() {_		sinkPartitionerType = CONNECTOR_SINK_PARTITIONER_VALUE_FIXED__		sinkPartitionerClass = null__		return this__	};configures,how,to,partition,records,from,flink,s,partitions,into,kafka,s,partitions,p,this,strategy,ensures,that,each,flink,partition,ends,up,in,one,kafka,partition,p,note,one,kafka,partition,can,contain,multiple,flink,partitions,examples,p,more,flink,partitions,than,kafka,partitions,some,or,all,kafka,partitions,contain,the,output,of,more,than,one,flink,partition,pre,flink,sinks,kafka,partitions,1,gt,1,2,3,4,pre,p,fewer,flink,partitions,than,kafka,partitions,pre,flink,sinks,kafka,partitions,1,gt,1,2,gt,2,pre,see,org,apache,flink,streaming,connectors,kafka,partitioner,flink,fixed,partitioner;public,kafka,sink,partitioner,fixed,sink,partitioner,type,sink,partitioner,class,null,return,this
Kafka -> public Kafka sinkPartitionerFixed();1540894120;Configures how to partition records from Flink's partitions into Kafka's partitions.__<p>This strategy ensures that each Flink partition ends up in one Kafka partition.__<p>Note: One Kafka partition can contain multiple Flink partitions. Examples:__<p>More Flink partitions than Kafka partitions. Some (or all) Kafka partitions contain_the output of more than one flink partition:_<pre>_Flink Sinks            Kafka Partitions_1    ----------------&gt_    1_2    --------------/_3    -------------/_4    ------------/_</pre>___<p>Fewer Flink partitions than Kafka partitions:_<pre>_Flink Sinks            Kafka Partitions_1    ----------------&gt_    1_2    ----------------&gt_    2____</pre>__@see org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner;public Kafka sinkPartitionerFixed() {_		sinkPartitionerType = CONNECTOR_SINK_PARTITIONER_VALUE_FIXED__		sinkPartitionerClass = null__		return this__	};configures,how,to,partition,records,from,flink,s,partitions,into,kafka,s,partitions,p,this,strategy,ensures,that,each,flink,partition,ends,up,in,one,kafka,partition,p,note,one,kafka,partition,can,contain,multiple,flink,partitions,examples,p,more,flink,partitions,than,kafka,partitions,some,or,all,kafka,partitions,contain,the,output,of,more,than,one,flink,partition,pre,flink,sinks,kafka,partitions,1,gt,1,2,3,4,pre,p,fewer,flink,partitions,than,kafka,partitions,pre,flink,sinks,kafka,partitions,1,gt,1,2,gt,2,pre,see,org,apache,flink,streaming,connectors,kafka,partitioner,flink,fixed,partitioner;public,kafka,sink,partitioner,fixed,sink,partitioner,type,sink,partitioner,class,null,return,this
Kafka -> public Kafka startFromEarliest();1519754754;Configures to start reading from the earliest offset for all partitions.__@see FlinkKafkaConsumerBase#setStartFromEarliest();public Kafka startFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificOffsets = null__		return this__	};configures,to,start,reading,from,the,earliest,offset,for,all,partitions,see,flink,kafka,consumer,base,set,start,from,earliest;public,kafka,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,offsets,null,return,this
Kafka -> public Kafka startFromEarliest();1533133404;Configures to start reading from the earliest offset for all partitions.__@see FlinkKafkaConsumerBase#setStartFromEarliest();public Kafka startFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificOffsets = null__		return this__	};configures,to,start,reading,from,the,earliest,offset,for,all,partitions,see,flink,kafka,consumer,base,set,start,from,earliest;public,kafka,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,offsets,null,return,this
Kafka -> public Kafka startFromEarliest();1540894120;Configures to start reading from the earliest offset for all partitions.__@see FlinkKafkaConsumerBase#setStartFromEarliest();public Kafka startFromEarliest() {_		this.startupMode = StartupMode.EARLIEST__		this.specificOffsets = null__		return this__	};configures,to,start,reading,from,the,earliest,offset,for,all,partitions,see,flink,kafka,consumer,base,set,start,from,earliest;public,kafka,start,from,earliest,this,startup,mode,startup,mode,earliest,this,specific,offsets,null,return,this
Kafka -> public Kafka startFromSpecificOffsets(Map<Integer, Long> specificOffsets);1519754754;Configures to start reading partitions from specific offsets, set independently for each partition._Resets previously set offsets.__@param specificOffsets the specified offsets for partitions_@see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map);public Kafka startFromSpecificOffsets(Map<Integer, Long> specificOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificOffsets = Preconditions.checkNotNull(specificOffsets)__		return this__	};configures,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,resets,previously,set,offsets,param,specific,offsets,the,specified,offsets,for,partitions,see,flink,kafka,consumer,base,set,start,from,specific,offsets,map;public,kafka,start,from,specific,offsets,map,integer,long,specific,offsets,this,startup,mode,startup,mode,this,specific,offsets,preconditions,check,not,null,specific,offsets,return,this
Kafka -> public Kafka startFromSpecificOffsets(Map<Integer, Long> specificOffsets);1533133404;Configures to start reading partitions from specific offsets, set independently for each partition._Resets previously set offsets.__@param specificOffsets the specified offsets for partitions_@see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map);public Kafka startFromSpecificOffsets(Map<Integer, Long> specificOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificOffsets = Preconditions.checkNotNull(specificOffsets)__		return this__	};configures,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,resets,previously,set,offsets,param,specific,offsets,the,specified,offsets,for,partitions,see,flink,kafka,consumer,base,set,start,from,specific,offsets,map;public,kafka,start,from,specific,offsets,map,integer,long,specific,offsets,this,startup,mode,startup,mode,this,specific,offsets,preconditions,check,not,null,specific,offsets,return,this
Kafka -> public Kafka startFromSpecificOffsets(Map<Integer, Long> specificOffsets);1540894120;Configures to start reading partitions from specific offsets, set independently for each partition._Resets previously set offsets.__@param specificOffsets the specified offsets for partitions_@see FlinkKafkaConsumerBase#setStartFromSpecificOffsets(Map);public Kafka startFromSpecificOffsets(Map<Integer, Long> specificOffsets) {_		this.startupMode = StartupMode.SPECIFIC_OFFSETS__		this.specificOffsets = Preconditions.checkNotNull(specificOffsets)__		return this__	};configures,to,start,reading,partitions,from,specific,offsets,set,independently,for,each,partition,resets,previously,set,offsets,param,specific,offsets,the,specified,offsets,for,partitions,see,flink,kafka,consumer,base,set,start,from,specific,offsets,map;public,kafka,start,from,specific,offsets,map,integer,long,specific,offsets,this,startup,mode,startup,mode,this,specific,offsets,preconditions,check,not,null,specific,offsets,return,this
Kafka -> public Kafka property(String key, String value);1519754754;Adds a configuration properties for the Kafka consumer.__@param key property key for the Kafka consumer_@param value property value for the Kafka consumer;public Kafka property(String key, String value) {_		Preconditions.checkNotNull(key)__		Preconditions.checkNotNull(value)__		if (this.kafkaProperties == null) {_			this.kafkaProperties = new HashMap<>()__		}_		kafkaProperties.put(key, value)__		return this__	};adds,a,configuration,properties,for,the,kafka,consumer,param,key,property,key,for,the,kafka,consumer,param,value,property,value,for,the,kafka,consumer;public,kafka,property,string,key,string,value,preconditions,check,not,null,key,preconditions,check,not,null,value,if,this,kafka,properties,null,this,kafka,properties,new,hash,map,kafka,properties,put,key,value,return,this
Kafka -> public Kafka property(String key, String value);1533133404;Adds a configuration properties for the Kafka consumer.__@param key property key for the Kafka consumer_@param value property value for the Kafka consumer;public Kafka property(String key, String value) {_		Preconditions.checkNotNull(key)__		Preconditions.checkNotNull(value)__		if (this.kafkaProperties == null) {_			this.kafkaProperties = new HashMap<>()__		}_		kafkaProperties.put(key, value)__		return this__	};adds,a,configuration,properties,for,the,kafka,consumer,param,key,property,key,for,the,kafka,consumer,param,value,property,value,for,the,kafka,consumer;public,kafka,property,string,key,string,value,preconditions,check,not,null,key,preconditions,check,not,null,value,if,this,kafka,properties,null,this,kafka,properties,new,hash,map,kafka,properties,put,key,value,return,this
Kafka -> public Kafka property(String key, String value);1540894120;Adds a configuration properties for the Kafka consumer.__@param key property key for the Kafka consumer_@param value property value for the Kafka consumer;public Kafka property(String key, String value) {_		Preconditions.checkNotNull(key)__		Preconditions.checkNotNull(value)__		if (this.kafkaProperties == null) {_			this.kafkaProperties = new HashMap<>()__		}_		kafkaProperties.put(key, value)__		return this__	};adds,a,configuration,properties,for,the,kafka,consumer,param,key,property,key,for,the,kafka,consumer,param,value,property,value,for,the,kafka,consumer;public,kafka,property,string,key,string,value,preconditions,check,not,null,key,preconditions,check,not,null,value,if,this,kafka,properties,null,this,kafka,properties,new,hash,map,kafka,properties,put,key,value,return,this
Kafka -> public Kafka startFromLatest();1519754754;Configures to start reading from the latest offset for all partitions.__@see FlinkKafkaConsumerBase#setStartFromLatest();public Kafka startFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificOffsets = null__		return this__	};configures,to,start,reading,from,the,latest,offset,for,all,partitions,see,flink,kafka,consumer,base,set,start,from,latest;public,kafka,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,offsets,null,return,this
Kafka -> public Kafka startFromLatest();1533133404;Configures to start reading from the latest offset for all partitions.__@see FlinkKafkaConsumerBase#setStartFromLatest();public Kafka startFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificOffsets = null__		return this__	};configures,to,start,reading,from,the,latest,offset,for,all,partitions,see,flink,kafka,consumer,base,set,start,from,latest;public,kafka,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,offsets,null,return,this
Kafka -> public Kafka startFromLatest();1540894120;Configures to start reading from the latest offset for all partitions.__@see FlinkKafkaConsumerBase#setStartFromLatest();public Kafka startFromLatest() {_		this.startupMode = StartupMode.LATEST__		this.specificOffsets = null__		return this__	};configures,to,start,reading,from,the,latest,offset,for,all,partitions,see,flink,kafka,consumer,base,set,start,from,latest;public,kafka,start,from,latest,this,startup,mode,startup,mode,latest,this,specific,offsets,null,return,this
Kafka -> public Kafka sinkPartitionerRoundRobin();1533133404;Configures how to partition records from Flink's partitions into Kafka's partitions.__<p>This strategy ensures that records will be distributed to Kafka partitions in a_round-robin fashion.__<p>Note: This strategy is useful to avoid an unbalanced partitioning. However, it will_cause a lot of network connections between all the Flink instances and all the Kafka brokers.;public Kafka sinkPartitionerRoundRobin() {_		sinkPartitionerType = CONNECTOR_SINK_PARTITIONER_VALUE_ROUND_ROBIN__		sinkPartitionerClass = null__		return this__	};configures,how,to,partition,records,from,flink,s,partitions,into,kafka,s,partitions,p,this,strategy,ensures,that,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,p,note,this,strategy,is,useful,to,avoid,an,unbalanced,partitioning,however,it,will,cause,a,lot,of,network,connections,between,all,the,flink,instances,and,all,the,kafka,brokers;public,kafka,sink,partitioner,round,robin,sink,partitioner,type,sink,partitioner,class,null,return,this
Kafka -> public Kafka sinkPartitionerRoundRobin();1540894120;Configures how to partition records from Flink's partitions into Kafka's partitions.__<p>This strategy ensures that records will be distributed to Kafka partitions in a_round-robin fashion.__<p>Note: This strategy is useful to avoid an unbalanced partitioning. However, it will_cause a lot of network connections between all the Flink instances and all the Kafka brokers.;public Kafka sinkPartitionerRoundRobin() {_		sinkPartitionerType = CONNECTOR_SINK_PARTITIONER_VALUE_ROUND_ROBIN__		sinkPartitionerClass = null__		return this__	};configures,how,to,partition,records,from,flink,s,partitions,into,kafka,s,partitions,p,this,strategy,ensures,that,records,will,be,distributed,to,kafka,partitions,in,a,round,robin,fashion,p,note,this,strategy,is,useful,to,avoid,an,unbalanced,partitioning,however,it,will,cause,a,lot,of,network,connections,between,all,the,flink,instances,and,all,the,kafka,brokers;public,kafka,sink,partitioner,round,robin,sink,partitioner,type,sink,partitioner,class,null,return,this
Kafka -> public Kafka sinkPartitionerCustom(Class<? extends FlinkKafkaPartitioner> partitionerClass);1533133404;Configures how to partition records from Flink's partitions into Kafka's partitions.__<p>This strategy allows for a custom partitioner by providing an implementation_of {@link FlinkKafkaPartitioner}.;public Kafka sinkPartitionerCustom(Class<? extends FlinkKafkaPartitioner> partitionerClass) {_		sinkPartitionerType = CONNECTOR_SINK_PARTITIONER_VALUE_CUSTOM__		sinkPartitionerClass = Preconditions.checkNotNull(partitionerClass)__		return this__	};configures,how,to,partition,records,from,flink,s,partitions,into,kafka,s,partitions,p,this,strategy,allows,for,a,custom,partitioner,by,providing,an,implementation,of,link,flink,kafka,partitioner;public,kafka,sink,partitioner,custom,class,extends,flink,kafka,partitioner,partitioner,class,sink,partitioner,type,sink,partitioner,class,preconditions,check,not,null,partitioner,class,return,this
Kafka -> public Kafka sinkPartitionerCustom(Class<? extends FlinkKafkaPartitioner> partitionerClass);1540894120;Configures how to partition records from Flink's partitions into Kafka's partitions.__<p>This strategy allows for a custom partitioner by providing an implementation_of {@link FlinkKafkaPartitioner}.;public Kafka sinkPartitionerCustom(Class<? extends FlinkKafkaPartitioner> partitionerClass) {_		sinkPartitionerType = CONNECTOR_SINK_PARTITIONER_VALUE_CUSTOM__		sinkPartitionerClass = Preconditions.checkNotNull(partitionerClass)__		return this__	};configures,how,to,partition,records,from,flink,s,partitions,into,kafka,s,partitions,p,this,strategy,allows,for,a,custom,partitioner,by,providing,an,implementation,of,link,flink,kafka,partitioner;public,kafka,sink,partitioner,custom,class,extends,flink,kafka,partitioner,partitioner,class,sink,partitioner,type,sink,partitioner,class,preconditions,check,not,null,partitioner,class,return,this
Kafka -> public Kafka();1519754754;Connector descriptor for the Apache Kafka message queue.;public Kafka() {_		super(CONNECTOR_TYPE_VALUE_KAFKA, 1, true)__	};connector,descriptor,for,the,apache,kafka,message,queue;public,kafka,super,1,true
Kafka -> public Kafka();1533133404;Connector descriptor for the Apache Kafka message queue.;public Kafka() {_		super(CONNECTOR_TYPE_VALUE_KAFKA, 1, true)__	};connector,descriptor,for,the,apache,kafka,message,queue;public,kafka,super,1,true
Kafka -> public Kafka();1540894120;Connector descriptor for the Apache Kafka message queue.;public Kafka() {_		super(CONNECTOR_TYPE_VALUE_KAFKA, 1, true)__	};connector,descriptor,for,the,apache,kafka,message,queue;public,kafka,super,1,true
Kafka -> @Override 	public void addConnectorProperties(DescriptorProperties properties);1519754754;Internal method for connector properties conversion.;@Override_	public void addConnectorProperties(DescriptorProperties properties) {_		if (version != null) {_			properties.putString(CONNECTOR_VERSION(), version)__		}__		if (topic != null) {_			properties.putString(CONNECTOR_TOPIC, topic)__		}__		if (startupMode != null) {_			properties.putString(CONNECTOR_STARTUP_MODE, KafkaValidator.normalizeStartupMode(startupMode))__		}__		if (specificOffsets != null) {_			final List<List<String>> values = new ArrayList<>()__			for (Map.Entry<Integer, Long> specificOffset : specificOffsets.entrySet()) {_				values.add(Arrays.asList(specificOffset.getKey().toString(), specificOffset.getValue().toString()))__			}_			properties.putIndexedFixedProperties(_				CONNECTOR_SPECIFIC_OFFSETS,_				Arrays.asList(CONNECTOR_SPECIFIC_OFFSETS_PARTITION, CONNECTOR_SPECIFIC_OFFSETS_OFFSET),_				values)__		}__		if (kafkaProperties != null) {_			properties.putIndexedFixedProperties(_				CONNECTOR_PROPERTIES,_				Arrays.asList(CONNECTOR_PROPERTIES_KEY, CONNECTOR_PROPERTIES_VALUE),_				this.kafkaProperties.entrySet().stream()_					.map(e -> Arrays.asList(e.getKey(), e.getValue()))_					.collect(Collectors.toList())_				)__		}_	};internal,method,for,connector,properties,conversion;override,public,void,add,connector,properties,descriptor,properties,properties,if,version,null,properties,put,string,version,if,topic,null,properties,put,string,topic,if,startup,mode,null,properties,put,string,kafka,validator,normalize,startup,mode,startup,mode,if,specific,offsets,null,final,list,list,string,values,new,array,list,for,map,entry,integer,long,specific,offset,specific,offsets,entry,set,values,add,arrays,as,list,specific,offset,get,key,to,string,specific,offset,get,value,to,string,properties,put,indexed,fixed,properties,arrays,as,list,values,if,kafka,properties,null,properties,put,indexed,fixed,properties,arrays,as,list,this,kafka,properties,entry,set,stream,map,e,arrays,as,list,e,get,key,e,get,value,collect,collectors,to,list
Kafka -> @Override 	public void addConnectorProperties(DescriptorProperties properties);1533133404;Internal method for connector properties conversion.;@Override_	public void addConnectorProperties(DescriptorProperties properties) {_		if (version != null) {_			properties.putString(CONNECTOR_VERSION(), version)__		}__		if (topic != null) {_			properties.putString(CONNECTOR_TOPIC, topic)__		}__		if (startupMode != null) {_			properties.putString(CONNECTOR_STARTUP_MODE, KafkaValidator.normalizeStartupMode(startupMode))__		}__		if (specificOffsets != null) {_			final List<List<String>> values = new ArrayList<>()__			for (Map.Entry<Integer, Long> specificOffset : specificOffsets.entrySet()) {_				values.add(Arrays.asList(specificOffset.getKey().toString(), specificOffset.getValue().toString()))__			}_			properties.putIndexedFixedProperties(_				CONNECTOR_SPECIFIC_OFFSETS,_				Arrays.asList(CONNECTOR_SPECIFIC_OFFSETS_PARTITION, CONNECTOR_SPECIFIC_OFFSETS_OFFSET),_				values)__		}__		if (kafkaProperties != null) {_			properties.putIndexedFixedProperties(_				CONNECTOR_PROPERTIES,_				Arrays.asList(CONNECTOR_PROPERTIES_KEY, CONNECTOR_PROPERTIES_VALUE),_				this.kafkaProperties.entrySet().stream()_					.map(e -> Arrays.asList(e.getKey(), e.getValue()))_					.collect(Collectors.toList())_				)__		}__		if (sinkPartitionerType != null) {_			properties.putString(CONNECTOR_SINK_PARTITIONER, sinkPartitionerType)__			if (sinkPartitionerClass != null) {_				properties.putClass(CONNECTOR_SINK_PARTITIONER_CLASS, sinkPartitionerClass)__			}_		}_	};internal,method,for,connector,properties,conversion;override,public,void,add,connector,properties,descriptor,properties,properties,if,version,null,properties,put,string,version,if,topic,null,properties,put,string,topic,if,startup,mode,null,properties,put,string,kafka,validator,normalize,startup,mode,startup,mode,if,specific,offsets,null,final,list,list,string,values,new,array,list,for,map,entry,integer,long,specific,offset,specific,offsets,entry,set,values,add,arrays,as,list,specific,offset,get,key,to,string,specific,offset,get,value,to,string,properties,put,indexed,fixed,properties,arrays,as,list,values,if,kafka,properties,null,properties,put,indexed,fixed,properties,arrays,as,list,this,kafka,properties,entry,set,stream,map,e,arrays,as,list,e,get,key,e,get,value,collect,collectors,to,list,if,sink,partitioner,type,null,properties,put,string,sink,partitioner,type,if,sink,partitioner,class,null,properties,put,class,sink,partitioner,class
Kafka -> public Kafka version(String version);1519751194;Sets the kafka version.__@param version_Could be {@link KafkaValidator#KAFKA_VERSION_VALUE_011},_{@link KafkaValidator#KAFKA_VERSION_VALUE_010},_{@link KafkaValidator#KAFKA_VERSION_VALUE_09},_or {@link KafkaValidator#KAFKA_VERSION_VALUE_08}.;public Kafka version(String version) {_		this.version = Optional.of(version)__		return this__	};sets,the,kafka,version,param,version,could,be,link,kafka,validator,link,kafka,validator,link,kafka,validator,or,link,kafka,validator;public,kafka,version,string,version,this,version,optional,of,version,return,this
Kafka -> public Kafka version(String version);1519754754;Sets the Kafka version to be used.__@param version Kafka version. E.g., "0.8", "0.11", etc.;public Kafka version(String version) {_		Preconditions.checkNotNull(version)__		this.version = version__		return this__	};sets,the,kafka,version,to,be,used,param,version,kafka,version,e,g,0,8,0,11,etc;public,kafka,version,string,version,preconditions,check,not,null,version,this,version,version,return,this
Kafka -> public Kafka version(String version);1533133404;Sets the Kafka version to be used.__@param version Kafka version. E.g., "0.8", "0.11", etc.;public Kafka version(String version) {_		Preconditions.checkNotNull(version)__		this.version = version__		return this__	};sets,the,kafka,version,to,be,used,param,version,kafka,version,e,g,0,8,0,11,etc;public,kafka,version,string,version,preconditions,check,not,null,version,this,version,version,return,this
Kafka -> public Kafka version(String version);1540894120;Sets the Kafka version to be used.__@param version Kafka version. E.g., "0.8", "0.11", etc.;public Kafka version(String version) {_		Preconditions.checkNotNull(version)__		this.version = version__		return this__	};sets,the,kafka,version,to,be,used,param,version,kafka,version,e,g,0,8,0,11,etc;public,kafka,version,string,version,preconditions,check,not,null,version,this,version,version,return,this
Kafka -> public Kafka topic(String topic);1519751194;Sets the topic to consume.;public Kafka topic(String topic) {_		this.topic = Optional.of(topic)__		return this__	};sets,the,topic,to,consume;public,kafka,topic,string,topic,this,topic,optional,of,topic,return,this
Kafka -> public Kafka topic(String topic);1519754754;Sets the topic from which the table is read.__@param topic The topic from which the table is read.;public Kafka topic(String topic) {_		Preconditions.checkNotNull(topic)__		this.topic = topic__		return this__	};sets,the,topic,from,which,the,table,is,read,param,topic,the,topic,from,which,the,table,is,read;public,kafka,topic,string,topic,preconditions,check,not,null,topic,this,topic,topic,return,this
Kafka -> public Kafka topic(String topic);1533133404;Sets the topic from which the table is read.__@param topic The topic from which the table is read.;public Kafka topic(String topic) {_		Preconditions.checkNotNull(topic)__		this.topic = topic__		return this__	};sets,the,topic,from,which,the,table,is,read,param,topic,the,topic,from,which,the,table,is,read;public,kafka,topic,string,topic,preconditions,check,not,null,topic,this,topic,topic,return,this
Kafka -> public Kafka topic(String topic);1540894120;Sets the topic from which the table is read.__@param topic The topic from which the table is read.;public Kafka topic(String topic) {_		Preconditions.checkNotNull(topic)__		this.topic = topic__		return this__	};sets,the,topic,from,which,the,table,is,read,param,topic,the,topic,from,which,the,table,is,read;public,kafka,topic,string,topic,preconditions,check,not,null,topic,this,topic,topic,return,this
Kafka -> public Kafka properties(Properties properties);1519754754;Sets the configuration properties for the Kafka consumer. Resets previously set properties.__@param properties The configuration properties for the Kafka consumer.;public Kafka properties(Properties properties) {_		Preconditions.checkNotNull(properties)__		if (this.kafkaProperties == null) {_			this.kafkaProperties = new HashMap<>()__		}_		this.kafkaProperties.clear()__		properties.forEach((k, v) -> this.kafkaProperties.put((String) k, (String) v))__		return this__	};sets,the,configuration,properties,for,the,kafka,consumer,resets,previously,set,properties,param,properties,the,configuration,properties,for,the,kafka,consumer;public,kafka,properties,properties,properties,preconditions,check,not,null,properties,if,this,kafka,properties,null,this,kafka,properties,new,hash,map,this,kafka,properties,clear,properties,for,each,k,v,this,kafka,properties,put,string,k,string,v,return,this
Kafka -> public Kafka properties(Properties properties);1533133404;Sets the configuration properties for the Kafka consumer. Resets previously set properties.__@param properties The configuration properties for the Kafka consumer.;public Kafka properties(Properties properties) {_		Preconditions.checkNotNull(properties)__		if (this.kafkaProperties == null) {_			this.kafkaProperties = new HashMap<>()__		}_		this.kafkaProperties.clear()__		properties.forEach((k, v) -> this.kafkaProperties.put((String) k, (String) v))__		return this__	};sets,the,configuration,properties,for,the,kafka,consumer,resets,previously,set,properties,param,properties,the,configuration,properties,for,the,kafka,consumer;public,kafka,properties,properties,properties,preconditions,check,not,null,properties,if,this,kafka,properties,null,this,kafka,properties,new,hash,map,this,kafka,properties,clear,properties,for,each,k,v,this,kafka,properties,put,string,k,string,v,return,this
Kafka -> public Kafka properties(Properties properties);1540894120;Sets the configuration properties for the Kafka consumer. Resets previously set properties.__@param properties The configuration properties for the Kafka consumer.;public Kafka properties(Properties properties) {_		Preconditions.checkNotNull(properties)__		if (this.kafkaProperties == null) {_			this.kafkaProperties = new HashMap<>()__		}_		this.kafkaProperties.clear()__		properties.forEach((k, v) -> this.kafkaProperties.put((String) k, (String) v))__		return this__	};sets,the,configuration,properties,for,the,kafka,consumer,resets,previously,set,properties,param,properties,the,configuration,properties,for,the,kafka,consumer;public,kafka,properties,properties,properties,preconditions,check,not,null,properties,if,this,kafka,properties,null,this,kafka,properties,new,hash,map,this,kafka,properties,clear,properties,for,each,k,v,this,kafka,properties,put,string,k,string,v,return,this
